LongformerModel(
  (embeddings): LongformerEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=1)
    (position_embeddings): Embedding(512, 768, padding_idx=1)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): LongformerEncoder(
    (layer): ModuleList(
      (0): LongformerLayer(
        (attention): LongformerAttention(
          (self): LongformerSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (query_global): Linear(in_features=768, out_features=768, bias=True)
            (key_global): Linear(in_features=768, out_features=768, bias=True)
            (value_global): Linear(in_features=768, out_features=768, bias=True)
          )
          (output): LongformerSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): LongformerIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): LongformerOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): LongformerLayer(
        (attention): LongformerAttention(
          (self): LongformerSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (query_global): Linear(in_features=768, out_features=768, bias=True)
            (key_global): Linear(in_features=768, out_features=768, bias=True)
            (value_global): Linear(in_features=768, out_features=768, bias=True)
          )
          (output): LongformerSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): LongformerIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): LongformerOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (2): LongformerLayer(
        (attention): LongformerAttention(
          (self): LongformerSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (query_global): Linear(in_features=768, out_features=768, bias=True)
            (key_global): Linear(in_features=768, out_features=768, bias=True)
            (value_global): Linear(in_features=768, out_features=768, bias=True)
          )
          (output): LongformerSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): LongformerIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): LongformerOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (3): LongformerLayer(
        (attention): LongformerAttention(
          (self): LongformerSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (query_global): Linear(in_features=768, out_features=768, bias=True)
            (key_global): Linear(in_features=768, out_features=768, bias=True)
            (value_global): Linear(in_features=768, out_features=768, bias=True)
          )
          (output): LongformerSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): LongformerIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): LongformerOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (4): LongformerLayer(
        (attention): LongformerAttention(
          (self): LongformerSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (query_global): Linear(in_features=768, out_features=768, bias=True)
            (key_global): Linear(in_features=768, out_features=768, bias=True)
            (value_global): Linear(in_features=768, out_features=768, bias=True)
          )
          (output): LongformerSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): LongformerIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): LongformerOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (5): LongformerLayer(
        (attention): LongformerAttention(
          (self): LongformerSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (query_global): Linear(in_features=768, out_features=768, bias=True)
            (key_global): Linear(in_features=768, out_features=768, bias=True)
            (value_global): Linear(in_features=768, out_features=768, bias=True)
          )
          (output): LongformerSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): LongformerIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): LongformerOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (6): LongformerLayer(
        (attention): LongformerAttention(
          (self): LongformerSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (query_global): Linear(in_features=768, out_features=768, bias=True)
            (key_global): Linear(in_features=768, out_features=768, bias=True)
            (value_global): Linear(in_features=768, out_features=768, bias=True)
          )
          (output): LongformerSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): LongformerIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): LongformerOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (7): LongformerLayer(
        (attention): LongformerAttention(
          (self): LongformerSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (query_global): Linear(in_features=768, out_features=768, bias=True)
            (key_global): Linear(in_features=768, out_features=768, bias=True)
            (value_global): Linear(in_features=768, out_features=768, bias=True)
          )
          (output): LongformerSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): LongformerIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): LongformerOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (8): LongformerLayer(
        (attention): LongformerAttention(
          (self): LongformerSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (query_global): Linear(in_features=768, out_features=768, bias=True)
            (key_global): Linear(in_features=768, out_features=768, bias=True)
            (value_global): Linear(in_features=768, out_features=768, bias=True)
          )
          (output): LongformerSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): LongformerIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): LongformerOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (9): LongformerLayer(
        (attention): LongformerAttention(
          (self): LongformerSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (query_global): Linear(in_features=768, out_features=768, bias=True)
            (key_global): Linear(in_features=768, out_features=768, bias=True)
            (value_global): Linear(in_features=768, out_features=768, bias=True)
          )
          (output): LongformerSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): LongformerIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): LongformerOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (10): LongformerLayer(
        (attention): LongformerAttention(
          (self): LongformerSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (query_global): Linear(in_features=768, out_features=768, bias=True)
            (key_global): Linear(in_features=768, out_features=768, bias=True)
            (value_global): Linear(in_features=768, out_features=768, bias=True)
          )
          (output): LongformerSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): LongformerIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): LongformerOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (11): LongformerLayer(
        (attention): LongformerAttention(
          (self): LongformerSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (query_global): Linear(in_features=768, out_features=768, bias=True)
            (key_global): Linear(in_features=768, out_features=768, bias=True)
            (value_global): Linear(in_features=768, out_features=768, bias=True)
          )
          (output): LongformerSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): LongformerIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): LongformerOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): LongformerPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)

LongformerModel._actual_script_module
LongformerModel.forward
  graph(%self.1 : __torch__.transformers.modeling_longformer.LongformerModel,
        %input_ids.1 : Long(17:13, 13:1),
        %input.1 : Long(17:13, 13:1)):
    %18861 : __torch__.transformers.modeling_longformer.LongformerPooler = prim::GetAttr[name="pooler"](%self.1)
    %18856 : __torch__.transformers.modeling_longformer.LongformerEncoder = prim::GetAttr[name="encoder"](%self.1)
    %18459 : __torch__.transformers.modeling_longformer.LongformerEmbeddings = prim::GetAttr[name="embeddings"](%self.1)
    %754 : int = prim::Constant[value=0]() # transformers/modeling_longformer.py:1228:0
    %755 : int = aten::size(%input_ids.1, %754) # transformers/modeling_longformer.py:1228:0
    %756 : Long() = prim::NumToTensor(%755)
    %760 : int = aten::Int(%756)
    %757 : int = prim::Constant[value=1]() # transformers/modeling_longformer.py:1228:0
    %758 : int = aten::size(%input_ids.1, %757) # transformers/modeling_longformer.py:1228:0
    %759 : Long() = prim::NumToTensor(%758)
    %761 : int = aten::Int(%759)
    %762 : int[] = prim::ListConstruct(%760, %761)
    %763 : int = prim::Constant[value=4]() # transformers/modeling_longformer.py:1239:0
    %764 : int = prim::Constant[value=0]() # transformers/modeling_longformer.py:1239:0
    %765 : Device = prim::Constant[value="cpu"]() # transformers/modeling_longformer.py:1239:0
    %766 : bool = prim::Constant[value=0]() # transformers/modeling_longformer.py:1239:0
    %input.2 : Long(17:13, 13:1) = aten::zeros(%762, %763, %764, %765, %766) # transformers/modeling_longformer.py:1239:0
    %771 : int = prim::Constant[value=1]() # transformers/modeling_longformer.py:1136:0
    %772 : int = aten::size(%input_ids.1, %771) # transformers/modeling_longformer.py:1136:0
    %seq_len.1 : Long() = prim::NumToTensor(%772)
    %774 : int = prim::Constant[value=512]() # transformers/modeling_longformer.py:1139:0
    %775 : Long() = aten::remainder(%seq_len.1, %774) # transformers/modeling_longformer.py:1139:0
    %776 : int = prim::Constant[value=512]() # torch/tensor.py:396:0
    %777 : int = prim::Constant[value=1]() # torch/tensor.py:396:0
    %778 : Long() = aten::rsub(%775, %776, %777) # torch/tensor.py:396:0
    %779 : int = prim::Constant[value=512]() # transformers/modeling_longformer.py:1139:0
    %padding_len : Long() = aten::remainder(%778, %779) # transformers/modeling_longformer.py:1139:0
    %795 : int = aten::Int(%padding_len)
    %790 : int = aten::Int(%padding_len)
    %785 : int = aten::Int(%padding_len)
    %786 : int = prim::Constant[value=0]() # torch/nn/functional.py:3552:0
    %787 : int[] = prim::ListConstruct(%786, %785)
    %788 : int = prim::Constant[value=1]() # torch/nn/functional.py:3552:0
    %input_ids : Long(17:512, 512:1) = aten::constant_pad_nd(%input_ids.1, %787, %788) # torch/nn/functional.py:3552:0
    %791 : int = prim::Constant[value=0]() # torch/nn/functional.py:3552:0
    %792 : int[] = prim::ListConstruct(%791, %790)
    %793 : int = prim::Constant[value=0]() # torch/nn/functional.py:3552:0
    %attention_mask.1 : Long(17:512, 512:1) = aten::constant_pad_nd(%input.1, %792, %793) # torch/nn/functional.py:3552:0
    %796 : int = prim::Constant[value=0]() # torch/nn/functional.py:3552:0
    %797 : int[] = prim::ListConstruct(%796, %795)
    %798 : int = prim::Constant[value=0]() # torch/nn/functional.py:3552:0
    %input.4 : Long(17:512, 512:1) = aten::constant_pad_nd(%input.2, %797, %798) # torch/nn/functional.py:3552:0
    %800 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
    %801 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
    %802 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_utils.py:244:0
    %803 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
    %804 : Long(17:512, 512:1) = aten::slice(%attention_mask.1, %800, %801, %802, %803) # transformers/modeling_utils.py:244:0
    %805 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
    %806 : Long(17:512, 1:512, 512:1) = aten::unsqueeze(%804, %805) # transformers/modeling_utils.py:244:0
    %807 : int = prim::Constant[value=2]() # transformers/modeling_utils.py:244:0
    %808 : Long(17:512, 1:512, 1:512, 512:1) = aten::unsqueeze(%806, %807) # transformers/modeling_utils.py:244:0
    %809 : int = prim::Constant[value=3]() # transformers/modeling_utils.py:244:0
    %810 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
    %811 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_utils.py:244:0
    %812 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
    %extended_attention_mask : Long(17:512, 1:512, 1:512, 512:1) = aten::slice(%808, %809, %810, %811, %812) # transformers/modeling_utils.py:244:0
    %814 : int = prim::Constant[value=6]() # transformers/modeling_utils.py:257:0
    %815 : bool = prim::Constant[value=0]() # transformers/modeling_utils.py:257:0
    %816 : bool = prim::Constant[value=0]() # transformers/modeling_utils.py:257:0
    %817 : None = prim::Constant()
    %818 : Float(17:512, 1:512, 1:512, 512:1) = aten::to(%extended_attention_mask, %814, %815, %816, %817) # transformers/modeling_utils.py:257:0
    %819 : float = prim::Constant[value=1.]() # torch/tensor.py:396:0
    %820 : int = prim::Constant[value=1]() # torch/tensor.py:396:0
    %821 : Float(17:512, 1:512, 1:512, 512:1) = aten::rsub(%818, %819, %820) # torch/tensor.py:396:0
    %822 : Double() = prim::Constant[value={-10000}]() # transformers/modeling_utils.py:258:0
    %attention_mask : Float(17:512, 1:512, 1:512, 512:1) = aten::mul(%821, %822) # transformers/modeling_utils.py:258:0
    %19064 : Tensor = prim::CallMethod[name="forward"](%18459, %input_ids, %input.4)
    %19065 : Tensor = prim::CallMethod[name="forward"](%18856, %attention_mask, %19064)
    %19066 : Tensor = prim::CallMethod[name="forward"](%18861, %19065)
    %18235 : Long() = aten::neg(%padding_len) # transformers/modeling_longformer.py:1275:0
    %18241 : int = aten::Int(%18235)
    %18236 : int = prim::Constant[value=0]() # transformers/modeling_longformer.py:1275:0
    %18237 : int = prim::Constant[value=0]() # transformers/modeling_longformer.py:1275:0
    %18238 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_longformer.py:1275:0
    %18239 : int = prim::Constant[value=1]() # transformers/modeling_longformer.py:1275:0
    %18240 : Float(17:393216, 512:768, 768:1) = aten::slice(%19065, %18236, %18237, %18238, %18239) # transformers/modeling_longformer.py:1275:0
    %18242 : int = prim::Constant[value=1]() # transformers/modeling_longformer.py:1275:0
    %18243 : int = prim::Constant[value=0]() # transformers/modeling_longformer.py:1275:0
    %18244 : int = prim::Constant[value=1]() # transformers/modeling_longformer.py:1275:0
    %18245 : Float(17:393216, 13:768, 768:1) = aten::slice(%18240, %18242, %18243, %18241, %18244) # transformers/modeling_longformer.py:1275:0
    %18246 : (Float(17:393216, 13:768, 768:1), Float(17:768, 768:1)) = prim::TupleConstruct(%18245, %19066)
    return (%18246)

LongformerModel.embeddings
LongformerEmbeddings._actual_script_module
  graph(%self.2 : __torch__.transformers.modeling_longformer.LongformerEmbeddings,
        %input_ids : Long(17:512, 512:1),
        %input.4 : Long(17:512, 512:1)):
    %1 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.2)
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.2)
    %3 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="token_type_embeddings"](%self.2)
    %4 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="position_embeddings"](%self.2)
    %5 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="word_embeddings"](%self.2)
    %6 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_longformer.py:112:0
    %7 : Bool(17:512, 512:1) = aten::ne(%input_ids, %6), scope: __module.embeddings # transformers/modeling_longformer.py:112:0
    %9 : int = prim::Constant[value=3](), scope: __module.embeddings # transformers/modeling_longformer.py:112:0
    %10 : bool = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_longformer.py:112:0
    %11 : bool = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_longformer.py:112:0
    %12 : None = prim::Constant(), scope: __module.embeddings
    %mask : Int(17:512, 512:1) = aten::to(%7, %9, %10, %11, %12), scope: __module.embeddings # transformers/modeling_longformer.py:112:0
    %14 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_longformer.py:113:0
    %15 : None = prim::Constant(), scope: __module.embeddings
    %16 : Long(17:512, 512:1) = aten::cumsum(%mask, %14, %15), scope: __module.embeddings # transformers/modeling_longformer.py:113:0
    %17 : Int(17:512, 512:1) = aten::type_as(%16, %mask), scope: __module.embeddings # transformers/modeling_longformer.py:113:0
    %incremental_indices : Int(17:512, 512:1) = aten::mul(%17, %mask), scope: __module.embeddings # transformers/modeling_longformer.py:113:0
    %19 : int = prim::Constant[value=4](), scope: __module.embeddings # transformers/modeling_longformer.py:114:0
    %20 : bool = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_longformer.py:114:0
    %21 : bool = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_longformer.py:114:0
    %22 : None = prim::Constant(), scope: __module.embeddings
    %23 : Long(17:512, 512:1) = aten::to(%incremental_indices, %19, %20, %21, %22), scope: __module.embeddings # transformers/modeling_longformer.py:114:0
    %24 : Long() = prim::Constant[value={1}](), scope: __module.embeddings # transformers/modeling_longformer.py:114:0
    %25 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_longformer.py:114:0
    %26 : Long(17:512, 512:1) = aten::add(%23, %24, %25), scope: __module.embeddings # transformers/modeling_longformer.py:114:0
    %27 : int = prim::Constant[value=4](), scope: __module.embeddings # transformers/modeling_longformer.py:147:0
    %28 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_longformer.py:147:0
    %29 : Device = prim::Constant[value="cpu"](), scope: __module.embeddings # transformers/modeling_longformer.py:147:0
    %30 : bool = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_longformer.py:147:0
    %31 : bool = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_longformer.py:147:0
    %32 : bool = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_longformer.py:147:0
    %33 : None = prim::Constant(), scope: __module.embeddings
    %input.3 : Long(17:512, 512:1) = aten::to(%26, %27, %28, %29, %30, %31, %32, %33), scope: __module.embeddings # transformers/modeling_longformer.py:147:0
    %51 : Tensor = prim::CallMethod[name="forward"](%5, %input_ids)
    %52 : Tensor = prim::CallMethod[name="forward"](%4, %input.3)
    %53 : Tensor = prim::CallMethod[name="forward"](%3, %input.4)
    %45 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_longformer.py:170:0
    %46 : Float(17:393216, 512:768, 768:1) = aten::add(%51, %52, %45), scope: __module.embeddings # transformers/modeling_longformer.py:170:0
    %47 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_longformer.py:170:0
    %input.5 : Float(17:393216, 512:768, 768:1) = aten::add(%46, %53, %47), scope: __module.embeddings # transformers/modeling_longformer.py:170:0
    %54 : Tensor = prim::CallMethod[name="forward"](%2, %input.5)
    %55 : Tensor = prim::CallMethod[name="forward"](%1, %54)
    return (%55)

LongformerModel.encoder
LongformerEncoder._actual_script_module
  graph(%self.8 : __torch__.transformers.modeling_longformer.LongformerEncoder,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %27 : Float(17:393216, 512:768, 768:1)):
    %1 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %2 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="11"](%1)
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %4 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="10"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %6 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="9"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %8 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="8"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %10 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="7"](%9)
    %11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %12 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="6"](%11)
    %13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %14 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="5"](%13)
    %15 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %16 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="4"](%15)
    %17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %18 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="3"](%17)
    %19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %20 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="2"](%19)
    %21 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %22 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="1"](%21)
    %23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %24 : __torch__.transformers.modeling_longformer.LongformerLayer = prim::GetAttr[name="0"](%23)
    %39 : Tensor = prim::CallMethod[name="forward"](%24, %attention_mask, %27)
    %40 : Tensor = prim::CallMethod[name="forward"](%22, %attention_mask, %39)
    %41 : Tensor = prim::CallMethod[name="forward"](%20, %attention_mask, %40)
    %42 : Tensor = prim::CallMethod[name="forward"](%18, %attention_mask, %41)
    %43 : Tensor = prim::CallMethod[name="forward"](%16, %attention_mask, %42)
    %44 : Tensor = prim::CallMethod[name="forward"](%14, %attention_mask, %43)
    %45 : Tensor = prim::CallMethod[name="forward"](%12, %attention_mask, %44)
    %46 : Tensor = prim::CallMethod[name="forward"](%10, %attention_mask, %45)
    %47 : Tensor = prim::CallMethod[name="forward"](%8, %attention_mask, %46)
    %48 : Tensor = prim::CallMethod[name="forward"](%6, %attention_mask, %47)
    %49 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %48)
    %50 : Tensor = prim::CallMethod[name="forward"](%2, %attention_mask, %49)
    return (%50)

LongformerModel.pooler
LongformerPooler._actual_script_module
  graph(%self.201 : __torch__.transformers.modeling_longformer.LongformerPooler,
        %8 : Float(17:393216, 512:768, 768:1)):
    %1 : __torch__.torch.nn.modules.activation.Tanh = prim::GetAttr[name="activation"](%self.201)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.201)
    %3 : int = prim::Constant[value=0](), scope: __module.pooler # transformers/modeling_longformer.py:934:0
    %4 : int = prim::Constant[value=0](), scope: __module.pooler # transformers/modeling_longformer.py:934:0
    %5 : int = prim::Constant[value=9223372036854775807](), scope: __module.pooler # transformers/modeling_longformer.py:934:0
    %6 : int = prim::Constant[value=1](), scope: __module.pooler # transformers/modeling_longformer.py:934:0
    %7 : Float(17:393216, 512:768, 768:1) = aten::slice(%8, %3, %4, %5, %6), scope: __module.pooler # transformers/modeling_longformer.py:934:0
    %9 : int = prim::Constant[value=1](), scope: __module.pooler # transformers/modeling_longformer.py:934:0
    %10 : int = prim::Constant[value=0](), scope: __module.pooler # transformers/modeling_longformer.py:934:0
    %input.151 : Float(17:393216, 768:1) = aten::select(%7, %9, %10), scope: __module.pooler # transformers/modeling_longformer.py:934:0
    %14 : Tensor = prim::CallMethod[name="forward"](%2, %input.151)
    %15 : Tensor = prim::CallMethod[name="forward"](%1, %14)
    return (%15)

LongformerEmbeddings.LayerNorm
LayerNorm._actual_script_module
  graph(%self.6 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.5 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.6)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.6)
    %4 : int = prim::Constant[value=768](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.embeddings/__module.embeddings.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    %input.6 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.5, %5, %3, %2, %6, %7), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.6)

LongformerEmbeddings.dropout
Dropout._actual_script_module
  graph(%self.7 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
    %hidden_states.1 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.1)

LongformerEmbeddings.position_embeddings
Embedding._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.3 : Long(17:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.4)
    %8 : int = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %9 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %10 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %position_embeddings : Float(17:393216, 512:768, 768:1) = aten::embedding(%2, %input.3, %8, %9, %10), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    return (%position_embeddings)

LongformerEmbeddings.token_type_embeddings
Embedding._actual_script_module
  graph(%self.5 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.4 : Long(17:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.5)
    %3 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
    %token_type_embeddings : Float(17:393216, 512:768, 768:1) = aten::embedding(%2, %input.4, %3, %4, %5), scope: __module.embeddings/__module.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
    return (%token_type_embeddings)

LongformerEmbeddings.word_embeddings
Embedding._actual_script_module
  graph(%self.3 : __torch__.torch.nn.modules.sparse.Embedding,
        %input_ids : Long(17:512, 512:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.3)
    %8 : int = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %9 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %10 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %inputs_embeds : Float(17:393216, 512:768, 768:1) = aten::embedding(%2, %input_ids, %8, %9, %10), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    return (%inputs_embeds)

ModuleList.*
Linear.*
  module had no methods with graph attrs.

LongformerLayer._actual_script_module
  graph(%self.9 : __torch__.transformers.modeling_longformer.LongformerLayer,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%self.9)
    %4 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%self.9)
    %5 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%self.9)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %attention_mask, %2)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

LongformerLayer.attention
LongformerAttention._actual_script_module
  graph(%self.10 : __torch__.transformers.modeling_longformer.LongformerAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%self.10)
    %4 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%self.10)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %2)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %2)
    return (%8)

LongformerLayer.intermediate
LongformerIntermediate._actual_script_module
  graph(%self.19 : __torch__.transformers.modeling_longformer.LongformerIntermediate,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.19)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.16 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
    return (%input.16)

LongformerLayer.output
LongformerOutput._actual_script_module
  graph(%self.21 : __torch__.transformers.modeling_longformer.LongformerOutput,
        %1 : Float(17:1572864, 512:3072, 3072:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.21)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.21)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.21)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output # transformers/modeling_longformer.py:830:0
    %input.18 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output # transformers/modeling_longformer.py:830:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.18)
    return (%13)

LongformerAttention.output
LongformerSelfOutput._actual_script_module
  graph(%self.15 : __torch__.transformers.modeling_longformer.LongformerSelfOutput,
        %1 : Float(17:768, 512:13056, 768:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.15)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.15)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.15)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output # transformers/modeling_longformer.py:758:0
    %input.14 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output # transformers/modeling_longformer.py:758:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.14)
    return (%13)

LongformerAttention.self
LongformerSelfAttention._actual_script_module
  graph(%self.11 : __torch__.transformers.modeling_longformer.LongformerSelfAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.11)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.11)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.11)
    %6 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:241:0
    %7 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:241:0
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:241:0
    %9 : Float(17:512, 512:1) = aten::squeeze(%7, %8), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:241:0
    %10 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:22:0
    %is_index_masked.1 : Bool(17:512, 512:1) = aten::lt(%9, %10), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:22:0
    %18 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:248:0
    %19 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:248:0
    %input.7 : Float(512:768, 17:393216, 768:1) = aten::transpose(%2, %18, %19), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:248:0
    %1387 : Tensor = prim::CallMethod[name="forward"](%5, %input.7)
    %1388 : Tensor = prim::CallMethod[name="forward"](%4, %input.7)
    %1389 : Tensor = prim::CallMethod[name="forward"](%3, %input.7)
    %24 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
    %25 : int = aten::size(%input.7, %24), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
    %seq_len.2 : Long() = prim::NumToTensor(%25), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %27 : int = aten::Int(%seq_len.2), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %28 : int = aten::Int(%seq_len.2), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %29 : int = aten::Int(%seq_len.2), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %30 : int = aten::Int(%seq_len.2), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %31 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
    %32 : int = aten::size(%input.7, %31), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
    %batch_size.1 : Long() = prim::NumToTensor(%32), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %34 : int = aten::Int(%batch_size.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %35 : int = aten::Int(%batch_size.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %36 : int = aten::Int(%batch_size.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %37 : int = aten::Int(%batch_size.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %38 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
    %39 : int = aten::size(%input.7, %38), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
    %embed_dim.1 : Long() = prim::NumToTensor(%39), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %41 : int = aten::Int(%embed_dim.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %44 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:261:0
    %query_vectors.2 : Float(512:13056, 17:768, 768:1) = aten::div_(%1387, %44), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:261:0
    %46 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
    %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
    %48 : int[] = prim::ListConstruct(%30, %37, %46, %47), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %49 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.2, %48), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
    %50 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
    %query.1 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%49, %50, %51), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
    %53 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:264:0
    %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:264:0
    %55 : int[] = prim::ListConstruct(%29, %36, %53, %54), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %56 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1388, %55), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:264:0
    %57 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:264:0
    %58 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:264:0
    %key.1 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%56, %57, %58), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:264:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %61 : int = aten::size(%query.1, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.2 : Long() = prim::NumToTensor(%61), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %63 : int = aten::Int(%batch_size.2), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %65 : int = aten::size(%query.1, %64), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.3 : Long() = prim::NumToTensor(%65), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %67 : int = aten::Int(%seq_len.3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %68 : int = aten::Int(%seq_len.3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %69 : int = aten::Int(%seq_len.3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %70 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %71 : int = aten::size(%query.1, %70), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.1 : Long() = prim::NumToTensor(%71), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %73 : int = aten::Int(%num_heads.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %74 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %75 : int = aten::size(%query.1, %74), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.1 : Long() = prim::NumToTensor(%75), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %77 : int = aten::Int(%head_dim.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %78 : int = aten::Int(%head_dim.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %111 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %112 : Long() = aten::floor_divide(%seq_len.3, %111), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %113 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:478:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.1 : Long() = aten::sub(%112, %113, %114), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:478:0
    %116 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
    %117 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
    %118 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.1, %116, %117), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
    %119 : Long() = aten::mul(%batch_size.2, %num_heads.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
    %120 : int = aten::Int(%119), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %121 : int[] = prim::ListConstruct(%120, %69, %78), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %hidden_states.2 : Float(204:64, 512:13056, 64:1) = aten::reshape(%118, %121), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
    %123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
    %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
    %125 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.1, %123, %124), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
    %126 : Long() = aten::mul(%batch_size.2, %num_heads.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
    %127 : int = aten::Int(%126), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %128 : int[] = prim::ListConstruct(%127, %68, %77), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %hidden_states.4 : Float(204:64, 512:13056, 64:1) = aten::reshape(%125, %128), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
    %130 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
    %131 : int = aten::size(%hidden_states.2, %130), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
    %132 : Long() = prim::NumToTensor(%131), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %133 : int = aten::Int(%132), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %134 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
    %135 : int = aten::size(%hidden_states.2, %134), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
    %136 : Long() = prim::NumToTensor(%135), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %137 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %138 : Long() = aten::floor_divide(%136, %137), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %139 : int = aten::Int(%138), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %140 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
    %141 : int = aten::size(%hidden_states.2, %140), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
    %142 : Long() = prim::NumToTensor(%141), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %143 : int = aten::Int(%142), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %144 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
    %145 : int[] = prim::ListConstruct(%133, %139, %144, %143), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %hidden_states.3 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.2, %145), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
    %147 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %148 : int = aten::size(%hidden_states.3, %147), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %149 : Long() = prim::NumToTensor(%148), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %150 : int = aten::Int(%149), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %151 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %152 : int = aten::size(%hidden_states.3, %151), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %153 : Long() = prim::NumToTensor(%152), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %154 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %155 : int = aten::size(%hidden_states.3, %154), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %156 : Long() = prim::NumToTensor(%155), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %157 : int = aten::Int(%156), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %158 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %159 : int = aten::size(%hidden_states.3, %158), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %160 : Long() = prim::NumToTensor(%159), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %161 : int = aten::Int(%160), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %162 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %163 : Long() = aten::mul(%153, %162), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %164 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %165 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %166 : Long() = aten::sub(%163, %164, %165), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %167 : int = aten::Int(%166), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %168 : int[] = prim::ListConstruct(%150, %167, %157, %161), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %169 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %170 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %171 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %172 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %173 : int[] = prim::ListConstruct(%169, %170, %171, %172), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %174 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %175 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.3, %168, %173, %174), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %176 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
    %177 : int = aten::size(%hidden_states.4, %176), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %179 : int = aten::Int(%178), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
    %181 : int = aten::size(%hidden_states.4, %180), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
    %182 : Long() = prim::NumToTensor(%181), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %183 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %184 : Long() = aten::floor_divide(%182, %183), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %185 : int = aten::Int(%184), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %186 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
    %187 : int = aten::size(%hidden_states.4, %186), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
    %188 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %189 : int = aten::Int(%188), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %190 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
    %191 : int[] = prim::ListConstruct(%179, %185, %190, %189), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %hidden_states.5 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.4, %191), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
    %193 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %194 : int = aten::size(%hidden_states.5, %193), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %195 : Long() = prim::NumToTensor(%194), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %196 : int = aten::Int(%195), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %197 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %198 : int = aten::size(%hidden_states.5, %197), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %199 : Long() = prim::NumToTensor(%198), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %200 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %201 : int = aten::size(%hidden_states.5, %200), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %203 : int = aten::Int(%202), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %204 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %205 : int = aten::size(%hidden_states.5, %204), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %207 : int = aten::Int(%206), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %208 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %209 : Long() = aten::mul(%199, %208), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %210 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %211 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %212 : Long() = aten::sub(%209, %210, %211), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %213 : int = aten::Int(%212), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %214 : int[] = prim::ListConstruct(%196, %213, %203, %207), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %216 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %217 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %218 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %219 : int[] = prim::ListConstruct(%215, %216, %217, %218), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %220 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.5, %214, %219, %220), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %222 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/functional.py:327:0
    %223 : Tensor[] = prim::ListConstruct(%175, %221), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %input.8 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%222, %223), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/functional.py:327:0
    %225 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %226 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %227 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %229 : int[] = prim::ListConstruct(%225, %226, %227, %228), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %230 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.1 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.8, %229, %230), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %232 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %233 : int = aten::size(%hidden_states_padded.1, %232), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %234 : Long() = prim::NumToTensor(%233), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %235 : int = aten::Int(%234), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %236 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %237 : int = aten::size(%hidden_states_padded.1, %236), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %238 : Long() = prim::NumToTensor(%237), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %239 : int = aten::Int(%238), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %246 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %247 : int = aten::size(%hidden_states_padded.1, %246), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %248 : Long() = prim::NumToTensor(%247), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %249 : int = aten::Int(%248), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %250 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %251 : int = aten::size(%hidden_states_padded.1, %250), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %252 : Long() = prim::NumToTensor(%251), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %253 : int = aten::Int(%252), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %254 : int[] = prim::ListConstruct(%235, %239, %249, %253), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %diagonal_chunked_attention_scores.1 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.1, %254), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:400:0
    %256 : Long() = aten::mul(%batch_size.2, %num_heads.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
    %257 : int = aten::Int(%256), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %258 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
    %259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
    %260 : Long() = aten::add(%chunks_count.1, %258, %259), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
    %261 : int = aten::Int(%260), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %263 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %264 : int[] = prim::ListConstruct(%257, %261, %262, %263), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %265 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %266 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %268 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.1 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.1, %264, %265, %266, %267, %268), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %270 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %271 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %273 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %270, %271, %272, %273), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %275 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %276 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %277 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %279 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%274, %275, %276, %277, %278), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %280 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %281 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %282 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %284 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%279, %280, %281, %282, %283), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %285 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %286 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %287 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %289 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%284, %285, %286, %287, %288), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %290 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %291 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %292 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %294 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %290, %291, %292, %293), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %295 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %296 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %297 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %299 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%294, %295, %296, %297, %298), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %300 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %301 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %302 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %303 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %304 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%299, %300, %301, %302, %303), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %305 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %306 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %307 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %308 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %309 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%304, %305, %306, %307, %308), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %310 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %311 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %312 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %313 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %314 : int[] = prim::ListConstruct(%310, %311, %312, %313), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %315 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%289, %314), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %316 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %317 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%309, %315, %316), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %319 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %320 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %321 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %322 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %318, %319, %320, %321), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %323 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %324 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %325 : Float(204:262656, 512:513, 513:1) = aten::select(%322, %323, %324), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %327 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %328 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %329 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %330 : Float(204:262656, 256:513, 513:1) = aten::slice(%325, %326, %327, %328, %329), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %331 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %333 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %335 : Float(204:262656, 256:513, 257:1) = aten::slice(%330, %331, %332, %333, %334), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %336 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %340 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %336, %337, %338, %339), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %341 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %342 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %343 : Float(204:262656, 256:513, 513:1) = aten::select(%340, %341, %342), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %345 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %347 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %348 : Float(204:262656, 256:513, 513:1) = aten::slice(%343, %344, %345, %346, %347), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %349 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %350 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %352 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %353 : Float(204:262656, 256:513, 257:1) = aten::slice(%348, %349, %350, %351, %352), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %354 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %355 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %356 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %357 : int[] = prim::ListConstruct(%354, %355, %356), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %358 : Float(204:262656, 256:513, 257:1) = aten::view(%335, %357), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %359 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %360 : Float(204:262656, 256:513, 257:1) = aten::copy_(%353, %358, %359), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %361 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %362 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %363 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %364 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %365 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %361, %362, %363, %364), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %366 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %367 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %368 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %369 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %370 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%365, %366, %367, %368, %369), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %371 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %372 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %373 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %374 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %375 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%370, %371, %372, %373, %374), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %376 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %377 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %379 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %380 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%375, %376, %377, %378, %379), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %383 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %384 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %385 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %381, %382, %383, %384), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %386 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %387 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %388 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %389 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %390 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%385, %386, %387, %388, %389), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %391 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %392 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %393 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %394 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %395 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%390, %391, %392, %393, %394), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %397 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %398 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %399 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %400 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%395, %396, %397, %398, %399), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %401 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %402 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %403 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %404 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %405 : int[] = prim::ListConstruct(%401, %402, %403, %404), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %406 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%380, %405), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %407 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %408 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%400, %406, %407), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %409 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %410 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %411 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %412 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %413 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %409, %410, %411, %412), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %414 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %415 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %416 : Float(204:262656, 512:513, 513:1) = aten::select(%413, %414, %415), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %417 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %418 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %419 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %420 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %421 : Float(204:262656, 255:513, 513:1) = aten::slice(%416, %417, %418, %419, %420), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %422 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %423 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %425 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %426 : Float(204:262656, 255:513, 255:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %427 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %429 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %430 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %431 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %427, %428, %429, %430), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %432 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %433 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %434 : Float(204:262656, 256:513, 513:1) = aten::select(%431, %432, %433), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %435 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %436 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %437 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %438 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %439 : Float(204:262656, 255:513, 513:1) = aten::slice(%434, %435, %436, %437, %438), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %440 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %441 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %442 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %443 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %444 : Float(204:262656, 255:513, 255:1) = aten::slice(%439, %440, %441, %442, %443), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %445 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %446 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %447 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %448 : int[] = prim::ListConstruct(%445, %446, %447), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %449 : Float(204:262656, 255:513, 255:1) = aten::view(%426, %448), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %450 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %451 : Float(204:262656, 255:513, 255:1) = aten::copy_(%444, %449, %450), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %452 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
    %453 : int[] = prim::ListConstruct(%63, %73, %67, %452), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %454 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.1, %453), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
    %455 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
    %456 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.1 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%454, %455, %456), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
    %458 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %459 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %460 : int[] = prim::ListConstruct(%458, %459), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %461 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %462 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %463 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %464 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %465 : Float(256:257, 257:1) = aten::ones(%460, %461, %462, %463, %464), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %466 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %467 : Float(256:257, 257:1) = aten::tril(%465, %466), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %468 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %469 : int[] = prim::ListConstruct(%468), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %beginning_mask_2d.1 : Float(256:257, 257:1) = aten::flip(%467, %469), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %471 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %472 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.1, %471), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %473 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %474 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %475 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %476 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %477 : Float(1:65792, 256:257, 257:1) = aten::slice(%472, %473, %474, %475, %476), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %478 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %479 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%477, %478), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %480 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %481 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %482 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %483 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.1 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%479, %480, %481, %482, %483), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %485 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:460:0
    %486 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:460:0
    %487 : int[] = prim::ListConstruct(%485, %486), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %ending_mask.1 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.1, %487), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:460:0
    %489 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %490 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %491 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %493 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.1, %489, %490, %491, %492), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %494 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %495 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %496 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %497 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %498 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%493, %494, %495, %496, %497), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %499 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %500 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %501 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %502 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %503 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%498, %499, %500, %501, %502), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %504 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %505 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %506 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %507 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.1 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%503, %504, %505, %506, %507), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %509 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %510 : int = aten::size(%beginning_input.1, %509), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %511 : Long() = prim::NumToTensor(%510), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %512 : int = aten::Int(%511), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %513 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %514 : int = aten::size(%beginning_input.1, %513), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %515 : Long() = prim::NumToTensor(%514), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %516 : int = aten::Int(%515), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %517 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %518 : int = aten::size(%beginning_input.1, %517), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %519 : Long() = prim::NumToTensor(%518), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %520 : int = aten::Int(%519), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %521 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %522 : int = aten::size(%beginning_input.1, %521), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %523 : Long() = prim::NumToTensor(%522), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %524 : int = aten::Int(%523), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %525 : int[] = prim::ListConstruct(%512, %516, %520, %524), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %526 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %527 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.1, %525, %526), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %528 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:22:0
    %529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%527, %528), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:22:0
    %530 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
    %531 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.1, %529, %530), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
    %532 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %533 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %534 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %535 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %536 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.1, %532, %533, %534, %535), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %537 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %538 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %539 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %541 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%536, %537, %538, %539, %540), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %542 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %543 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %544 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %545 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %546 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%541, %542, %543, %544, %545), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %547 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %548 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %549 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.1 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%546, %547, %548, %549, %550), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %552 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %553 : int = aten::size(%ending_input.1, %552), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %554 : Long() = prim::NumToTensor(%553), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %555 : int = aten::Int(%554), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %556 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %557 : int = aten::size(%ending_input.1, %556), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %558 : Long() = prim::NumToTensor(%557), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %559 : int = aten::Int(%558), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %560 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %561 : int = aten::size(%ending_input.1, %560), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %562 : Long() = prim::NumToTensor(%561), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %563 : int = aten::Int(%562), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %564 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %565 : int = aten::size(%ending_input.1, %564), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %566 : Long() = prim::NumToTensor(%565), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %567 : int = aten::Int(%566), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %568 : int[] = prim::ListConstruct(%555, %559, %563, %567), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %569 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %570 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.1, %568, %569), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %571 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:22:0
    %572 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%570, %571), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:22:0
    %573 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:466:0
    %574 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.1, %572, %573), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:466:0
    %575 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:22:0
    %576 : Bool(17:512, 512:1) = aten::ne(%9, %575), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:22:0
    %577 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %578 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %579 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %580 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %581 : Bool(17:512, 512:1) = aten::slice(%576, %577, %578, %579, %580), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %582 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %583 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %584 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %585 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %586 : Bool(17:512, 512:1) = aten::slice(%581, %582, %583, %584, %585), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %587 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %588 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%586, %587), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %589 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %remove_from_windowed_attention_mask.1 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%588, %589), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
    %591 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.1, %query.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:275:0
    %592 : float = prim::Constant[value=-10000.](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:275:0
    %float_mask.1 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%591, %remove_from_windowed_attention_mask.1, %592), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:275:0
    %594 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %595 : int = aten::size(%float_mask.1, %594), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %596 : Long() = prim::NumToTensor(%595), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %597 : int = aten::Int(%596), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %598 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %599 : int = aten::size(%float_mask.1, %598), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %600 : Long() = prim::NumToTensor(%599), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %601 : int = aten::Int(%600), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %602 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %603 : int = aten::size(%float_mask.1, %602), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %604 : Long() = prim::NumToTensor(%603), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %605 : int = aten::Int(%604), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %606 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %607 : int = aten::size(%float_mask.1, %606), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %608 : Long() = prim::NumToTensor(%607), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %609 : int = aten::Int(%608), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %610 : int[] = prim::ListConstruct(%597, %601, %605, %609), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %611 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %612 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %613 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %614 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %query.2 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%610, %611, %612, %613, %614), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
    %616 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %617 : int = aten::size(%query.2, %616), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.3 : Long() = prim::NumToTensor(%617), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %619 : int = aten::Int(%batch_size.3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %620 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %621 : int = aten::size(%query.2, %620), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.4 : Long() = prim::NumToTensor(%621), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %623 : int = aten::Int(%seq_len.4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %624 : int = aten::Int(%seq_len.4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %625 : int = aten::Int(%seq_len.4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %626 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %627 : int = aten::size(%query.2, %626), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.2 : Long() = prim::NumToTensor(%627), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %629 : int = aten::Int(%num_heads.2), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %630 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %631 : int = aten::size(%query.2, %630), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.2 : Long() = prim::NumToTensor(%631), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %633 : int = aten::Int(%head_dim.2), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %634 : int = aten::Int(%head_dim.2), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %667 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %668 : Long() = aten::floor_divide(%seq_len.4, %667), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %669 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:478:0
    %670 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.2 : Long() = aten::sub(%668, %669, %670), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:478:0
    %672 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
    %673 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
    %674 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.2, %672, %673), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
    %675 : Long() = aten::mul(%batch_size.3, %num_heads.2), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
    %676 : int = aten::Int(%675), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %677 : int[] = prim::ListConstruct(%676, %625, %634), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %hidden_states.6 : Float(17:512, 512:1, 1:1) = aten::reshape(%674, %677), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
    %679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
    %680 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
    %681 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.1, %679, %680), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
    %682 : Long() = aten::mul(%batch_size.3, %num_heads.2), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
    %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %684 : int[] = prim::ListConstruct(%683, %624, %633), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %hidden_states.8 : Float(17:512, 512:1, 1:1) = aten::reshape(%681, %684), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
    %686 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
    %687 : int = aten::size(%hidden_states.6, %686), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
    %688 : Long() = prim::NumToTensor(%687), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %690 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
    %691 : int = aten::size(%hidden_states.6, %690), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
    %692 : Long() = prim::NumToTensor(%691), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %693 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %694 : Long() = aten::floor_divide(%692, %693), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %696 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
    %697 : int = aten::size(%hidden_states.6, %696), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
    %698 : Long() = prim::NumToTensor(%697), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %699 : int = aten::Int(%698), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %700 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
    %701 : int[] = prim::ListConstruct(%689, %695, %700, %699), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %hidden_states.7 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.6, %701), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
    %703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %704 : int = aten::size(%hidden_states.7, %703), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %705 : Long() = prim::NumToTensor(%704), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %706 : int = aten::Int(%705), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %708 : int = aten::size(%hidden_states.7, %707), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %709 : Long() = prim::NumToTensor(%708), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %710 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %711 : int = aten::size(%hidden_states.7, %710), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %712 : Long() = prim::NumToTensor(%711), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %713 : int = aten::Int(%712), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %714 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %715 : int = aten::size(%hidden_states.7, %714), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %716 : Long() = prim::NumToTensor(%715), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %717 : int = aten::Int(%716), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %718 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %719 : Long() = aten::mul(%709, %718), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %720 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %721 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %722 : Long() = aten::sub(%719, %720, %721), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %723 : int = aten::Int(%722), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %724 : int[] = prim::ListConstruct(%706, %723, %713, %717), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %725 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %726 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %727 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %728 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %729 : int[] = prim::ListConstruct(%725, %726, %727, %728), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %730 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %731 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.7, %724, %729, %730), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %732 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
    %733 : int = aten::size(%hidden_states.8, %732), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
    %734 : Long() = prim::NumToTensor(%733), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %735 : int = aten::Int(%734), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %736 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
    %737 : int = aten::size(%hidden_states.8, %736), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
    %738 : Long() = prim::NumToTensor(%737), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %739 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %740 : Long() = aten::floor_divide(%738, %739), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %741 : int = aten::Int(%740), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
    %743 : int = aten::size(%hidden_states.8, %742), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
    %744 : Long() = prim::NumToTensor(%743), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %745 : int = aten::Int(%744), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %746 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
    %747 : int[] = prim::ListConstruct(%735, %741, %746, %745), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %hidden_states.9 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.8, %747), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
    %749 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %750 : int = aten::size(%hidden_states.9, %749), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %751 : Long() = prim::NumToTensor(%750), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %752 : int = aten::Int(%751), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %754 : int = aten::size(%hidden_states.9, %753), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %755 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %756 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %757 : int = aten::size(%hidden_states.9, %756), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %758 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %759 : int = aten::Int(%758), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %760 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %761 : int = aten::size(%hidden_states.9, %760), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
    %762 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %764 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %765 : Long() = aten::mul(%755, %764), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %766 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %767 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %768 : Long() = aten::sub(%765, %766, %767), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
    %769 : int = aten::Int(%768), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %770 : int[] = prim::ListConstruct(%752, %769, %759, %763), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %771 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %772 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %773 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %776 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.9, %770, %775, %776), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
    %778 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/functional.py:327:0
    %779 : Tensor[] = prim::ListConstruct(%731, %777), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %input.9 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%778, %779), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/functional.py:327:0
    %781 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %782 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %783 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %786 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.2 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.9, %785, %786), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %788 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %789 : int = aten::size(%hidden_states_padded.2, %788), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %790 : Long() = prim::NumToTensor(%789), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %791 : int = aten::Int(%790), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %792 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %793 : int = aten::size(%hidden_states_padded.2, %792), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %794 : Long() = prim::NumToTensor(%793), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %795 : int = aten::Int(%794), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %802 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %803 : int = aten::size(%hidden_states_padded.2, %802), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %804 : Long() = prim::NumToTensor(%803), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %805 : int = aten::Int(%804), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %806 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %807 : int = aten::size(%hidden_states_padded.2, %806), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
    %808 : Long() = prim::NumToTensor(%807), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %809 : int = aten::Int(%808), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %810 : int[] = prim::ListConstruct(%791, %795, %805, %809), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %diagonal_chunked_attention_scores.2 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.2, %810), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:400:0
    %812 : Long() = aten::mul(%batch_size.3, %num_heads.2), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
    %813 : int = aten::Int(%812), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %814 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
    %815 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
    %816 : Long() = aten::add(%chunks_count.2, %814, %815), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
    %817 : int = aten::Int(%816), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %818 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %819 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %820 : int[] = prim::ListConstruct(%813, %817, %818, %819), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %821 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %822 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %823 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %824 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.2 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.2, %820, %821, %822, %823, %824), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
    %826 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %827 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %828 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %829 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %830 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %826, %827, %828, %829), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %831 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %832 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %833 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %834 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%830, %831, %832, %833, %834), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %836 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %837 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %838 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %839 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%835, %836, %837, %838, %839), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %841 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %842 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %843 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %845 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%840, %841, %842, %843, %844), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %846 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %847 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %848 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %849 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %846, %847, %848, %849), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %851 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %852 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %853 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %855 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%850, %851, %852, %853, %854), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %856 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %857 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %858 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %859 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %860 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%855, %856, %857, %858, %859), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %861 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %862 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %863 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %864 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %865 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%860, %861, %862, %863, %864), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %866 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %867 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %868 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %869 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %870 : int[] = prim::ListConstruct(%866, %867, %868, %869), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %871 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%845, %870), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %872 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %873 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%865, %871, %872), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
    %874 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %875 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %876 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %877 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %874, %875, %876, %877), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %879 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %880 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %881 : Float(17:262656, 512:513, 513:1) = aten::select(%878, %879, %880), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %883 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %884 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %885 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %886 : Float(17:262656, 256:513, 513:1) = aten::slice(%881, %882, %883, %884, %885), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %887 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %889 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%886, %887, %888, %889, %890), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %892 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %893 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %894 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %895 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %896 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %892, %893, %894, %895), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %897 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %898 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %899 : Float(17:262656, 256:513, 513:1) = aten::select(%896, %897, %898), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %901 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %902 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %903 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %904 : Float(17:262656, 256:513, 513:1) = aten::slice(%899, %900, %901, %902, %903), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %905 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %906 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %907 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %909 : Float(17:262656, 256:513, 257:1) = aten::slice(%904, %905, %906, %907, %908), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %910 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %911 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %912 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %913 : int[] = prim::ListConstruct(%910, %911, %912), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %914 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %913), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %915 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %916 : Float(17:262656, 256:513, 257:1) = aten::copy_(%909, %914, %915), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
    %917 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %918 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %919 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %920 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %921 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %917, %918, %919, %920), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %922 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %923 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %924 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %925 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %926 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%921, %922, %923, %924, %925), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %927 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %928 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %929 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %930 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %931 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%926, %927, %928, %929, %930), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %932 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %933 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %934 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %935 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %936 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%931, %932, %933, %934, %935), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %938 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %939 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %940 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %941 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %937, %938, %939, %940), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %942 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %943 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %944 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %945 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %946 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%941, %942, %943, %944, %945), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %947 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %948 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %949 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %950 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %951 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%946, %947, %948, %949, %950), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %952 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %953 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %954 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %955 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %956 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%951, %952, %953, %954, %955), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %957 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %959 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %960 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %962 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%936, %961), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %963 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %964 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%956, %962, %963), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
    %965 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %966 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %967 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %969 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %965, %966, %967, %968), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %970 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %971 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %972 : Float(17:262656, 512:513, 513:1) = aten::select(%969, %970, %971), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %973 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %974 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %975 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %976 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %977 : Float(17:262656, 255:513, 513:1) = aten::slice(%972, %973, %974, %975, %976), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %978 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %979 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %980 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %981 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %982 : Float(17:262656, 255:513, 255:1) = aten::slice(%977, %978, %979, %980, %981), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %983 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %984 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %985 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %986 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %987 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %983, %984, %985, %986), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %988 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %989 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %990 : Float(17:262656, 256:513, 513:1) = aten::select(%987, %988, %989), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %991 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %993 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %994 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %995 : Float(17:262656, 255:513, 513:1) = aten::slice(%990, %991, %992, %993, %994), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %996 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %998 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %999 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %1000 : Float(17:262656, 255:513, 255:1) = aten::slice(%995, %996, %997, %998, %999), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %1001 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %1002 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %1003 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %1004 : int[] = prim::ListConstruct(%1001, %1002, %1003), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1005 : Float(17:262656, 255:513, 255:1) = aten::view(%982, %1004), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %1006 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1007 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1000, %1005, %1006), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
    %1008 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
    %1009 : int[] = prim::ListConstruct(%619, %629, %623, %1008), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1010 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.2, %1009), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
    %1011 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
    %1012 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.2 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1010, %1011, %1012), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
    %1014 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %1015 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %1016 : int[] = prim::ListConstruct(%1014, %1015), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1017 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %1018 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %1019 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %1020 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %1021 : Float(256:257, 257:1) = aten::ones(%1016, %1017, %1018, %1019, %1020), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %1022 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %1023 : Float(256:257, 257:1) = aten::tril(%1021, %1022), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %1024 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %1025 : int[] = prim::ListConstruct(%1024), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %beginning_mask_2d.2 : Float(256:257, 257:1) = aten::flip(%1023, %1025), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
    %1027 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1028 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.2, %1027), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1029 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1030 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1031 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1032 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1033 : Float(1:65792, 256:257, 257:1) = aten::slice(%1028, %1029, %1030, %1031, %1032), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1034 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1035 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1033, %1034), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1036 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1037 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1038 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1039 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.2 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1035, %1036, %1037, %1038, %1039), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
    %1041 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:460:0
    %1042 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:460:0
    %1043 : int[] = prim::ListConstruct(%1041, %1042), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %ending_mask.2 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.2, %1043), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:460:0
    %1045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1046 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1047 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1048 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1049 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.2, %1045, %1046, %1047, %1048), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1050 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1052 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1053 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1054 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1049, %1050, %1051, %1052, %1053), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1055 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1056 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1057 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1058 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1059 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1054, %1055, %1056, %1057, %1058), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1060 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1061 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1062 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.2 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1059, %1060, %1061, %1062, %1063), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
    %1065 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %1066 : int = aten::size(%beginning_input.2, %1065), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %1067 : Long() = prim::NumToTensor(%1066), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1068 : int = aten::Int(%1067), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1069 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %1070 : int = aten::size(%beginning_input.2, %1069), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %1071 : Long() = prim::NumToTensor(%1070), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1072 : int = aten::Int(%1071), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1073 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %1074 : int = aten::size(%beginning_input.2, %1073), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %1075 : Long() = prim::NumToTensor(%1074), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1076 : int = aten::Int(%1075), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1077 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %1078 : int = aten::size(%beginning_input.2, %1077), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %1079 : Long() = prim::NumToTensor(%1078), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1080 : int = aten::Int(%1079), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1081 : int[] = prim::ListConstruct(%1068, %1072, %1076, %1080), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1082 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %1083 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.2, %1081, %1082), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
    %1084 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:22:0
    %1085 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1083, %1084), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:22:0
    %1086 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
    %1087 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.2, %1085, %1086), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
    %1088 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1089 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1090 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1091 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1092 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.2, %1088, %1089, %1090, %1091), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1094 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1095 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1096 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1097 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1092, %1093, %1094, %1095, %1096), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1098 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1099 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1100 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1101 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1102 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1097, %1098, %1099, %1100, %1101), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1103 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1104 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1105 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1106 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.2 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1102, %1103, %1104, %1105, %1106), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
    %1108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %1109 : int = aten::size(%ending_input.2, %1108), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %1110 : Long() = prim::NumToTensor(%1109), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1111 : int = aten::Int(%1110), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1112 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %1113 : int = aten::size(%ending_input.2, %1112), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %1114 : Long() = prim::NumToTensor(%1113), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1115 : int = aten::Int(%1114), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1116 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %1117 : int = aten::size(%ending_input.2, %1116), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %1118 : Long() = prim::NumToTensor(%1117), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1119 : int = aten::Int(%1118), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1120 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %1121 : int = aten::size(%ending_input.2, %1120), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %1122 : Long() = prim::NumToTensor(%1121), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1124 : int[] = prim::ListConstruct(%1111, %1115, %1119, %1123), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1125 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %1126 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.2, %1124, %1125), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
    %1127 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:22:0
    %1128 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1126, %1127), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:22:0
    %1129 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:466:0
    %1130 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.2, %1128, %1129), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:466:0
    %1131 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:284:0
    %attn_scores.1 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.1, %input_tensor.2, %1131), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:284:0
    %1151 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:1500:0
    %1152 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:1500:0
    %attn_probs_fp32.1 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.1, %1151, %1152), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:1500:0
    %attn_probs.1 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::type_as(%attn_probs_fp32.1, %attn_scores.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:320:0
    %1155 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1156 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1157 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1158 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1159 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.1, %1155, %1156, %1157, %1158), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1161 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1162 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1163 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1164 : Bool(17:512, 512:1) = aten::slice(%1159, %1160, %1161, %1162, %1163), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1165 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1166 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1164, %1165), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1167 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1168 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1166, %1167), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1169 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %input.10 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs.1, %1168, %1169), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
    %1171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:973:0
    %1172 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:973:0
    %attn_probs.2 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.10, %1171, %1172), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:973:0
    %1174 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:331:0
    %1175 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:331:0
    %1176 : int[] = prim::ListConstruct(%28, %35, %1174, %1175), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1177 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1389, %1176), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:331:0
    %1178 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:331:0
    %1179 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:331:0
    %value.1 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1177, %1178, %1179), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:331:0
    %1181 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
    %1182 : int = aten::size(%value.1, %1181), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
    %batch_size.4 : Long() = prim::NumToTensor(%1182), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1184 : int = aten::Int(%batch_size.4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1185 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
    %1186 : int = aten::size(%value.1, %1185), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
    %seq_len.5 : Long() = prim::NumToTensor(%1186), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1188 : int = aten::Int(%seq_len.5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1189 : int = aten::Int(%seq_len.5), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1190 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
    %1191 : int = aten::size(%value.1, %1190), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
    %num_heads.3 : Long() = prim::NumToTensor(%1191), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1193 : int = aten::Int(%num_heads.3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1194 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
    %1195 : int = aten::size(%value.1, %1194), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
    %head_dim.3 : Long() = prim::NumToTensor(%1195), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1197 : int = aten::Int(%head_dim.3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1198 : int = aten::Int(%head_dim.3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1199 : int = aten::Int(%head_dim.3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1236 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %1237 : Long() = aten::floor_divide(%seq_len.5, %1236), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %1238 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:542:0
    %1239 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:542:0
    %chunks_count.3 : Long() = aten::sub(%1237, %1238, %1239), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:542:0
    %1241 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:545:0
    %1242 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:545:0
    %1243 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.2, %1241, %1242), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:545:0
    %1244 : Long() = aten::mul(%batch_size.4, %num_heads.3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:546:0
    %1245 : int = aten::Int(%1244), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1246 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %1247 : Long() = aten::floor_divide(%seq_len.5, %1246), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/tensor.py:424:0
    %1248 : int = aten::Int(%1247), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1249 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:545:0
    %1250 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:545:0
    %1251 : int[] = prim::ListConstruct(%1245, %1248, %1249, %1250), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %chunked_hidden_states.1 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1243, %1251), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:545:0
    %1253 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
    %1254 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
    %1255 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.1, %1253, %1254), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
    %1256 : Long() = aten::mul(%batch_size.4, %num_heads.3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
    %1257 : int = aten::Int(%1256), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1258 : int[] = prim::ListConstruct(%1257, %1189, %1199), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %input.11 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1255, %1258), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
    %1260 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %1261 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %1262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %1263 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %1264 : int[] = prim::ListConstruct(%1260, %1261, %1262, %1263), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1265 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %padded_value.1 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.11, %1264, %1265), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %1267 : Long() = aten::mul(%batch_size.4, %num_heads.3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:556:0
    %1268 : int = aten::Int(%1267), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1269 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:556:0
    %1270 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:556:0
    %1271 : Long() = aten::add(%chunks_count.3, %1269, %1270), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:556:0
    %1272 : int = aten::Int(%1271), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
    %1274 : int[] = prim::ListConstruct(%1268, %1272, %1273, %1198), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1275 : int = prim::Constant[value=65536](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
    %1276 : int = prim::Constant[value=16384](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
    %1277 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
    %1278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
    %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1280 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1281 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.1, %1274, %1279, %1280), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
    %1282 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
    %1283 : int = aten::size(%chunked_hidden_states.1, %1282), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
    %total_num_heads.1 : Long() = prim::NumToTensor(%1283), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1285 : int = aten::Int(%total_num_heads.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1286 : int = aten::Int(%total_num_heads.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1287 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
    %1288 : int = aten::size(%chunked_hidden_states.1, %1287), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
    %num_chunks.1 : Long() = prim::NumToTensor(%1288), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1290 : int = aten::Int(%num_chunks.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1291 : int = aten::Int(%num_chunks.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1292 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
    %1293 : int = aten::size(%chunked_hidden_states.1, %1292), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
    %window_overlap.1 : Long() = prim::NumToTensor(%1293), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1295 : int = aten::Int(%window_overlap.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1296 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
    %1297 : int = aten::size(%chunked_hidden_states.1, %1296), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
    %hidden_dim.1 : Long() = prim::NumToTensor(%1297), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1299 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:422:0
    %1300 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:422:0
    %1301 : Long() = aten::add(%window_overlap.1, %1299, %1300), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:422:0
    %1302 : int = aten::Int(%1301), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1303 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %1304 : int[] = prim::ListConstruct(%1303, %1302), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1305 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %chunked_hidden_states.2 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.1, %1304, %1305), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
    %1307 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:424:0
    %1308 : int[] = prim::ListConstruct(%1286, %1291, %1307), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %chunked_hidden_states.3 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.2, %1308), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:424:0
    %1310 : Long() = aten::neg(%window_overlap.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:428:0
    %1311 : int = aten::Int(%1310), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1312 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1313 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1314 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1315 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1316 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.3, %1312, %1313, %1314, %1315), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1319 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1320 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1316, %1317, %1318, %1319, %1320), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1322 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1323 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1324 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %chunked_hidden_states.4 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1321, %1322, %1323, %1311, %1324), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
    %1326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:431:0
    %1327 : Long() = aten::add(%window_overlap.1, %hidden_dim.1, %1326), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:431:0
    %1328 : int = aten::Int(%1327), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1329 : int[] = prim::ListConstruct(%1285, %1290, %1295, %1328), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %chunked_hidden_states.5 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.4, %1329), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:430:0
    %1331 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1333 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1335 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.5, %1331, %1332, %1333, %1334), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1336 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1340 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1335, %1336, %1337, %1338, %1339), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1341 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1342 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1343 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1345 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1340, %1341, %1342, %1343, %1344), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1346 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1347 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1348 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1350 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1345, %1346, %1347, %1348, %1349), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
    %1351 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/functional.py:327:0
    %1352 : Tensor[] = prim::ListConstruct(%1350, %1281), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %context.1 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%1351, %1352), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/functional.py:327:0
    %1354 : int[] = prim::ListConstruct(%1184, %1193, %1188, %1197), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1355 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.1, %1354), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:569:0
    %1356 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:569:0
    %1357 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:569:0
    %attn_output.1 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1355, %1356, %1357), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:569:0
    %1377 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
    %1378 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
    %1379 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.1, %1377, %1378), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
    %1380 : int[] = prim::ListConstruct(%27, %34, %41), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
    %1381 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1379, %1380), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
    %1382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
    %attn_output.2 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1381, %1382), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
    %1384 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:374:0
    %1385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:374:0
    %input.12 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.2, %1384, %1385), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_longformer.py:374:0
    return (%input.12)

LongformerSelfAttention.key
Linear._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.linear.Linear,
        %input.7 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.13)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.13)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
    %output.2 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.7, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
    %key_vectors.1 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.2, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
    return (%key_vectors.1)

LongformerSelfAttention.query
Linear._actual_script_module
  graph(%self.12 : __torch__.torch.nn.modules.linear.Linear,
        %input.7 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.12)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.12)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
    %output.1 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.7, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
    %query_vectors.1 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.1, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
    return (%query_vectors.1)

LongformerSelfAttention.value
Linear._actual_script_module
  graph(%self.14 : __torch__.torch.nn.modules.linear.Linear,
        %input.7 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.14)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.14)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
    %output.3 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.7, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
    %value_vectors.1 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.3, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
    return (%value_vectors.1)

LongformerSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.18 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.14 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.18)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.18)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.3 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.14, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.3)

LongformerSelfOutput.dense
Linear._actual_script_module
  graph(%self.16 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:768, 512:13056, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.16)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.16)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
    %output.4 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
    %input.13 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.4, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.13)

LongformerSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.17 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.10 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.10)

LongformerIntermediate.dense
Linear._actual_script_module
  graph(%self.20 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.20)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.20)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.5 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.15 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.5, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.15)

LongformerOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.24 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.18 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.24)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.24)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
    %hidden_states.12 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.18, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%hidden_states.12)

LongformerOutput.dense
Linear._actual_script_module
  graph(%self.22 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1572864, 512:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.22)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.22)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
    %output.6 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
    %input.17 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.6, %2, %6), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
    return (%input.17)

LongformerOutput.dropout
Dropout._actual_script_module
  graph(%self.23 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.11 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.11)

LongformerLayer._actual_script_module
  graph(%self.25 : __torch__.transformers.modeling_longformer.LongformerLayer,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%self.25)
    %4 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%self.25)
    %5 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%self.25)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %attention_mask, %2)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

LongformerLayer.attention
LongformerAttention._actual_script_module
  graph(%self.26 : __torch__.transformers.modeling_longformer.LongformerAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%self.26)
    %4 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%self.26)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %2)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %2)
    return (%8)

LongformerLayer.intermediate
LongformerIntermediate._actual_script_module
  graph(%self.35 : __torch__.transformers.modeling_longformer.LongformerIntermediate,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.35)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.28 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
    return (%input.28)

LongformerLayer.output
LongformerOutput._actual_script_module
  graph(%self.37 : __torch__.transformers.modeling_longformer.LongformerOutput,
        %1 : Float(17:1572864, 512:3072, 3072:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.37)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.37)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.37)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output # transformers/modeling_longformer.py:830:0
    %input.30 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output # transformers/modeling_longformer.py:830:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.30)
    return (%13)

LongformerAttention.output
LongformerSelfOutput._actual_script_module
  graph(%self.31 : __torch__.transformers.modeling_longformer.LongformerSelfOutput,
        %1 : Float(17:768, 512:13056, 768:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.31)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.31)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.31)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output # transformers/modeling_longformer.py:758:0
    %input.26 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output # transformers/modeling_longformer.py:758:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.26)
    return (%13)

LongformerAttention.self
LongformerSelfAttention._actual_script_module
  graph(%self.27 : __torch__.transformers.modeling_longformer.LongformerSelfAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.27)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.27)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.27)
    %6 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:241:0
    %7 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:241:0
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:241:0
    %9 : Float(17:512, 512:1) = aten::squeeze(%7, %8), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:241:0
    %10 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:22:0
    %is_index_masked.2 : Bool(17:512, 512:1) = aten::lt(%9, %10), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:22:0
    %18 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:248:0
    %19 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:248:0
    %input.19 : Float(512:768, 17:393216, 768:1) = aten::transpose(%2, %18, %19), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:248:0
    %1387 : Tensor = prim::CallMethod[name="forward"](%5, %input.19)
    %1388 : Tensor = prim::CallMethod[name="forward"](%4, %input.19)
    %1389 : Tensor = prim::CallMethod[name="forward"](%3, %input.19)
    %24 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
    %25 : int = aten::size(%input.19, %24), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
    %seq_len.6 : Long() = prim::NumToTensor(%25), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %27 : int = aten::Int(%seq_len.6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %28 : int = aten::Int(%seq_len.6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %29 : int = aten::Int(%seq_len.6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %30 : int = aten::Int(%seq_len.6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %31 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
    %32 : int = aten::size(%input.19, %31), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
    %batch_size.5 : Long() = prim::NumToTensor(%32), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %34 : int = aten::Int(%batch_size.5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %35 : int = aten::Int(%batch_size.5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %36 : int = aten::Int(%batch_size.5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %37 : int = aten::Int(%batch_size.5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %38 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
    %39 : int = aten::size(%input.19, %38), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
    %embed_dim.2 : Long() = prim::NumToTensor(%39), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %41 : int = aten::Int(%embed_dim.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %44 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:261:0
    %query_vectors.4 : Float(512:13056, 17:768, 768:1) = aten::div_(%1387, %44), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:261:0
    %46 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:263:0
    %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:263:0
    %48 : int[] = prim::ListConstruct(%30, %37, %46, %47), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %49 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.4, %48), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:263:0
    %50 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:263:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:263:0
    %query.3 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%49, %50, %51), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:263:0
    %53 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:264:0
    %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:264:0
    %55 : int[] = prim::ListConstruct(%29, %36, %53, %54), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %56 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1388, %55), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:264:0
    %57 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:264:0
    %58 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:264:0
    %key.2 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%56, %57, %58), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:264:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %61 : int = aten::size(%query.3, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.6 : Long() = prim::NumToTensor(%61), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %63 : int = aten::Int(%batch_size.6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %65 : int = aten::size(%query.3, %64), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.7 : Long() = prim::NumToTensor(%65), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %67 : int = aten::Int(%seq_len.7), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %68 : int = aten::Int(%seq_len.7), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %69 : int = aten::Int(%seq_len.7), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %70 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %71 : int = aten::size(%query.3, %70), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.4 : Long() = prim::NumToTensor(%71), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %73 : int = aten::Int(%num_heads.4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %74 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %75 : int = aten::size(%query.3, %74), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.4 : Long() = prim::NumToTensor(%75), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %77 : int = aten::Int(%head_dim.4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %78 : int = aten::Int(%head_dim.4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %111 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %112 : Long() = aten::floor_divide(%seq_len.7, %111), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %113 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:478:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.4 : Long() = aten::sub(%112, %113, %114), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:478:0
    %116 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
    %117 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
    %118 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.3, %116, %117), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
    %119 : Long() = aten::mul(%batch_size.6, %num_heads.4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
    %120 : int = aten::Int(%119), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %121 : int[] = prim::ListConstruct(%120, %69, %78), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %hidden_states.13 : Float(204:64, 512:13056, 64:1) = aten::reshape(%118, %121), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
    %123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
    %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
    %125 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.2, %123, %124), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
    %126 : Long() = aten::mul(%batch_size.6, %num_heads.4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
    %127 : int = aten::Int(%126), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %128 : int[] = prim::ListConstruct(%127, %68, %77), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %hidden_states.15 : Float(204:64, 512:13056, 64:1) = aten::reshape(%125, %128), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
    %130 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
    %131 : int = aten::size(%hidden_states.13, %130), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
    %132 : Long() = prim::NumToTensor(%131), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %133 : int = aten::Int(%132), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %134 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
    %135 : int = aten::size(%hidden_states.13, %134), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
    %136 : Long() = prim::NumToTensor(%135), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %137 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %138 : Long() = aten::floor_divide(%136, %137), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %139 : int = aten::Int(%138), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %140 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
    %141 : int = aten::size(%hidden_states.13, %140), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
    %142 : Long() = prim::NumToTensor(%141), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %143 : int = aten::Int(%142), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %144 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
    %145 : int[] = prim::ListConstruct(%133, %139, %144, %143), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %hidden_states.14 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.13, %145), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
    %147 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %148 : int = aten::size(%hidden_states.14, %147), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %149 : Long() = prim::NumToTensor(%148), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %150 : int = aten::Int(%149), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %151 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %152 : int = aten::size(%hidden_states.14, %151), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %153 : Long() = prim::NumToTensor(%152), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %154 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %155 : int = aten::size(%hidden_states.14, %154), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %156 : Long() = prim::NumToTensor(%155), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %157 : int = aten::Int(%156), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %158 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %159 : int = aten::size(%hidden_states.14, %158), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %160 : Long() = prim::NumToTensor(%159), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %161 : int = aten::Int(%160), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %162 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %163 : Long() = aten::mul(%153, %162), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %164 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %165 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %166 : Long() = aten::sub(%163, %164, %165), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %167 : int = aten::Int(%166), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %168 : int[] = prim::ListConstruct(%150, %167, %157, %161), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %169 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %170 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %171 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %172 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %173 : int[] = prim::ListConstruct(%169, %170, %171, %172), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %174 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %175 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.14, %168, %173, %174), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %176 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
    %177 : int = aten::size(%hidden_states.15, %176), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %179 : int = aten::Int(%178), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
    %181 : int = aten::size(%hidden_states.15, %180), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
    %182 : Long() = prim::NumToTensor(%181), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %183 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %184 : Long() = aten::floor_divide(%182, %183), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %185 : int = aten::Int(%184), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %186 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
    %187 : int = aten::size(%hidden_states.15, %186), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
    %188 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %189 : int = aten::Int(%188), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %190 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
    %191 : int[] = prim::ListConstruct(%179, %185, %190, %189), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %hidden_states.16 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.15, %191), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
    %193 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %194 : int = aten::size(%hidden_states.16, %193), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %195 : Long() = prim::NumToTensor(%194), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %196 : int = aten::Int(%195), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %197 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %198 : int = aten::size(%hidden_states.16, %197), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %199 : Long() = prim::NumToTensor(%198), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %200 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %201 : int = aten::size(%hidden_states.16, %200), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %203 : int = aten::Int(%202), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %204 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %205 : int = aten::size(%hidden_states.16, %204), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %207 : int = aten::Int(%206), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %208 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %209 : Long() = aten::mul(%199, %208), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %210 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %211 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %212 : Long() = aten::sub(%209, %210, %211), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %213 : int = aten::Int(%212), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %214 : int[] = prim::ListConstruct(%196, %213, %203, %207), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %216 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %217 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %218 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %219 : int[] = prim::ListConstruct(%215, %216, %217, %218), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %220 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.16, %214, %219, %220), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %222 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/functional.py:327:0
    %223 : Tensor[] = prim::ListConstruct(%175, %221), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %input.20 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%222, %223), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/functional.py:327:0
    %225 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %226 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %227 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %229 : int[] = prim::ListConstruct(%225, %226, %227, %228), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %230 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.3 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.20, %229, %230), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %232 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %233 : int = aten::size(%hidden_states_padded.3, %232), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %234 : Long() = prim::NumToTensor(%233), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %235 : int = aten::Int(%234), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %236 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %237 : int = aten::size(%hidden_states_padded.3, %236), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %238 : Long() = prim::NumToTensor(%237), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %239 : int = aten::Int(%238), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %246 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %247 : int = aten::size(%hidden_states_padded.3, %246), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %248 : Long() = prim::NumToTensor(%247), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %249 : int = aten::Int(%248), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %250 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %251 : int = aten::size(%hidden_states_padded.3, %250), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %252 : Long() = prim::NumToTensor(%251), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %253 : int = aten::Int(%252), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %254 : int[] = prim::ListConstruct(%235, %239, %249, %253), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %diagonal_chunked_attention_scores.3 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.3, %254), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:400:0
    %256 : Long() = aten::mul(%batch_size.6, %num_heads.4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
    %257 : int = aten::Int(%256), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %258 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
    %259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
    %260 : Long() = aten::add(%chunks_count.4, %258, %259), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
    %261 : int = aten::Int(%260), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %263 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %264 : int[] = prim::ListConstruct(%257, %261, %262, %263), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %265 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %266 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %268 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.3 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.3, %264, %265, %266, %267, %268), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %270 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %271 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %273 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %270, %271, %272, %273), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %275 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %276 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %277 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %279 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%274, %275, %276, %277, %278), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %280 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %281 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %282 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %284 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%279, %280, %281, %282, %283), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %285 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %286 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %287 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %289 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%284, %285, %286, %287, %288), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %290 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %291 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %292 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %294 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %290, %291, %292, %293), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %295 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %296 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %297 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %299 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%294, %295, %296, %297, %298), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %300 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %301 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %302 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %303 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %304 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%299, %300, %301, %302, %303), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %305 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %306 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %307 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %308 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %309 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%304, %305, %306, %307, %308), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %310 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %311 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %312 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %313 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %314 : int[] = prim::ListConstruct(%310, %311, %312, %313), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %315 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%289, %314), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %316 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %317 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%309, %315, %316), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %319 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %320 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %321 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %322 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %318, %319, %320, %321), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %323 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %324 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %325 : Float(204:262656, 512:513, 513:1) = aten::select(%322, %323, %324), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %327 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %328 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %329 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %330 : Float(204:262656, 256:513, 513:1) = aten::slice(%325, %326, %327, %328, %329), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %331 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %333 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %335 : Float(204:262656, 256:513, 257:1) = aten::slice(%330, %331, %332, %333, %334), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %336 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %340 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %336, %337, %338, %339), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %341 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %342 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %343 : Float(204:262656, 256:513, 513:1) = aten::select(%340, %341, %342), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %345 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %347 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %348 : Float(204:262656, 256:513, 513:1) = aten::slice(%343, %344, %345, %346, %347), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %349 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %350 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %352 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %353 : Float(204:262656, 256:513, 257:1) = aten::slice(%348, %349, %350, %351, %352), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %354 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %355 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %356 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %357 : int[] = prim::ListConstruct(%354, %355, %356), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %358 : Float(204:262656, 256:513, 257:1) = aten::view(%335, %357), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %359 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %360 : Float(204:262656, 256:513, 257:1) = aten::copy_(%353, %358, %359), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %361 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %362 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %363 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %364 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %365 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %361, %362, %363, %364), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %366 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %367 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %368 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %369 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %370 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%365, %366, %367, %368, %369), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %371 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %372 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %373 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %374 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %375 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%370, %371, %372, %373, %374), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %376 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %377 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %379 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %380 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%375, %376, %377, %378, %379), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %383 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %384 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %385 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %381, %382, %383, %384), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %386 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %387 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %388 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %389 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %390 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%385, %386, %387, %388, %389), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %391 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %392 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %393 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %394 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %395 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%390, %391, %392, %393, %394), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %397 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %398 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %399 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %400 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%395, %396, %397, %398, %399), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %401 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %402 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %403 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %404 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %405 : int[] = prim::ListConstruct(%401, %402, %403, %404), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %406 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%380, %405), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %407 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %408 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%400, %406, %407), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %409 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %410 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %411 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %412 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %413 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %409, %410, %411, %412), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %414 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %415 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %416 : Float(204:262656, 512:513, 513:1) = aten::select(%413, %414, %415), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %417 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %418 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %419 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %420 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %421 : Float(204:262656, 255:513, 513:1) = aten::slice(%416, %417, %418, %419, %420), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %422 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %423 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %425 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %426 : Float(204:262656, 255:513, 255:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %427 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %429 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %430 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %431 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %427, %428, %429, %430), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %432 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %433 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %434 : Float(204:262656, 256:513, 513:1) = aten::select(%431, %432, %433), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %435 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %436 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %437 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %438 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %439 : Float(204:262656, 255:513, 513:1) = aten::slice(%434, %435, %436, %437, %438), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %440 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %441 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %442 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %443 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %444 : Float(204:262656, 255:513, 255:1) = aten::slice(%439, %440, %441, %442, %443), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %445 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %446 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %447 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %448 : int[] = prim::ListConstruct(%445, %446, %447), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %449 : Float(204:262656, 255:513, 255:1) = aten::view(%426, %448), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %450 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %451 : Float(204:262656, 255:513, 255:1) = aten::copy_(%444, %449, %450), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %452 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
    %453 : int[] = prim::ListConstruct(%63, %73, %67, %452), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %454 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.3, %453), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
    %455 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
    %456 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.4 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%454, %455, %456), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
    %458 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %459 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %460 : int[] = prim::ListConstruct(%458, %459), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %461 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %462 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %463 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %464 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %465 : Float(256:257, 257:1) = aten::ones(%460, %461, %462, %463, %464), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %466 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %467 : Float(256:257, 257:1) = aten::tril(%465, %466), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %468 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %469 : int[] = prim::ListConstruct(%468), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %beginning_mask_2d.3 : Float(256:257, 257:1) = aten::flip(%467, %469), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %471 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %472 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.3, %471), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %473 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %474 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %475 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %476 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %477 : Float(1:65792, 256:257, 257:1) = aten::slice(%472, %473, %474, %475, %476), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %478 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %479 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%477, %478), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %480 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %481 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %482 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %483 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.3 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%479, %480, %481, %482, %483), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %485 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:460:0
    %486 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:460:0
    %487 : int[] = prim::ListConstruct(%485, %486), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %ending_mask.3 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.3, %487), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:460:0
    %489 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %490 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %491 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %493 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.4, %489, %490, %491, %492), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %494 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %495 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %496 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %497 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %498 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%493, %494, %495, %496, %497), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %499 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %500 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %501 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %502 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %503 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%498, %499, %500, %501, %502), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %504 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %505 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %506 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %507 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.3 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%503, %504, %505, %506, %507), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %509 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %510 : int = aten::size(%beginning_input.3, %509), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %511 : Long() = prim::NumToTensor(%510), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %512 : int = aten::Int(%511), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %513 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %514 : int = aten::size(%beginning_input.3, %513), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %515 : Long() = prim::NumToTensor(%514), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %516 : int = aten::Int(%515), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %517 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %518 : int = aten::size(%beginning_input.3, %517), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %519 : Long() = prim::NumToTensor(%518), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %520 : int = aten::Int(%519), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %521 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %522 : int = aten::size(%beginning_input.3, %521), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %523 : Long() = prim::NumToTensor(%522), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %524 : int = aten::Int(%523), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %525 : int[] = prim::ListConstruct(%512, %516, %520, %524), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %526 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %527 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.3, %525, %526), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %528 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:22:0
    %529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%527, %528), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:22:0
    %530 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:463:0
    %531 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.3, %529, %530), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:463:0
    %532 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %533 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %534 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %535 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %536 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.4, %532, %533, %534, %535), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %537 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %538 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %539 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %541 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%536, %537, %538, %539, %540), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %542 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %543 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %544 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %545 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %546 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%541, %542, %543, %544, %545), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %547 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %548 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %549 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.3 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%546, %547, %548, %549, %550), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %552 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %553 : int = aten::size(%ending_input.3, %552), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %554 : Long() = prim::NumToTensor(%553), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %555 : int = aten::Int(%554), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %556 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %557 : int = aten::size(%ending_input.3, %556), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %558 : Long() = prim::NumToTensor(%557), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %559 : int = aten::Int(%558), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %560 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %561 : int = aten::size(%ending_input.3, %560), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %562 : Long() = prim::NumToTensor(%561), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %563 : int = aten::Int(%562), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %564 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %565 : int = aten::size(%ending_input.3, %564), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %566 : Long() = prim::NumToTensor(%565), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %567 : int = aten::Int(%566), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %568 : int[] = prim::ListConstruct(%555, %559, %563, %567), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %569 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %570 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.3, %568, %569), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %571 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:22:0
    %572 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%570, %571), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:22:0
    %573 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:466:0
    %574 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.3, %572, %573), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:466:0
    %575 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:22:0
    %576 : Bool(17:512, 512:1) = aten::ne(%9, %575), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:22:0
    %577 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %578 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %579 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %580 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %581 : Bool(17:512, 512:1) = aten::slice(%576, %577, %578, %579, %580), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %582 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %583 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %584 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %585 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %586 : Bool(17:512, 512:1) = aten::slice(%581, %582, %583, %584, %585), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %587 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %588 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%586, %587), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %589 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %remove_from_windowed_attention_mask.2 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%588, %589), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
    %591 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.2, %query.3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:275:0
    %592 : float = prim::Constant[value=-10000.](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:275:0
    %float_mask.2 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%591, %remove_from_windowed_attention_mask.2, %592), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:275:0
    %594 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %595 : int = aten::size(%float_mask.2, %594), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %596 : Long() = prim::NumToTensor(%595), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %597 : int = aten::Int(%596), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %598 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %599 : int = aten::size(%float_mask.2, %598), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %600 : Long() = prim::NumToTensor(%599), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %601 : int = aten::Int(%600), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %602 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %603 : int = aten::size(%float_mask.2, %602), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %604 : Long() = prim::NumToTensor(%603), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %605 : int = aten::Int(%604), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %606 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %607 : int = aten::size(%float_mask.2, %606), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %608 : Long() = prim::NumToTensor(%607), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %609 : int = aten::Int(%608), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %610 : int[] = prim::ListConstruct(%597, %601, %605, %609), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %611 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %612 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %613 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %614 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %query.4 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%610, %611, %612, %613, %614), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
    %616 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %617 : int = aten::size(%query.4, %616), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.7 : Long() = prim::NumToTensor(%617), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %619 : int = aten::Int(%batch_size.7), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %620 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %621 : int = aten::size(%query.4, %620), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.8 : Long() = prim::NumToTensor(%621), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %623 : int = aten::Int(%seq_len.8), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %624 : int = aten::Int(%seq_len.8), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %625 : int = aten::Int(%seq_len.8), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %626 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %627 : int = aten::size(%query.4, %626), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.5 : Long() = prim::NumToTensor(%627), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %629 : int = aten::Int(%num_heads.5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %630 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %631 : int = aten::size(%query.4, %630), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.5 : Long() = prim::NumToTensor(%631), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %633 : int = aten::Int(%head_dim.5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %634 : int = aten::Int(%head_dim.5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %667 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %668 : Long() = aten::floor_divide(%seq_len.8, %667), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %669 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:478:0
    %670 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.5 : Long() = aten::sub(%668, %669, %670), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:478:0
    %672 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
    %673 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
    %674 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.4, %672, %673), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
    %675 : Long() = aten::mul(%batch_size.7, %num_heads.5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
    %676 : int = aten::Int(%675), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %677 : int[] = prim::ListConstruct(%676, %625, %634), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %hidden_states.17 : Float(17:512, 512:1, 1:1) = aten::reshape(%674, %677), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
    %679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
    %680 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
    %681 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.2, %679, %680), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
    %682 : Long() = aten::mul(%batch_size.7, %num_heads.5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
    %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %684 : int[] = prim::ListConstruct(%683, %624, %633), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %hidden_states.19 : Float(17:512, 512:1, 1:1) = aten::reshape(%681, %684), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
    %686 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
    %687 : int = aten::size(%hidden_states.17, %686), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
    %688 : Long() = prim::NumToTensor(%687), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %690 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
    %691 : int = aten::size(%hidden_states.17, %690), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
    %692 : Long() = prim::NumToTensor(%691), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %693 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %694 : Long() = aten::floor_divide(%692, %693), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %696 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
    %697 : int = aten::size(%hidden_states.17, %696), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
    %698 : Long() = prim::NumToTensor(%697), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %699 : int = aten::Int(%698), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %700 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
    %701 : int[] = prim::ListConstruct(%689, %695, %700, %699), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %hidden_states.18 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.17, %701), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
    %703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %704 : int = aten::size(%hidden_states.18, %703), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %705 : Long() = prim::NumToTensor(%704), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %706 : int = aten::Int(%705), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %708 : int = aten::size(%hidden_states.18, %707), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %709 : Long() = prim::NumToTensor(%708), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %710 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %711 : int = aten::size(%hidden_states.18, %710), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %712 : Long() = prim::NumToTensor(%711), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %713 : int = aten::Int(%712), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %714 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %715 : int = aten::size(%hidden_states.18, %714), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %716 : Long() = prim::NumToTensor(%715), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %717 : int = aten::Int(%716), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %718 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %719 : Long() = aten::mul(%709, %718), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %720 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %721 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %722 : Long() = aten::sub(%719, %720, %721), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %723 : int = aten::Int(%722), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %724 : int[] = prim::ListConstruct(%706, %723, %713, %717), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %725 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %726 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %727 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %728 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %729 : int[] = prim::ListConstruct(%725, %726, %727, %728), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %730 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %731 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.18, %724, %729, %730), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %732 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
    %733 : int = aten::size(%hidden_states.19, %732), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
    %734 : Long() = prim::NumToTensor(%733), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %735 : int = aten::Int(%734), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %736 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
    %737 : int = aten::size(%hidden_states.19, %736), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
    %738 : Long() = prim::NumToTensor(%737), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %739 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %740 : Long() = aten::floor_divide(%738, %739), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %741 : int = aten::Int(%740), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
    %743 : int = aten::size(%hidden_states.19, %742), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
    %744 : Long() = prim::NumToTensor(%743), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %745 : int = aten::Int(%744), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %746 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
    %747 : int[] = prim::ListConstruct(%735, %741, %746, %745), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %hidden_states.20 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.19, %747), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
    %749 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %750 : int = aten::size(%hidden_states.20, %749), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %751 : Long() = prim::NumToTensor(%750), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %752 : int = aten::Int(%751), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %754 : int = aten::size(%hidden_states.20, %753), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %755 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %756 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %757 : int = aten::size(%hidden_states.20, %756), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %758 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %759 : int = aten::Int(%758), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %760 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %761 : int = aten::size(%hidden_states.20, %760), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
    %762 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %764 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %765 : Long() = aten::mul(%755, %764), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %766 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %767 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %768 : Long() = aten::sub(%765, %766, %767), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
    %769 : int = aten::Int(%768), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %770 : int[] = prim::ListConstruct(%752, %769, %759, %763), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %771 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %772 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %773 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %776 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.20, %770, %775, %776), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
    %778 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/functional.py:327:0
    %779 : Tensor[] = prim::ListConstruct(%731, %777), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %input.21 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%778, %779), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/functional.py:327:0
    %781 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %782 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %783 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %786 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.4 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.21, %785, %786), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %788 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %789 : int = aten::size(%hidden_states_padded.4, %788), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %790 : Long() = prim::NumToTensor(%789), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %791 : int = aten::Int(%790), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %792 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %793 : int = aten::size(%hidden_states_padded.4, %792), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %794 : Long() = prim::NumToTensor(%793), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %795 : int = aten::Int(%794), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %802 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %803 : int = aten::size(%hidden_states_padded.4, %802), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %804 : Long() = prim::NumToTensor(%803), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %805 : int = aten::Int(%804), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %806 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %807 : int = aten::size(%hidden_states_padded.4, %806), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
    %808 : Long() = prim::NumToTensor(%807), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %809 : int = aten::Int(%808), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %810 : int[] = prim::ListConstruct(%791, %795, %805, %809), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %diagonal_chunked_attention_scores.4 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.4, %810), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:400:0
    %812 : Long() = aten::mul(%batch_size.7, %num_heads.5), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
    %813 : int = aten::Int(%812), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %814 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
    %815 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
    %816 : Long() = aten::add(%chunks_count.5, %814, %815), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
    %817 : int = aten::Int(%816), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %818 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %819 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %820 : int[] = prim::ListConstruct(%813, %817, %818, %819), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %821 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %822 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %823 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %824 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.4 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.4, %820, %821, %822, %823, %824), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
    %826 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %827 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %828 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %829 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %830 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %826, %827, %828, %829), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %831 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %832 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %833 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %834 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%830, %831, %832, %833, %834), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %836 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %837 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %838 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %839 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%835, %836, %837, %838, %839), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %841 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %842 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %843 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %845 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%840, %841, %842, %843, %844), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %846 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %847 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %848 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %849 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %846, %847, %848, %849), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %851 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %852 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %853 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %855 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%850, %851, %852, %853, %854), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %856 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %857 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %858 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %859 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %860 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%855, %856, %857, %858, %859), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %861 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %862 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %863 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %864 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %865 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%860, %861, %862, %863, %864), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %866 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %867 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %868 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %869 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %870 : int[] = prim::ListConstruct(%866, %867, %868, %869), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %871 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%845, %870), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %872 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %873 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%865, %871, %872), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
    %874 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %875 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %876 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %877 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %874, %875, %876, %877), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %879 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %880 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %881 : Float(17:262656, 512:513, 513:1) = aten::select(%878, %879, %880), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %883 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %884 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %885 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %886 : Float(17:262656, 256:513, 513:1) = aten::slice(%881, %882, %883, %884, %885), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %887 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %889 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%886, %887, %888, %889, %890), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %892 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %893 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %894 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %895 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %896 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %892, %893, %894, %895), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %897 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %898 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %899 : Float(17:262656, 256:513, 513:1) = aten::select(%896, %897, %898), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %901 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %902 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %903 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %904 : Float(17:262656, 256:513, 513:1) = aten::slice(%899, %900, %901, %902, %903), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %905 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %906 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %907 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %909 : Float(17:262656, 256:513, 257:1) = aten::slice(%904, %905, %906, %907, %908), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %910 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %911 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %912 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %913 : int[] = prim::ListConstruct(%910, %911, %912), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %914 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %913), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %915 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %916 : Float(17:262656, 256:513, 257:1) = aten::copy_(%909, %914, %915), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
    %917 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %918 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %919 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %920 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %921 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %917, %918, %919, %920), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %922 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %923 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %924 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %925 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %926 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%921, %922, %923, %924, %925), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %927 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %928 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %929 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %930 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %931 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%926, %927, %928, %929, %930), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %932 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %933 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %934 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %935 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %936 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%931, %932, %933, %934, %935), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %938 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %939 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %940 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %941 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %937, %938, %939, %940), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %942 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %943 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %944 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %945 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %946 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%941, %942, %943, %944, %945), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %947 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %948 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %949 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %950 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %951 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%946, %947, %948, %949, %950), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %952 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %953 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %954 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %955 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %956 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%951, %952, %953, %954, %955), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %957 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %959 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %960 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %962 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%936, %961), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %963 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %964 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%956, %962, %963), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
    %965 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %966 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %967 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %969 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %965, %966, %967, %968), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %970 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %971 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %972 : Float(17:262656, 512:513, 513:1) = aten::select(%969, %970, %971), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %973 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %974 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %975 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %976 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %977 : Float(17:262656, 255:513, 513:1) = aten::slice(%972, %973, %974, %975, %976), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %978 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %979 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %980 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %981 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %982 : Float(17:262656, 255:513, 255:1) = aten::slice(%977, %978, %979, %980, %981), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %983 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %984 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %985 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %986 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %987 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %983, %984, %985, %986), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %988 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %989 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %990 : Float(17:262656, 256:513, 513:1) = aten::select(%987, %988, %989), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %991 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %993 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %994 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %995 : Float(17:262656, 255:513, 513:1) = aten::slice(%990, %991, %992, %993, %994), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %996 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %998 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %999 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %1000 : Float(17:262656, 255:513, 255:1) = aten::slice(%995, %996, %997, %998, %999), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %1001 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %1002 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %1003 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %1004 : int[] = prim::ListConstruct(%1001, %1002, %1003), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1005 : Float(17:262656, 255:513, 255:1) = aten::view(%982, %1004), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %1006 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1007 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1000, %1005, %1006), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
    %1008 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
    %1009 : int[] = prim::ListConstruct(%619, %629, %623, %1008), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1010 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.4, %1009), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
    %1011 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
    %1012 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.5 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1010, %1011, %1012), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
    %1014 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %1015 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %1016 : int[] = prim::ListConstruct(%1014, %1015), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1017 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %1018 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %1019 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %1020 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %1021 : Float(256:257, 257:1) = aten::ones(%1016, %1017, %1018, %1019, %1020), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %1022 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %1023 : Float(256:257, 257:1) = aten::tril(%1021, %1022), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %1024 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %1025 : int[] = prim::ListConstruct(%1024), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %beginning_mask_2d.4 : Float(256:257, 257:1) = aten::flip(%1023, %1025), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
    %1027 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1028 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.4, %1027), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1029 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1030 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1031 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1032 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1033 : Float(1:65792, 256:257, 257:1) = aten::slice(%1028, %1029, %1030, %1031, %1032), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1034 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1035 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1033, %1034), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1036 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1037 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1038 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1039 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.4 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1035, %1036, %1037, %1038, %1039), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
    %1041 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:460:0
    %1042 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:460:0
    %1043 : int[] = prim::ListConstruct(%1041, %1042), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %ending_mask.4 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.4, %1043), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:460:0
    %1045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1046 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1047 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1048 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1049 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.5, %1045, %1046, %1047, %1048), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1050 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1052 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1053 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1054 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1049, %1050, %1051, %1052, %1053), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1055 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1056 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1057 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1058 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1059 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1054, %1055, %1056, %1057, %1058), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1060 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1061 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1062 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.4 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1059, %1060, %1061, %1062, %1063), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
    %1065 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %1066 : int = aten::size(%beginning_input.4, %1065), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %1067 : Long() = prim::NumToTensor(%1066), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1068 : int = aten::Int(%1067), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1069 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %1070 : int = aten::size(%beginning_input.4, %1069), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %1071 : Long() = prim::NumToTensor(%1070), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1072 : int = aten::Int(%1071), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1073 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %1074 : int = aten::size(%beginning_input.4, %1073), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %1075 : Long() = prim::NumToTensor(%1074), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1076 : int = aten::Int(%1075), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1077 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %1078 : int = aten::size(%beginning_input.4, %1077), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %1079 : Long() = prim::NumToTensor(%1078), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1080 : int = aten::Int(%1079), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1081 : int[] = prim::ListConstruct(%1068, %1072, %1076, %1080), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1082 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %1083 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.4, %1081, %1082), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
    %1084 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:22:0
    %1085 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1083, %1084), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:22:0
    %1086 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:463:0
    %1087 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.4, %1085, %1086), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:463:0
    %1088 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1089 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1090 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1091 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1092 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.5, %1088, %1089, %1090, %1091), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1094 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1095 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1096 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1097 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1092, %1093, %1094, %1095, %1096), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1098 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1099 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1100 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1101 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1102 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1097, %1098, %1099, %1100, %1101), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1103 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1104 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1105 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1106 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.4 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1102, %1103, %1104, %1105, %1106), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
    %1108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %1109 : int = aten::size(%ending_input.4, %1108), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %1110 : Long() = prim::NumToTensor(%1109), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1111 : int = aten::Int(%1110), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1112 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %1113 : int = aten::size(%ending_input.4, %1112), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %1114 : Long() = prim::NumToTensor(%1113), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1115 : int = aten::Int(%1114), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1116 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %1117 : int = aten::size(%ending_input.4, %1116), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %1118 : Long() = prim::NumToTensor(%1117), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1119 : int = aten::Int(%1118), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1120 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %1121 : int = aten::size(%ending_input.4, %1120), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %1122 : Long() = prim::NumToTensor(%1121), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1124 : int[] = prim::ListConstruct(%1111, %1115, %1119, %1123), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1125 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %1126 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.4, %1124, %1125), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
    %1127 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:22:0
    %1128 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1126, %1127), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:22:0
    %1129 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:466:0
    %1130 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.4, %1128, %1129), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:466:0
    %1131 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:284:0
    %attn_scores.2 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.4, %input_tensor.5, %1131), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:284:0
    %1151 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:1500:0
    %1152 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:1500:0
    %attn_probs_fp32.2 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.2, %1151, %1152), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:1500:0
    %attn_probs.3 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::type_as(%attn_probs_fp32.2, %attn_scores.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:320:0
    %1155 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1156 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1157 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1158 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1159 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.2, %1155, %1156, %1157, %1158), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1161 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1162 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1163 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1164 : Bool(17:512, 512:1) = aten::slice(%1159, %1160, %1161, %1162, %1163), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1165 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1166 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1164, %1165), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1167 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1168 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1166, %1167), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1169 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %input.22 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs.3, %1168, %1169), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
    %1171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:973:0
    %1172 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:973:0
    %attn_probs.4 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.22, %1171, %1172), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:973:0
    %1174 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:331:0
    %1175 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:331:0
    %1176 : int[] = prim::ListConstruct(%28, %35, %1174, %1175), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1177 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1389, %1176), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:331:0
    %1178 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:331:0
    %1179 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:331:0
    %value.2 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1177, %1178, %1179), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:331:0
    %1181 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
    %1182 : int = aten::size(%value.2, %1181), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
    %batch_size.8 : Long() = prim::NumToTensor(%1182), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1184 : int = aten::Int(%batch_size.8), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1185 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
    %1186 : int = aten::size(%value.2, %1185), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
    %seq_len.9 : Long() = prim::NumToTensor(%1186), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1188 : int = aten::Int(%seq_len.9), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1189 : int = aten::Int(%seq_len.9), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1190 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
    %1191 : int = aten::size(%value.2, %1190), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
    %num_heads.6 : Long() = prim::NumToTensor(%1191), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1193 : int = aten::Int(%num_heads.6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1194 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
    %1195 : int = aten::size(%value.2, %1194), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
    %head_dim.6 : Long() = prim::NumToTensor(%1195), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1197 : int = aten::Int(%head_dim.6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1198 : int = aten::Int(%head_dim.6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1199 : int = aten::Int(%head_dim.6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1236 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %1237 : Long() = aten::floor_divide(%seq_len.9, %1236), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %1238 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:542:0
    %1239 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:542:0
    %chunks_count.6 : Long() = aten::sub(%1237, %1238, %1239), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:542:0
    %1241 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:545:0
    %1242 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:545:0
    %1243 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.4, %1241, %1242), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:545:0
    %1244 : Long() = aten::mul(%batch_size.8, %num_heads.6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:546:0
    %1245 : int = aten::Int(%1244), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1246 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %1247 : Long() = aten::floor_divide(%seq_len.9, %1246), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/tensor.py:424:0
    %1248 : int = aten::Int(%1247), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1249 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:545:0
    %1250 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:545:0
    %1251 : int[] = prim::ListConstruct(%1245, %1248, %1249, %1250), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %chunked_hidden_states.6 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1243, %1251), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:545:0
    %1253 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
    %1254 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
    %1255 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.2, %1253, %1254), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
    %1256 : Long() = aten::mul(%batch_size.8, %num_heads.6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
    %1257 : int = aten::Int(%1256), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1258 : int[] = prim::ListConstruct(%1257, %1189, %1199), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %input.23 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1255, %1258), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
    %1260 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %1261 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %1262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %1263 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %1264 : int[] = prim::ListConstruct(%1260, %1261, %1262, %1263), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1265 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %padded_value.2 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.23, %1264, %1265), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %1267 : Long() = aten::mul(%batch_size.8, %num_heads.6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:556:0
    %1268 : int = aten::Int(%1267), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1269 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:556:0
    %1270 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:556:0
    %1271 : Long() = aten::add(%chunks_count.6, %1269, %1270), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:556:0
    %1272 : int = aten::Int(%1271), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:564:0
    %1274 : int[] = prim::ListConstruct(%1268, %1272, %1273, %1198), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1275 : int = prim::Constant[value=65536](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:564:0
    %1276 : int = prim::Constant[value=16384](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:564:0
    %1277 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:564:0
    %1278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:564:0
    %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1280 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1281 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.2, %1274, %1279, %1280), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:564:0
    %1282 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
    %1283 : int = aten::size(%chunked_hidden_states.6, %1282), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
    %total_num_heads.2 : Long() = prim::NumToTensor(%1283), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1285 : int = aten::Int(%total_num_heads.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1286 : int = aten::Int(%total_num_heads.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1287 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
    %1288 : int = aten::size(%chunked_hidden_states.6, %1287), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
    %num_chunks.2 : Long() = prim::NumToTensor(%1288), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1290 : int = aten::Int(%num_chunks.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1291 : int = aten::Int(%num_chunks.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1292 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
    %1293 : int = aten::size(%chunked_hidden_states.6, %1292), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
    %window_overlap.2 : Long() = prim::NumToTensor(%1293), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1295 : int = aten::Int(%window_overlap.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1296 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
    %1297 : int = aten::size(%chunked_hidden_states.6, %1296), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
    %hidden_dim.2 : Long() = prim::NumToTensor(%1297), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1299 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:422:0
    %1300 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:422:0
    %1301 : Long() = aten::add(%window_overlap.2, %1299, %1300), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:422:0
    %1302 : int = aten::Int(%1301), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1303 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %1304 : int[] = prim::ListConstruct(%1303, %1302), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1305 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %chunked_hidden_states.7 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.6, %1304, %1305), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
    %1307 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:424:0
    %1308 : int[] = prim::ListConstruct(%1286, %1291, %1307), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %chunked_hidden_states.8 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.7, %1308), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:424:0
    %1310 : Long() = aten::neg(%window_overlap.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:428:0
    %1311 : int = aten::Int(%1310), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1312 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1313 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1314 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1315 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1316 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.8, %1312, %1313, %1314, %1315), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1319 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1320 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1316, %1317, %1318, %1319, %1320), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1322 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1323 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1324 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %chunked_hidden_states.9 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1321, %1322, %1323, %1311, %1324), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
    %1326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:431:0
    %1327 : Long() = aten::add(%window_overlap.2, %hidden_dim.2, %1326), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:431:0
    %1328 : int = aten::Int(%1327), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1329 : int[] = prim::ListConstruct(%1285, %1290, %1295, %1328), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %chunked_hidden_states.10 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.9, %1329), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:430:0
    %1331 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1333 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1335 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.10, %1331, %1332, %1333, %1334), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1336 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1340 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1335, %1336, %1337, %1338, %1339), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1341 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1342 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1343 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1345 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1340, %1341, %1342, %1343, %1344), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1346 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1347 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1348 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1350 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1345, %1346, %1347, %1348, %1349), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
    %1351 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/functional.py:327:0
    %1352 : Tensor[] = prim::ListConstruct(%1350, %1281), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %context.2 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%1351, %1352), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/functional.py:327:0
    %1354 : int[] = prim::ListConstruct(%1184, %1193, %1188, %1197), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1355 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.2, %1354), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:569:0
    %1356 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:569:0
    %1357 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:569:0
    %attn_output.3 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1355, %1356, %1357), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:569:0
    %1377 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
    %1378 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
    %1379 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.3, %1377, %1378), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
    %1380 : int[] = prim::ListConstruct(%27, %34, %41), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
    %1381 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1379, %1380), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
    %1382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
    %attn_output.4 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1381, %1382), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
    %1384 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:374:0
    %1385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:374:0
    %input.24 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.4, %1384, %1385), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_longformer.py:374:0
    return (%input.24)

LongformerSelfAttention.key
Linear._actual_script_module
  graph(%self.29 : __torch__.torch.nn.modules.linear.Linear,
        %input.19 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.29)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.29)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
    %output.8 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.19, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
    %key_vectors.2 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.8, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
    return (%key_vectors.2)

LongformerSelfAttention.query
Linear._actual_script_module
  graph(%self.28 : __torch__.torch.nn.modules.linear.Linear,
        %input.19 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.28)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.28)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
    %output.7 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.19, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
    %query_vectors.3 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.7, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
    return (%query_vectors.3)

LongformerSelfAttention.value
Linear._actual_script_module
  graph(%self.30 : __torch__.torch.nn.modules.linear.Linear,
        %input.19 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.30)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.30)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
    %output.9 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.19, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
    %value_vectors.2 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.9, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
    return (%value_vectors.2)

LongformerSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.34 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.26 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.34)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.34)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.6 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.26, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.6)

LongformerSelfOutput.dense
Linear._actual_script_module
  graph(%self.32 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:768, 512:13056, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.32)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.32)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
    %output.10 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
    %input.25 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.10, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.25)

LongformerSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.33 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.21 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.21)

LongformerIntermediate.dense
Linear._actual_script_module
  graph(%self.36 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.36)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.36)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.11 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.27 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.11, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.27)

LongformerOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.40 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.30 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.40)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.40)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
    %hidden_states.23 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.30, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%hidden_states.23)

LongformerOutput.dense
Linear._actual_script_module
  graph(%self.38 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1572864, 512:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.38)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.38)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
    %output.12 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
    %input.29 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.12, %2, %6), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
    return (%input.29)

LongformerOutput.dropout
Dropout._actual_script_module
  graph(%self.39 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.22 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.22)

LongformerLayer._actual_script_module
  graph(%self.41 : __torch__.transformers.modeling_longformer.LongformerLayer,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%self.41)
    %4 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%self.41)
    %5 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%self.41)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %attention_mask, %2)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

LongformerLayer.attention
LongformerAttention._actual_script_module
  graph(%self.42 : __torch__.transformers.modeling_longformer.LongformerAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%self.42)
    %4 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%self.42)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %2)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %2)
    return (%8)

LongformerLayer.intermediate
LongformerIntermediate._actual_script_module
  graph(%self.51 : __torch__.transformers.modeling_longformer.LongformerIntermediate,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.51)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.40 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
    return (%input.40)

LongformerLayer.output
LongformerOutput._actual_script_module
  graph(%self.53 : __torch__.transformers.modeling_longformer.LongformerOutput,
        %1 : Float(17:1572864, 512:3072, 3072:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.53)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.53)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.53)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output # transformers/modeling_longformer.py:830:0
    %input.42 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output # transformers/modeling_longformer.py:830:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.42)
    return (%13)

LongformerAttention.output
LongformerSelfOutput._actual_script_module
  graph(%self.47 : __torch__.transformers.modeling_longformer.LongformerSelfOutput,
        %1 : Float(17:768, 512:13056, 768:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.47)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.47)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.47)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output # transformers/modeling_longformer.py:758:0
    %input.38 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output # transformers/modeling_longformer.py:758:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.38)
    return (%13)

LongformerAttention.self
LongformerSelfAttention._actual_script_module
  graph(%self.43 : __torch__.transformers.modeling_longformer.LongformerSelfAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.43)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.43)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.43)
    %6 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:241:0
    %7 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:241:0
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:241:0
    %9 : Float(17:512, 512:1) = aten::squeeze(%7, %8), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:241:0
    %10 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:22:0
    %is_index_masked.3 : Bool(17:512, 512:1) = aten::lt(%9, %10), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:22:0
    %18 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:248:0
    %19 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:248:0
    %input.31 : Float(512:768, 17:393216, 768:1) = aten::transpose(%2, %18, %19), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:248:0
    %1387 : Tensor = prim::CallMethod[name="forward"](%5, %input.31)
    %1388 : Tensor = prim::CallMethod[name="forward"](%4, %input.31)
    %1389 : Tensor = prim::CallMethod[name="forward"](%3, %input.31)
    %24 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
    %25 : int = aten::size(%input.31, %24), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
    %seq_len.10 : Long() = prim::NumToTensor(%25), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %27 : int = aten::Int(%seq_len.10), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %28 : int = aten::Int(%seq_len.10), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %29 : int = aten::Int(%seq_len.10), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %30 : int = aten::Int(%seq_len.10), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %31 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
    %32 : int = aten::size(%input.31, %31), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
    %batch_size.9 : Long() = prim::NumToTensor(%32), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %34 : int = aten::Int(%batch_size.9), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %35 : int = aten::Int(%batch_size.9), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %36 : int = aten::Int(%batch_size.9), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %37 : int = aten::Int(%batch_size.9), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %38 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
    %39 : int = aten::size(%input.31, %38), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
    %embed_dim.3 : Long() = prim::NumToTensor(%39), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %41 : int = aten::Int(%embed_dim.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %44 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:261:0
    %query_vectors.6 : Float(512:13056, 17:768, 768:1) = aten::div_(%1387, %44), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:261:0
    %46 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:263:0
    %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:263:0
    %48 : int[] = prim::ListConstruct(%30, %37, %46, %47), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %49 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.6, %48), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:263:0
    %50 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:263:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:263:0
    %query.5 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%49, %50, %51), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:263:0
    %53 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:264:0
    %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:264:0
    %55 : int[] = prim::ListConstruct(%29, %36, %53, %54), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %56 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1388, %55), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:264:0
    %57 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:264:0
    %58 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:264:0
    %key.3 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%56, %57, %58), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:264:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %61 : int = aten::size(%query.5, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.10 : Long() = prim::NumToTensor(%61), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %63 : int = aten::Int(%batch_size.10), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %65 : int = aten::size(%query.5, %64), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.11 : Long() = prim::NumToTensor(%65), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %67 : int = aten::Int(%seq_len.11), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %68 : int = aten::Int(%seq_len.11), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %69 : int = aten::Int(%seq_len.11), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %70 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %71 : int = aten::size(%query.5, %70), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.7 : Long() = prim::NumToTensor(%71), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %73 : int = aten::Int(%num_heads.7), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %74 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %75 : int = aten::size(%query.5, %74), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.7 : Long() = prim::NumToTensor(%75), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %77 : int = aten::Int(%head_dim.7), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %78 : int = aten::Int(%head_dim.7), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %111 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %112 : Long() = aten::floor_divide(%seq_len.11, %111), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %113 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:478:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.7 : Long() = aten::sub(%112, %113, %114), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:478:0
    %116 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
    %117 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
    %118 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.5, %116, %117), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
    %119 : Long() = aten::mul(%batch_size.10, %num_heads.7), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
    %120 : int = aten::Int(%119), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %121 : int[] = prim::ListConstruct(%120, %69, %78), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %hidden_states.24 : Float(204:64, 512:13056, 64:1) = aten::reshape(%118, %121), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
    %123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
    %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
    %125 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.3, %123, %124), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
    %126 : Long() = aten::mul(%batch_size.10, %num_heads.7), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
    %127 : int = aten::Int(%126), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %128 : int[] = prim::ListConstruct(%127, %68, %77), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %hidden_states.26 : Float(204:64, 512:13056, 64:1) = aten::reshape(%125, %128), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
    %130 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
    %131 : int = aten::size(%hidden_states.24, %130), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
    %132 : Long() = prim::NumToTensor(%131), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %133 : int = aten::Int(%132), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %134 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
    %135 : int = aten::size(%hidden_states.24, %134), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
    %136 : Long() = prim::NumToTensor(%135), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %137 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %138 : Long() = aten::floor_divide(%136, %137), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %139 : int = aten::Int(%138), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %140 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
    %141 : int = aten::size(%hidden_states.24, %140), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
    %142 : Long() = prim::NumToTensor(%141), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %143 : int = aten::Int(%142), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %144 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
    %145 : int[] = prim::ListConstruct(%133, %139, %144, %143), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %hidden_states.25 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.24, %145), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
    %147 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %148 : int = aten::size(%hidden_states.25, %147), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %149 : Long() = prim::NumToTensor(%148), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %150 : int = aten::Int(%149), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %151 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %152 : int = aten::size(%hidden_states.25, %151), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %153 : Long() = prim::NumToTensor(%152), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %154 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %155 : int = aten::size(%hidden_states.25, %154), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %156 : Long() = prim::NumToTensor(%155), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %157 : int = aten::Int(%156), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %158 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %159 : int = aten::size(%hidden_states.25, %158), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %160 : Long() = prim::NumToTensor(%159), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %161 : int = aten::Int(%160), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %162 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %163 : Long() = aten::mul(%153, %162), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %164 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %165 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %166 : Long() = aten::sub(%163, %164, %165), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %167 : int = aten::Int(%166), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %168 : int[] = prim::ListConstruct(%150, %167, %157, %161), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %169 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %170 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %171 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %172 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %173 : int[] = prim::ListConstruct(%169, %170, %171, %172), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %174 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %175 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.25, %168, %173, %174), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %176 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
    %177 : int = aten::size(%hidden_states.26, %176), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %179 : int = aten::Int(%178), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
    %181 : int = aten::size(%hidden_states.26, %180), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
    %182 : Long() = prim::NumToTensor(%181), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %183 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %184 : Long() = aten::floor_divide(%182, %183), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %185 : int = aten::Int(%184), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %186 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
    %187 : int = aten::size(%hidden_states.26, %186), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
    %188 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %189 : int = aten::Int(%188), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %190 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
    %191 : int[] = prim::ListConstruct(%179, %185, %190, %189), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %hidden_states.27 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.26, %191), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
    %193 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %194 : int = aten::size(%hidden_states.27, %193), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %195 : Long() = prim::NumToTensor(%194), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %196 : int = aten::Int(%195), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %197 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %198 : int = aten::size(%hidden_states.27, %197), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %199 : Long() = prim::NumToTensor(%198), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %200 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %201 : int = aten::size(%hidden_states.27, %200), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %203 : int = aten::Int(%202), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %204 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %205 : int = aten::size(%hidden_states.27, %204), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %207 : int = aten::Int(%206), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %208 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %209 : Long() = aten::mul(%199, %208), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %210 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %211 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %212 : Long() = aten::sub(%209, %210, %211), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %213 : int = aten::Int(%212), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %214 : int[] = prim::ListConstruct(%196, %213, %203, %207), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %216 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %217 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %218 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %219 : int[] = prim::ListConstruct(%215, %216, %217, %218), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %220 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.27, %214, %219, %220), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %222 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/functional.py:327:0
    %223 : Tensor[] = prim::ListConstruct(%175, %221), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %input.32 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%222, %223), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/functional.py:327:0
    %225 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %226 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %227 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %229 : int[] = prim::ListConstruct(%225, %226, %227, %228), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %230 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.5 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.32, %229, %230), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %232 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %233 : int = aten::size(%hidden_states_padded.5, %232), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %234 : Long() = prim::NumToTensor(%233), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %235 : int = aten::Int(%234), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %236 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %237 : int = aten::size(%hidden_states_padded.5, %236), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %238 : Long() = prim::NumToTensor(%237), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %239 : int = aten::Int(%238), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %246 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %247 : int = aten::size(%hidden_states_padded.5, %246), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %248 : Long() = prim::NumToTensor(%247), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %249 : int = aten::Int(%248), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %250 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %251 : int = aten::size(%hidden_states_padded.5, %250), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %252 : Long() = prim::NumToTensor(%251), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %253 : int = aten::Int(%252), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %254 : int[] = prim::ListConstruct(%235, %239, %249, %253), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %diagonal_chunked_attention_scores.5 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.5, %254), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:400:0
    %256 : Long() = aten::mul(%batch_size.10, %num_heads.7), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
    %257 : int = aten::Int(%256), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %258 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
    %259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
    %260 : Long() = aten::add(%chunks_count.7, %258, %259), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
    %261 : int = aten::Int(%260), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %263 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %264 : int[] = prim::ListConstruct(%257, %261, %262, %263), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %265 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %266 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %268 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.5 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.5, %264, %265, %266, %267, %268), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %270 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %271 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %273 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %270, %271, %272, %273), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %275 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %276 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %277 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %279 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%274, %275, %276, %277, %278), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %280 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %281 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %282 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %284 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%279, %280, %281, %282, %283), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %285 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %286 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %287 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %289 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%284, %285, %286, %287, %288), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %290 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %291 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %292 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %294 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %290, %291, %292, %293), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %295 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %296 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %297 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %299 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%294, %295, %296, %297, %298), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %300 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %301 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %302 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %303 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %304 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%299, %300, %301, %302, %303), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %305 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %306 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %307 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %308 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %309 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%304, %305, %306, %307, %308), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %310 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %311 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %312 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %313 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %314 : int[] = prim::ListConstruct(%310, %311, %312, %313), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %315 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%289, %314), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %316 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %317 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%309, %315, %316), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %319 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %320 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %321 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %322 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %318, %319, %320, %321), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %323 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %324 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %325 : Float(204:262656, 512:513, 513:1) = aten::select(%322, %323, %324), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %327 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %328 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %329 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %330 : Float(204:262656, 256:513, 513:1) = aten::slice(%325, %326, %327, %328, %329), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %331 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %333 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %335 : Float(204:262656, 256:513, 257:1) = aten::slice(%330, %331, %332, %333, %334), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %336 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %340 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %336, %337, %338, %339), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %341 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %342 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %343 : Float(204:262656, 256:513, 513:1) = aten::select(%340, %341, %342), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %345 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %347 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %348 : Float(204:262656, 256:513, 513:1) = aten::slice(%343, %344, %345, %346, %347), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %349 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %350 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %352 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %353 : Float(204:262656, 256:513, 257:1) = aten::slice(%348, %349, %350, %351, %352), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %354 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %355 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %356 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %357 : int[] = prim::ListConstruct(%354, %355, %356), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %358 : Float(204:262656, 256:513, 257:1) = aten::view(%335, %357), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %359 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %360 : Float(204:262656, 256:513, 257:1) = aten::copy_(%353, %358, %359), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %361 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %362 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %363 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %364 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %365 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %361, %362, %363, %364), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %366 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %367 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %368 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %369 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %370 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%365, %366, %367, %368, %369), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %371 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %372 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %373 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %374 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %375 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%370, %371, %372, %373, %374), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %376 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %377 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %379 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %380 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%375, %376, %377, %378, %379), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %383 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %384 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %385 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %381, %382, %383, %384), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %386 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %387 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %388 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %389 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %390 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%385, %386, %387, %388, %389), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %391 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %392 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %393 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %394 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %395 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%390, %391, %392, %393, %394), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %397 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %398 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %399 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %400 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%395, %396, %397, %398, %399), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %401 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %402 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %403 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %404 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %405 : int[] = prim::ListConstruct(%401, %402, %403, %404), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %406 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%380, %405), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %407 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %408 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%400, %406, %407), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %409 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %410 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %411 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %412 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %413 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %409, %410, %411, %412), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %414 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %415 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %416 : Float(204:262656, 512:513, 513:1) = aten::select(%413, %414, %415), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %417 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %418 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %419 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %420 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %421 : Float(204:262656, 255:513, 513:1) = aten::slice(%416, %417, %418, %419, %420), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %422 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %423 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %425 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %426 : Float(204:262656, 255:513, 255:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %427 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %429 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %430 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %431 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %427, %428, %429, %430), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %432 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %433 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %434 : Float(204:262656, 256:513, 513:1) = aten::select(%431, %432, %433), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %435 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %436 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %437 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %438 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %439 : Float(204:262656, 255:513, 513:1) = aten::slice(%434, %435, %436, %437, %438), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %440 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %441 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %442 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %443 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %444 : Float(204:262656, 255:513, 255:1) = aten::slice(%439, %440, %441, %442, %443), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %445 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %446 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %447 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %448 : int[] = prim::ListConstruct(%445, %446, %447), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %449 : Float(204:262656, 255:513, 255:1) = aten::view(%426, %448), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %450 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %451 : Float(204:262656, 255:513, 255:1) = aten::copy_(%444, %449, %450), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %452 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
    %453 : int[] = prim::ListConstruct(%63, %73, %67, %452), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %454 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.5, %453), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
    %455 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
    %456 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.7 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%454, %455, %456), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
    %458 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %459 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %460 : int[] = prim::ListConstruct(%458, %459), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %461 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %462 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %463 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %464 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %465 : Float(256:257, 257:1) = aten::ones(%460, %461, %462, %463, %464), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %466 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %467 : Float(256:257, 257:1) = aten::tril(%465, %466), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %468 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %469 : int[] = prim::ListConstruct(%468), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %beginning_mask_2d.5 : Float(256:257, 257:1) = aten::flip(%467, %469), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %471 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %472 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.5, %471), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %473 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %474 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %475 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %476 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %477 : Float(1:65792, 256:257, 257:1) = aten::slice(%472, %473, %474, %475, %476), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %478 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %479 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%477, %478), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %480 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %481 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %482 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %483 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.5 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%479, %480, %481, %482, %483), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %485 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:460:0
    %486 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:460:0
    %487 : int[] = prim::ListConstruct(%485, %486), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %ending_mask.5 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.5, %487), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:460:0
    %489 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %490 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %491 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %493 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.7, %489, %490, %491, %492), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %494 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %495 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %496 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %497 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %498 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%493, %494, %495, %496, %497), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %499 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %500 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %501 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %502 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %503 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%498, %499, %500, %501, %502), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %504 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %505 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %506 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %507 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.5 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%503, %504, %505, %506, %507), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %509 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %510 : int = aten::size(%beginning_input.5, %509), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %511 : Long() = prim::NumToTensor(%510), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %512 : int = aten::Int(%511), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %513 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %514 : int = aten::size(%beginning_input.5, %513), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %515 : Long() = prim::NumToTensor(%514), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %516 : int = aten::Int(%515), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %517 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %518 : int = aten::size(%beginning_input.5, %517), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %519 : Long() = prim::NumToTensor(%518), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %520 : int = aten::Int(%519), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %521 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %522 : int = aten::size(%beginning_input.5, %521), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %523 : Long() = prim::NumToTensor(%522), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %524 : int = aten::Int(%523), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %525 : int[] = prim::ListConstruct(%512, %516, %520, %524), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %526 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %527 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.5, %525, %526), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %528 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:22:0
    %529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%527, %528), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:22:0
    %530 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:463:0
    %531 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.5, %529, %530), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:463:0
    %532 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %533 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %534 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %535 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %536 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.7, %532, %533, %534, %535), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %537 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %538 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %539 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %541 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%536, %537, %538, %539, %540), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %542 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %543 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %544 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %545 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %546 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%541, %542, %543, %544, %545), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %547 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %548 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %549 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.5 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%546, %547, %548, %549, %550), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %552 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %553 : int = aten::size(%ending_input.5, %552), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %554 : Long() = prim::NumToTensor(%553), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %555 : int = aten::Int(%554), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %556 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %557 : int = aten::size(%ending_input.5, %556), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %558 : Long() = prim::NumToTensor(%557), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %559 : int = aten::Int(%558), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %560 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %561 : int = aten::size(%ending_input.5, %560), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %562 : Long() = prim::NumToTensor(%561), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %563 : int = aten::Int(%562), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %564 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %565 : int = aten::size(%ending_input.5, %564), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %566 : Long() = prim::NumToTensor(%565), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %567 : int = aten::Int(%566), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %568 : int[] = prim::ListConstruct(%555, %559, %563, %567), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %569 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %570 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.5, %568, %569), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %571 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:22:0
    %572 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%570, %571), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:22:0
    %573 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:466:0
    %574 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.5, %572, %573), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:466:0
    %575 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:22:0
    %576 : Bool(17:512, 512:1) = aten::ne(%9, %575), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:22:0
    %577 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %578 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %579 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %580 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %581 : Bool(17:512, 512:1) = aten::slice(%576, %577, %578, %579, %580), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %582 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %583 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %584 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %585 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %586 : Bool(17:512, 512:1) = aten::slice(%581, %582, %583, %584, %585), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %587 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %588 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%586, %587), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %589 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %remove_from_windowed_attention_mask.3 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%588, %589), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
    %591 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.3, %query.5), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:275:0
    %592 : float = prim::Constant[value=-10000.](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:275:0
    %float_mask.3 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%591, %remove_from_windowed_attention_mask.3, %592), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:275:0
    %594 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %595 : int = aten::size(%float_mask.3, %594), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %596 : Long() = prim::NumToTensor(%595), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %597 : int = aten::Int(%596), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %598 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %599 : int = aten::size(%float_mask.3, %598), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %600 : Long() = prim::NumToTensor(%599), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %601 : int = aten::Int(%600), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %602 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %603 : int = aten::size(%float_mask.3, %602), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %604 : Long() = prim::NumToTensor(%603), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %605 : int = aten::Int(%604), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %606 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %607 : int = aten::size(%float_mask.3, %606), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %608 : Long() = prim::NumToTensor(%607), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %609 : int = aten::Int(%608), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %610 : int[] = prim::ListConstruct(%597, %601, %605, %609), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %611 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %612 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %613 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %614 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %query.6 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%610, %611, %612, %613, %614), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
    %616 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %617 : int = aten::size(%query.6, %616), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.11 : Long() = prim::NumToTensor(%617), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %619 : int = aten::Int(%batch_size.11), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %620 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %621 : int = aten::size(%query.6, %620), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.12 : Long() = prim::NumToTensor(%621), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %623 : int = aten::Int(%seq_len.12), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %624 : int = aten::Int(%seq_len.12), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %625 : int = aten::Int(%seq_len.12), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %626 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %627 : int = aten::size(%query.6, %626), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.8 : Long() = prim::NumToTensor(%627), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %629 : int = aten::Int(%num_heads.8), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %630 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %631 : int = aten::size(%query.6, %630), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.8 : Long() = prim::NumToTensor(%631), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %633 : int = aten::Int(%head_dim.8), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %634 : int = aten::Int(%head_dim.8), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %667 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %668 : Long() = aten::floor_divide(%seq_len.12, %667), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %669 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:478:0
    %670 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.8 : Long() = aten::sub(%668, %669, %670), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:478:0
    %672 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
    %673 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
    %674 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.6, %672, %673), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
    %675 : Long() = aten::mul(%batch_size.11, %num_heads.8), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
    %676 : int = aten::Int(%675), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %677 : int[] = prim::ListConstruct(%676, %625, %634), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %hidden_states.28 : Float(17:512, 512:1, 1:1) = aten::reshape(%674, %677), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
    %679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
    %680 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
    %681 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.3, %679, %680), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
    %682 : Long() = aten::mul(%batch_size.11, %num_heads.8), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
    %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %684 : int[] = prim::ListConstruct(%683, %624, %633), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %hidden_states.30 : Float(17:512, 512:1, 1:1) = aten::reshape(%681, %684), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
    %686 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
    %687 : int = aten::size(%hidden_states.28, %686), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
    %688 : Long() = prim::NumToTensor(%687), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %690 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
    %691 : int = aten::size(%hidden_states.28, %690), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
    %692 : Long() = prim::NumToTensor(%691), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %693 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %694 : Long() = aten::floor_divide(%692, %693), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %696 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
    %697 : int = aten::size(%hidden_states.28, %696), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
    %698 : Long() = prim::NumToTensor(%697), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %699 : int = aten::Int(%698), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %700 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
    %701 : int[] = prim::ListConstruct(%689, %695, %700, %699), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %hidden_states.29 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.28, %701), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
    %703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %704 : int = aten::size(%hidden_states.29, %703), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %705 : Long() = prim::NumToTensor(%704), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %706 : int = aten::Int(%705), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %708 : int = aten::size(%hidden_states.29, %707), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %709 : Long() = prim::NumToTensor(%708), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %710 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %711 : int = aten::size(%hidden_states.29, %710), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %712 : Long() = prim::NumToTensor(%711), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %713 : int = aten::Int(%712), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %714 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %715 : int = aten::size(%hidden_states.29, %714), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %716 : Long() = prim::NumToTensor(%715), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %717 : int = aten::Int(%716), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %718 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %719 : Long() = aten::mul(%709, %718), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %720 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %721 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %722 : Long() = aten::sub(%719, %720, %721), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %723 : int = aten::Int(%722), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %724 : int[] = prim::ListConstruct(%706, %723, %713, %717), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %725 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %726 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %727 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %728 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %729 : int[] = prim::ListConstruct(%725, %726, %727, %728), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %730 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %731 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.29, %724, %729, %730), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %732 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
    %733 : int = aten::size(%hidden_states.30, %732), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
    %734 : Long() = prim::NumToTensor(%733), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %735 : int = aten::Int(%734), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %736 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
    %737 : int = aten::size(%hidden_states.30, %736), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
    %738 : Long() = prim::NumToTensor(%737), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %739 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %740 : Long() = aten::floor_divide(%738, %739), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %741 : int = aten::Int(%740), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
    %743 : int = aten::size(%hidden_states.30, %742), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
    %744 : Long() = prim::NumToTensor(%743), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %745 : int = aten::Int(%744), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %746 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
    %747 : int[] = prim::ListConstruct(%735, %741, %746, %745), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %hidden_states.31 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.30, %747), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
    %749 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %750 : int = aten::size(%hidden_states.31, %749), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %751 : Long() = prim::NumToTensor(%750), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %752 : int = aten::Int(%751), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %754 : int = aten::size(%hidden_states.31, %753), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %755 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %756 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %757 : int = aten::size(%hidden_states.31, %756), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %758 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %759 : int = aten::Int(%758), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %760 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %761 : int = aten::size(%hidden_states.31, %760), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
    %762 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %764 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %765 : Long() = aten::mul(%755, %764), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %766 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %767 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %768 : Long() = aten::sub(%765, %766, %767), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
    %769 : int = aten::Int(%768), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %770 : int[] = prim::ListConstruct(%752, %769, %759, %763), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %771 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %772 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %773 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %776 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.31, %770, %775, %776), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
    %778 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/functional.py:327:0
    %779 : Tensor[] = prim::ListConstruct(%731, %777), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %input.33 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%778, %779), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/functional.py:327:0
    %781 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %782 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %783 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %786 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.6 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.33, %785, %786), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %788 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %789 : int = aten::size(%hidden_states_padded.6, %788), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %790 : Long() = prim::NumToTensor(%789), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %791 : int = aten::Int(%790), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %792 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %793 : int = aten::size(%hidden_states_padded.6, %792), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %794 : Long() = prim::NumToTensor(%793), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %795 : int = aten::Int(%794), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %802 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %803 : int = aten::size(%hidden_states_padded.6, %802), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %804 : Long() = prim::NumToTensor(%803), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %805 : int = aten::Int(%804), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %806 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %807 : int = aten::size(%hidden_states_padded.6, %806), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
    %808 : Long() = prim::NumToTensor(%807), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %809 : int = aten::Int(%808), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %810 : int[] = prim::ListConstruct(%791, %795, %805, %809), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %diagonal_chunked_attention_scores.6 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.6, %810), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:400:0
    %812 : Long() = aten::mul(%batch_size.11, %num_heads.8), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
    %813 : int = aten::Int(%812), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %814 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
    %815 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
    %816 : Long() = aten::add(%chunks_count.8, %814, %815), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
    %817 : int = aten::Int(%816), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %818 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %819 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %820 : int[] = prim::ListConstruct(%813, %817, %818, %819), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %821 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %822 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %823 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %824 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.6 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.6, %820, %821, %822, %823, %824), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
    %826 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %827 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %828 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %829 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %830 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %826, %827, %828, %829), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %831 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %832 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %833 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %834 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%830, %831, %832, %833, %834), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %836 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %837 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %838 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %839 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%835, %836, %837, %838, %839), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %841 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %842 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %843 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %845 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%840, %841, %842, %843, %844), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %846 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %847 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %848 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %849 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %846, %847, %848, %849), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %851 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %852 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %853 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %855 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%850, %851, %852, %853, %854), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %856 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %857 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %858 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %859 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %860 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%855, %856, %857, %858, %859), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %861 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %862 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %863 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %864 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %865 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%860, %861, %862, %863, %864), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %866 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %867 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %868 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %869 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %870 : int[] = prim::ListConstruct(%866, %867, %868, %869), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %871 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%845, %870), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %872 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %873 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%865, %871, %872), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
    %874 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %875 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %876 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %877 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %874, %875, %876, %877), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %879 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %880 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %881 : Float(17:262656, 512:513, 513:1) = aten::select(%878, %879, %880), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %883 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %884 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %885 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %886 : Float(17:262656, 256:513, 513:1) = aten::slice(%881, %882, %883, %884, %885), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %887 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %889 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%886, %887, %888, %889, %890), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %892 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %893 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %894 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %895 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %896 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %892, %893, %894, %895), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %897 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %898 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %899 : Float(17:262656, 256:513, 513:1) = aten::select(%896, %897, %898), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %901 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %902 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %903 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %904 : Float(17:262656, 256:513, 513:1) = aten::slice(%899, %900, %901, %902, %903), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %905 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %906 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %907 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %909 : Float(17:262656, 256:513, 257:1) = aten::slice(%904, %905, %906, %907, %908), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %910 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %911 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %912 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %913 : int[] = prim::ListConstruct(%910, %911, %912), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %914 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %913), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %915 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %916 : Float(17:262656, 256:513, 257:1) = aten::copy_(%909, %914, %915), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
    %917 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %918 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %919 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %920 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %921 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %917, %918, %919, %920), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %922 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %923 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %924 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %925 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %926 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%921, %922, %923, %924, %925), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %927 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %928 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %929 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %930 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %931 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%926, %927, %928, %929, %930), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %932 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %933 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %934 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %935 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %936 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%931, %932, %933, %934, %935), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %938 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %939 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %940 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %941 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %937, %938, %939, %940), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %942 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %943 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %944 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %945 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %946 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%941, %942, %943, %944, %945), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %947 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %948 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %949 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %950 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %951 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%946, %947, %948, %949, %950), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %952 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %953 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %954 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %955 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %956 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%951, %952, %953, %954, %955), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %957 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %959 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %960 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %962 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%936, %961), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %963 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %964 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%956, %962, %963), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
    %965 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %966 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %967 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %969 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %965, %966, %967, %968), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %970 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %971 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %972 : Float(17:262656, 512:513, 513:1) = aten::select(%969, %970, %971), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %973 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %974 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %975 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %976 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %977 : Float(17:262656, 255:513, 513:1) = aten::slice(%972, %973, %974, %975, %976), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %978 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %979 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %980 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %981 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %982 : Float(17:262656, 255:513, 255:1) = aten::slice(%977, %978, %979, %980, %981), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %983 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %984 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %985 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %986 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %987 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %983, %984, %985, %986), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %988 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %989 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %990 : Float(17:262656, 256:513, 513:1) = aten::select(%987, %988, %989), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %991 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %993 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %994 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %995 : Float(17:262656, 255:513, 513:1) = aten::slice(%990, %991, %992, %993, %994), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %996 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %998 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %999 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %1000 : Float(17:262656, 255:513, 255:1) = aten::slice(%995, %996, %997, %998, %999), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %1001 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %1002 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %1003 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %1004 : int[] = prim::ListConstruct(%1001, %1002, %1003), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1005 : Float(17:262656, 255:513, 255:1) = aten::view(%982, %1004), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %1006 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1007 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1000, %1005, %1006), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
    %1008 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
    %1009 : int[] = prim::ListConstruct(%619, %629, %623, %1008), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1010 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.6, %1009), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
    %1011 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
    %1012 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.8 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1010, %1011, %1012), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
    %1014 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %1015 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %1016 : int[] = prim::ListConstruct(%1014, %1015), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1017 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %1018 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %1019 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %1020 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %1021 : Float(256:257, 257:1) = aten::ones(%1016, %1017, %1018, %1019, %1020), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %1022 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %1023 : Float(256:257, 257:1) = aten::tril(%1021, %1022), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %1024 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %1025 : int[] = prim::ListConstruct(%1024), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %beginning_mask_2d.6 : Float(256:257, 257:1) = aten::flip(%1023, %1025), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
    %1027 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1028 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.6, %1027), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1029 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1030 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1031 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1032 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1033 : Float(1:65792, 256:257, 257:1) = aten::slice(%1028, %1029, %1030, %1031, %1032), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1034 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1035 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1033, %1034), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1036 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1037 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1038 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1039 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.6 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1035, %1036, %1037, %1038, %1039), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
    %1041 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:460:0
    %1042 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:460:0
    %1043 : int[] = prim::ListConstruct(%1041, %1042), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %ending_mask.6 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.6, %1043), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:460:0
    %1045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1046 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1047 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1048 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1049 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.8, %1045, %1046, %1047, %1048), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1050 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1052 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1053 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1054 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1049, %1050, %1051, %1052, %1053), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1055 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1056 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1057 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1058 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1059 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1054, %1055, %1056, %1057, %1058), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1060 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1061 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1062 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.6 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1059, %1060, %1061, %1062, %1063), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
    %1065 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %1066 : int = aten::size(%beginning_input.6, %1065), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %1067 : Long() = prim::NumToTensor(%1066), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1068 : int = aten::Int(%1067), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1069 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %1070 : int = aten::size(%beginning_input.6, %1069), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %1071 : Long() = prim::NumToTensor(%1070), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1072 : int = aten::Int(%1071), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1073 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %1074 : int = aten::size(%beginning_input.6, %1073), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %1075 : Long() = prim::NumToTensor(%1074), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1076 : int = aten::Int(%1075), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1077 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %1078 : int = aten::size(%beginning_input.6, %1077), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %1079 : Long() = prim::NumToTensor(%1078), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1080 : int = aten::Int(%1079), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1081 : int[] = prim::ListConstruct(%1068, %1072, %1076, %1080), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1082 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %1083 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.6, %1081, %1082), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
    %1084 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:22:0
    %1085 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1083, %1084), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:22:0
    %1086 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:463:0
    %1087 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.6, %1085, %1086), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:463:0
    %1088 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1089 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1090 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1091 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1092 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.8, %1088, %1089, %1090, %1091), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1094 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1095 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1096 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1097 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1092, %1093, %1094, %1095, %1096), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1098 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1099 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1100 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1101 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1102 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1097, %1098, %1099, %1100, %1101), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1103 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1104 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1105 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1106 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.6 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1102, %1103, %1104, %1105, %1106), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
    %1108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %1109 : int = aten::size(%ending_input.6, %1108), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %1110 : Long() = prim::NumToTensor(%1109), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1111 : int = aten::Int(%1110), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1112 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %1113 : int = aten::size(%ending_input.6, %1112), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %1114 : Long() = prim::NumToTensor(%1113), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1115 : int = aten::Int(%1114), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1116 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %1117 : int = aten::size(%ending_input.6, %1116), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %1118 : Long() = prim::NumToTensor(%1117), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1119 : int = aten::Int(%1118), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1120 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %1121 : int = aten::size(%ending_input.6, %1120), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %1122 : Long() = prim::NumToTensor(%1121), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1124 : int[] = prim::ListConstruct(%1111, %1115, %1119, %1123), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1125 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %1126 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.6, %1124, %1125), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
    %1127 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:22:0
    %1128 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1126, %1127), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:22:0
    %1129 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:466:0
    %1130 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.6, %1128, %1129), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:466:0
    %1131 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:284:0
    %attn_scores.3 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.7, %input_tensor.8, %1131), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:284:0
    %1151 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:1500:0
    %1152 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:1500:0
    %attn_probs_fp32.3 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.3, %1151, %1152), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:1500:0
    %attn_probs.5 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::type_as(%attn_probs_fp32.3, %attn_scores.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:320:0
    %1155 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1156 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1157 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1158 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1159 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.3, %1155, %1156, %1157, %1158), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1161 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1162 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1163 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1164 : Bool(17:512, 512:1) = aten::slice(%1159, %1160, %1161, %1162, %1163), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1165 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1166 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1164, %1165), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1167 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1168 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1166, %1167), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1169 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %input.34 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs.5, %1168, %1169), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
    %1171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:973:0
    %1172 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:973:0
    %attn_probs.6 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.34, %1171, %1172), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:973:0
    %1174 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:331:0
    %1175 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:331:0
    %1176 : int[] = prim::ListConstruct(%28, %35, %1174, %1175), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1177 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1389, %1176), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:331:0
    %1178 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:331:0
    %1179 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:331:0
    %value.3 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1177, %1178, %1179), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:331:0
    %1181 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
    %1182 : int = aten::size(%value.3, %1181), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
    %batch_size.12 : Long() = prim::NumToTensor(%1182), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1184 : int = aten::Int(%batch_size.12), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1185 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
    %1186 : int = aten::size(%value.3, %1185), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
    %seq_len.13 : Long() = prim::NumToTensor(%1186), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1188 : int = aten::Int(%seq_len.13), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1189 : int = aten::Int(%seq_len.13), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1190 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
    %1191 : int = aten::size(%value.3, %1190), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
    %num_heads.9 : Long() = prim::NumToTensor(%1191), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1193 : int = aten::Int(%num_heads.9), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1194 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
    %1195 : int = aten::size(%value.3, %1194), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
    %head_dim.9 : Long() = prim::NumToTensor(%1195), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1197 : int = aten::Int(%head_dim.9), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1198 : int = aten::Int(%head_dim.9), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1199 : int = aten::Int(%head_dim.9), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1236 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %1237 : Long() = aten::floor_divide(%seq_len.13, %1236), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %1238 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:542:0
    %1239 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:542:0
    %chunks_count.9 : Long() = aten::sub(%1237, %1238, %1239), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:542:0
    %1241 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:545:0
    %1242 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:545:0
    %1243 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.6, %1241, %1242), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:545:0
    %1244 : Long() = aten::mul(%batch_size.12, %num_heads.9), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:546:0
    %1245 : int = aten::Int(%1244), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1246 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %1247 : Long() = aten::floor_divide(%seq_len.13, %1246), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/tensor.py:424:0
    %1248 : int = aten::Int(%1247), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1249 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:545:0
    %1250 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:545:0
    %1251 : int[] = prim::ListConstruct(%1245, %1248, %1249, %1250), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %chunked_hidden_states.11 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1243, %1251), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:545:0
    %1253 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
    %1254 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
    %1255 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.3, %1253, %1254), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
    %1256 : Long() = aten::mul(%batch_size.12, %num_heads.9), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
    %1257 : int = aten::Int(%1256), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1258 : int[] = prim::ListConstruct(%1257, %1189, %1199), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %input.35 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1255, %1258), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
    %1260 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %1261 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %1262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %1263 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %1264 : int[] = prim::ListConstruct(%1260, %1261, %1262, %1263), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1265 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %padded_value.3 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.35, %1264, %1265), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %1267 : Long() = aten::mul(%batch_size.12, %num_heads.9), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:556:0
    %1268 : int = aten::Int(%1267), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1269 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:556:0
    %1270 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:556:0
    %1271 : Long() = aten::add(%chunks_count.9, %1269, %1270), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:556:0
    %1272 : int = aten::Int(%1271), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:564:0
    %1274 : int[] = prim::ListConstruct(%1268, %1272, %1273, %1198), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1275 : int = prim::Constant[value=65536](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:564:0
    %1276 : int = prim::Constant[value=16384](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:564:0
    %1277 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:564:0
    %1278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:564:0
    %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1280 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1281 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.3, %1274, %1279, %1280), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:564:0
    %1282 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
    %1283 : int = aten::size(%chunked_hidden_states.11, %1282), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
    %total_num_heads.3 : Long() = prim::NumToTensor(%1283), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1285 : int = aten::Int(%total_num_heads.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1286 : int = aten::Int(%total_num_heads.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1287 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
    %1288 : int = aten::size(%chunked_hidden_states.11, %1287), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
    %num_chunks.3 : Long() = prim::NumToTensor(%1288), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1290 : int = aten::Int(%num_chunks.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1291 : int = aten::Int(%num_chunks.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1292 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
    %1293 : int = aten::size(%chunked_hidden_states.11, %1292), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
    %window_overlap.3 : Long() = prim::NumToTensor(%1293), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1295 : int = aten::Int(%window_overlap.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1296 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
    %1297 : int = aten::size(%chunked_hidden_states.11, %1296), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
    %hidden_dim.3 : Long() = prim::NumToTensor(%1297), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1299 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:422:0
    %1300 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:422:0
    %1301 : Long() = aten::add(%window_overlap.3, %1299, %1300), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:422:0
    %1302 : int = aten::Int(%1301), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1303 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %1304 : int[] = prim::ListConstruct(%1303, %1302), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1305 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %chunked_hidden_states.12 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.11, %1304, %1305), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
    %1307 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:424:0
    %1308 : int[] = prim::ListConstruct(%1286, %1291, %1307), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %chunked_hidden_states.13 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.12, %1308), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:424:0
    %1310 : Long() = aten::neg(%window_overlap.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:428:0
    %1311 : int = aten::Int(%1310), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1312 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1313 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1314 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1315 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1316 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.13, %1312, %1313, %1314, %1315), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1319 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1320 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1316, %1317, %1318, %1319, %1320), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1322 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1323 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1324 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %chunked_hidden_states.14 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1321, %1322, %1323, %1311, %1324), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
    %1326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:431:0
    %1327 : Long() = aten::add(%window_overlap.3, %hidden_dim.3, %1326), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:431:0
    %1328 : int = aten::Int(%1327), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1329 : int[] = prim::ListConstruct(%1285, %1290, %1295, %1328), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %chunked_hidden_states.15 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.14, %1329), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:430:0
    %1331 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1333 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1335 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.15, %1331, %1332, %1333, %1334), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1336 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1340 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1335, %1336, %1337, %1338, %1339), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1341 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1342 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1343 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1345 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1340, %1341, %1342, %1343, %1344), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1346 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1347 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1348 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1350 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1345, %1346, %1347, %1348, %1349), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
    %1351 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/functional.py:327:0
    %1352 : Tensor[] = prim::ListConstruct(%1350, %1281), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %context.3 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%1351, %1352), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/functional.py:327:0
    %1354 : int[] = prim::ListConstruct(%1184, %1193, %1188, %1197), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1355 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.3, %1354), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:569:0
    %1356 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:569:0
    %1357 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:569:0
    %attn_output.5 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1355, %1356, %1357), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:569:0
    %1377 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
    %1378 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
    %1379 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.5, %1377, %1378), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
    %1380 : int[] = prim::ListConstruct(%27, %34, %41), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
    %1381 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1379, %1380), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
    %1382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
    %attn_output.6 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1381, %1382), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
    %1384 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:374:0
    %1385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:374:0
    %input.36 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.6, %1384, %1385), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_longformer.py:374:0
    return (%input.36)

LongformerSelfAttention.key
Linear._actual_script_module
  graph(%self.45 : __torch__.torch.nn.modules.linear.Linear,
        %input.31 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.45)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.45)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
    %output.14 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.31, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
    %key_vectors.3 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.14, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
    return (%key_vectors.3)

LongformerSelfAttention.query
Linear._actual_script_module
  graph(%self.44 : __torch__.torch.nn.modules.linear.Linear,
        %input.31 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.44)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.44)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
    %output.13 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.31, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
    %query_vectors.5 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.13, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
    return (%query_vectors.5)

LongformerSelfAttention.value
Linear._actual_script_module
  graph(%self.46 : __torch__.torch.nn.modules.linear.Linear,
        %input.31 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.46)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.46)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
    %output.15 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.31, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
    %value_vectors.3 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.15, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
    return (%value_vectors.3)

LongformerSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.50 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.38 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.50)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.50)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.9 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.38, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.9)

LongformerSelfOutput.dense
Linear._actual_script_module
  graph(%self.48 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:768, 512:13056, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.48)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.48)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
    %output.16 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
    %input.37 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.16, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.37)

LongformerSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.49 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.32 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.32)

LongformerIntermediate.dense
Linear._actual_script_module
  graph(%self.52 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.52)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.52)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.17 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.39 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.17, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.39)

LongformerOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.56 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.42 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.56)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.56)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
    %hidden_states.34 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.42, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%hidden_states.34)

LongformerOutput.dense
Linear._actual_script_module
  graph(%self.54 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1572864, 512:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.54)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.54)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
    %output.18 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
    %input.41 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.18, %2, %6), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
    return (%input.41)

LongformerOutput.dropout
Dropout._actual_script_module
  graph(%self.55 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.33 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.33)

LongformerLayer._actual_script_module
  graph(%self.57 : __torch__.transformers.modeling_longformer.LongformerLayer,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%self.57)
    %4 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%self.57)
    %5 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%self.57)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %attention_mask, %2)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

LongformerLayer.attention
LongformerAttention._actual_script_module
  graph(%self.58 : __torch__.transformers.modeling_longformer.LongformerAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%self.58)
    %4 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%self.58)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %2)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %2)
    return (%8)

LongformerLayer.intermediate
LongformerIntermediate._actual_script_module
  graph(%self.67 : __torch__.transformers.modeling_longformer.LongformerIntermediate,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.67)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.52 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%5), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
    return (%input.52)

LongformerLayer.output
LongformerOutput._actual_script_module
  graph(%self.69 : __torch__.transformers.modeling_longformer.LongformerOutput,
        %1 : Float(17:1572864, 512:3072, 3072:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.69)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.69)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.69)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output # transformers/modeling_longformer.py:830:0
    %input.54 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output # transformers/modeling_longformer.py:830:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.54)
    return (%13)

LongformerAttention.output
LongformerSelfOutput._actual_script_module
  graph(%self.63 : __torch__.transformers.modeling_longformer.LongformerSelfOutput,
        %1 : Float(17:768, 512:13056, 768:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.63)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.63)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.63)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output # transformers/modeling_longformer.py:758:0
    %input.50 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output # transformers/modeling_longformer.py:758:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.50)
    return (%13)

LongformerAttention.self
LongformerSelfAttention._actual_script_module
  graph(%self.59 : __torch__.transformers.modeling_longformer.LongformerSelfAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.59)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.59)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.59)
    %6 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:241:0
    %7 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:241:0
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:241:0
    %9 : Float(17:512, 512:1) = aten::squeeze(%7, %8), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:241:0
    %10 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:22:0
    %is_index_masked.4 : Bool(17:512, 512:1) = aten::lt(%9, %10), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:22:0
    %18 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:248:0
    %19 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:248:0
    %input.43 : Float(512:768, 17:393216, 768:1) = aten::transpose(%2, %18, %19), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:248:0
    %1387 : Tensor = prim::CallMethod[name="forward"](%5, %input.43)
    %1388 : Tensor = prim::CallMethod[name="forward"](%4, %input.43)
    %1389 : Tensor = prim::CallMethod[name="forward"](%3, %input.43)
    %24 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
    %25 : int = aten::size(%input.43, %24), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
    %seq_len.14 : Long() = prim::NumToTensor(%25), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %27 : int = aten::Int(%seq_len.14), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %28 : int = aten::Int(%seq_len.14), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %29 : int = aten::Int(%seq_len.14), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %30 : int = aten::Int(%seq_len.14), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %31 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
    %32 : int = aten::size(%input.43, %31), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
    %batch_size.13 : Long() = prim::NumToTensor(%32), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %34 : int = aten::Int(%batch_size.13), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %35 : int = aten::Int(%batch_size.13), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %36 : int = aten::Int(%batch_size.13), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %37 : int = aten::Int(%batch_size.13), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %38 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
    %39 : int = aten::size(%input.43, %38), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
    %embed_dim.4 : Long() = prim::NumToTensor(%39), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %41 : int = aten::Int(%embed_dim.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %44 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:261:0
    %query_vectors.8 : Float(512:13056, 17:768, 768:1) = aten::div_(%1387, %44), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:261:0
    %46 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:263:0
    %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:263:0
    %48 : int[] = prim::ListConstruct(%30, %37, %46, %47), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %49 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.8, %48), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:263:0
    %50 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:263:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:263:0
    %query.7 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%49, %50, %51), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:263:0
    %53 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:264:0
    %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:264:0
    %55 : int[] = prim::ListConstruct(%29, %36, %53, %54), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %56 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1388, %55), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:264:0
    %57 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:264:0
    %58 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:264:0
    %key.4 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%56, %57, %58), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:264:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %61 : int = aten::size(%query.7, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.14 : Long() = prim::NumToTensor(%61), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %63 : int = aten::Int(%batch_size.14), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %65 : int = aten::size(%query.7, %64), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.15 : Long() = prim::NumToTensor(%65), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %67 : int = aten::Int(%seq_len.15), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %68 : int = aten::Int(%seq_len.15), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %69 : int = aten::Int(%seq_len.15), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %70 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %71 : int = aten::size(%query.7, %70), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.10 : Long() = prim::NumToTensor(%71), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %73 : int = aten::Int(%num_heads.10), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %74 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %75 : int = aten::size(%query.7, %74), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.10 : Long() = prim::NumToTensor(%75), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %77 : int = aten::Int(%head_dim.10), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %78 : int = aten::Int(%head_dim.10), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %111 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %112 : Long() = aten::floor_divide(%seq_len.15, %111), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %113 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:478:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.10 : Long() = aten::sub(%112, %113, %114), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:478:0
    %116 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
    %117 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
    %118 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.7, %116, %117), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
    %119 : Long() = aten::mul(%batch_size.14, %num_heads.10), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
    %120 : int = aten::Int(%119), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %121 : int[] = prim::ListConstruct(%120, %69, %78), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %hidden_states.35 : Float(204:64, 512:13056, 64:1) = aten::reshape(%118, %121), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
    %123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
    %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
    %125 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.4, %123, %124), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
    %126 : Long() = aten::mul(%batch_size.14, %num_heads.10), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
    %127 : int = aten::Int(%126), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %128 : int[] = prim::ListConstruct(%127, %68, %77), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %hidden_states.37 : Float(204:64, 512:13056, 64:1) = aten::reshape(%125, %128), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
    %130 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
    %131 : int = aten::size(%hidden_states.35, %130), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
    %132 : Long() = prim::NumToTensor(%131), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %133 : int = aten::Int(%132), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %134 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
    %135 : int = aten::size(%hidden_states.35, %134), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
    %136 : Long() = prim::NumToTensor(%135), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %137 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %138 : Long() = aten::floor_divide(%136, %137), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %139 : int = aten::Int(%138), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %140 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
    %141 : int = aten::size(%hidden_states.35, %140), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
    %142 : Long() = prim::NumToTensor(%141), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %143 : int = aten::Int(%142), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %144 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
    %145 : int[] = prim::ListConstruct(%133, %139, %144, %143), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %hidden_states.36 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.35, %145), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
    %147 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %148 : int = aten::size(%hidden_states.36, %147), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %149 : Long() = prim::NumToTensor(%148), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %150 : int = aten::Int(%149), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %151 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %152 : int = aten::size(%hidden_states.36, %151), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %153 : Long() = prim::NumToTensor(%152), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %154 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %155 : int = aten::size(%hidden_states.36, %154), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %156 : Long() = prim::NumToTensor(%155), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %157 : int = aten::Int(%156), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %158 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %159 : int = aten::size(%hidden_states.36, %158), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %160 : Long() = prim::NumToTensor(%159), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %161 : int = aten::Int(%160), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %162 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %163 : Long() = aten::mul(%153, %162), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %164 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %165 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %166 : Long() = aten::sub(%163, %164, %165), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %167 : int = aten::Int(%166), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %168 : int[] = prim::ListConstruct(%150, %167, %157, %161), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %169 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %170 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %171 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %172 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %173 : int[] = prim::ListConstruct(%169, %170, %171, %172), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %174 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %175 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.36, %168, %173, %174), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %176 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
    %177 : int = aten::size(%hidden_states.37, %176), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %179 : int = aten::Int(%178), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
    %181 : int = aten::size(%hidden_states.37, %180), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
    %182 : Long() = prim::NumToTensor(%181), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %183 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %184 : Long() = aten::floor_divide(%182, %183), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %185 : int = aten::Int(%184), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %186 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
    %187 : int = aten::size(%hidden_states.37, %186), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
    %188 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %189 : int = aten::Int(%188), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %190 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
    %191 : int[] = prim::ListConstruct(%179, %185, %190, %189), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %hidden_states.38 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.37, %191), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
    %193 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %194 : int = aten::size(%hidden_states.38, %193), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %195 : Long() = prim::NumToTensor(%194), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %196 : int = aten::Int(%195), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %197 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %198 : int = aten::size(%hidden_states.38, %197), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %199 : Long() = prim::NumToTensor(%198), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %200 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %201 : int = aten::size(%hidden_states.38, %200), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %203 : int = aten::Int(%202), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %204 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %205 : int = aten::size(%hidden_states.38, %204), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %207 : int = aten::Int(%206), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %208 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %209 : Long() = aten::mul(%199, %208), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %210 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %211 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %212 : Long() = aten::sub(%209, %210, %211), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %213 : int = aten::Int(%212), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %214 : int[] = prim::ListConstruct(%196, %213, %203, %207), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %216 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %217 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %218 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %219 : int[] = prim::ListConstruct(%215, %216, %217, %218), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %220 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.38, %214, %219, %220), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %222 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/functional.py:327:0
    %223 : Tensor[] = prim::ListConstruct(%175, %221), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %input.44 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%222, %223), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/functional.py:327:0
    %225 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %226 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %227 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %229 : int[] = prim::ListConstruct(%225, %226, %227, %228), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %230 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.7 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.44, %229, %230), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %232 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %233 : int = aten::size(%hidden_states_padded.7, %232), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %234 : Long() = prim::NumToTensor(%233), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %235 : int = aten::Int(%234), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %236 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %237 : int = aten::size(%hidden_states_padded.7, %236), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %238 : Long() = prim::NumToTensor(%237), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %239 : int = aten::Int(%238), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %246 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %247 : int = aten::size(%hidden_states_padded.7, %246), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %248 : Long() = prim::NumToTensor(%247), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %249 : int = aten::Int(%248), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %250 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %251 : int = aten::size(%hidden_states_padded.7, %250), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %252 : Long() = prim::NumToTensor(%251), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %253 : int = aten::Int(%252), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %254 : int[] = prim::ListConstruct(%235, %239, %249, %253), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %diagonal_chunked_attention_scores.7 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.7, %254), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:400:0
    %256 : Long() = aten::mul(%batch_size.14, %num_heads.10), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
    %257 : int = aten::Int(%256), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %258 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
    %259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
    %260 : Long() = aten::add(%chunks_count.10, %258, %259), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
    %261 : int = aten::Int(%260), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %263 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %264 : int[] = prim::ListConstruct(%257, %261, %262, %263), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %265 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %266 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %268 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.7 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.7, %264, %265, %266, %267, %268), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %270 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %271 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %273 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %270, %271, %272, %273), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %275 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %276 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %277 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %279 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%274, %275, %276, %277, %278), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %280 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %281 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %282 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %284 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%279, %280, %281, %282, %283), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %285 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %286 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %287 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %289 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%284, %285, %286, %287, %288), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %290 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %291 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %292 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %294 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %290, %291, %292, %293), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %295 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %296 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %297 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %299 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%294, %295, %296, %297, %298), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %300 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %301 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %302 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %303 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %304 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%299, %300, %301, %302, %303), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %305 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %306 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %307 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %308 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %309 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%304, %305, %306, %307, %308), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %310 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %311 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %312 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %313 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %314 : int[] = prim::ListConstruct(%310, %311, %312, %313), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %315 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%289, %314), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %316 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %317 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%309, %315, %316), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %319 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %320 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %321 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %322 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %318, %319, %320, %321), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %323 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %324 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %325 : Float(204:262656, 512:513, 513:1) = aten::select(%322, %323, %324), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %327 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %328 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %329 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %330 : Float(204:262656, 256:513, 513:1) = aten::slice(%325, %326, %327, %328, %329), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %331 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %333 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %335 : Float(204:262656, 256:513, 257:1) = aten::slice(%330, %331, %332, %333, %334), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %336 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %340 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %336, %337, %338, %339), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %341 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %342 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %343 : Float(204:262656, 256:513, 513:1) = aten::select(%340, %341, %342), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %345 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %347 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %348 : Float(204:262656, 256:513, 513:1) = aten::slice(%343, %344, %345, %346, %347), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %349 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %350 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %352 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %353 : Float(204:262656, 256:513, 257:1) = aten::slice(%348, %349, %350, %351, %352), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %354 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %355 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %356 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %357 : int[] = prim::ListConstruct(%354, %355, %356), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %358 : Float(204:262656, 256:513, 257:1) = aten::view(%335, %357), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %359 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %360 : Float(204:262656, 256:513, 257:1) = aten::copy_(%353, %358, %359), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %361 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %362 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %363 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %364 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %365 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %361, %362, %363, %364), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %366 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %367 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %368 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %369 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %370 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%365, %366, %367, %368, %369), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %371 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %372 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %373 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %374 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %375 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%370, %371, %372, %373, %374), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %376 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %377 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %379 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %380 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%375, %376, %377, %378, %379), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %383 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %384 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %385 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %381, %382, %383, %384), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %386 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %387 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %388 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %389 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %390 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%385, %386, %387, %388, %389), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %391 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %392 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %393 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %394 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %395 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%390, %391, %392, %393, %394), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %397 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %398 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %399 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %400 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%395, %396, %397, %398, %399), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %401 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %402 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %403 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %404 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %405 : int[] = prim::ListConstruct(%401, %402, %403, %404), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %406 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%380, %405), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %407 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %408 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%400, %406, %407), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %409 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %410 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %411 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %412 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %413 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %409, %410, %411, %412), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %414 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %415 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %416 : Float(204:262656, 512:513, 513:1) = aten::select(%413, %414, %415), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %417 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %418 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %419 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %420 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %421 : Float(204:262656, 255:513, 513:1) = aten::slice(%416, %417, %418, %419, %420), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %422 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %423 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %425 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %426 : Float(204:262656, 255:513, 255:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %427 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %429 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %430 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %431 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %427, %428, %429, %430), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %432 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %433 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %434 : Float(204:262656, 256:513, 513:1) = aten::select(%431, %432, %433), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %435 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %436 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %437 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %438 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %439 : Float(204:262656, 255:513, 513:1) = aten::slice(%434, %435, %436, %437, %438), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %440 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %441 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %442 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %443 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %444 : Float(204:262656, 255:513, 255:1) = aten::slice(%439, %440, %441, %442, %443), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %445 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %446 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %447 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %448 : int[] = prim::ListConstruct(%445, %446, %447), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %449 : Float(204:262656, 255:513, 255:1) = aten::view(%426, %448), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %450 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %451 : Float(204:262656, 255:513, 255:1) = aten::copy_(%444, %449, %450), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %452 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
    %453 : int[] = prim::ListConstruct(%63, %73, %67, %452), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %454 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.7, %453), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
    %455 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
    %456 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.10 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%454, %455, %456), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
    %458 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %459 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %460 : int[] = prim::ListConstruct(%458, %459), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %461 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %462 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %463 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %464 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %465 : Float(256:257, 257:1) = aten::ones(%460, %461, %462, %463, %464), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %466 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %467 : Float(256:257, 257:1) = aten::tril(%465, %466), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %468 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %469 : int[] = prim::ListConstruct(%468), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %beginning_mask_2d.7 : Float(256:257, 257:1) = aten::flip(%467, %469), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %471 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %472 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.7, %471), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %473 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %474 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %475 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %476 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %477 : Float(1:65792, 256:257, 257:1) = aten::slice(%472, %473, %474, %475, %476), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %478 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %479 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%477, %478), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %480 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %481 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %482 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %483 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.7 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%479, %480, %481, %482, %483), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %485 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:460:0
    %486 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:460:0
    %487 : int[] = prim::ListConstruct(%485, %486), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %ending_mask.7 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.7, %487), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:460:0
    %489 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %490 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %491 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %493 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.10, %489, %490, %491, %492), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %494 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %495 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %496 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %497 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %498 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%493, %494, %495, %496, %497), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %499 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %500 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %501 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %502 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %503 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%498, %499, %500, %501, %502), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %504 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %505 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %506 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %507 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.7 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%503, %504, %505, %506, %507), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %509 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %510 : int = aten::size(%beginning_input.7, %509), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %511 : Long() = prim::NumToTensor(%510), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %512 : int = aten::Int(%511), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %513 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %514 : int = aten::size(%beginning_input.7, %513), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %515 : Long() = prim::NumToTensor(%514), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %516 : int = aten::Int(%515), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %517 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %518 : int = aten::size(%beginning_input.7, %517), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %519 : Long() = prim::NumToTensor(%518), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %520 : int = aten::Int(%519), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %521 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %522 : int = aten::size(%beginning_input.7, %521), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %523 : Long() = prim::NumToTensor(%522), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %524 : int = aten::Int(%523), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %525 : int[] = prim::ListConstruct(%512, %516, %520, %524), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %526 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %527 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.7, %525, %526), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %528 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:22:0
    %529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%527, %528), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:22:0
    %530 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:463:0
    %531 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.7, %529, %530), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:463:0
    %532 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %533 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %534 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %535 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %536 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.10, %532, %533, %534, %535), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %537 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %538 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %539 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %541 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%536, %537, %538, %539, %540), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %542 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %543 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %544 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %545 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %546 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%541, %542, %543, %544, %545), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %547 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %548 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %549 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.7 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%546, %547, %548, %549, %550), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %552 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %553 : int = aten::size(%ending_input.7, %552), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %554 : Long() = prim::NumToTensor(%553), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %555 : int = aten::Int(%554), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %556 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %557 : int = aten::size(%ending_input.7, %556), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %558 : Long() = prim::NumToTensor(%557), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %559 : int = aten::Int(%558), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %560 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %561 : int = aten::size(%ending_input.7, %560), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %562 : Long() = prim::NumToTensor(%561), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %563 : int = aten::Int(%562), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %564 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %565 : int = aten::size(%ending_input.7, %564), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %566 : Long() = prim::NumToTensor(%565), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %567 : int = aten::Int(%566), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %568 : int[] = prim::ListConstruct(%555, %559, %563, %567), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %569 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %570 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.7, %568, %569), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %571 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:22:0
    %572 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%570, %571), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:22:0
    %573 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:466:0
    %574 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.7, %572, %573), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:466:0
    %575 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:22:0
    %576 : Bool(17:512, 512:1) = aten::ne(%9, %575), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:22:0
    %577 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %578 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %579 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %580 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %581 : Bool(17:512, 512:1) = aten::slice(%576, %577, %578, %579, %580), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %582 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %583 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %584 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %585 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %586 : Bool(17:512, 512:1) = aten::slice(%581, %582, %583, %584, %585), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %587 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %588 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%586, %587), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %589 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %remove_from_windowed_attention_mask.4 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%588, %589), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
    %591 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.4, %query.7), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:275:0
    %592 : float = prim::Constant[value=-10000.](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:275:0
    %float_mask.4 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%591, %remove_from_windowed_attention_mask.4, %592), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:275:0
    %594 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %595 : int = aten::size(%float_mask.4, %594), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %596 : Long() = prim::NumToTensor(%595), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %597 : int = aten::Int(%596), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %598 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %599 : int = aten::size(%float_mask.4, %598), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %600 : Long() = prim::NumToTensor(%599), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %601 : int = aten::Int(%600), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %602 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %603 : int = aten::size(%float_mask.4, %602), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %604 : Long() = prim::NumToTensor(%603), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %605 : int = aten::Int(%604), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %606 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %607 : int = aten::size(%float_mask.4, %606), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %608 : Long() = prim::NumToTensor(%607), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %609 : int = aten::Int(%608), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %610 : int[] = prim::ListConstruct(%597, %601, %605, %609), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %611 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %612 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %613 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %614 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %query.8 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%610, %611, %612, %613, %614), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
    %616 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %617 : int = aten::size(%query.8, %616), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.15 : Long() = prim::NumToTensor(%617), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %619 : int = aten::Int(%batch_size.15), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %620 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %621 : int = aten::size(%query.8, %620), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.16 : Long() = prim::NumToTensor(%621), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %623 : int = aten::Int(%seq_len.16), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %624 : int = aten::Int(%seq_len.16), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %625 : int = aten::Int(%seq_len.16), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %626 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %627 : int = aten::size(%query.8, %626), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.11 : Long() = prim::NumToTensor(%627), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %629 : int = aten::Int(%num_heads.11), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %630 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %631 : int = aten::size(%query.8, %630), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.11 : Long() = prim::NumToTensor(%631), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %633 : int = aten::Int(%head_dim.11), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %634 : int = aten::Int(%head_dim.11), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %667 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %668 : Long() = aten::floor_divide(%seq_len.16, %667), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %669 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:478:0
    %670 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.11 : Long() = aten::sub(%668, %669, %670), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:478:0
    %672 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
    %673 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
    %674 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.8, %672, %673), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
    %675 : Long() = aten::mul(%batch_size.15, %num_heads.11), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
    %676 : int = aten::Int(%675), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %677 : int[] = prim::ListConstruct(%676, %625, %634), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %hidden_states.39 : Float(17:512, 512:1, 1:1) = aten::reshape(%674, %677), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
    %679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
    %680 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
    %681 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.4, %679, %680), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
    %682 : Long() = aten::mul(%batch_size.15, %num_heads.11), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
    %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %684 : int[] = prim::ListConstruct(%683, %624, %633), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %hidden_states.41 : Float(17:512, 512:1, 1:1) = aten::reshape(%681, %684), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
    %686 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
    %687 : int = aten::size(%hidden_states.39, %686), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
    %688 : Long() = prim::NumToTensor(%687), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %690 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
    %691 : int = aten::size(%hidden_states.39, %690), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
    %692 : Long() = prim::NumToTensor(%691), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %693 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %694 : Long() = aten::floor_divide(%692, %693), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %696 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
    %697 : int = aten::size(%hidden_states.39, %696), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
    %698 : Long() = prim::NumToTensor(%697), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %699 : int = aten::Int(%698), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %700 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
    %701 : int[] = prim::ListConstruct(%689, %695, %700, %699), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %hidden_states.40 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.39, %701), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
    %703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %704 : int = aten::size(%hidden_states.40, %703), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %705 : Long() = prim::NumToTensor(%704), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %706 : int = aten::Int(%705), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %708 : int = aten::size(%hidden_states.40, %707), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %709 : Long() = prim::NumToTensor(%708), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %710 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %711 : int = aten::size(%hidden_states.40, %710), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %712 : Long() = prim::NumToTensor(%711), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %713 : int = aten::Int(%712), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %714 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %715 : int = aten::size(%hidden_states.40, %714), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %716 : Long() = prim::NumToTensor(%715), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %717 : int = aten::Int(%716), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %718 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %719 : Long() = aten::mul(%709, %718), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %720 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %721 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %722 : Long() = aten::sub(%719, %720, %721), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %723 : int = aten::Int(%722), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %724 : int[] = prim::ListConstruct(%706, %723, %713, %717), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %725 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %726 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %727 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %728 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %729 : int[] = prim::ListConstruct(%725, %726, %727, %728), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %730 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %731 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.40, %724, %729, %730), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %732 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
    %733 : int = aten::size(%hidden_states.41, %732), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
    %734 : Long() = prim::NumToTensor(%733), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %735 : int = aten::Int(%734), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %736 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
    %737 : int = aten::size(%hidden_states.41, %736), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
    %738 : Long() = prim::NumToTensor(%737), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %739 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %740 : Long() = aten::floor_divide(%738, %739), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %741 : int = aten::Int(%740), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
    %743 : int = aten::size(%hidden_states.41, %742), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
    %744 : Long() = prim::NumToTensor(%743), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %745 : int = aten::Int(%744), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %746 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
    %747 : int[] = prim::ListConstruct(%735, %741, %746, %745), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %hidden_states.42 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.41, %747), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
    %749 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %750 : int = aten::size(%hidden_states.42, %749), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %751 : Long() = prim::NumToTensor(%750), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %752 : int = aten::Int(%751), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %754 : int = aten::size(%hidden_states.42, %753), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %755 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %756 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %757 : int = aten::size(%hidden_states.42, %756), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %758 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %759 : int = aten::Int(%758), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %760 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %761 : int = aten::size(%hidden_states.42, %760), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
    %762 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %764 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %765 : Long() = aten::mul(%755, %764), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %766 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %767 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %768 : Long() = aten::sub(%765, %766, %767), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
    %769 : int = aten::Int(%768), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %770 : int[] = prim::ListConstruct(%752, %769, %759, %763), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %771 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %772 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %773 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %776 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.42, %770, %775, %776), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
    %778 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/functional.py:327:0
    %779 : Tensor[] = prim::ListConstruct(%731, %777), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %input.45 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%778, %779), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/functional.py:327:0
    %781 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %782 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %783 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %786 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.8 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.45, %785, %786), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %788 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %789 : int = aten::size(%hidden_states_padded.8, %788), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %790 : Long() = prim::NumToTensor(%789), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %791 : int = aten::Int(%790), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %792 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %793 : int = aten::size(%hidden_states_padded.8, %792), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %794 : Long() = prim::NumToTensor(%793), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %795 : int = aten::Int(%794), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %802 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %803 : int = aten::size(%hidden_states_padded.8, %802), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %804 : Long() = prim::NumToTensor(%803), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %805 : int = aten::Int(%804), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %806 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %807 : int = aten::size(%hidden_states_padded.8, %806), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
    %808 : Long() = prim::NumToTensor(%807), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %809 : int = aten::Int(%808), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %810 : int[] = prim::ListConstruct(%791, %795, %805, %809), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %diagonal_chunked_attention_scores.8 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.8, %810), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:400:0
    %812 : Long() = aten::mul(%batch_size.15, %num_heads.11), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
    %813 : int = aten::Int(%812), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %814 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
    %815 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
    %816 : Long() = aten::add(%chunks_count.11, %814, %815), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
    %817 : int = aten::Int(%816), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %818 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %819 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %820 : int[] = prim::ListConstruct(%813, %817, %818, %819), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %821 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %822 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %823 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %824 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.8 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.8, %820, %821, %822, %823, %824), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
    %826 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %827 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %828 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %829 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %830 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %826, %827, %828, %829), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %831 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %832 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %833 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %834 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%830, %831, %832, %833, %834), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %836 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %837 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %838 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %839 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%835, %836, %837, %838, %839), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %841 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %842 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %843 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %845 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%840, %841, %842, %843, %844), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %846 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %847 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %848 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %849 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %846, %847, %848, %849), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %851 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %852 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %853 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %855 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%850, %851, %852, %853, %854), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %856 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %857 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %858 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %859 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %860 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%855, %856, %857, %858, %859), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %861 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %862 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %863 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %864 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %865 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%860, %861, %862, %863, %864), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %866 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %867 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %868 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %869 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %870 : int[] = prim::ListConstruct(%866, %867, %868, %869), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %871 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%845, %870), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %872 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %873 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%865, %871, %872), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
    %874 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %875 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %876 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %877 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %874, %875, %876, %877), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %879 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %880 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %881 : Float(17:262656, 512:513, 513:1) = aten::select(%878, %879, %880), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %883 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %884 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %885 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %886 : Float(17:262656, 256:513, 513:1) = aten::slice(%881, %882, %883, %884, %885), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %887 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %889 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%886, %887, %888, %889, %890), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %892 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %893 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %894 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %895 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %896 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %892, %893, %894, %895), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %897 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %898 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %899 : Float(17:262656, 256:513, 513:1) = aten::select(%896, %897, %898), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %901 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %902 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %903 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %904 : Float(17:262656, 256:513, 513:1) = aten::slice(%899, %900, %901, %902, %903), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %905 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %906 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %907 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %909 : Float(17:262656, 256:513, 257:1) = aten::slice(%904, %905, %906, %907, %908), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %910 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %911 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %912 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %913 : int[] = prim::ListConstruct(%910, %911, %912), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %914 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %913), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %915 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %916 : Float(17:262656, 256:513, 257:1) = aten::copy_(%909, %914, %915), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
    %917 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %918 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %919 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %920 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %921 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %917, %918, %919, %920), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %922 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %923 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %924 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %925 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %926 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%921, %922, %923, %924, %925), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %927 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %928 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %929 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %930 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %931 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%926, %927, %928, %929, %930), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %932 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %933 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %934 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %935 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %936 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%931, %932, %933, %934, %935), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %938 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %939 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %940 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %941 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %937, %938, %939, %940), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %942 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %943 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %944 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %945 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %946 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%941, %942, %943, %944, %945), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %947 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %948 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %949 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %950 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %951 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%946, %947, %948, %949, %950), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %952 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %953 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %954 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %955 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %956 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%951, %952, %953, %954, %955), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %957 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %959 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %960 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %962 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%936, %961), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %963 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %964 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%956, %962, %963), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
    %965 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %966 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %967 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %969 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %965, %966, %967, %968), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %970 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %971 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %972 : Float(17:262656, 512:513, 513:1) = aten::select(%969, %970, %971), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %973 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %974 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %975 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %976 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %977 : Float(17:262656, 255:513, 513:1) = aten::slice(%972, %973, %974, %975, %976), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %978 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %979 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %980 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %981 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %982 : Float(17:262656, 255:513, 255:1) = aten::slice(%977, %978, %979, %980, %981), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %983 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %984 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %985 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %986 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %987 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %983, %984, %985, %986), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %988 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %989 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %990 : Float(17:262656, 256:513, 513:1) = aten::select(%987, %988, %989), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %991 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %993 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %994 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %995 : Float(17:262656, 255:513, 513:1) = aten::slice(%990, %991, %992, %993, %994), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %996 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %998 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %999 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %1000 : Float(17:262656, 255:513, 255:1) = aten::slice(%995, %996, %997, %998, %999), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %1001 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %1002 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %1003 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %1004 : int[] = prim::ListConstruct(%1001, %1002, %1003), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1005 : Float(17:262656, 255:513, 255:1) = aten::view(%982, %1004), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %1006 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1007 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1000, %1005, %1006), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
    %1008 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
    %1009 : int[] = prim::ListConstruct(%619, %629, %623, %1008), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1010 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.8, %1009), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
    %1011 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
    %1012 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.11 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1010, %1011, %1012), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
    %1014 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %1015 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %1016 : int[] = prim::ListConstruct(%1014, %1015), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1017 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %1018 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %1019 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %1020 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %1021 : Float(256:257, 257:1) = aten::ones(%1016, %1017, %1018, %1019, %1020), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %1022 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %1023 : Float(256:257, 257:1) = aten::tril(%1021, %1022), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %1024 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %1025 : int[] = prim::ListConstruct(%1024), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %beginning_mask_2d.8 : Float(256:257, 257:1) = aten::flip(%1023, %1025), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
    %1027 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1028 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.8, %1027), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1029 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1030 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1031 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1032 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1033 : Float(1:65792, 256:257, 257:1) = aten::slice(%1028, %1029, %1030, %1031, %1032), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1034 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1035 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1033, %1034), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1036 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1037 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1038 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1039 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.8 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1035, %1036, %1037, %1038, %1039), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
    %1041 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:460:0
    %1042 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:460:0
    %1043 : int[] = prim::ListConstruct(%1041, %1042), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %ending_mask.8 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.8, %1043), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:460:0
    %1045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1046 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1047 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1048 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1049 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.11, %1045, %1046, %1047, %1048), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1050 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1052 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1053 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1054 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1049, %1050, %1051, %1052, %1053), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1055 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1056 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1057 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1058 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1059 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1054, %1055, %1056, %1057, %1058), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1060 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1061 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1062 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.8 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1059, %1060, %1061, %1062, %1063), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
    %1065 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %1066 : int = aten::size(%beginning_input.8, %1065), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %1067 : Long() = prim::NumToTensor(%1066), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1068 : int = aten::Int(%1067), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1069 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %1070 : int = aten::size(%beginning_input.8, %1069), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %1071 : Long() = prim::NumToTensor(%1070), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1072 : int = aten::Int(%1071), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1073 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %1074 : int = aten::size(%beginning_input.8, %1073), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %1075 : Long() = prim::NumToTensor(%1074), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1076 : int = aten::Int(%1075), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1077 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %1078 : int = aten::size(%beginning_input.8, %1077), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %1079 : Long() = prim::NumToTensor(%1078), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1080 : int = aten::Int(%1079), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1081 : int[] = prim::ListConstruct(%1068, %1072, %1076, %1080), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1082 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %1083 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.8, %1081, %1082), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
    %1084 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:22:0
    %1085 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1083, %1084), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:22:0
    %1086 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:463:0
    %1087 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.8, %1085, %1086), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:463:0
    %1088 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1089 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1090 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1091 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1092 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.11, %1088, %1089, %1090, %1091), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1094 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1095 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1096 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1097 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1092, %1093, %1094, %1095, %1096), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1098 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1099 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1100 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1101 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1102 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1097, %1098, %1099, %1100, %1101), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1103 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1104 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1105 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1106 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.8 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1102, %1103, %1104, %1105, %1106), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
    %1108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %1109 : int = aten::size(%ending_input.8, %1108), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %1110 : Long() = prim::NumToTensor(%1109), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1111 : int = aten::Int(%1110), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1112 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %1113 : int = aten::size(%ending_input.8, %1112), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %1114 : Long() = prim::NumToTensor(%1113), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1115 : int = aten::Int(%1114), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1116 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %1117 : int = aten::size(%ending_input.8, %1116), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %1118 : Long() = prim::NumToTensor(%1117), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1119 : int = aten::Int(%1118), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1120 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %1121 : int = aten::size(%ending_input.8, %1120), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %1122 : Long() = prim::NumToTensor(%1121), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1124 : int[] = prim::ListConstruct(%1111, %1115, %1119, %1123), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1125 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %1126 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.8, %1124, %1125), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
    %1127 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:22:0
    %1128 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1126, %1127), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:22:0
    %1129 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:466:0
    %1130 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.8, %1128, %1129), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:466:0
    %1131 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:284:0
    %attn_scores.4 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.10, %input_tensor.11, %1131), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:284:0
    %1151 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:1500:0
    %1152 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:1500:0
    %attn_probs_fp32.4 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.4, %1151, %1152), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:1500:0
    %attn_probs.7 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::type_as(%attn_probs_fp32.4, %attn_scores.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:320:0
    %1155 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1156 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1157 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1158 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1159 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.4, %1155, %1156, %1157, %1158), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1161 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1162 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1163 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1164 : Bool(17:512, 512:1) = aten::slice(%1159, %1160, %1161, %1162, %1163), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1165 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1166 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1164, %1165), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1167 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1168 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1166, %1167), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1169 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %input.46 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs.7, %1168, %1169), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
    %1171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:973:0
    %1172 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:973:0
    %attn_probs.8 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.46, %1171, %1172), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:973:0
    %1174 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:331:0
    %1175 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:331:0
    %1176 : int[] = prim::ListConstruct(%28, %35, %1174, %1175), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1177 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1389, %1176), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:331:0
    %1178 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:331:0
    %1179 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:331:0
    %value.4 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1177, %1178, %1179), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:331:0
    %1181 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
    %1182 : int = aten::size(%value.4, %1181), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
    %batch_size.16 : Long() = prim::NumToTensor(%1182), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1184 : int = aten::Int(%batch_size.16), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1185 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
    %1186 : int = aten::size(%value.4, %1185), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
    %seq_len.17 : Long() = prim::NumToTensor(%1186), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1188 : int = aten::Int(%seq_len.17), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1189 : int = aten::Int(%seq_len.17), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1190 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
    %1191 : int = aten::size(%value.4, %1190), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
    %num_heads.12 : Long() = prim::NumToTensor(%1191), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1193 : int = aten::Int(%num_heads.12), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1194 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
    %1195 : int = aten::size(%value.4, %1194), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
    %head_dim.12 : Long() = prim::NumToTensor(%1195), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1197 : int = aten::Int(%head_dim.12), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1198 : int = aten::Int(%head_dim.12), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1199 : int = aten::Int(%head_dim.12), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1236 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %1237 : Long() = aten::floor_divide(%seq_len.17, %1236), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %1238 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:542:0
    %1239 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:542:0
    %chunks_count.12 : Long() = aten::sub(%1237, %1238, %1239), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:542:0
    %1241 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:545:0
    %1242 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:545:0
    %1243 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.8, %1241, %1242), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:545:0
    %1244 : Long() = aten::mul(%batch_size.16, %num_heads.12), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:546:0
    %1245 : int = aten::Int(%1244), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1246 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %1247 : Long() = aten::floor_divide(%seq_len.17, %1246), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/tensor.py:424:0
    %1248 : int = aten::Int(%1247), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1249 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:545:0
    %1250 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:545:0
    %1251 : int[] = prim::ListConstruct(%1245, %1248, %1249, %1250), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %chunked_hidden_states.16 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1243, %1251), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:545:0
    %1253 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
    %1254 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
    %1255 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.4, %1253, %1254), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
    %1256 : Long() = aten::mul(%batch_size.16, %num_heads.12), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
    %1257 : int = aten::Int(%1256), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1258 : int[] = prim::ListConstruct(%1257, %1189, %1199), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %input.47 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1255, %1258), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
    %1260 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %1261 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %1262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %1263 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %1264 : int[] = prim::ListConstruct(%1260, %1261, %1262, %1263), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1265 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %padded_value.4 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.47, %1264, %1265), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %1267 : Long() = aten::mul(%batch_size.16, %num_heads.12), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:556:0
    %1268 : int = aten::Int(%1267), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1269 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:556:0
    %1270 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:556:0
    %1271 : Long() = aten::add(%chunks_count.12, %1269, %1270), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:556:0
    %1272 : int = aten::Int(%1271), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:564:0
    %1274 : int[] = prim::ListConstruct(%1268, %1272, %1273, %1198), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1275 : int = prim::Constant[value=65536](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:564:0
    %1276 : int = prim::Constant[value=16384](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:564:0
    %1277 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:564:0
    %1278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:564:0
    %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1280 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1281 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.4, %1274, %1279, %1280), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:564:0
    %1282 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
    %1283 : int = aten::size(%chunked_hidden_states.16, %1282), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
    %total_num_heads.4 : Long() = prim::NumToTensor(%1283), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1285 : int = aten::Int(%total_num_heads.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1286 : int = aten::Int(%total_num_heads.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1287 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
    %1288 : int = aten::size(%chunked_hidden_states.16, %1287), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
    %num_chunks.4 : Long() = prim::NumToTensor(%1288), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1290 : int = aten::Int(%num_chunks.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1291 : int = aten::Int(%num_chunks.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1292 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
    %1293 : int = aten::size(%chunked_hidden_states.16, %1292), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
    %window_overlap.4 : Long() = prim::NumToTensor(%1293), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1295 : int = aten::Int(%window_overlap.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1296 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
    %1297 : int = aten::size(%chunked_hidden_states.16, %1296), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
    %hidden_dim.4 : Long() = prim::NumToTensor(%1297), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1299 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:422:0
    %1300 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:422:0
    %1301 : Long() = aten::add(%window_overlap.4, %1299, %1300), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:422:0
    %1302 : int = aten::Int(%1301), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1303 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %1304 : int[] = prim::ListConstruct(%1303, %1302), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1305 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %chunked_hidden_states.17 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.16, %1304, %1305), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
    %1307 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:424:0
    %1308 : int[] = prim::ListConstruct(%1286, %1291, %1307), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %chunked_hidden_states.18 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.17, %1308), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:424:0
    %1310 : Long() = aten::neg(%window_overlap.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:428:0
    %1311 : int = aten::Int(%1310), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1312 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1313 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1314 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1315 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1316 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.18, %1312, %1313, %1314, %1315), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1319 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1320 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1316, %1317, %1318, %1319, %1320), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1322 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1323 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1324 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %chunked_hidden_states.19 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1321, %1322, %1323, %1311, %1324), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
    %1326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:431:0
    %1327 : Long() = aten::add(%window_overlap.4, %hidden_dim.4, %1326), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:431:0
    %1328 : int = aten::Int(%1327), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1329 : int[] = prim::ListConstruct(%1285, %1290, %1295, %1328), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %chunked_hidden_states.20 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.19, %1329), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:430:0
    %1331 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1333 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1335 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.20, %1331, %1332, %1333, %1334), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1336 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1340 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1335, %1336, %1337, %1338, %1339), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1341 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1342 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1343 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1345 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1340, %1341, %1342, %1343, %1344), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1346 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1347 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1348 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1350 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1345, %1346, %1347, %1348, %1349), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
    %1351 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/functional.py:327:0
    %1352 : Tensor[] = prim::ListConstruct(%1350, %1281), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %context.4 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%1351, %1352), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/functional.py:327:0
    %1354 : int[] = prim::ListConstruct(%1184, %1193, %1188, %1197), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1355 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.4, %1354), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:569:0
    %1356 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:569:0
    %1357 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:569:0
    %attn_output.7 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1355, %1356, %1357), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:569:0
    %1377 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
    %1378 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
    %1379 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.7, %1377, %1378), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
    %1380 : int[] = prim::ListConstruct(%27, %34, %41), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
    %1381 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1379, %1380), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
    %1382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
    %attn_output.8 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1381, %1382), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
    %1384 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:374:0
    %1385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:374:0
    %input.48 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.8, %1384, %1385), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_longformer.py:374:0
    return (%input.48)

LongformerSelfAttention.key
Linear._actual_script_module
  graph(%self.61 : __torch__.torch.nn.modules.linear.Linear,
        %input.43 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.61)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.61)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
    %output.20 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.43, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
    %key_vectors.4 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.20, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
    return (%key_vectors.4)

LongformerSelfAttention.query
Linear._actual_script_module
  graph(%self.60 : __torch__.torch.nn.modules.linear.Linear,
        %input.43 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.60)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.60)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
    %output.19 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.43, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
    %query_vectors.7 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.19, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
    return (%query_vectors.7)

LongformerSelfAttention.value
Linear._actual_script_module
  graph(%self.62 : __torch__.torch.nn.modules.linear.Linear,
        %input.43 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.62)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.62)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
    %output.21 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.43, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
    %value_vectors.4 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.21, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
    return (%value_vectors.4)

LongformerSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.66 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.50 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.66)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.66)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.12 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.50, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.12)

LongformerSelfOutput.dense
Linear._actual_script_module
  graph(%self.64 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:768, 512:13056, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.64)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.64)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
    %output.22 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
    %input.49 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.22, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.49)

LongformerSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.65 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.43 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.43)

LongformerIntermediate.dense
Linear._actual_script_module
  graph(%self.68 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.68)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.68)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
    %output.23 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
    %input.51 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.23, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.51)

LongformerOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.72 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.54 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.72)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.72)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
    %hidden_states.45 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.54, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%hidden_states.45)

LongformerOutput.dense
Linear._actual_script_module
  graph(%self.70 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1572864, 512:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.70)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.70)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
    %output.24 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
    %input.53 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.24, %2, %6), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
    return (%input.53)

LongformerOutput.dropout
Dropout._actual_script_module
  graph(%self.71 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.44 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.44)

LongformerLayer._actual_script_module
  graph(%self.73 : __torch__.transformers.modeling_longformer.LongformerLayer,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%self.73)
    %4 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%self.73)
    %5 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%self.73)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %attention_mask, %2)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

LongformerLayer.attention
LongformerAttention._actual_script_module
  graph(%self.74 : __torch__.transformers.modeling_longformer.LongformerAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%self.74)
    %4 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%self.74)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %2)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %2)
    return (%8)

LongformerLayer.intermediate
LongformerIntermediate._actual_script_module
  graph(%self.83 : __torch__.transformers.modeling_longformer.LongformerIntermediate,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.83)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.64 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
    return (%input.64)

LongformerLayer.output
LongformerOutput._actual_script_module
  graph(%self.85 : __torch__.transformers.modeling_longformer.LongformerOutput,
        %1 : Float(17:1572864, 512:3072, 3072:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.85)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.85)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.85)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output # transformers/modeling_longformer.py:830:0
    %input.66 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output # transformers/modeling_longformer.py:830:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.66)
    return (%13)

LongformerAttention.output
LongformerSelfOutput._actual_script_module
  graph(%self.79 : __torch__.transformers.modeling_longformer.LongformerSelfOutput,
        %1 : Float(17:768, 512:13056, 768:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.79)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.79)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.79)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output # transformers/modeling_longformer.py:758:0
    %input.62 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output # transformers/modeling_longformer.py:758:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.62)
    return (%13)

LongformerAttention.self
LongformerSelfAttention._actual_script_module
  graph(%self.75 : __torch__.transformers.modeling_longformer.LongformerSelfAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.75)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.75)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.75)
    %6 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:241:0
    %7 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:241:0
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:241:0
    %9 : Float(17:512, 512:1) = aten::squeeze(%7, %8), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:241:0
    %10 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:22:0
    %is_index_masked.5 : Bool(17:512, 512:1) = aten::lt(%9, %10), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:22:0
    %18 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:248:0
    %19 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:248:0
    %input.55 : Float(512:768, 17:393216, 768:1) = aten::transpose(%2, %18, %19), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:248:0
    %1387 : Tensor = prim::CallMethod[name="forward"](%5, %input.55)
    %1388 : Tensor = prim::CallMethod[name="forward"](%4, %input.55)
    %1389 : Tensor = prim::CallMethod[name="forward"](%3, %input.55)
    %24 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
    %25 : int = aten::size(%input.55, %24), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
    %seq_len.18 : Long() = prim::NumToTensor(%25), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %27 : int = aten::Int(%seq_len.18), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %28 : int = aten::Int(%seq_len.18), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %29 : int = aten::Int(%seq_len.18), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %30 : int = aten::Int(%seq_len.18), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %31 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
    %32 : int = aten::size(%input.55, %31), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
    %batch_size.17 : Long() = prim::NumToTensor(%32), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %34 : int = aten::Int(%batch_size.17), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %35 : int = aten::Int(%batch_size.17), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %36 : int = aten::Int(%batch_size.17), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %37 : int = aten::Int(%batch_size.17), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %38 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
    %39 : int = aten::size(%input.55, %38), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
    %embed_dim.5 : Long() = prim::NumToTensor(%39), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %41 : int = aten::Int(%embed_dim.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %44 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:261:0
    %query_vectors.10 : Float(512:13056, 17:768, 768:1) = aten::div_(%1387, %44), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:261:0
    %46 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:263:0
    %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:263:0
    %48 : int[] = prim::ListConstruct(%30, %37, %46, %47), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %49 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.10, %48), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:263:0
    %50 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:263:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:263:0
    %query.9 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%49, %50, %51), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:263:0
    %53 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:264:0
    %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:264:0
    %55 : int[] = prim::ListConstruct(%29, %36, %53, %54), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %56 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1388, %55), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:264:0
    %57 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:264:0
    %58 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:264:0
    %key.5 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%56, %57, %58), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:264:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %61 : int = aten::size(%query.9, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.18 : Long() = prim::NumToTensor(%61), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %63 : int = aten::Int(%batch_size.18), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %65 : int = aten::size(%query.9, %64), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.19 : Long() = prim::NumToTensor(%65), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %67 : int = aten::Int(%seq_len.19), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %68 : int = aten::Int(%seq_len.19), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %69 : int = aten::Int(%seq_len.19), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %70 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %71 : int = aten::size(%query.9, %70), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.13 : Long() = prim::NumToTensor(%71), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %73 : int = aten::Int(%num_heads.13), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %74 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %75 : int = aten::size(%query.9, %74), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.13 : Long() = prim::NumToTensor(%75), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %77 : int = aten::Int(%head_dim.13), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %78 : int = aten::Int(%head_dim.13), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %111 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %112 : Long() = aten::floor_divide(%seq_len.19, %111), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %113 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:478:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.13 : Long() = aten::sub(%112, %113, %114), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:478:0
    %116 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
    %117 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
    %118 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.9, %116, %117), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
    %119 : Long() = aten::mul(%batch_size.18, %num_heads.13), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
    %120 : int = aten::Int(%119), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %121 : int[] = prim::ListConstruct(%120, %69, %78), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %hidden_states.46 : Float(204:64, 512:13056, 64:1) = aten::reshape(%118, %121), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
    %123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
    %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
    %125 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.5, %123, %124), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
    %126 : Long() = aten::mul(%batch_size.18, %num_heads.13), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
    %127 : int = aten::Int(%126), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %128 : int[] = prim::ListConstruct(%127, %68, %77), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %hidden_states.48 : Float(204:64, 512:13056, 64:1) = aten::reshape(%125, %128), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
    %130 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
    %131 : int = aten::size(%hidden_states.46, %130), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
    %132 : Long() = prim::NumToTensor(%131), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %133 : int = aten::Int(%132), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %134 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
    %135 : int = aten::size(%hidden_states.46, %134), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
    %136 : Long() = prim::NumToTensor(%135), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %137 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %138 : Long() = aten::floor_divide(%136, %137), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %139 : int = aten::Int(%138), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %140 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
    %141 : int = aten::size(%hidden_states.46, %140), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
    %142 : Long() = prim::NumToTensor(%141), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %143 : int = aten::Int(%142), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %144 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
    %145 : int[] = prim::ListConstruct(%133, %139, %144, %143), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %hidden_states.47 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.46, %145), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
    %147 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %148 : int = aten::size(%hidden_states.47, %147), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %149 : Long() = prim::NumToTensor(%148), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %150 : int = aten::Int(%149), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %151 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %152 : int = aten::size(%hidden_states.47, %151), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %153 : Long() = prim::NumToTensor(%152), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %154 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %155 : int = aten::size(%hidden_states.47, %154), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %156 : Long() = prim::NumToTensor(%155), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %157 : int = aten::Int(%156), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %158 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %159 : int = aten::size(%hidden_states.47, %158), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %160 : Long() = prim::NumToTensor(%159), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %161 : int = aten::Int(%160), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %162 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %163 : Long() = aten::mul(%153, %162), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %164 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %165 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %166 : Long() = aten::sub(%163, %164, %165), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %167 : int = aten::Int(%166), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %168 : int[] = prim::ListConstruct(%150, %167, %157, %161), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %169 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %170 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %171 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %172 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %173 : int[] = prim::ListConstruct(%169, %170, %171, %172), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %174 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %175 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.47, %168, %173, %174), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %176 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
    %177 : int = aten::size(%hidden_states.48, %176), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %179 : int = aten::Int(%178), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
    %181 : int = aten::size(%hidden_states.48, %180), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
    %182 : Long() = prim::NumToTensor(%181), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %183 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %184 : Long() = aten::floor_divide(%182, %183), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %185 : int = aten::Int(%184), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %186 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
    %187 : int = aten::size(%hidden_states.48, %186), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
    %188 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %189 : int = aten::Int(%188), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %190 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
    %191 : int[] = prim::ListConstruct(%179, %185, %190, %189), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %hidden_states.49 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.48, %191), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
    %193 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %194 : int = aten::size(%hidden_states.49, %193), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %195 : Long() = prim::NumToTensor(%194), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %196 : int = aten::Int(%195), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %197 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %198 : int = aten::size(%hidden_states.49, %197), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %199 : Long() = prim::NumToTensor(%198), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %200 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %201 : int = aten::size(%hidden_states.49, %200), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %203 : int = aten::Int(%202), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %204 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %205 : int = aten::size(%hidden_states.49, %204), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %207 : int = aten::Int(%206), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %208 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %209 : Long() = aten::mul(%199, %208), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %210 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %211 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %212 : Long() = aten::sub(%209, %210, %211), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %213 : int = aten::Int(%212), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %214 : int[] = prim::ListConstruct(%196, %213, %203, %207), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %216 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %217 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %218 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %219 : int[] = prim::ListConstruct(%215, %216, %217, %218), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %220 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.49, %214, %219, %220), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %222 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/functional.py:327:0
    %223 : Tensor[] = prim::ListConstruct(%175, %221), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %input.56 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%222, %223), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/functional.py:327:0
    %225 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %226 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %227 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %229 : int[] = prim::ListConstruct(%225, %226, %227, %228), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %230 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.9 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.56, %229, %230), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %232 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %233 : int = aten::size(%hidden_states_padded.9, %232), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %234 : Long() = prim::NumToTensor(%233), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %235 : int = aten::Int(%234), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %236 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %237 : int = aten::size(%hidden_states_padded.9, %236), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %238 : Long() = prim::NumToTensor(%237), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %239 : int = aten::Int(%238), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %246 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %247 : int = aten::size(%hidden_states_padded.9, %246), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %248 : Long() = prim::NumToTensor(%247), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %249 : int = aten::Int(%248), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %250 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %251 : int = aten::size(%hidden_states_padded.9, %250), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %252 : Long() = prim::NumToTensor(%251), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %253 : int = aten::Int(%252), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %254 : int[] = prim::ListConstruct(%235, %239, %249, %253), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %diagonal_chunked_attention_scores.9 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.9, %254), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:400:0
    %256 : Long() = aten::mul(%batch_size.18, %num_heads.13), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
    %257 : int = aten::Int(%256), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %258 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
    %259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
    %260 : Long() = aten::add(%chunks_count.13, %258, %259), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
    %261 : int = aten::Int(%260), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %263 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %264 : int[] = prim::ListConstruct(%257, %261, %262, %263), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %265 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %266 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %268 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.9 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.9, %264, %265, %266, %267, %268), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %270 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %271 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %273 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %270, %271, %272, %273), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %275 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %276 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %277 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %279 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%274, %275, %276, %277, %278), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %280 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %281 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %282 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %284 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%279, %280, %281, %282, %283), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %285 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %286 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %287 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %289 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%284, %285, %286, %287, %288), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %290 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %291 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %292 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %294 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %290, %291, %292, %293), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %295 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %296 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %297 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %299 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%294, %295, %296, %297, %298), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %300 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %301 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %302 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %303 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %304 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%299, %300, %301, %302, %303), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %305 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %306 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %307 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %308 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %309 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%304, %305, %306, %307, %308), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %310 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %311 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %312 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %313 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %314 : int[] = prim::ListConstruct(%310, %311, %312, %313), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %315 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%289, %314), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %316 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %317 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%309, %315, %316), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %319 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %320 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %321 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %322 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %318, %319, %320, %321), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %323 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %324 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %325 : Float(204:262656, 512:513, 513:1) = aten::select(%322, %323, %324), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %327 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %328 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %329 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %330 : Float(204:262656, 256:513, 513:1) = aten::slice(%325, %326, %327, %328, %329), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %331 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %333 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %335 : Float(204:262656, 256:513, 257:1) = aten::slice(%330, %331, %332, %333, %334), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %336 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %340 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %336, %337, %338, %339), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %341 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %342 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %343 : Float(204:262656, 256:513, 513:1) = aten::select(%340, %341, %342), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %345 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %347 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %348 : Float(204:262656, 256:513, 513:1) = aten::slice(%343, %344, %345, %346, %347), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %349 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %350 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %352 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %353 : Float(204:262656, 256:513, 257:1) = aten::slice(%348, %349, %350, %351, %352), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %354 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %355 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %356 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %357 : int[] = prim::ListConstruct(%354, %355, %356), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %358 : Float(204:262656, 256:513, 257:1) = aten::view(%335, %357), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %359 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %360 : Float(204:262656, 256:513, 257:1) = aten::copy_(%353, %358, %359), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %361 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %362 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %363 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %364 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %365 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %361, %362, %363, %364), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %366 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %367 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %368 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %369 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %370 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%365, %366, %367, %368, %369), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %371 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %372 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %373 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %374 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %375 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%370, %371, %372, %373, %374), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %376 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %377 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %379 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %380 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%375, %376, %377, %378, %379), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %383 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %384 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %385 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %381, %382, %383, %384), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %386 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %387 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %388 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %389 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %390 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%385, %386, %387, %388, %389), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %391 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %392 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %393 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %394 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %395 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%390, %391, %392, %393, %394), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %397 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %398 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %399 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %400 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%395, %396, %397, %398, %399), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %401 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %402 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %403 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %404 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %405 : int[] = prim::ListConstruct(%401, %402, %403, %404), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %406 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%380, %405), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %407 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %408 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%400, %406, %407), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %409 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %410 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %411 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %412 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %413 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %409, %410, %411, %412), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %414 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %415 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %416 : Float(204:262656, 512:513, 513:1) = aten::select(%413, %414, %415), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %417 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %418 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %419 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %420 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %421 : Float(204:262656, 255:513, 513:1) = aten::slice(%416, %417, %418, %419, %420), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %422 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %423 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %425 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %426 : Float(204:262656, 255:513, 255:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %427 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %429 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %430 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %431 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %427, %428, %429, %430), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %432 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %433 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %434 : Float(204:262656, 256:513, 513:1) = aten::select(%431, %432, %433), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %435 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %436 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %437 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %438 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %439 : Float(204:262656, 255:513, 513:1) = aten::slice(%434, %435, %436, %437, %438), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %440 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %441 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %442 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %443 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %444 : Float(204:262656, 255:513, 255:1) = aten::slice(%439, %440, %441, %442, %443), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %445 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %446 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %447 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %448 : int[] = prim::ListConstruct(%445, %446, %447), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %449 : Float(204:262656, 255:513, 255:1) = aten::view(%426, %448), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %450 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %451 : Float(204:262656, 255:513, 255:1) = aten::copy_(%444, %449, %450), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %452 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
    %453 : int[] = prim::ListConstruct(%63, %73, %67, %452), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %454 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.9, %453), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
    %455 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
    %456 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.13 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%454, %455, %456), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
    %458 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %459 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %460 : int[] = prim::ListConstruct(%458, %459), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %461 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %462 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %463 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %464 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %465 : Float(256:257, 257:1) = aten::ones(%460, %461, %462, %463, %464), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %466 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %467 : Float(256:257, 257:1) = aten::tril(%465, %466), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %468 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %469 : int[] = prim::ListConstruct(%468), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %beginning_mask_2d.9 : Float(256:257, 257:1) = aten::flip(%467, %469), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %471 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %472 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.9, %471), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %473 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %474 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %475 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %476 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %477 : Float(1:65792, 256:257, 257:1) = aten::slice(%472, %473, %474, %475, %476), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %478 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %479 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%477, %478), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %480 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %481 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %482 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %483 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.9 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%479, %480, %481, %482, %483), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %485 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:460:0
    %486 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:460:0
    %487 : int[] = prim::ListConstruct(%485, %486), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %ending_mask.9 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.9, %487), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:460:0
    %489 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %490 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %491 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %493 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.13, %489, %490, %491, %492), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %494 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %495 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %496 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %497 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %498 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%493, %494, %495, %496, %497), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %499 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %500 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %501 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %502 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %503 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%498, %499, %500, %501, %502), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %504 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %505 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %506 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %507 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.9 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%503, %504, %505, %506, %507), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %509 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %510 : int = aten::size(%beginning_input.9, %509), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %511 : Long() = prim::NumToTensor(%510), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %512 : int = aten::Int(%511), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %513 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %514 : int = aten::size(%beginning_input.9, %513), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %515 : Long() = prim::NumToTensor(%514), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %516 : int = aten::Int(%515), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %517 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %518 : int = aten::size(%beginning_input.9, %517), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %519 : Long() = prim::NumToTensor(%518), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %520 : int = aten::Int(%519), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %521 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %522 : int = aten::size(%beginning_input.9, %521), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %523 : Long() = prim::NumToTensor(%522), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %524 : int = aten::Int(%523), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %525 : int[] = prim::ListConstruct(%512, %516, %520, %524), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %526 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %527 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.9, %525, %526), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %528 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:22:0
    %529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%527, %528), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:22:0
    %530 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:463:0
    %531 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.9, %529, %530), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:463:0
    %532 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %533 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %534 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %535 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %536 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.13, %532, %533, %534, %535), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %537 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %538 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %539 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %541 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%536, %537, %538, %539, %540), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %542 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %543 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %544 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %545 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %546 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%541, %542, %543, %544, %545), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %547 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %548 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %549 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.9 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%546, %547, %548, %549, %550), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %552 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %553 : int = aten::size(%ending_input.9, %552), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %554 : Long() = prim::NumToTensor(%553), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %555 : int = aten::Int(%554), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %556 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %557 : int = aten::size(%ending_input.9, %556), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %558 : Long() = prim::NumToTensor(%557), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %559 : int = aten::Int(%558), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %560 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %561 : int = aten::size(%ending_input.9, %560), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %562 : Long() = prim::NumToTensor(%561), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %563 : int = aten::Int(%562), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %564 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %565 : int = aten::size(%ending_input.9, %564), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %566 : Long() = prim::NumToTensor(%565), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %567 : int = aten::Int(%566), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %568 : int[] = prim::ListConstruct(%555, %559, %563, %567), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %569 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %570 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.9, %568, %569), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %571 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:22:0
    %572 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%570, %571), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:22:0
    %573 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:466:0
    %574 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.9, %572, %573), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:466:0
    %575 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:22:0
    %576 : Bool(17:512, 512:1) = aten::ne(%9, %575), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:22:0
    %577 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %578 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %579 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %580 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %581 : Bool(17:512, 512:1) = aten::slice(%576, %577, %578, %579, %580), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %582 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %583 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %584 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %585 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %586 : Bool(17:512, 512:1) = aten::slice(%581, %582, %583, %584, %585), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %587 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %588 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%586, %587), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %589 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %remove_from_windowed_attention_mask.5 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%588, %589), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
    %591 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.5, %query.9), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:275:0
    %592 : float = prim::Constant[value=-10000.](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:275:0
    %float_mask.5 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%591, %remove_from_windowed_attention_mask.5, %592), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:275:0
    %594 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %595 : int = aten::size(%float_mask.5, %594), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %596 : Long() = prim::NumToTensor(%595), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %597 : int = aten::Int(%596), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %598 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %599 : int = aten::size(%float_mask.5, %598), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %600 : Long() = prim::NumToTensor(%599), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %601 : int = aten::Int(%600), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %602 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %603 : int = aten::size(%float_mask.5, %602), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %604 : Long() = prim::NumToTensor(%603), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %605 : int = aten::Int(%604), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %606 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %607 : int = aten::size(%float_mask.5, %606), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %608 : Long() = prim::NumToTensor(%607), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %609 : int = aten::Int(%608), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %610 : int[] = prim::ListConstruct(%597, %601, %605, %609), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %611 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %612 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %613 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %614 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %query.10 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%610, %611, %612, %613, %614), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
    %616 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %617 : int = aten::size(%query.10, %616), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.19 : Long() = prim::NumToTensor(%617), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %619 : int = aten::Int(%batch_size.19), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %620 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %621 : int = aten::size(%query.10, %620), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.20 : Long() = prim::NumToTensor(%621), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %623 : int = aten::Int(%seq_len.20), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %624 : int = aten::Int(%seq_len.20), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %625 : int = aten::Int(%seq_len.20), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %626 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %627 : int = aten::size(%query.10, %626), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.14 : Long() = prim::NumToTensor(%627), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %629 : int = aten::Int(%num_heads.14), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %630 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %631 : int = aten::size(%query.10, %630), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.14 : Long() = prim::NumToTensor(%631), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %633 : int = aten::Int(%head_dim.14), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %634 : int = aten::Int(%head_dim.14), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %667 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %668 : Long() = aten::floor_divide(%seq_len.20, %667), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %669 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:478:0
    %670 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.14 : Long() = aten::sub(%668, %669, %670), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:478:0
    %672 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
    %673 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
    %674 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.10, %672, %673), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
    %675 : Long() = aten::mul(%batch_size.19, %num_heads.14), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
    %676 : int = aten::Int(%675), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %677 : int[] = prim::ListConstruct(%676, %625, %634), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %hidden_states.50 : Float(17:512, 512:1, 1:1) = aten::reshape(%674, %677), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
    %679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
    %680 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
    %681 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.5, %679, %680), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
    %682 : Long() = aten::mul(%batch_size.19, %num_heads.14), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
    %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %684 : int[] = prim::ListConstruct(%683, %624, %633), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %hidden_states.52 : Float(17:512, 512:1, 1:1) = aten::reshape(%681, %684), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
    %686 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
    %687 : int = aten::size(%hidden_states.50, %686), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
    %688 : Long() = prim::NumToTensor(%687), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %690 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
    %691 : int = aten::size(%hidden_states.50, %690), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
    %692 : Long() = prim::NumToTensor(%691), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %693 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %694 : Long() = aten::floor_divide(%692, %693), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %696 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
    %697 : int = aten::size(%hidden_states.50, %696), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
    %698 : Long() = prim::NumToTensor(%697), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %699 : int = aten::Int(%698), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %700 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
    %701 : int[] = prim::ListConstruct(%689, %695, %700, %699), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %hidden_states.51 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.50, %701), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
    %703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %704 : int = aten::size(%hidden_states.51, %703), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %705 : Long() = prim::NumToTensor(%704), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %706 : int = aten::Int(%705), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %708 : int = aten::size(%hidden_states.51, %707), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %709 : Long() = prim::NumToTensor(%708), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %710 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %711 : int = aten::size(%hidden_states.51, %710), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %712 : Long() = prim::NumToTensor(%711), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %713 : int = aten::Int(%712), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %714 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %715 : int = aten::size(%hidden_states.51, %714), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %716 : Long() = prim::NumToTensor(%715), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %717 : int = aten::Int(%716), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %718 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %719 : Long() = aten::mul(%709, %718), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %720 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %721 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %722 : Long() = aten::sub(%719, %720, %721), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %723 : int = aten::Int(%722), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %724 : int[] = prim::ListConstruct(%706, %723, %713, %717), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %725 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %726 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %727 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %728 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %729 : int[] = prim::ListConstruct(%725, %726, %727, %728), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %730 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %731 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.51, %724, %729, %730), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %732 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
    %733 : int = aten::size(%hidden_states.52, %732), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
    %734 : Long() = prim::NumToTensor(%733), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %735 : int = aten::Int(%734), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %736 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
    %737 : int = aten::size(%hidden_states.52, %736), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
    %738 : Long() = prim::NumToTensor(%737), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %739 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %740 : Long() = aten::floor_divide(%738, %739), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %741 : int = aten::Int(%740), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
    %743 : int = aten::size(%hidden_states.52, %742), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
    %744 : Long() = prim::NumToTensor(%743), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %745 : int = aten::Int(%744), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %746 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
    %747 : int[] = prim::ListConstruct(%735, %741, %746, %745), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %hidden_states.53 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.52, %747), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
    %749 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %750 : int = aten::size(%hidden_states.53, %749), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %751 : Long() = prim::NumToTensor(%750), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %752 : int = aten::Int(%751), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %754 : int = aten::size(%hidden_states.53, %753), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %755 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %756 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %757 : int = aten::size(%hidden_states.53, %756), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %758 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %759 : int = aten::Int(%758), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %760 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %761 : int = aten::size(%hidden_states.53, %760), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
    %762 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %764 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %765 : Long() = aten::mul(%755, %764), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %766 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %767 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %768 : Long() = aten::sub(%765, %766, %767), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
    %769 : int = aten::Int(%768), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %770 : int[] = prim::ListConstruct(%752, %769, %759, %763), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %771 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %772 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %773 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %776 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.53, %770, %775, %776), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
    %778 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/functional.py:327:0
    %779 : Tensor[] = prim::ListConstruct(%731, %777), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %input.57 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%778, %779), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/functional.py:327:0
    %781 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %782 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %783 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %786 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.10 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.57, %785, %786), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %788 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %789 : int = aten::size(%hidden_states_padded.10, %788), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %790 : Long() = prim::NumToTensor(%789), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %791 : int = aten::Int(%790), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %792 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %793 : int = aten::size(%hidden_states_padded.10, %792), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %794 : Long() = prim::NumToTensor(%793), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %795 : int = aten::Int(%794), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %802 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %803 : int = aten::size(%hidden_states_padded.10, %802), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %804 : Long() = prim::NumToTensor(%803), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %805 : int = aten::Int(%804), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %806 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %807 : int = aten::size(%hidden_states_padded.10, %806), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
    %808 : Long() = prim::NumToTensor(%807), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %809 : int = aten::Int(%808), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %810 : int[] = prim::ListConstruct(%791, %795, %805, %809), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %diagonal_chunked_attention_scores.10 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.10, %810), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:400:0
    %812 : Long() = aten::mul(%batch_size.19, %num_heads.14), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
    %813 : int = aten::Int(%812), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %814 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
    %815 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
    %816 : Long() = aten::add(%chunks_count.14, %814, %815), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
    %817 : int = aten::Int(%816), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %818 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %819 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %820 : int[] = prim::ListConstruct(%813, %817, %818, %819), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %821 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %822 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %823 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %824 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.10 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.10, %820, %821, %822, %823, %824), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
    %826 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %827 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %828 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %829 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %830 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %826, %827, %828, %829), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %831 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %832 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %833 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %834 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%830, %831, %832, %833, %834), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %836 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %837 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %838 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %839 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%835, %836, %837, %838, %839), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %841 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %842 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %843 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %845 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%840, %841, %842, %843, %844), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %846 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %847 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %848 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %849 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %846, %847, %848, %849), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %851 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %852 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %853 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %855 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%850, %851, %852, %853, %854), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %856 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %857 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %858 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %859 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %860 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%855, %856, %857, %858, %859), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %861 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %862 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %863 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %864 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %865 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%860, %861, %862, %863, %864), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %866 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %867 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %868 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %869 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %870 : int[] = prim::ListConstruct(%866, %867, %868, %869), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %871 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%845, %870), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %872 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %873 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%865, %871, %872), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
    %874 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %875 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %876 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %877 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %874, %875, %876, %877), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %879 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %880 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %881 : Float(17:262656, 512:513, 513:1) = aten::select(%878, %879, %880), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %883 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %884 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %885 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %886 : Float(17:262656, 256:513, 513:1) = aten::slice(%881, %882, %883, %884, %885), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %887 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %889 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%886, %887, %888, %889, %890), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %892 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %893 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %894 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %895 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %896 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %892, %893, %894, %895), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %897 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %898 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %899 : Float(17:262656, 256:513, 513:1) = aten::select(%896, %897, %898), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %901 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %902 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %903 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %904 : Float(17:262656, 256:513, 513:1) = aten::slice(%899, %900, %901, %902, %903), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %905 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %906 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %907 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %909 : Float(17:262656, 256:513, 257:1) = aten::slice(%904, %905, %906, %907, %908), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %910 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %911 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %912 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %913 : int[] = prim::ListConstruct(%910, %911, %912), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %914 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %913), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %915 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %916 : Float(17:262656, 256:513, 257:1) = aten::copy_(%909, %914, %915), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
    %917 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %918 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %919 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %920 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %921 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %917, %918, %919, %920), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %922 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %923 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %924 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %925 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %926 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%921, %922, %923, %924, %925), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %927 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %928 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %929 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %930 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %931 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%926, %927, %928, %929, %930), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %932 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %933 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %934 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %935 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %936 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%931, %932, %933, %934, %935), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %938 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %939 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %940 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %941 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %937, %938, %939, %940), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %942 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %943 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %944 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %945 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %946 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%941, %942, %943, %944, %945), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %947 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %948 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %949 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %950 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %951 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%946, %947, %948, %949, %950), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %952 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %953 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %954 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %955 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %956 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%951, %952, %953, %954, %955), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %957 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %959 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %960 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %962 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%936, %961), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %963 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %964 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%956, %962, %963), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
    %965 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %966 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %967 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %969 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %965, %966, %967, %968), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %970 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %971 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %972 : Float(17:262656, 512:513, 513:1) = aten::select(%969, %970, %971), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %973 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %974 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %975 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %976 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %977 : Float(17:262656, 255:513, 513:1) = aten::slice(%972, %973, %974, %975, %976), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %978 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %979 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %980 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %981 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %982 : Float(17:262656, 255:513, 255:1) = aten::slice(%977, %978, %979, %980, %981), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %983 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %984 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %985 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %986 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %987 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %983, %984, %985, %986), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %988 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %989 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %990 : Float(17:262656, 256:513, 513:1) = aten::select(%987, %988, %989), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %991 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %993 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %994 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %995 : Float(17:262656, 255:513, 513:1) = aten::slice(%990, %991, %992, %993, %994), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %996 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %998 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %999 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %1000 : Float(17:262656, 255:513, 255:1) = aten::slice(%995, %996, %997, %998, %999), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %1001 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %1002 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %1003 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %1004 : int[] = prim::ListConstruct(%1001, %1002, %1003), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1005 : Float(17:262656, 255:513, 255:1) = aten::view(%982, %1004), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %1006 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1007 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1000, %1005, %1006), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
    %1008 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
    %1009 : int[] = prim::ListConstruct(%619, %629, %623, %1008), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1010 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.10, %1009), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
    %1011 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
    %1012 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.14 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1010, %1011, %1012), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
    %1014 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %1015 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %1016 : int[] = prim::ListConstruct(%1014, %1015), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1017 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %1018 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %1019 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %1020 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %1021 : Float(256:257, 257:1) = aten::ones(%1016, %1017, %1018, %1019, %1020), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %1022 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %1023 : Float(256:257, 257:1) = aten::tril(%1021, %1022), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %1024 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %1025 : int[] = prim::ListConstruct(%1024), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %beginning_mask_2d.10 : Float(256:257, 257:1) = aten::flip(%1023, %1025), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
    %1027 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1028 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.10, %1027), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1029 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1030 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1031 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1032 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1033 : Float(1:65792, 256:257, 257:1) = aten::slice(%1028, %1029, %1030, %1031, %1032), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1034 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1035 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1033, %1034), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1036 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1037 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1038 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1039 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.10 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1035, %1036, %1037, %1038, %1039), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
    %1041 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:460:0
    %1042 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:460:0
    %1043 : int[] = prim::ListConstruct(%1041, %1042), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %ending_mask.10 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.10, %1043), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:460:0
    %1045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1046 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1047 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1048 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1049 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.14, %1045, %1046, %1047, %1048), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1050 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1052 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1053 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1054 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1049, %1050, %1051, %1052, %1053), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1055 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1056 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1057 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1058 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1059 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1054, %1055, %1056, %1057, %1058), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1060 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1061 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1062 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.10 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1059, %1060, %1061, %1062, %1063), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
    %1065 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %1066 : int = aten::size(%beginning_input.10, %1065), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %1067 : Long() = prim::NumToTensor(%1066), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1068 : int = aten::Int(%1067), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1069 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %1070 : int = aten::size(%beginning_input.10, %1069), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %1071 : Long() = prim::NumToTensor(%1070), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1072 : int = aten::Int(%1071), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1073 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %1074 : int = aten::size(%beginning_input.10, %1073), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %1075 : Long() = prim::NumToTensor(%1074), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1076 : int = aten::Int(%1075), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1077 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %1078 : int = aten::size(%beginning_input.10, %1077), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %1079 : Long() = prim::NumToTensor(%1078), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1080 : int = aten::Int(%1079), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1081 : int[] = prim::ListConstruct(%1068, %1072, %1076, %1080), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1082 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %1083 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.10, %1081, %1082), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
    %1084 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:22:0
    %1085 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1083, %1084), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:22:0
    %1086 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:463:0
    %1087 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.10, %1085, %1086), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:463:0
    %1088 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1089 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1090 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1091 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1092 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.14, %1088, %1089, %1090, %1091), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1094 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1095 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1096 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1097 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1092, %1093, %1094, %1095, %1096), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1098 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1099 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1100 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1101 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1102 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1097, %1098, %1099, %1100, %1101), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1103 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1104 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1105 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1106 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.10 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1102, %1103, %1104, %1105, %1106), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
    %1108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %1109 : int = aten::size(%ending_input.10, %1108), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %1110 : Long() = prim::NumToTensor(%1109), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1111 : int = aten::Int(%1110), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1112 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %1113 : int = aten::size(%ending_input.10, %1112), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %1114 : Long() = prim::NumToTensor(%1113), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1115 : int = aten::Int(%1114), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1116 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %1117 : int = aten::size(%ending_input.10, %1116), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %1118 : Long() = prim::NumToTensor(%1117), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1119 : int = aten::Int(%1118), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1120 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %1121 : int = aten::size(%ending_input.10, %1120), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %1122 : Long() = prim::NumToTensor(%1121), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1124 : int[] = prim::ListConstruct(%1111, %1115, %1119, %1123), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1125 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %1126 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.10, %1124, %1125), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
    %1127 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:22:0
    %1128 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1126, %1127), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:22:0
    %1129 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:466:0
    %1130 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.10, %1128, %1129), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:466:0
    %1131 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:284:0
    %attn_scores.5 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.13, %input_tensor.14, %1131), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:284:0
    %1151 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:1500:0
    %1152 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:1500:0
    %attn_probs_fp32.5 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.5, %1151, %1152), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:1500:0
    %attn_probs.9 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::type_as(%attn_probs_fp32.5, %attn_scores.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:320:0
    %1155 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1156 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1157 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1158 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1159 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.5, %1155, %1156, %1157, %1158), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1161 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1162 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1163 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1164 : Bool(17:512, 512:1) = aten::slice(%1159, %1160, %1161, %1162, %1163), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1165 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1166 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1164, %1165), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1167 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1168 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1166, %1167), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1169 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %input.58 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs.9, %1168, %1169), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
    %1171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:973:0
    %1172 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:973:0
    %attn_probs.10 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.58, %1171, %1172), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:973:0
    %1174 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:331:0
    %1175 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:331:0
    %1176 : int[] = prim::ListConstruct(%28, %35, %1174, %1175), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1177 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1389, %1176), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:331:0
    %1178 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:331:0
    %1179 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:331:0
    %value.5 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1177, %1178, %1179), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:331:0
    %1181 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
    %1182 : int = aten::size(%value.5, %1181), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
    %batch_size.20 : Long() = prim::NumToTensor(%1182), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1184 : int = aten::Int(%batch_size.20), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1185 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
    %1186 : int = aten::size(%value.5, %1185), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
    %seq_len.21 : Long() = prim::NumToTensor(%1186), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1188 : int = aten::Int(%seq_len.21), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1189 : int = aten::Int(%seq_len.21), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1190 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
    %1191 : int = aten::size(%value.5, %1190), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
    %num_heads.15 : Long() = prim::NumToTensor(%1191), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1193 : int = aten::Int(%num_heads.15), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1194 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
    %1195 : int = aten::size(%value.5, %1194), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
    %head_dim.15 : Long() = prim::NumToTensor(%1195), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1197 : int = aten::Int(%head_dim.15), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1198 : int = aten::Int(%head_dim.15), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1199 : int = aten::Int(%head_dim.15), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1236 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %1237 : Long() = aten::floor_divide(%seq_len.21, %1236), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %1238 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:542:0
    %1239 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:542:0
    %chunks_count.15 : Long() = aten::sub(%1237, %1238, %1239), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:542:0
    %1241 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:545:0
    %1242 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:545:0
    %1243 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.10, %1241, %1242), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:545:0
    %1244 : Long() = aten::mul(%batch_size.20, %num_heads.15), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:546:0
    %1245 : int = aten::Int(%1244), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1246 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %1247 : Long() = aten::floor_divide(%seq_len.21, %1246), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/tensor.py:424:0
    %1248 : int = aten::Int(%1247), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1249 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:545:0
    %1250 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:545:0
    %1251 : int[] = prim::ListConstruct(%1245, %1248, %1249, %1250), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %chunked_hidden_states.21 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1243, %1251), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:545:0
    %1253 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
    %1254 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
    %1255 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.5, %1253, %1254), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
    %1256 : Long() = aten::mul(%batch_size.20, %num_heads.15), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
    %1257 : int = aten::Int(%1256), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1258 : int[] = prim::ListConstruct(%1257, %1189, %1199), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %input.59 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1255, %1258), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
    %1260 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %1261 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %1262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %1263 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %1264 : int[] = prim::ListConstruct(%1260, %1261, %1262, %1263), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1265 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %padded_value.5 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.59, %1264, %1265), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %1267 : Long() = aten::mul(%batch_size.20, %num_heads.15), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:556:0
    %1268 : int = aten::Int(%1267), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1269 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:556:0
    %1270 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:556:0
    %1271 : Long() = aten::add(%chunks_count.15, %1269, %1270), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:556:0
    %1272 : int = aten::Int(%1271), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:564:0
    %1274 : int[] = prim::ListConstruct(%1268, %1272, %1273, %1198), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1275 : int = prim::Constant[value=65536](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:564:0
    %1276 : int = prim::Constant[value=16384](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:564:0
    %1277 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:564:0
    %1278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:564:0
    %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1280 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1281 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.5, %1274, %1279, %1280), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:564:0
    %1282 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
    %1283 : int = aten::size(%chunked_hidden_states.21, %1282), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
    %total_num_heads.5 : Long() = prim::NumToTensor(%1283), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1285 : int = aten::Int(%total_num_heads.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1286 : int = aten::Int(%total_num_heads.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1287 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
    %1288 : int = aten::size(%chunked_hidden_states.21, %1287), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
    %num_chunks.5 : Long() = prim::NumToTensor(%1288), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1290 : int = aten::Int(%num_chunks.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1291 : int = aten::Int(%num_chunks.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1292 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
    %1293 : int = aten::size(%chunked_hidden_states.21, %1292), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
    %window_overlap.5 : Long() = prim::NumToTensor(%1293), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1295 : int = aten::Int(%window_overlap.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1296 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
    %1297 : int = aten::size(%chunked_hidden_states.21, %1296), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
    %hidden_dim.5 : Long() = prim::NumToTensor(%1297), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1299 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:422:0
    %1300 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:422:0
    %1301 : Long() = aten::add(%window_overlap.5, %1299, %1300), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:422:0
    %1302 : int = aten::Int(%1301), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1303 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %1304 : int[] = prim::ListConstruct(%1303, %1302), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1305 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %chunked_hidden_states.22 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.21, %1304, %1305), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
    %1307 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:424:0
    %1308 : int[] = prim::ListConstruct(%1286, %1291, %1307), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %chunked_hidden_states.23 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.22, %1308), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:424:0
    %1310 : Long() = aten::neg(%window_overlap.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:428:0
    %1311 : int = aten::Int(%1310), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1312 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1313 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1314 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1315 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1316 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.23, %1312, %1313, %1314, %1315), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1319 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1320 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1316, %1317, %1318, %1319, %1320), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1322 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1323 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1324 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %chunked_hidden_states.24 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1321, %1322, %1323, %1311, %1324), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
    %1326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:431:0
    %1327 : Long() = aten::add(%window_overlap.5, %hidden_dim.5, %1326), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:431:0
    %1328 : int = aten::Int(%1327), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1329 : int[] = prim::ListConstruct(%1285, %1290, %1295, %1328), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %chunked_hidden_states.25 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.24, %1329), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:430:0
    %1331 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1333 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1335 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.25, %1331, %1332, %1333, %1334), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1336 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1340 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1335, %1336, %1337, %1338, %1339), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1341 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1342 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1343 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1345 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1340, %1341, %1342, %1343, %1344), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1346 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1347 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1348 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1350 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1345, %1346, %1347, %1348, %1349), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
    %1351 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/functional.py:327:0
    %1352 : Tensor[] = prim::ListConstruct(%1350, %1281), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %context.5 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%1351, %1352), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/functional.py:327:0
    %1354 : int[] = prim::ListConstruct(%1184, %1193, %1188, %1197), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1355 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.5, %1354), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:569:0
    %1356 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:569:0
    %1357 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:569:0
    %attn_output.9 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1355, %1356, %1357), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:569:0
    %1377 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
    %1378 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
    %1379 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.9, %1377, %1378), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
    %1380 : int[] = prim::ListConstruct(%27, %34, %41), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
    %1381 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1379, %1380), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
    %1382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
    %attn_output.10 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1381, %1382), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
    %1384 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:374:0
    %1385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:374:0
    %input.60 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.10, %1384, %1385), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_longformer.py:374:0
    return (%input.60)

LongformerSelfAttention.key
Linear._actual_script_module
  graph(%self.77 : __torch__.torch.nn.modules.linear.Linear,
        %input.55 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.77)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.77)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
    %output.26 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.55, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
    %key_vectors.5 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.26, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
    return (%key_vectors.5)

LongformerSelfAttention.query
Linear._actual_script_module
  graph(%self.76 : __torch__.torch.nn.modules.linear.Linear,
        %input.55 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.76)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.76)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
    %output.25 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.55, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
    %query_vectors.9 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.25, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
    return (%query_vectors.9)

LongformerSelfAttention.value
Linear._actual_script_module
  graph(%self.78 : __torch__.torch.nn.modules.linear.Linear,
        %input.55 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.78)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.78)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
    %output.27 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.55, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
    %value_vectors.5 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.27, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
    return (%value_vectors.5)

LongformerSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.82 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.62 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.82)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.82)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.15 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.62, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.15)

LongformerSelfOutput.dense
Linear._actual_script_module
  graph(%self.80 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:768, 512:13056, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.80)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.80)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
    %output.28 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
    %input.61 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.28, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.61)

LongformerSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.81 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.54 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.54)

LongformerIntermediate.dense
Linear._actual_script_module
  graph(%self.84 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.84)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.84)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
    %output.29 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
    %input.63 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.29, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.63)

LongformerOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.88 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.66 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.88)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.88)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
    %hidden_states.56 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.66, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%hidden_states.56)

LongformerOutput.dense
Linear._actual_script_module
  graph(%self.86 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1572864, 512:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.86)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.86)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
    %output.30 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
    %input.65 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.30, %2, %6), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
    return (%input.65)

LongformerOutput.dropout
Dropout._actual_script_module
  graph(%self.87 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.55 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.55)

LongformerLayer._actual_script_module
  graph(%self.89 : __torch__.transformers.modeling_longformer.LongformerLayer,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%self.89)
    %4 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%self.89)
    %5 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%self.89)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %attention_mask, %2)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

LongformerLayer.attention
LongformerAttention._actual_script_module
  graph(%self.90 : __torch__.transformers.modeling_longformer.LongformerAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%self.90)
    %4 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%self.90)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %2)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %2)
    return (%8)

LongformerLayer.intermediate
LongformerIntermediate._actual_script_module
  graph(%self.99 : __torch__.transformers.modeling_longformer.LongformerIntermediate,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.99)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.76 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%5), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
    return (%input.76)

LongformerLayer.output
LongformerOutput._actual_script_module
  graph(%self.101 : __torch__.transformers.modeling_longformer.LongformerOutput,
        %1 : Float(17:1572864, 512:3072, 3072:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.101)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.101)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.101)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output # transformers/modeling_longformer.py:830:0
    %input.78 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output # transformers/modeling_longformer.py:830:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.78)
    return (%13)

LongformerAttention.output
LongformerSelfOutput._actual_script_module
  graph(%self.95 : __torch__.transformers.modeling_longformer.LongformerSelfOutput,
        %1 : Float(17:768, 512:13056, 768:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.95)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.95)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.95)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output # transformers/modeling_longformer.py:758:0
    %input.74 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output # transformers/modeling_longformer.py:758:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.74)
    return (%13)

LongformerAttention.self
LongformerSelfAttention._actual_script_module
  graph(%self.91 : __torch__.transformers.modeling_longformer.LongformerSelfAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.91)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.91)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.91)
    %6 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:241:0
    %7 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:241:0
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:241:0
    %9 : Float(17:512, 512:1) = aten::squeeze(%7, %8), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:241:0
    %10 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:22:0
    %is_index_masked.6 : Bool(17:512, 512:1) = aten::lt(%9, %10), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:22:0
    %18 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:248:0
    %19 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:248:0
    %input.67 : Float(512:768, 17:393216, 768:1) = aten::transpose(%2, %18, %19), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:248:0
    %1387 : Tensor = prim::CallMethod[name="forward"](%5, %input.67)
    %1388 : Tensor = prim::CallMethod[name="forward"](%4, %input.67)
    %1389 : Tensor = prim::CallMethod[name="forward"](%3, %input.67)
    %24 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
    %25 : int = aten::size(%input.67, %24), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
    %seq_len.22 : Long() = prim::NumToTensor(%25), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %27 : int = aten::Int(%seq_len.22), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %28 : int = aten::Int(%seq_len.22), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %29 : int = aten::Int(%seq_len.22), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %30 : int = aten::Int(%seq_len.22), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %31 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
    %32 : int = aten::size(%input.67, %31), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
    %batch_size.21 : Long() = prim::NumToTensor(%32), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %34 : int = aten::Int(%batch_size.21), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %35 : int = aten::Int(%batch_size.21), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %36 : int = aten::Int(%batch_size.21), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %37 : int = aten::Int(%batch_size.21), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %38 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
    %39 : int = aten::size(%input.67, %38), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
    %embed_dim.6 : Long() = prim::NumToTensor(%39), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %41 : int = aten::Int(%embed_dim.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %44 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:261:0
    %query_vectors.12 : Float(512:13056, 17:768, 768:1) = aten::div_(%1387, %44), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:261:0
    %46 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:263:0
    %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:263:0
    %48 : int[] = prim::ListConstruct(%30, %37, %46, %47), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %49 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.12, %48), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:263:0
    %50 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:263:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:263:0
    %query.11 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%49, %50, %51), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:263:0
    %53 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:264:0
    %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:264:0
    %55 : int[] = prim::ListConstruct(%29, %36, %53, %54), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %56 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1388, %55), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:264:0
    %57 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:264:0
    %58 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:264:0
    %key.6 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%56, %57, %58), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:264:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %61 : int = aten::size(%query.11, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.22 : Long() = prim::NumToTensor(%61), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %63 : int = aten::Int(%batch_size.22), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %65 : int = aten::size(%query.11, %64), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.23 : Long() = prim::NumToTensor(%65), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %67 : int = aten::Int(%seq_len.23), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %68 : int = aten::Int(%seq_len.23), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %69 : int = aten::Int(%seq_len.23), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %70 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %71 : int = aten::size(%query.11, %70), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.16 : Long() = prim::NumToTensor(%71), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %73 : int = aten::Int(%num_heads.16), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %74 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %75 : int = aten::size(%query.11, %74), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.16 : Long() = prim::NumToTensor(%75), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %77 : int = aten::Int(%head_dim.16), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %78 : int = aten::Int(%head_dim.16), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %111 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %112 : Long() = aten::floor_divide(%seq_len.23, %111), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %113 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:478:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.16 : Long() = aten::sub(%112, %113, %114), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:478:0
    %116 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
    %117 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
    %118 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.11, %116, %117), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
    %119 : Long() = aten::mul(%batch_size.22, %num_heads.16), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
    %120 : int = aten::Int(%119), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %121 : int[] = prim::ListConstruct(%120, %69, %78), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %hidden_states.57 : Float(204:64, 512:13056, 64:1) = aten::reshape(%118, %121), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
    %123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
    %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
    %125 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.6, %123, %124), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
    %126 : Long() = aten::mul(%batch_size.22, %num_heads.16), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
    %127 : int = aten::Int(%126), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %128 : int[] = prim::ListConstruct(%127, %68, %77), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %hidden_states.59 : Float(204:64, 512:13056, 64:1) = aten::reshape(%125, %128), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
    %130 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
    %131 : int = aten::size(%hidden_states.57, %130), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
    %132 : Long() = prim::NumToTensor(%131), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %133 : int = aten::Int(%132), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %134 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
    %135 : int = aten::size(%hidden_states.57, %134), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
    %136 : Long() = prim::NumToTensor(%135), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %137 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %138 : Long() = aten::floor_divide(%136, %137), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %139 : int = aten::Int(%138), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %140 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
    %141 : int = aten::size(%hidden_states.57, %140), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
    %142 : Long() = prim::NumToTensor(%141), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %143 : int = aten::Int(%142), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %144 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
    %145 : int[] = prim::ListConstruct(%133, %139, %144, %143), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %hidden_states.58 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.57, %145), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
    %147 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %148 : int = aten::size(%hidden_states.58, %147), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %149 : Long() = prim::NumToTensor(%148), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %150 : int = aten::Int(%149), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %151 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %152 : int = aten::size(%hidden_states.58, %151), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %153 : Long() = prim::NumToTensor(%152), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %154 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %155 : int = aten::size(%hidden_states.58, %154), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %156 : Long() = prim::NumToTensor(%155), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %157 : int = aten::Int(%156), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %158 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %159 : int = aten::size(%hidden_states.58, %158), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %160 : Long() = prim::NumToTensor(%159), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %161 : int = aten::Int(%160), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %162 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %163 : Long() = aten::mul(%153, %162), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %164 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %165 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %166 : Long() = aten::sub(%163, %164, %165), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %167 : int = aten::Int(%166), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %168 : int[] = prim::ListConstruct(%150, %167, %157, %161), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %169 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %170 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %171 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %172 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %173 : int[] = prim::ListConstruct(%169, %170, %171, %172), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %174 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %175 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.58, %168, %173, %174), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %176 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
    %177 : int = aten::size(%hidden_states.59, %176), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %179 : int = aten::Int(%178), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
    %181 : int = aten::size(%hidden_states.59, %180), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
    %182 : Long() = prim::NumToTensor(%181), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %183 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %184 : Long() = aten::floor_divide(%182, %183), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %185 : int = aten::Int(%184), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %186 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
    %187 : int = aten::size(%hidden_states.59, %186), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
    %188 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %189 : int = aten::Int(%188), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %190 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
    %191 : int[] = prim::ListConstruct(%179, %185, %190, %189), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %hidden_states.60 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.59, %191), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
    %193 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %194 : int = aten::size(%hidden_states.60, %193), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %195 : Long() = prim::NumToTensor(%194), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %196 : int = aten::Int(%195), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %197 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %198 : int = aten::size(%hidden_states.60, %197), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %199 : Long() = prim::NumToTensor(%198), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %200 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %201 : int = aten::size(%hidden_states.60, %200), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %203 : int = aten::Int(%202), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %204 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %205 : int = aten::size(%hidden_states.60, %204), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %207 : int = aten::Int(%206), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %208 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %209 : Long() = aten::mul(%199, %208), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %210 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %211 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %212 : Long() = aten::sub(%209, %210, %211), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %213 : int = aten::Int(%212), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %214 : int[] = prim::ListConstruct(%196, %213, %203, %207), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %216 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %217 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %218 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %219 : int[] = prim::ListConstruct(%215, %216, %217, %218), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %220 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.60, %214, %219, %220), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %222 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/functional.py:327:0
    %223 : Tensor[] = prim::ListConstruct(%175, %221), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %input.68 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%222, %223), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/functional.py:327:0
    %225 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %226 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %227 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %229 : int[] = prim::ListConstruct(%225, %226, %227, %228), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %230 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.11 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.68, %229, %230), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %232 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %233 : int = aten::size(%hidden_states_padded.11, %232), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %234 : Long() = prim::NumToTensor(%233), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %235 : int = aten::Int(%234), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %236 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %237 : int = aten::size(%hidden_states_padded.11, %236), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %238 : Long() = prim::NumToTensor(%237), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %239 : int = aten::Int(%238), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %246 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %247 : int = aten::size(%hidden_states_padded.11, %246), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %248 : Long() = prim::NumToTensor(%247), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %249 : int = aten::Int(%248), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %250 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %251 : int = aten::size(%hidden_states_padded.11, %250), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %252 : Long() = prim::NumToTensor(%251), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %253 : int = aten::Int(%252), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %254 : int[] = prim::ListConstruct(%235, %239, %249, %253), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %diagonal_chunked_attention_scores.11 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.11, %254), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:400:0
    %256 : Long() = aten::mul(%batch_size.22, %num_heads.16), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
    %257 : int = aten::Int(%256), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %258 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
    %259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
    %260 : Long() = aten::add(%chunks_count.16, %258, %259), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
    %261 : int = aten::Int(%260), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %263 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %264 : int[] = prim::ListConstruct(%257, %261, %262, %263), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %265 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %266 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %268 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.11 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.11, %264, %265, %266, %267, %268), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %270 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %271 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %273 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %270, %271, %272, %273), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %275 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %276 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %277 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %279 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%274, %275, %276, %277, %278), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %280 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %281 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %282 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %284 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%279, %280, %281, %282, %283), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %285 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %286 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %287 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %289 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%284, %285, %286, %287, %288), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %290 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %291 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %292 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %294 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %290, %291, %292, %293), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %295 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %296 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %297 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %299 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%294, %295, %296, %297, %298), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %300 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %301 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %302 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %303 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %304 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%299, %300, %301, %302, %303), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %305 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %306 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %307 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %308 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %309 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%304, %305, %306, %307, %308), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %310 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %311 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %312 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %313 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %314 : int[] = prim::ListConstruct(%310, %311, %312, %313), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %315 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%289, %314), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %316 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %317 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%309, %315, %316), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %319 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %320 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %321 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %322 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %318, %319, %320, %321), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %323 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %324 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %325 : Float(204:262656, 512:513, 513:1) = aten::select(%322, %323, %324), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %327 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %328 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %329 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %330 : Float(204:262656, 256:513, 513:1) = aten::slice(%325, %326, %327, %328, %329), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %331 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %333 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %335 : Float(204:262656, 256:513, 257:1) = aten::slice(%330, %331, %332, %333, %334), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %336 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %340 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %336, %337, %338, %339), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %341 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %342 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %343 : Float(204:262656, 256:513, 513:1) = aten::select(%340, %341, %342), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %345 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %347 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %348 : Float(204:262656, 256:513, 513:1) = aten::slice(%343, %344, %345, %346, %347), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %349 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %350 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %352 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %353 : Float(204:262656, 256:513, 257:1) = aten::slice(%348, %349, %350, %351, %352), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %354 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %355 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %356 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %357 : int[] = prim::ListConstruct(%354, %355, %356), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %358 : Float(204:262656, 256:513, 257:1) = aten::view(%335, %357), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %359 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %360 : Float(204:262656, 256:513, 257:1) = aten::copy_(%353, %358, %359), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %361 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %362 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %363 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %364 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %365 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %361, %362, %363, %364), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %366 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %367 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %368 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %369 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %370 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%365, %366, %367, %368, %369), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %371 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %372 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %373 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %374 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %375 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%370, %371, %372, %373, %374), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %376 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %377 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %379 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %380 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%375, %376, %377, %378, %379), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %383 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %384 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %385 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %381, %382, %383, %384), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %386 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %387 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %388 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %389 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %390 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%385, %386, %387, %388, %389), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %391 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %392 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %393 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %394 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %395 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%390, %391, %392, %393, %394), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %397 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %398 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %399 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %400 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%395, %396, %397, %398, %399), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %401 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %402 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %403 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %404 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %405 : int[] = prim::ListConstruct(%401, %402, %403, %404), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %406 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%380, %405), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %407 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %408 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%400, %406, %407), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %409 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %410 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %411 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %412 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %413 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %409, %410, %411, %412), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %414 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %415 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %416 : Float(204:262656, 512:513, 513:1) = aten::select(%413, %414, %415), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %417 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %418 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %419 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %420 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %421 : Float(204:262656, 255:513, 513:1) = aten::slice(%416, %417, %418, %419, %420), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %422 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %423 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %425 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %426 : Float(204:262656, 255:513, 255:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %427 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %429 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %430 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %431 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %427, %428, %429, %430), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %432 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %433 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %434 : Float(204:262656, 256:513, 513:1) = aten::select(%431, %432, %433), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %435 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %436 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %437 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %438 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %439 : Float(204:262656, 255:513, 513:1) = aten::slice(%434, %435, %436, %437, %438), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %440 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %441 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %442 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %443 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %444 : Float(204:262656, 255:513, 255:1) = aten::slice(%439, %440, %441, %442, %443), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %445 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %446 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %447 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %448 : int[] = prim::ListConstruct(%445, %446, %447), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %449 : Float(204:262656, 255:513, 255:1) = aten::view(%426, %448), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %450 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %451 : Float(204:262656, 255:513, 255:1) = aten::copy_(%444, %449, %450), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %452 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
    %453 : int[] = prim::ListConstruct(%63, %73, %67, %452), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %454 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.11, %453), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
    %455 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
    %456 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.16 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%454, %455, %456), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
    %458 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %459 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %460 : int[] = prim::ListConstruct(%458, %459), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %461 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %462 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %463 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %464 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %465 : Float(256:257, 257:1) = aten::ones(%460, %461, %462, %463, %464), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %466 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %467 : Float(256:257, 257:1) = aten::tril(%465, %466), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %468 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %469 : int[] = prim::ListConstruct(%468), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %beginning_mask_2d.11 : Float(256:257, 257:1) = aten::flip(%467, %469), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %471 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %472 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.11, %471), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %473 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %474 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %475 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %476 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %477 : Float(1:65792, 256:257, 257:1) = aten::slice(%472, %473, %474, %475, %476), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %478 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %479 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%477, %478), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %480 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %481 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %482 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %483 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.11 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%479, %480, %481, %482, %483), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %485 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:460:0
    %486 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:460:0
    %487 : int[] = prim::ListConstruct(%485, %486), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %ending_mask.11 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.11, %487), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:460:0
    %489 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %490 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %491 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %493 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.16, %489, %490, %491, %492), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %494 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %495 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %496 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %497 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %498 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%493, %494, %495, %496, %497), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %499 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %500 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %501 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %502 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %503 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%498, %499, %500, %501, %502), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %504 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %505 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %506 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %507 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.11 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%503, %504, %505, %506, %507), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %509 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %510 : int = aten::size(%beginning_input.11, %509), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %511 : Long() = prim::NumToTensor(%510), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %512 : int = aten::Int(%511), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %513 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %514 : int = aten::size(%beginning_input.11, %513), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %515 : Long() = prim::NumToTensor(%514), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %516 : int = aten::Int(%515), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %517 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %518 : int = aten::size(%beginning_input.11, %517), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %519 : Long() = prim::NumToTensor(%518), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %520 : int = aten::Int(%519), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %521 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %522 : int = aten::size(%beginning_input.11, %521), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %523 : Long() = prim::NumToTensor(%522), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %524 : int = aten::Int(%523), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %525 : int[] = prim::ListConstruct(%512, %516, %520, %524), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %526 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %527 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.11, %525, %526), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %528 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:22:0
    %529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%527, %528), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:22:0
    %530 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:463:0
    %531 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.11, %529, %530), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:463:0
    %532 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %533 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %534 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %535 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %536 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.16, %532, %533, %534, %535), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %537 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %538 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %539 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %541 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%536, %537, %538, %539, %540), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %542 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %543 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %544 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %545 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %546 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%541, %542, %543, %544, %545), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %547 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %548 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %549 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.11 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%546, %547, %548, %549, %550), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %552 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %553 : int = aten::size(%ending_input.11, %552), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %554 : Long() = prim::NumToTensor(%553), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %555 : int = aten::Int(%554), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %556 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %557 : int = aten::size(%ending_input.11, %556), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %558 : Long() = prim::NumToTensor(%557), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %559 : int = aten::Int(%558), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %560 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %561 : int = aten::size(%ending_input.11, %560), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %562 : Long() = prim::NumToTensor(%561), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %563 : int = aten::Int(%562), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %564 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %565 : int = aten::size(%ending_input.11, %564), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %566 : Long() = prim::NumToTensor(%565), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %567 : int = aten::Int(%566), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %568 : int[] = prim::ListConstruct(%555, %559, %563, %567), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %569 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %570 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.11, %568, %569), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %571 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:22:0
    %572 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%570, %571), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:22:0
    %573 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:466:0
    %574 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.11, %572, %573), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:466:0
    %575 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:22:0
    %576 : Bool(17:512, 512:1) = aten::ne(%9, %575), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:22:0
    %577 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %578 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %579 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %580 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %581 : Bool(17:512, 512:1) = aten::slice(%576, %577, %578, %579, %580), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %582 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %583 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %584 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %585 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %586 : Bool(17:512, 512:1) = aten::slice(%581, %582, %583, %584, %585), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %587 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %588 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%586, %587), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %589 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %remove_from_windowed_attention_mask.6 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%588, %589), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
    %591 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.6, %query.11), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:275:0
    %592 : float = prim::Constant[value=-10000.](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:275:0
    %float_mask.6 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%591, %remove_from_windowed_attention_mask.6, %592), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:275:0
    %594 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %595 : int = aten::size(%float_mask.6, %594), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %596 : Long() = prim::NumToTensor(%595), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %597 : int = aten::Int(%596), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %598 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %599 : int = aten::size(%float_mask.6, %598), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %600 : Long() = prim::NumToTensor(%599), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %601 : int = aten::Int(%600), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %602 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %603 : int = aten::size(%float_mask.6, %602), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %604 : Long() = prim::NumToTensor(%603), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %605 : int = aten::Int(%604), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %606 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %607 : int = aten::size(%float_mask.6, %606), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %608 : Long() = prim::NumToTensor(%607), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %609 : int = aten::Int(%608), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %610 : int[] = prim::ListConstruct(%597, %601, %605, %609), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %611 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %612 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %613 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %614 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %query.12 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%610, %611, %612, %613, %614), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
    %616 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %617 : int = aten::size(%query.12, %616), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.23 : Long() = prim::NumToTensor(%617), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %619 : int = aten::Int(%batch_size.23), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %620 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %621 : int = aten::size(%query.12, %620), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.24 : Long() = prim::NumToTensor(%621), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %623 : int = aten::Int(%seq_len.24), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %624 : int = aten::Int(%seq_len.24), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %625 : int = aten::Int(%seq_len.24), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %626 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %627 : int = aten::size(%query.12, %626), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.17 : Long() = prim::NumToTensor(%627), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %629 : int = aten::Int(%num_heads.17), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %630 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %631 : int = aten::size(%query.12, %630), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.17 : Long() = prim::NumToTensor(%631), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %633 : int = aten::Int(%head_dim.17), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %634 : int = aten::Int(%head_dim.17), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %667 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %668 : Long() = aten::floor_divide(%seq_len.24, %667), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %669 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:478:0
    %670 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.17 : Long() = aten::sub(%668, %669, %670), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:478:0
    %672 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
    %673 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
    %674 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.12, %672, %673), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
    %675 : Long() = aten::mul(%batch_size.23, %num_heads.17), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
    %676 : int = aten::Int(%675), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %677 : int[] = prim::ListConstruct(%676, %625, %634), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %hidden_states.61 : Float(17:512, 512:1, 1:1) = aten::reshape(%674, %677), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
    %679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
    %680 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
    %681 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.6, %679, %680), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
    %682 : Long() = aten::mul(%batch_size.23, %num_heads.17), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
    %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %684 : int[] = prim::ListConstruct(%683, %624, %633), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %hidden_states.63 : Float(17:512, 512:1, 1:1) = aten::reshape(%681, %684), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
    %686 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
    %687 : int = aten::size(%hidden_states.61, %686), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
    %688 : Long() = prim::NumToTensor(%687), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %690 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
    %691 : int = aten::size(%hidden_states.61, %690), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
    %692 : Long() = prim::NumToTensor(%691), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %693 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %694 : Long() = aten::floor_divide(%692, %693), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %696 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
    %697 : int = aten::size(%hidden_states.61, %696), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
    %698 : Long() = prim::NumToTensor(%697), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %699 : int = aten::Int(%698), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %700 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
    %701 : int[] = prim::ListConstruct(%689, %695, %700, %699), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %hidden_states.62 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.61, %701), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
    %703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %704 : int = aten::size(%hidden_states.62, %703), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %705 : Long() = prim::NumToTensor(%704), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %706 : int = aten::Int(%705), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %708 : int = aten::size(%hidden_states.62, %707), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %709 : Long() = prim::NumToTensor(%708), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %710 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %711 : int = aten::size(%hidden_states.62, %710), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %712 : Long() = prim::NumToTensor(%711), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %713 : int = aten::Int(%712), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %714 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %715 : int = aten::size(%hidden_states.62, %714), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %716 : Long() = prim::NumToTensor(%715), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %717 : int = aten::Int(%716), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %718 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %719 : Long() = aten::mul(%709, %718), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %720 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %721 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %722 : Long() = aten::sub(%719, %720, %721), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %723 : int = aten::Int(%722), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %724 : int[] = prim::ListConstruct(%706, %723, %713, %717), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %725 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %726 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %727 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %728 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %729 : int[] = prim::ListConstruct(%725, %726, %727, %728), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %730 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %731 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.62, %724, %729, %730), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %732 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
    %733 : int = aten::size(%hidden_states.63, %732), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
    %734 : Long() = prim::NumToTensor(%733), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %735 : int = aten::Int(%734), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %736 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
    %737 : int = aten::size(%hidden_states.63, %736), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
    %738 : Long() = prim::NumToTensor(%737), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %739 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %740 : Long() = aten::floor_divide(%738, %739), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %741 : int = aten::Int(%740), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
    %743 : int = aten::size(%hidden_states.63, %742), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
    %744 : Long() = prim::NumToTensor(%743), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %745 : int = aten::Int(%744), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %746 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
    %747 : int[] = prim::ListConstruct(%735, %741, %746, %745), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %hidden_states.64 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.63, %747), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
    %749 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %750 : int = aten::size(%hidden_states.64, %749), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %751 : Long() = prim::NumToTensor(%750), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %752 : int = aten::Int(%751), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %754 : int = aten::size(%hidden_states.64, %753), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %755 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %756 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %757 : int = aten::size(%hidden_states.64, %756), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %758 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %759 : int = aten::Int(%758), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %760 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %761 : int = aten::size(%hidden_states.64, %760), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
    %762 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %764 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %765 : Long() = aten::mul(%755, %764), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %766 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %767 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %768 : Long() = aten::sub(%765, %766, %767), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
    %769 : int = aten::Int(%768), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %770 : int[] = prim::ListConstruct(%752, %769, %759, %763), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %771 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %772 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %773 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %776 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.64, %770, %775, %776), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
    %778 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/functional.py:327:0
    %779 : Tensor[] = prim::ListConstruct(%731, %777), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %input.69 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%778, %779), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/functional.py:327:0
    %781 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %782 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %783 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %786 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.12 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.69, %785, %786), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %788 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %789 : int = aten::size(%hidden_states_padded.12, %788), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %790 : Long() = prim::NumToTensor(%789), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %791 : int = aten::Int(%790), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %792 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %793 : int = aten::size(%hidden_states_padded.12, %792), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %794 : Long() = prim::NumToTensor(%793), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %795 : int = aten::Int(%794), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %802 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %803 : int = aten::size(%hidden_states_padded.12, %802), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %804 : Long() = prim::NumToTensor(%803), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %805 : int = aten::Int(%804), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %806 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %807 : int = aten::size(%hidden_states_padded.12, %806), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
    %808 : Long() = prim::NumToTensor(%807), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %809 : int = aten::Int(%808), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %810 : int[] = prim::ListConstruct(%791, %795, %805, %809), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %diagonal_chunked_attention_scores.12 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.12, %810), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:400:0
    %812 : Long() = aten::mul(%batch_size.23, %num_heads.17), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
    %813 : int = aten::Int(%812), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %814 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
    %815 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
    %816 : Long() = aten::add(%chunks_count.17, %814, %815), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
    %817 : int = aten::Int(%816), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %818 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %819 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %820 : int[] = prim::ListConstruct(%813, %817, %818, %819), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %821 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %822 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %823 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %824 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.12 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.12, %820, %821, %822, %823, %824), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
    %826 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %827 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %828 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %829 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %830 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %826, %827, %828, %829), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %831 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %832 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %833 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %834 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%830, %831, %832, %833, %834), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %836 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %837 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %838 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %839 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%835, %836, %837, %838, %839), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %841 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %842 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %843 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %845 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%840, %841, %842, %843, %844), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %846 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %847 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %848 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %849 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %846, %847, %848, %849), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %851 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %852 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %853 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %855 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%850, %851, %852, %853, %854), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %856 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %857 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %858 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %859 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %860 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%855, %856, %857, %858, %859), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %861 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %862 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %863 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %864 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %865 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%860, %861, %862, %863, %864), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %866 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %867 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %868 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %869 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %870 : int[] = prim::ListConstruct(%866, %867, %868, %869), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %871 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%845, %870), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %872 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %873 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%865, %871, %872), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
    %874 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %875 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %876 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %877 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %874, %875, %876, %877), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %879 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %880 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %881 : Float(17:262656, 512:513, 513:1) = aten::select(%878, %879, %880), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %883 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %884 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %885 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %886 : Float(17:262656, 256:513, 513:1) = aten::slice(%881, %882, %883, %884, %885), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %887 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %889 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%886, %887, %888, %889, %890), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %892 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %893 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %894 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %895 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %896 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %892, %893, %894, %895), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %897 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %898 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %899 : Float(17:262656, 256:513, 513:1) = aten::select(%896, %897, %898), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %901 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %902 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %903 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %904 : Float(17:262656, 256:513, 513:1) = aten::slice(%899, %900, %901, %902, %903), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %905 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %906 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %907 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %909 : Float(17:262656, 256:513, 257:1) = aten::slice(%904, %905, %906, %907, %908), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %910 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %911 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %912 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %913 : int[] = prim::ListConstruct(%910, %911, %912), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %914 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %913), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %915 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %916 : Float(17:262656, 256:513, 257:1) = aten::copy_(%909, %914, %915), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
    %917 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %918 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %919 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %920 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %921 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %917, %918, %919, %920), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %922 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %923 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %924 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %925 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %926 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%921, %922, %923, %924, %925), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %927 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %928 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %929 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %930 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %931 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%926, %927, %928, %929, %930), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %932 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %933 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %934 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %935 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %936 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%931, %932, %933, %934, %935), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %938 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %939 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %940 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %941 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %937, %938, %939, %940), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %942 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %943 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %944 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %945 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %946 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%941, %942, %943, %944, %945), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %947 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %948 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %949 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %950 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %951 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%946, %947, %948, %949, %950), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %952 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %953 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %954 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %955 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %956 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%951, %952, %953, %954, %955), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %957 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %959 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %960 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %962 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%936, %961), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %963 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %964 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%956, %962, %963), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
    %965 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %966 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %967 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %969 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %965, %966, %967, %968), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %970 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %971 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %972 : Float(17:262656, 512:513, 513:1) = aten::select(%969, %970, %971), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %973 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %974 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %975 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %976 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %977 : Float(17:262656, 255:513, 513:1) = aten::slice(%972, %973, %974, %975, %976), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %978 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %979 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %980 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %981 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %982 : Float(17:262656, 255:513, 255:1) = aten::slice(%977, %978, %979, %980, %981), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %983 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %984 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %985 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %986 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %987 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %983, %984, %985, %986), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %988 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %989 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %990 : Float(17:262656, 256:513, 513:1) = aten::select(%987, %988, %989), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %991 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %993 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %994 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %995 : Float(17:262656, 255:513, 513:1) = aten::slice(%990, %991, %992, %993, %994), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %996 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %998 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %999 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %1000 : Float(17:262656, 255:513, 255:1) = aten::slice(%995, %996, %997, %998, %999), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %1001 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %1002 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %1003 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %1004 : int[] = prim::ListConstruct(%1001, %1002, %1003), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1005 : Float(17:262656, 255:513, 255:1) = aten::view(%982, %1004), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %1006 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1007 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1000, %1005, %1006), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
    %1008 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
    %1009 : int[] = prim::ListConstruct(%619, %629, %623, %1008), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1010 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.12, %1009), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
    %1011 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
    %1012 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.17 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1010, %1011, %1012), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
    %1014 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %1015 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %1016 : int[] = prim::ListConstruct(%1014, %1015), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1017 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %1018 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %1019 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %1020 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %1021 : Float(256:257, 257:1) = aten::ones(%1016, %1017, %1018, %1019, %1020), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %1022 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %1023 : Float(256:257, 257:1) = aten::tril(%1021, %1022), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %1024 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %1025 : int[] = prim::ListConstruct(%1024), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %beginning_mask_2d.12 : Float(256:257, 257:1) = aten::flip(%1023, %1025), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
    %1027 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1028 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.12, %1027), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1029 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1030 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1031 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1032 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1033 : Float(1:65792, 256:257, 257:1) = aten::slice(%1028, %1029, %1030, %1031, %1032), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1034 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1035 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1033, %1034), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1036 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1037 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1038 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1039 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.12 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1035, %1036, %1037, %1038, %1039), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
    %1041 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:460:0
    %1042 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:460:0
    %1043 : int[] = prim::ListConstruct(%1041, %1042), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %ending_mask.12 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.12, %1043), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:460:0
    %1045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1046 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1047 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1048 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1049 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.17, %1045, %1046, %1047, %1048), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1050 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1052 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1053 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1054 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1049, %1050, %1051, %1052, %1053), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1055 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1056 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1057 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1058 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1059 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1054, %1055, %1056, %1057, %1058), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1060 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1061 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1062 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.12 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1059, %1060, %1061, %1062, %1063), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
    %1065 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %1066 : int = aten::size(%beginning_input.12, %1065), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %1067 : Long() = prim::NumToTensor(%1066), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1068 : int = aten::Int(%1067), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1069 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %1070 : int = aten::size(%beginning_input.12, %1069), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %1071 : Long() = prim::NumToTensor(%1070), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1072 : int = aten::Int(%1071), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1073 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %1074 : int = aten::size(%beginning_input.12, %1073), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %1075 : Long() = prim::NumToTensor(%1074), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1076 : int = aten::Int(%1075), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1077 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %1078 : int = aten::size(%beginning_input.12, %1077), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %1079 : Long() = prim::NumToTensor(%1078), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1080 : int = aten::Int(%1079), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1081 : int[] = prim::ListConstruct(%1068, %1072, %1076, %1080), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1082 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %1083 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.12, %1081, %1082), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
    %1084 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:22:0
    %1085 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1083, %1084), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:22:0
    %1086 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:463:0
    %1087 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.12, %1085, %1086), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:463:0
    %1088 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1089 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1090 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1091 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1092 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.17, %1088, %1089, %1090, %1091), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1094 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1095 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1096 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1097 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1092, %1093, %1094, %1095, %1096), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1098 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1099 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1100 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1101 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1102 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1097, %1098, %1099, %1100, %1101), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1103 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1104 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1105 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1106 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.12 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1102, %1103, %1104, %1105, %1106), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
    %1108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %1109 : int = aten::size(%ending_input.12, %1108), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %1110 : Long() = prim::NumToTensor(%1109), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1111 : int = aten::Int(%1110), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1112 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %1113 : int = aten::size(%ending_input.12, %1112), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %1114 : Long() = prim::NumToTensor(%1113), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1115 : int = aten::Int(%1114), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1116 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %1117 : int = aten::size(%ending_input.12, %1116), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %1118 : Long() = prim::NumToTensor(%1117), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1119 : int = aten::Int(%1118), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1120 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %1121 : int = aten::size(%ending_input.12, %1120), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %1122 : Long() = prim::NumToTensor(%1121), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1124 : int[] = prim::ListConstruct(%1111, %1115, %1119, %1123), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1125 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %1126 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.12, %1124, %1125), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
    %1127 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:22:0
    %1128 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1126, %1127), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:22:0
    %1129 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:466:0
    %1130 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.12, %1128, %1129), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:466:0
    %1131 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:284:0
    %attn_scores.6 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.16, %input_tensor.17, %1131), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:284:0
    %1151 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:1500:0
    %1152 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:1500:0
    %attn_probs_fp32.6 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.6, %1151, %1152), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:1500:0
    %attn_probs.11 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::type_as(%attn_probs_fp32.6, %attn_scores.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:320:0
    %1155 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1156 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1157 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1158 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1159 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.6, %1155, %1156, %1157, %1158), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1161 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1162 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1163 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1164 : Bool(17:512, 512:1) = aten::slice(%1159, %1160, %1161, %1162, %1163), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1165 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1166 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1164, %1165), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1167 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1168 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1166, %1167), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1169 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %input.70 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs.11, %1168, %1169), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
    %1171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:973:0
    %1172 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:973:0
    %attn_probs.12 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.70, %1171, %1172), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:973:0
    %1174 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:331:0
    %1175 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:331:0
    %1176 : int[] = prim::ListConstruct(%28, %35, %1174, %1175), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1177 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1389, %1176), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:331:0
    %1178 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:331:0
    %1179 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:331:0
    %value.6 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1177, %1178, %1179), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:331:0
    %1181 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
    %1182 : int = aten::size(%value.6, %1181), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
    %batch_size.24 : Long() = prim::NumToTensor(%1182), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1184 : int = aten::Int(%batch_size.24), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1185 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
    %1186 : int = aten::size(%value.6, %1185), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
    %seq_len.25 : Long() = prim::NumToTensor(%1186), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1188 : int = aten::Int(%seq_len.25), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1189 : int = aten::Int(%seq_len.25), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1190 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
    %1191 : int = aten::size(%value.6, %1190), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
    %num_heads.18 : Long() = prim::NumToTensor(%1191), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1193 : int = aten::Int(%num_heads.18), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1194 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
    %1195 : int = aten::size(%value.6, %1194), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
    %head_dim.18 : Long() = prim::NumToTensor(%1195), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1197 : int = aten::Int(%head_dim.18), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1198 : int = aten::Int(%head_dim.18), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1199 : int = aten::Int(%head_dim.18), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1236 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %1237 : Long() = aten::floor_divide(%seq_len.25, %1236), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %1238 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:542:0
    %1239 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:542:0
    %chunks_count.18 : Long() = aten::sub(%1237, %1238, %1239), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:542:0
    %1241 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:545:0
    %1242 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:545:0
    %1243 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.12, %1241, %1242), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:545:0
    %1244 : Long() = aten::mul(%batch_size.24, %num_heads.18), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:546:0
    %1245 : int = aten::Int(%1244), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1246 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %1247 : Long() = aten::floor_divide(%seq_len.25, %1246), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/tensor.py:424:0
    %1248 : int = aten::Int(%1247), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1249 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:545:0
    %1250 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:545:0
    %1251 : int[] = prim::ListConstruct(%1245, %1248, %1249, %1250), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %chunked_hidden_states.26 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1243, %1251), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:545:0
    %1253 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
    %1254 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
    %1255 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.6, %1253, %1254), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
    %1256 : Long() = aten::mul(%batch_size.24, %num_heads.18), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
    %1257 : int = aten::Int(%1256), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1258 : int[] = prim::ListConstruct(%1257, %1189, %1199), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %input.71 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1255, %1258), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
    %1260 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %1261 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %1262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %1263 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %1264 : int[] = prim::ListConstruct(%1260, %1261, %1262, %1263), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1265 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %padded_value.6 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.71, %1264, %1265), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %1267 : Long() = aten::mul(%batch_size.24, %num_heads.18), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:556:0
    %1268 : int = aten::Int(%1267), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1269 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:556:0
    %1270 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:556:0
    %1271 : Long() = aten::add(%chunks_count.18, %1269, %1270), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:556:0
    %1272 : int = aten::Int(%1271), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:564:0
    %1274 : int[] = prim::ListConstruct(%1268, %1272, %1273, %1198), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1275 : int = prim::Constant[value=65536](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:564:0
    %1276 : int = prim::Constant[value=16384](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:564:0
    %1277 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:564:0
    %1278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:564:0
    %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1280 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1281 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.6, %1274, %1279, %1280), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:564:0
    %1282 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
    %1283 : int = aten::size(%chunked_hidden_states.26, %1282), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
    %total_num_heads.6 : Long() = prim::NumToTensor(%1283), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1285 : int = aten::Int(%total_num_heads.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1286 : int = aten::Int(%total_num_heads.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1287 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
    %1288 : int = aten::size(%chunked_hidden_states.26, %1287), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
    %num_chunks.6 : Long() = prim::NumToTensor(%1288), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1290 : int = aten::Int(%num_chunks.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1291 : int = aten::Int(%num_chunks.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1292 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
    %1293 : int = aten::size(%chunked_hidden_states.26, %1292), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
    %window_overlap.6 : Long() = prim::NumToTensor(%1293), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1295 : int = aten::Int(%window_overlap.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1296 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
    %1297 : int = aten::size(%chunked_hidden_states.26, %1296), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
    %hidden_dim.6 : Long() = prim::NumToTensor(%1297), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1299 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:422:0
    %1300 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:422:0
    %1301 : Long() = aten::add(%window_overlap.6, %1299, %1300), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:422:0
    %1302 : int = aten::Int(%1301), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1303 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %1304 : int[] = prim::ListConstruct(%1303, %1302), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1305 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %chunked_hidden_states.27 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.26, %1304, %1305), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
    %1307 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:424:0
    %1308 : int[] = prim::ListConstruct(%1286, %1291, %1307), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %chunked_hidden_states.28 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.27, %1308), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:424:0
    %1310 : Long() = aten::neg(%window_overlap.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:428:0
    %1311 : int = aten::Int(%1310), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1312 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1313 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1314 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1315 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1316 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.28, %1312, %1313, %1314, %1315), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1319 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1320 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1316, %1317, %1318, %1319, %1320), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1322 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1323 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1324 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %chunked_hidden_states.29 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1321, %1322, %1323, %1311, %1324), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
    %1326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:431:0
    %1327 : Long() = aten::add(%window_overlap.6, %hidden_dim.6, %1326), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:431:0
    %1328 : int = aten::Int(%1327), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1329 : int[] = prim::ListConstruct(%1285, %1290, %1295, %1328), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %chunked_hidden_states.30 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.29, %1329), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:430:0
    %1331 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1333 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1335 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.30, %1331, %1332, %1333, %1334), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1336 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1340 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1335, %1336, %1337, %1338, %1339), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1341 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1342 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1343 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1345 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1340, %1341, %1342, %1343, %1344), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1346 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1347 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1348 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1350 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1345, %1346, %1347, %1348, %1349), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
    %1351 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/functional.py:327:0
    %1352 : Tensor[] = prim::ListConstruct(%1350, %1281), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %context.6 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%1351, %1352), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/functional.py:327:0
    %1354 : int[] = prim::ListConstruct(%1184, %1193, %1188, %1197), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1355 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.6, %1354), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:569:0
    %1356 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:569:0
    %1357 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:569:0
    %attn_output.11 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1355, %1356, %1357), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:569:0
    %1377 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
    %1378 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
    %1379 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.11, %1377, %1378), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
    %1380 : int[] = prim::ListConstruct(%27, %34, %41), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
    %1381 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1379, %1380), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
    %1382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
    %attn_output.12 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1381, %1382), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
    %1384 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:374:0
    %1385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:374:0
    %input.72 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.12, %1384, %1385), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_longformer.py:374:0
    return (%input.72)

LongformerSelfAttention.key
Linear._actual_script_module
  graph(%self.93 : __torch__.torch.nn.modules.linear.Linear,
        %input.67 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.93)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.93)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
    %output.32 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.67, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
    %key_vectors.6 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.32, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
    return (%key_vectors.6)

LongformerSelfAttention.query
Linear._actual_script_module
  graph(%self.92 : __torch__.torch.nn.modules.linear.Linear,
        %input.67 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.92)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.92)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
    %output.31 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.67, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
    %query_vectors.11 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.31, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
    return (%query_vectors.11)

LongformerSelfAttention.value
Linear._actual_script_module
  graph(%self.94 : __torch__.torch.nn.modules.linear.Linear,
        %input.67 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.94)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.94)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
    %output.33 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.67, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
    %value_vectors.6 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.33, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
    return (%value_vectors.6)

LongformerSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.98 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.74 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.98)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.98)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.18 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.74, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.18)

LongformerSelfOutput.dense
Linear._actual_script_module
  graph(%self.96 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:768, 512:13056, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.96)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.96)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
    %output.34 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
    %input.73 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.34, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.73)

LongformerSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.97 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.65 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.65)

LongformerIntermediate.dense
Linear._actual_script_module
  graph(%self.100 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.100)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.100)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
    %output.35 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
    %input.75 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.35, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.75)

LongformerOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.104 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.78 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.104)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.104)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
    %hidden_states.67 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.78, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%hidden_states.67)

LongformerOutput.dense
Linear._actual_script_module
  graph(%self.102 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1572864, 512:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.102)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.102)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
    %output.36 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
    %input.77 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.36, %2, %6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
    return (%input.77)

LongformerOutput.dropout
Dropout._actual_script_module
  graph(%self.103 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.66 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.66)

LongformerLayer._actual_script_module
  graph(%self.105 : __torch__.transformers.modeling_longformer.LongformerLayer,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%self.105)
    %4 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%self.105)
    %5 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%self.105)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %attention_mask, %2)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

LongformerLayer.attention
LongformerAttention._actual_script_module
  graph(%self.106 : __torch__.transformers.modeling_longformer.LongformerAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%self.106)
    %4 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%self.106)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %2)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %2)
    return (%8)

LongformerLayer.intermediate
LongformerIntermediate._actual_script_module
  graph(%self.115 : __torch__.transformers.modeling_longformer.LongformerIntermediate,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.115)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.88 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%5), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
    return (%input.88)

LongformerLayer.output
LongformerOutput._actual_script_module
  graph(%self.117 : __torch__.transformers.modeling_longformer.LongformerOutput,
        %1 : Float(17:1572864, 512:3072, 3072:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.117)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.117)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.117)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output # transformers/modeling_longformer.py:830:0
    %input.90 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output # transformers/modeling_longformer.py:830:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.90)
    return (%13)

LongformerAttention.output
LongformerSelfOutput._actual_script_module
  graph(%self.111 : __torch__.transformers.modeling_longformer.LongformerSelfOutput,
        %1 : Float(17:768, 512:13056, 768:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.111)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.111)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.111)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output # transformers/modeling_longformer.py:758:0
    %input.86 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output # transformers/modeling_longformer.py:758:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.86)
    return (%13)

LongformerAttention.self
LongformerSelfAttention._actual_script_module
  graph(%self.107 : __torch__.transformers.modeling_longformer.LongformerSelfAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.107)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.107)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.107)
    %6 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:241:0
    %7 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:241:0
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:241:0
    %9 : Float(17:512, 512:1) = aten::squeeze(%7, %8), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:241:0
    %10 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:22:0
    %is_index_masked.7 : Bool(17:512, 512:1) = aten::lt(%9, %10), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:22:0
    %18 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:248:0
    %19 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:248:0
    %input.79 : Float(512:768, 17:393216, 768:1) = aten::transpose(%2, %18, %19), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:248:0
    %1387 : Tensor = prim::CallMethod[name="forward"](%5, %input.79)
    %1388 : Tensor = prim::CallMethod[name="forward"](%4, %input.79)
    %1389 : Tensor = prim::CallMethod[name="forward"](%3, %input.79)
    %24 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
    %25 : int = aten::size(%input.79, %24), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
    %seq_len.26 : Long() = prim::NumToTensor(%25), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %27 : int = aten::Int(%seq_len.26), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %28 : int = aten::Int(%seq_len.26), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %29 : int = aten::Int(%seq_len.26), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %30 : int = aten::Int(%seq_len.26), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %31 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
    %32 : int = aten::size(%input.79, %31), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
    %batch_size.25 : Long() = prim::NumToTensor(%32), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %34 : int = aten::Int(%batch_size.25), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %35 : int = aten::Int(%batch_size.25), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %36 : int = aten::Int(%batch_size.25), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %37 : int = aten::Int(%batch_size.25), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %38 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
    %39 : int = aten::size(%input.79, %38), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
    %embed_dim.7 : Long() = prim::NumToTensor(%39), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %41 : int = aten::Int(%embed_dim.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %44 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:261:0
    %query_vectors.14 : Float(512:13056, 17:768, 768:1) = aten::div_(%1387, %44), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:261:0
    %46 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:263:0
    %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:263:0
    %48 : int[] = prim::ListConstruct(%30, %37, %46, %47), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %49 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.14, %48), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:263:0
    %50 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:263:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:263:0
    %query.13 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%49, %50, %51), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:263:0
    %53 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:264:0
    %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:264:0
    %55 : int[] = prim::ListConstruct(%29, %36, %53, %54), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %56 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1388, %55), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:264:0
    %57 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:264:0
    %58 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:264:0
    %key.7 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%56, %57, %58), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:264:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %61 : int = aten::size(%query.13, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.26 : Long() = prim::NumToTensor(%61), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %63 : int = aten::Int(%batch_size.26), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %65 : int = aten::size(%query.13, %64), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.27 : Long() = prim::NumToTensor(%65), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %67 : int = aten::Int(%seq_len.27), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %68 : int = aten::Int(%seq_len.27), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %69 : int = aten::Int(%seq_len.27), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %70 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %71 : int = aten::size(%query.13, %70), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.19 : Long() = prim::NumToTensor(%71), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %73 : int = aten::Int(%num_heads.19), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %74 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %75 : int = aten::size(%query.13, %74), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.19 : Long() = prim::NumToTensor(%75), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %77 : int = aten::Int(%head_dim.19), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %78 : int = aten::Int(%head_dim.19), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %111 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %112 : Long() = aten::floor_divide(%seq_len.27, %111), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %113 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:478:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.19 : Long() = aten::sub(%112, %113, %114), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:478:0
    %116 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
    %117 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
    %118 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.13, %116, %117), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
    %119 : Long() = aten::mul(%batch_size.26, %num_heads.19), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
    %120 : int = aten::Int(%119), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %121 : int[] = prim::ListConstruct(%120, %69, %78), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %hidden_states.68 : Float(204:64, 512:13056, 64:1) = aten::reshape(%118, %121), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
    %123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
    %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
    %125 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.7, %123, %124), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
    %126 : Long() = aten::mul(%batch_size.26, %num_heads.19), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
    %127 : int = aten::Int(%126), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %128 : int[] = prim::ListConstruct(%127, %68, %77), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %hidden_states.70 : Float(204:64, 512:13056, 64:1) = aten::reshape(%125, %128), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
    %130 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
    %131 : int = aten::size(%hidden_states.68, %130), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
    %132 : Long() = prim::NumToTensor(%131), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %133 : int = aten::Int(%132), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %134 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
    %135 : int = aten::size(%hidden_states.68, %134), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
    %136 : Long() = prim::NumToTensor(%135), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %137 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %138 : Long() = aten::floor_divide(%136, %137), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %139 : int = aten::Int(%138), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %140 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
    %141 : int = aten::size(%hidden_states.68, %140), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
    %142 : Long() = prim::NumToTensor(%141), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %143 : int = aten::Int(%142), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %144 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
    %145 : int[] = prim::ListConstruct(%133, %139, %144, %143), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %hidden_states.69 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.68, %145), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
    %147 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %148 : int = aten::size(%hidden_states.69, %147), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %149 : Long() = prim::NumToTensor(%148), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %150 : int = aten::Int(%149), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %151 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %152 : int = aten::size(%hidden_states.69, %151), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %153 : Long() = prim::NumToTensor(%152), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %154 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %155 : int = aten::size(%hidden_states.69, %154), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %156 : Long() = prim::NumToTensor(%155), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %157 : int = aten::Int(%156), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %158 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %159 : int = aten::size(%hidden_states.69, %158), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %160 : Long() = prim::NumToTensor(%159), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %161 : int = aten::Int(%160), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %162 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %163 : Long() = aten::mul(%153, %162), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %164 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %165 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %166 : Long() = aten::sub(%163, %164, %165), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %167 : int = aten::Int(%166), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %168 : int[] = prim::ListConstruct(%150, %167, %157, %161), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %169 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %170 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %171 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %172 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %173 : int[] = prim::ListConstruct(%169, %170, %171, %172), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %174 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %175 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.69, %168, %173, %174), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %176 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
    %177 : int = aten::size(%hidden_states.70, %176), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %179 : int = aten::Int(%178), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
    %181 : int = aten::size(%hidden_states.70, %180), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
    %182 : Long() = prim::NumToTensor(%181), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %183 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %184 : Long() = aten::floor_divide(%182, %183), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %185 : int = aten::Int(%184), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %186 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
    %187 : int = aten::size(%hidden_states.70, %186), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
    %188 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %189 : int = aten::Int(%188), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %190 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
    %191 : int[] = prim::ListConstruct(%179, %185, %190, %189), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %hidden_states.71 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.70, %191), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
    %193 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %194 : int = aten::size(%hidden_states.71, %193), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %195 : Long() = prim::NumToTensor(%194), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %196 : int = aten::Int(%195), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %197 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %198 : int = aten::size(%hidden_states.71, %197), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %199 : Long() = prim::NumToTensor(%198), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %200 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %201 : int = aten::size(%hidden_states.71, %200), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %203 : int = aten::Int(%202), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %204 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %205 : int = aten::size(%hidden_states.71, %204), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %207 : int = aten::Int(%206), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %208 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %209 : Long() = aten::mul(%199, %208), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %210 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %211 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %212 : Long() = aten::sub(%209, %210, %211), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %213 : int = aten::Int(%212), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %214 : int[] = prim::ListConstruct(%196, %213, %203, %207), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %216 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %217 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %218 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %219 : int[] = prim::ListConstruct(%215, %216, %217, %218), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %220 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.71, %214, %219, %220), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %222 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/functional.py:327:0
    %223 : Tensor[] = prim::ListConstruct(%175, %221), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %input.80 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%222, %223), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/functional.py:327:0
    %225 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %226 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %227 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %229 : int[] = prim::ListConstruct(%225, %226, %227, %228), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %230 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.13 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.80, %229, %230), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %232 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %233 : int = aten::size(%hidden_states_padded.13, %232), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %234 : Long() = prim::NumToTensor(%233), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %235 : int = aten::Int(%234), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %236 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %237 : int = aten::size(%hidden_states_padded.13, %236), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %238 : Long() = prim::NumToTensor(%237), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %239 : int = aten::Int(%238), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %246 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %247 : int = aten::size(%hidden_states_padded.13, %246), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %248 : Long() = prim::NumToTensor(%247), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %249 : int = aten::Int(%248), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %250 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %251 : int = aten::size(%hidden_states_padded.13, %250), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %252 : Long() = prim::NumToTensor(%251), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %253 : int = aten::Int(%252), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %254 : int[] = prim::ListConstruct(%235, %239, %249, %253), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %diagonal_chunked_attention_scores.13 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.13, %254), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:400:0
    %256 : Long() = aten::mul(%batch_size.26, %num_heads.19), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
    %257 : int = aten::Int(%256), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %258 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
    %259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
    %260 : Long() = aten::add(%chunks_count.19, %258, %259), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
    %261 : int = aten::Int(%260), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %263 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %264 : int[] = prim::ListConstruct(%257, %261, %262, %263), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %265 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %266 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %268 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.13 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.13, %264, %265, %266, %267, %268), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %270 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %271 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %273 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %270, %271, %272, %273), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %275 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %276 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %277 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %279 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%274, %275, %276, %277, %278), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %280 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %281 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %282 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %284 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%279, %280, %281, %282, %283), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %285 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %286 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %287 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %289 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%284, %285, %286, %287, %288), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %290 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %291 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %292 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %294 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %290, %291, %292, %293), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %295 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %296 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %297 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %299 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%294, %295, %296, %297, %298), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %300 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %301 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %302 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %303 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %304 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%299, %300, %301, %302, %303), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %305 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %306 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %307 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %308 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %309 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%304, %305, %306, %307, %308), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %310 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %311 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %312 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %313 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %314 : int[] = prim::ListConstruct(%310, %311, %312, %313), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %315 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%289, %314), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %316 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %317 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%309, %315, %316), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %319 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %320 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %321 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %322 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %318, %319, %320, %321), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %323 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %324 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %325 : Float(204:262656, 512:513, 513:1) = aten::select(%322, %323, %324), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %327 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %328 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %329 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %330 : Float(204:262656, 256:513, 513:1) = aten::slice(%325, %326, %327, %328, %329), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %331 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %333 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %335 : Float(204:262656, 256:513, 257:1) = aten::slice(%330, %331, %332, %333, %334), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %336 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %340 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %336, %337, %338, %339), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %341 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %342 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %343 : Float(204:262656, 256:513, 513:1) = aten::select(%340, %341, %342), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %345 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %347 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %348 : Float(204:262656, 256:513, 513:1) = aten::slice(%343, %344, %345, %346, %347), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %349 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %350 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %352 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %353 : Float(204:262656, 256:513, 257:1) = aten::slice(%348, %349, %350, %351, %352), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %354 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %355 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %356 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %357 : int[] = prim::ListConstruct(%354, %355, %356), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %358 : Float(204:262656, 256:513, 257:1) = aten::view(%335, %357), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %359 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %360 : Float(204:262656, 256:513, 257:1) = aten::copy_(%353, %358, %359), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %361 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %362 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %363 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %364 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %365 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %361, %362, %363, %364), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %366 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %367 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %368 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %369 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %370 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%365, %366, %367, %368, %369), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %371 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %372 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %373 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %374 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %375 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%370, %371, %372, %373, %374), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %376 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %377 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %379 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %380 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%375, %376, %377, %378, %379), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %383 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %384 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %385 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %381, %382, %383, %384), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %386 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %387 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %388 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %389 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %390 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%385, %386, %387, %388, %389), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %391 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %392 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %393 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %394 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %395 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%390, %391, %392, %393, %394), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %397 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %398 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %399 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %400 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%395, %396, %397, %398, %399), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %401 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %402 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %403 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %404 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %405 : int[] = prim::ListConstruct(%401, %402, %403, %404), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %406 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%380, %405), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %407 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %408 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%400, %406, %407), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %409 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %410 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %411 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %412 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %413 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %409, %410, %411, %412), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %414 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %415 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %416 : Float(204:262656, 512:513, 513:1) = aten::select(%413, %414, %415), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %417 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %418 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %419 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %420 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %421 : Float(204:262656, 255:513, 513:1) = aten::slice(%416, %417, %418, %419, %420), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %422 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %423 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %425 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %426 : Float(204:262656, 255:513, 255:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %427 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %429 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %430 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %431 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %427, %428, %429, %430), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %432 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %433 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %434 : Float(204:262656, 256:513, 513:1) = aten::select(%431, %432, %433), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %435 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %436 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %437 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %438 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %439 : Float(204:262656, 255:513, 513:1) = aten::slice(%434, %435, %436, %437, %438), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %440 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %441 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %442 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %443 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %444 : Float(204:262656, 255:513, 255:1) = aten::slice(%439, %440, %441, %442, %443), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %445 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %446 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %447 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %448 : int[] = prim::ListConstruct(%445, %446, %447), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %449 : Float(204:262656, 255:513, 255:1) = aten::view(%426, %448), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %450 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %451 : Float(204:262656, 255:513, 255:1) = aten::copy_(%444, %449, %450), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %452 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
    %453 : int[] = prim::ListConstruct(%63, %73, %67, %452), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %454 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.13, %453), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
    %455 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
    %456 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.19 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%454, %455, %456), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
    %458 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %459 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %460 : int[] = prim::ListConstruct(%458, %459), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %461 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %462 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %463 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %464 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %465 : Float(256:257, 257:1) = aten::ones(%460, %461, %462, %463, %464), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %466 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %467 : Float(256:257, 257:1) = aten::tril(%465, %466), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %468 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %469 : int[] = prim::ListConstruct(%468), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %beginning_mask_2d.13 : Float(256:257, 257:1) = aten::flip(%467, %469), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %471 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %472 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.13, %471), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %473 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %474 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %475 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %476 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %477 : Float(1:65792, 256:257, 257:1) = aten::slice(%472, %473, %474, %475, %476), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %478 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %479 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%477, %478), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %480 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %481 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %482 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %483 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.13 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%479, %480, %481, %482, %483), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %485 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:460:0
    %486 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:460:0
    %487 : int[] = prim::ListConstruct(%485, %486), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %ending_mask.13 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.13, %487), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:460:0
    %489 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %490 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %491 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %493 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.19, %489, %490, %491, %492), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %494 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %495 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %496 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %497 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %498 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%493, %494, %495, %496, %497), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %499 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %500 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %501 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %502 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %503 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%498, %499, %500, %501, %502), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %504 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %505 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %506 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %507 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.13 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%503, %504, %505, %506, %507), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %509 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %510 : int = aten::size(%beginning_input.13, %509), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %511 : Long() = prim::NumToTensor(%510), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %512 : int = aten::Int(%511), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %513 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %514 : int = aten::size(%beginning_input.13, %513), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %515 : Long() = prim::NumToTensor(%514), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %516 : int = aten::Int(%515), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %517 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %518 : int = aten::size(%beginning_input.13, %517), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %519 : Long() = prim::NumToTensor(%518), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %520 : int = aten::Int(%519), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %521 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %522 : int = aten::size(%beginning_input.13, %521), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %523 : Long() = prim::NumToTensor(%522), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %524 : int = aten::Int(%523), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %525 : int[] = prim::ListConstruct(%512, %516, %520, %524), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %526 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %527 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.13, %525, %526), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %528 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:22:0
    %529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%527, %528), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:22:0
    %530 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:463:0
    %531 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.13, %529, %530), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:463:0
    %532 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %533 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %534 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %535 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %536 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.19, %532, %533, %534, %535), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %537 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %538 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %539 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %541 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%536, %537, %538, %539, %540), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %542 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %543 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %544 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %545 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %546 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%541, %542, %543, %544, %545), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %547 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %548 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %549 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.13 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%546, %547, %548, %549, %550), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %552 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %553 : int = aten::size(%ending_input.13, %552), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %554 : Long() = prim::NumToTensor(%553), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %555 : int = aten::Int(%554), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %556 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %557 : int = aten::size(%ending_input.13, %556), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %558 : Long() = prim::NumToTensor(%557), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %559 : int = aten::Int(%558), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %560 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %561 : int = aten::size(%ending_input.13, %560), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %562 : Long() = prim::NumToTensor(%561), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %563 : int = aten::Int(%562), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %564 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %565 : int = aten::size(%ending_input.13, %564), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %566 : Long() = prim::NumToTensor(%565), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %567 : int = aten::Int(%566), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %568 : int[] = prim::ListConstruct(%555, %559, %563, %567), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %569 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %570 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.13, %568, %569), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %571 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:22:0
    %572 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%570, %571), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:22:0
    %573 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:466:0
    %574 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.13, %572, %573), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:466:0
    %575 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:22:0
    %576 : Bool(17:512, 512:1) = aten::ne(%9, %575), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:22:0
    %577 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %578 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %579 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %580 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %581 : Bool(17:512, 512:1) = aten::slice(%576, %577, %578, %579, %580), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %582 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %583 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %584 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %585 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %586 : Bool(17:512, 512:1) = aten::slice(%581, %582, %583, %584, %585), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %587 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %588 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%586, %587), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %589 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %remove_from_windowed_attention_mask.7 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%588, %589), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
    %591 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.7, %query.13), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:275:0
    %592 : float = prim::Constant[value=-10000.](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:275:0
    %float_mask.7 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%591, %remove_from_windowed_attention_mask.7, %592), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:275:0
    %594 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %595 : int = aten::size(%float_mask.7, %594), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %596 : Long() = prim::NumToTensor(%595), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %597 : int = aten::Int(%596), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %598 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %599 : int = aten::size(%float_mask.7, %598), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %600 : Long() = prim::NumToTensor(%599), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %601 : int = aten::Int(%600), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %602 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %603 : int = aten::size(%float_mask.7, %602), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %604 : Long() = prim::NumToTensor(%603), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %605 : int = aten::Int(%604), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %606 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %607 : int = aten::size(%float_mask.7, %606), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %608 : Long() = prim::NumToTensor(%607), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %609 : int = aten::Int(%608), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %610 : int[] = prim::ListConstruct(%597, %601, %605, %609), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %611 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %612 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %613 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %614 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %query.14 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%610, %611, %612, %613, %614), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
    %616 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %617 : int = aten::size(%query.14, %616), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.27 : Long() = prim::NumToTensor(%617), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %619 : int = aten::Int(%batch_size.27), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %620 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %621 : int = aten::size(%query.14, %620), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.28 : Long() = prim::NumToTensor(%621), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %623 : int = aten::Int(%seq_len.28), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %624 : int = aten::Int(%seq_len.28), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %625 : int = aten::Int(%seq_len.28), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %626 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %627 : int = aten::size(%query.14, %626), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.20 : Long() = prim::NumToTensor(%627), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %629 : int = aten::Int(%num_heads.20), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %630 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %631 : int = aten::size(%query.14, %630), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.20 : Long() = prim::NumToTensor(%631), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %633 : int = aten::Int(%head_dim.20), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %634 : int = aten::Int(%head_dim.20), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %667 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %668 : Long() = aten::floor_divide(%seq_len.28, %667), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %669 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:478:0
    %670 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.20 : Long() = aten::sub(%668, %669, %670), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:478:0
    %672 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
    %673 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
    %674 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.14, %672, %673), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
    %675 : Long() = aten::mul(%batch_size.27, %num_heads.20), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
    %676 : int = aten::Int(%675), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %677 : int[] = prim::ListConstruct(%676, %625, %634), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %hidden_states.72 : Float(17:512, 512:1, 1:1) = aten::reshape(%674, %677), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
    %679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
    %680 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
    %681 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.7, %679, %680), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
    %682 : Long() = aten::mul(%batch_size.27, %num_heads.20), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
    %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %684 : int[] = prim::ListConstruct(%683, %624, %633), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %hidden_states.74 : Float(17:512, 512:1, 1:1) = aten::reshape(%681, %684), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
    %686 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
    %687 : int = aten::size(%hidden_states.72, %686), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
    %688 : Long() = prim::NumToTensor(%687), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %690 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
    %691 : int = aten::size(%hidden_states.72, %690), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
    %692 : Long() = prim::NumToTensor(%691), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %693 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %694 : Long() = aten::floor_divide(%692, %693), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %696 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
    %697 : int = aten::size(%hidden_states.72, %696), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
    %698 : Long() = prim::NumToTensor(%697), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %699 : int = aten::Int(%698), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %700 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
    %701 : int[] = prim::ListConstruct(%689, %695, %700, %699), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %hidden_states.73 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.72, %701), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
    %703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %704 : int = aten::size(%hidden_states.73, %703), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %705 : Long() = prim::NumToTensor(%704), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %706 : int = aten::Int(%705), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %708 : int = aten::size(%hidden_states.73, %707), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %709 : Long() = prim::NumToTensor(%708), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %710 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %711 : int = aten::size(%hidden_states.73, %710), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %712 : Long() = prim::NumToTensor(%711), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %713 : int = aten::Int(%712), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %714 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %715 : int = aten::size(%hidden_states.73, %714), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %716 : Long() = prim::NumToTensor(%715), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %717 : int = aten::Int(%716), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %718 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %719 : Long() = aten::mul(%709, %718), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %720 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %721 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %722 : Long() = aten::sub(%719, %720, %721), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %723 : int = aten::Int(%722), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %724 : int[] = prim::ListConstruct(%706, %723, %713, %717), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %725 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %726 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %727 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %728 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %729 : int[] = prim::ListConstruct(%725, %726, %727, %728), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %730 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %731 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.73, %724, %729, %730), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %732 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
    %733 : int = aten::size(%hidden_states.74, %732), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
    %734 : Long() = prim::NumToTensor(%733), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %735 : int = aten::Int(%734), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %736 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
    %737 : int = aten::size(%hidden_states.74, %736), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
    %738 : Long() = prim::NumToTensor(%737), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %739 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %740 : Long() = aten::floor_divide(%738, %739), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %741 : int = aten::Int(%740), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
    %743 : int = aten::size(%hidden_states.74, %742), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
    %744 : Long() = prim::NumToTensor(%743), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %745 : int = aten::Int(%744), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %746 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
    %747 : int[] = prim::ListConstruct(%735, %741, %746, %745), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %hidden_states.75 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.74, %747), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
    %749 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %750 : int = aten::size(%hidden_states.75, %749), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %751 : Long() = prim::NumToTensor(%750), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %752 : int = aten::Int(%751), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %754 : int = aten::size(%hidden_states.75, %753), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %755 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %756 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %757 : int = aten::size(%hidden_states.75, %756), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %758 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %759 : int = aten::Int(%758), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %760 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %761 : int = aten::size(%hidden_states.75, %760), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
    %762 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %764 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %765 : Long() = aten::mul(%755, %764), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %766 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %767 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %768 : Long() = aten::sub(%765, %766, %767), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
    %769 : int = aten::Int(%768), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %770 : int[] = prim::ListConstruct(%752, %769, %759, %763), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %771 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %772 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %773 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %776 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.75, %770, %775, %776), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
    %778 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/functional.py:327:0
    %779 : Tensor[] = prim::ListConstruct(%731, %777), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %input.81 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%778, %779), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/functional.py:327:0
    %781 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %782 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %783 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %786 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.14 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.81, %785, %786), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %788 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %789 : int = aten::size(%hidden_states_padded.14, %788), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %790 : Long() = prim::NumToTensor(%789), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %791 : int = aten::Int(%790), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %792 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %793 : int = aten::size(%hidden_states_padded.14, %792), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %794 : Long() = prim::NumToTensor(%793), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %795 : int = aten::Int(%794), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %802 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %803 : int = aten::size(%hidden_states_padded.14, %802), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %804 : Long() = prim::NumToTensor(%803), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %805 : int = aten::Int(%804), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %806 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %807 : int = aten::size(%hidden_states_padded.14, %806), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
    %808 : Long() = prim::NumToTensor(%807), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %809 : int = aten::Int(%808), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %810 : int[] = prim::ListConstruct(%791, %795, %805, %809), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %diagonal_chunked_attention_scores.14 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.14, %810), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:400:0
    %812 : Long() = aten::mul(%batch_size.27, %num_heads.20), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
    %813 : int = aten::Int(%812), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %814 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
    %815 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
    %816 : Long() = aten::add(%chunks_count.20, %814, %815), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
    %817 : int = aten::Int(%816), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %818 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %819 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %820 : int[] = prim::ListConstruct(%813, %817, %818, %819), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %821 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %822 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %823 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %824 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.14 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.14, %820, %821, %822, %823, %824), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
    %826 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %827 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %828 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %829 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %830 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %826, %827, %828, %829), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %831 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %832 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %833 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %834 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%830, %831, %832, %833, %834), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %836 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %837 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %838 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %839 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%835, %836, %837, %838, %839), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %841 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %842 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %843 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %845 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%840, %841, %842, %843, %844), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %846 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %847 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %848 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %849 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %846, %847, %848, %849), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %851 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %852 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %853 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %855 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%850, %851, %852, %853, %854), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %856 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %857 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %858 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %859 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %860 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%855, %856, %857, %858, %859), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %861 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %862 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %863 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %864 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %865 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%860, %861, %862, %863, %864), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %866 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %867 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %868 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %869 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %870 : int[] = prim::ListConstruct(%866, %867, %868, %869), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %871 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%845, %870), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %872 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %873 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%865, %871, %872), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
    %874 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %875 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %876 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %877 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %874, %875, %876, %877), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %879 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %880 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %881 : Float(17:262656, 512:513, 513:1) = aten::select(%878, %879, %880), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %883 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %884 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %885 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %886 : Float(17:262656, 256:513, 513:1) = aten::slice(%881, %882, %883, %884, %885), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %887 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %889 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%886, %887, %888, %889, %890), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %892 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %893 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %894 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %895 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %896 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %892, %893, %894, %895), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %897 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %898 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %899 : Float(17:262656, 256:513, 513:1) = aten::select(%896, %897, %898), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %901 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %902 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %903 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %904 : Float(17:262656, 256:513, 513:1) = aten::slice(%899, %900, %901, %902, %903), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %905 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %906 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %907 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %909 : Float(17:262656, 256:513, 257:1) = aten::slice(%904, %905, %906, %907, %908), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %910 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %911 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %912 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %913 : int[] = prim::ListConstruct(%910, %911, %912), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %914 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %913), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %915 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %916 : Float(17:262656, 256:513, 257:1) = aten::copy_(%909, %914, %915), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
    %917 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %918 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %919 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %920 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %921 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %917, %918, %919, %920), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %922 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %923 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %924 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %925 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %926 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%921, %922, %923, %924, %925), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %927 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %928 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %929 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %930 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %931 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%926, %927, %928, %929, %930), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %932 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %933 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %934 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %935 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %936 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%931, %932, %933, %934, %935), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %938 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %939 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %940 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %941 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %937, %938, %939, %940), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %942 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %943 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %944 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %945 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %946 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%941, %942, %943, %944, %945), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %947 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %948 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %949 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %950 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %951 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%946, %947, %948, %949, %950), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %952 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %953 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %954 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %955 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %956 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%951, %952, %953, %954, %955), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %957 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %959 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %960 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %962 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%936, %961), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %963 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %964 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%956, %962, %963), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
    %965 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %966 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %967 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %969 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %965, %966, %967, %968), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %970 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %971 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %972 : Float(17:262656, 512:513, 513:1) = aten::select(%969, %970, %971), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %973 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %974 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %975 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %976 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %977 : Float(17:262656, 255:513, 513:1) = aten::slice(%972, %973, %974, %975, %976), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %978 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %979 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %980 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %981 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %982 : Float(17:262656, 255:513, 255:1) = aten::slice(%977, %978, %979, %980, %981), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %983 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %984 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %985 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %986 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %987 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %983, %984, %985, %986), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %988 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %989 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %990 : Float(17:262656, 256:513, 513:1) = aten::select(%987, %988, %989), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %991 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %993 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %994 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %995 : Float(17:262656, 255:513, 513:1) = aten::slice(%990, %991, %992, %993, %994), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %996 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %998 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %999 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %1000 : Float(17:262656, 255:513, 255:1) = aten::slice(%995, %996, %997, %998, %999), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %1001 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %1002 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %1003 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %1004 : int[] = prim::ListConstruct(%1001, %1002, %1003), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1005 : Float(17:262656, 255:513, 255:1) = aten::view(%982, %1004), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %1006 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1007 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1000, %1005, %1006), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
    %1008 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
    %1009 : int[] = prim::ListConstruct(%619, %629, %623, %1008), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1010 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.14, %1009), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
    %1011 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
    %1012 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.20 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1010, %1011, %1012), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
    %1014 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %1015 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %1016 : int[] = prim::ListConstruct(%1014, %1015), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1017 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %1018 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %1019 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %1020 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %1021 : Float(256:257, 257:1) = aten::ones(%1016, %1017, %1018, %1019, %1020), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %1022 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %1023 : Float(256:257, 257:1) = aten::tril(%1021, %1022), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %1024 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %1025 : int[] = prim::ListConstruct(%1024), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %beginning_mask_2d.14 : Float(256:257, 257:1) = aten::flip(%1023, %1025), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
    %1027 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1028 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.14, %1027), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1029 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1030 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1031 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1032 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1033 : Float(1:65792, 256:257, 257:1) = aten::slice(%1028, %1029, %1030, %1031, %1032), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1034 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1035 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1033, %1034), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1036 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1037 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1038 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1039 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.14 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1035, %1036, %1037, %1038, %1039), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
    %1041 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:460:0
    %1042 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:460:0
    %1043 : int[] = prim::ListConstruct(%1041, %1042), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %ending_mask.14 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.14, %1043), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:460:0
    %1045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1046 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1047 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1048 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1049 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.20, %1045, %1046, %1047, %1048), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1050 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1052 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1053 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1054 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1049, %1050, %1051, %1052, %1053), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1055 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1056 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1057 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1058 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1059 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1054, %1055, %1056, %1057, %1058), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1060 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1061 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1062 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.14 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1059, %1060, %1061, %1062, %1063), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
    %1065 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %1066 : int = aten::size(%beginning_input.14, %1065), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %1067 : Long() = prim::NumToTensor(%1066), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1068 : int = aten::Int(%1067), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1069 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %1070 : int = aten::size(%beginning_input.14, %1069), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %1071 : Long() = prim::NumToTensor(%1070), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1072 : int = aten::Int(%1071), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1073 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %1074 : int = aten::size(%beginning_input.14, %1073), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %1075 : Long() = prim::NumToTensor(%1074), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1076 : int = aten::Int(%1075), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1077 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %1078 : int = aten::size(%beginning_input.14, %1077), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %1079 : Long() = prim::NumToTensor(%1078), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1080 : int = aten::Int(%1079), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1081 : int[] = prim::ListConstruct(%1068, %1072, %1076, %1080), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1082 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %1083 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.14, %1081, %1082), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
    %1084 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:22:0
    %1085 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1083, %1084), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:22:0
    %1086 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:463:0
    %1087 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.14, %1085, %1086), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:463:0
    %1088 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1089 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1090 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1091 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1092 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.20, %1088, %1089, %1090, %1091), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1094 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1095 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1096 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1097 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1092, %1093, %1094, %1095, %1096), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1098 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1099 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1100 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1101 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1102 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1097, %1098, %1099, %1100, %1101), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1103 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1104 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1105 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1106 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.14 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1102, %1103, %1104, %1105, %1106), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
    %1108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %1109 : int = aten::size(%ending_input.14, %1108), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %1110 : Long() = prim::NumToTensor(%1109), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1111 : int = aten::Int(%1110), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1112 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %1113 : int = aten::size(%ending_input.14, %1112), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %1114 : Long() = prim::NumToTensor(%1113), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1115 : int = aten::Int(%1114), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1116 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %1117 : int = aten::size(%ending_input.14, %1116), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %1118 : Long() = prim::NumToTensor(%1117), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1119 : int = aten::Int(%1118), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1120 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %1121 : int = aten::size(%ending_input.14, %1120), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %1122 : Long() = prim::NumToTensor(%1121), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1124 : int[] = prim::ListConstruct(%1111, %1115, %1119, %1123), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1125 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %1126 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.14, %1124, %1125), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
    %1127 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:22:0
    %1128 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1126, %1127), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:22:0
    %1129 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:466:0
    %1130 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.14, %1128, %1129), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:466:0
    %1131 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:284:0
    %attn_scores.7 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.19, %input_tensor.20, %1131), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:284:0
    %1151 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:1500:0
    %1152 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:1500:0
    %attn_probs_fp32.7 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.7, %1151, %1152), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:1500:0
    %attn_probs.13 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::type_as(%attn_probs_fp32.7, %attn_scores.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:320:0
    %1155 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1156 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1157 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1158 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1159 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.7, %1155, %1156, %1157, %1158), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1161 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1162 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1163 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1164 : Bool(17:512, 512:1) = aten::slice(%1159, %1160, %1161, %1162, %1163), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1165 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1166 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1164, %1165), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1167 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1168 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1166, %1167), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1169 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %input.82 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs.13, %1168, %1169), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
    %1171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:973:0
    %1172 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:973:0
    %attn_probs.14 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.82, %1171, %1172), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:973:0
    %1174 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:331:0
    %1175 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:331:0
    %1176 : int[] = prim::ListConstruct(%28, %35, %1174, %1175), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1177 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1389, %1176), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:331:0
    %1178 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:331:0
    %1179 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:331:0
    %value.7 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1177, %1178, %1179), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:331:0
    %1181 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
    %1182 : int = aten::size(%value.7, %1181), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
    %batch_size.28 : Long() = prim::NumToTensor(%1182), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1184 : int = aten::Int(%batch_size.28), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1185 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
    %1186 : int = aten::size(%value.7, %1185), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
    %seq_len.29 : Long() = prim::NumToTensor(%1186), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1188 : int = aten::Int(%seq_len.29), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1189 : int = aten::Int(%seq_len.29), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1190 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
    %1191 : int = aten::size(%value.7, %1190), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
    %num_heads.21 : Long() = prim::NumToTensor(%1191), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1193 : int = aten::Int(%num_heads.21), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1194 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
    %1195 : int = aten::size(%value.7, %1194), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
    %head_dim.21 : Long() = prim::NumToTensor(%1195), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1197 : int = aten::Int(%head_dim.21), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1198 : int = aten::Int(%head_dim.21), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1199 : int = aten::Int(%head_dim.21), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1236 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %1237 : Long() = aten::floor_divide(%seq_len.29, %1236), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %1238 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:542:0
    %1239 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:542:0
    %chunks_count.21 : Long() = aten::sub(%1237, %1238, %1239), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:542:0
    %1241 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:545:0
    %1242 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:545:0
    %1243 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.14, %1241, %1242), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:545:0
    %1244 : Long() = aten::mul(%batch_size.28, %num_heads.21), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:546:0
    %1245 : int = aten::Int(%1244), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1246 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %1247 : Long() = aten::floor_divide(%seq_len.29, %1246), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/tensor.py:424:0
    %1248 : int = aten::Int(%1247), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1249 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:545:0
    %1250 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:545:0
    %1251 : int[] = prim::ListConstruct(%1245, %1248, %1249, %1250), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %chunked_hidden_states.31 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1243, %1251), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:545:0
    %1253 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
    %1254 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
    %1255 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.7, %1253, %1254), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
    %1256 : Long() = aten::mul(%batch_size.28, %num_heads.21), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
    %1257 : int = aten::Int(%1256), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1258 : int[] = prim::ListConstruct(%1257, %1189, %1199), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %input.83 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1255, %1258), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
    %1260 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %1261 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %1262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %1263 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %1264 : int[] = prim::ListConstruct(%1260, %1261, %1262, %1263), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1265 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %padded_value.7 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.83, %1264, %1265), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %1267 : Long() = aten::mul(%batch_size.28, %num_heads.21), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:556:0
    %1268 : int = aten::Int(%1267), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1269 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:556:0
    %1270 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:556:0
    %1271 : Long() = aten::add(%chunks_count.21, %1269, %1270), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:556:0
    %1272 : int = aten::Int(%1271), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:564:0
    %1274 : int[] = prim::ListConstruct(%1268, %1272, %1273, %1198), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1275 : int = prim::Constant[value=65536](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:564:0
    %1276 : int = prim::Constant[value=16384](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:564:0
    %1277 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:564:0
    %1278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:564:0
    %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1280 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1281 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.7, %1274, %1279, %1280), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:564:0
    %1282 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
    %1283 : int = aten::size(%chunked_hidden_states.31, %1282), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
    %total_num_heads.7 : Long() = prim::NumToTensor(%1283), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1285 : int = aten::Int(%total_num_heads.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1286 : int = aten::Int(%total_num_heads.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1287 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
    %1288 : int = aten::size(%chunked_hidden_states.31, %1287), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
    %num_chunks.7 : Long() = prim::NumToTensor(%1288), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1290 : int = aten::Int(%num_chunks.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1291 : int = aten::Int(%num_chunks.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1292 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
    %1293 : int = aten::size(%chunked_hidden_states.31, %1292), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
    %window_overlap.7 : Long() = prim::NumToTensor(%1293), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1295 : int = aten::Int(%window_overlap.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1296 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
    %1297 : int = aten::size(%chunked_hidden_states.31, %1296), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
    %hidden_dim.7 : Long() = prim::NumToTensor(%1297), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1299 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:422:0
    %1300 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:422:0
    %1301 : Long() = aten::add(%window_overlap.7, %1299, %1300), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:422:0
    %1302 : int = aten::Int(%1301), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1303 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %1304 : int[] = prim::ListConstruct(%1303, %1302), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1305 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %chunked_hidden_states.32 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.31, %1304, %1305), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
    %1307 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:424:0
    %1308 : int[] = prim::ListConstruct(%1286, %1291, %1307), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %chunked_hidden_states.33 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.32, %1308), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:424:0
    %1310 : Long() = aten::neg(%window_overlap.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:428:0
    %1311 : int = aten::Int(%1310), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1312 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1313 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1314 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1315 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1316 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.33, %1312, %1313, %1314, %1315), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1319 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1320 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1316, %1317, %1318, %1319, %1320), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1322 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1323 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1324 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %chunked_hidden_states.34 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1321, %1322, %1323, %1311, %1324), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
    %1326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:431:0
    %1327 : Long() = aten::add(%window_overlap.7, %hidden_dim.7, %1326), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:431:0
    %1328 : int = aten::Int(%1327), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1329 : int[] = prim::ListConstruct(%1285, %1290, %1295, %1328), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %chunked_hidden_states.35 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.34, %1329), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:430:0
    %1331 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1333 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1335 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.35, %1331, %1332, %1333, %1334), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1336 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1340 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1335, %1336, %1337, %1338, %1339), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1341 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1342 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1343 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1345 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1340, %1341, %1342, %1343, %1344), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1346 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1347 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1348 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1350 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1345, %1346, %1347, %1348, %1349), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
    %1351 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/functional.py:327:0
    %1352 : Tensor[] = prim::ListConstruct(%1350, %1281), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %context.7 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%1351, %1352), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/functional.py:327:0
    %1354 : int[] = prim::ListConstruct(%1184, %1193, %1188, %1197), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1355 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.7, %1354), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:569:0
    %1356 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:569:0
    %1357 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:569:0
    %attn_output.13 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1355, %1356, %1357), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:569:0
    %1377 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
    %1378 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
    %1379 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.13, %1377, %1378), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
    %1380 : int[] = prim::ListConstruct(%27, %34, %41), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
    %1381 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1379, %1380), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
    %1382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
    %attn_output.14 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1381, %1382), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
    %1384 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:374:0
    %1385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:374:0
    %input.84 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.14, %1384, %1385), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_longformer.py:374:0
    return (%input.84)

LongformerSelfAttention.key
Linear._actual_script_module
  graph(%self.109 : __torch__.torch.nn.modules.linear.Linear,
        %input.79 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.109)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.109)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
    %output.38 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.79, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
    %key_vectors.7 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.38, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
    return (%key_vectors.7)

LongformerSelfAttention.query
Linear._actual_script_module
  graph(%self.108 : __torch__.torch.nn.modules.linear.Linear,
        %input.79 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.108)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.108)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
    %output.37 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.79, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
    %query_vectors.13 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.37, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
    return (%query_vectors.13)

LongformerSelfAttention.value
Linear._actual_script_module
  graph(%self.110 : __torch__.torch.nn.modules.linear.Linear,
        %input.79 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.110)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.110)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
    %output.39 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.79, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
    %value_vectors.7 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.39, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
    return (%value_vectors.7)

LongformerSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.114 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.86 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.114)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.114)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.21 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.86, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.21)

LongformerSelfOutput.dense
Linear._actual_script_module
  graph(%self.112 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:768, 512:13056, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.112)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.112)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
    %output.40 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
    %input.85 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.40, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.85)

LongformerSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.113 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.76 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.76)

LongformerIntermediate.dense
Linear._actual_script_module
  graph(%self.116 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.116)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.116)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
    %output.41 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
    %input.87 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.41, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.87)

LongformerOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.120 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.90 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.120)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.120)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
    %hidden_states.78 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.90, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%hidden_states.78)

LongformerOutput.dense
Linear._actual_script_module
  graph(%self.118 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1572864, 512:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.118)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.118)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
    %output.42 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
    %input.89 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.42, %2, %6), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
    return (%input.89)

LongformerOutput.dropout
Dropout._actual_script_module
  graph(%self.119 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.77 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.77)

LongformerLayer._actual_script_module
  graph(%self.121 : __torch__.transformers.modeling_longformer.LongformerLayer,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%self.121)
    %4 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%self.121)
    %5 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%self.121)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %attention_mask, %2)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

LongformerLayer.attention
LongformerAttention._actual_script_module
  graph(%self.122 : __torch__.transformers.modeling_longformer.LongformerAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%self.122)
    %4 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%self.122)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %2)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %2)
    return (%8)

LongformerLayer.intermediate
LongformerIntermediate._actual_script_module
  graph(%self.131 : __torch__.transformers.modeling_longformer.LongformerIntermediate,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.131)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.100 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%5), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
    return (%input.100)

LongformerLayer.output
LongformerOutput._actual_script_module
  graph(%self.133 : __torch__.transformers.modeling_longformer.LongformerOutput,
        %1 : Float(17:1572864, 512:3072, 3072:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.133)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.133)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.133)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output # transformers/modeling_longformer.py:830:0
    %input.102 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output # transformers/modeling_longformer.py:830:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.102)
    return (%13)

LongformerAttention.output
LongformerSelfOutput._actual_script_module
  graph(%self.127 : __torch__.transformers.modeling_longformer.LongformerSelfOutput,
        %1 : Float(17:768, 512:13056, 768:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.127)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.127)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.127)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output # transformers/modeling_longformer.py:758:0
    %input.98 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output # transformers/modeling_longformer.py:758:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.98)
    return (%13)

LongformerAttention.self
LongformerSelfAttention._actual_script_module
  graph(%self.123 : __torch__.transformers.modeling_longformer.LongformerSelfAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.123)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.123)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.123)
    %6 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:241:0
    %7 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:241:0
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:241:0
    %9 : Float(17:512, 512:1) = aten::squeeze(%7, %8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:241:0
    %10 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:22:0
    %is_index_masked.8 : Bool(17:512, 512:1) = aten::lt(%9, %10), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:22:0
    %18 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:248:0
    %19 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:248:0
    %input.91 : Float(512:768, 17:393216, 768:1) = aten::transpose(%2, %18, %19), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:248:0
    %1387 : Tensor = prim::CallMethod[name="forward"](%5, %input.91)
    %1388 : Tensor = prim::CallMethod[name="forward"](%4, %input.91)
    %1389 : Tensor = prim::CallMethod[name="forward"](%3, %input.91)
    %24 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
    %25 : int = aten::size(%input.91, %24), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
    %seq_len.30 : Long() = prim::NumToTensor(%25), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %27 : int = aten::Int(%seq_len.30), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %28 : int = aten::Int(%seq_len.30), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %29 : int = aten::Int(%seq_len.30), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %30 : int = aten::Int(%seq_len.30), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %31 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
    %32 : int = aten::size(%input.91, %31), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
    %batch_size.29 : Long() = prim::NumToTensor(%32), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %34 : int = aten::Int(%batch_size.29), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %35 : int = aten::Int(%batch_size.29), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %36 : int = aten::Int(%batch_size.29), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %37 : int = aten::Int(%batch_size.29), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %38 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
    %39 : int = aten::size(%input.91, %38), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
    %embed_dim.8 : Long() = prim::NumToTensor(%39), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %41 : int = aten::Int(%embed_dim.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %44 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:261:0
    %query_vectors.16 : Float(512:13056, 17:768, 768:1) = aten::div_(%1387, %44), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:261:0
    %46 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:263:0
    %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:263:0
    %48 : int[] = prim::ListConstruct(%30, %37, %46, %47), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %49 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.16, %48), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:263:0
    %50 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:263:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:263:0
    %query.15 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%49, %50, %51), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:263:0
    %53 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:264:0
    %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:264:0
    %55 : int[] = prim::ListConstruct(%29, %36, %53, %54), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %56 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1388, %55), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:264:0
    %57 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:264:0
    %58 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:264:0
    %key.8 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%56, %57, %58), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:264:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %61 : int = aten::size(%query.15, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.30 : Long() = prim::NumToTensor(%61), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %63 : int = aten::Int(%batch_size.30), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %65 : int = aten::size(%query.15, %64), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.31 : Long() = prim::NumToTensor(%65), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %67 : int = aten::Int(%seq_len.31), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %68 : int = aten::Int(%seq_len.31), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %69 : int = aten::Int(%seq_len.31), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %70 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %71 : int = aten::size(%query.15, %70), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.22 : Long() = prim::NumToTensor(%71), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %73 : int = aten::Int(%num_heads.22), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %74 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %75 : int = aten::size(%query.15, %74), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.22 : Long() = prim::NumToTensor(%75), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %77 : int = aten::Int(%head_dim.22), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %78 : int = aten::Int(%head_dim.22), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %111 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %112 : Long() = aten::floor_divide(%seq_len.31, %111), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %113 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:478:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.22 : Long() = aten::sub(%112, %113, %114), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:478:0
    %116 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
    %117 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
    %118 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.15, %116, %117), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
    %119 : Long() = aten::mul(%batch_size.30, %num_heads.22), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
    %120 : int = aten::Int(%119), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %121 : int[] = prim::ListConstruct(%120, %69, %78), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %hidden_states.79 : Float(204:64, 512:13056, 64:1) = aten::reshape(%118, %121), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
    %123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
    %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
    %125 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.8, %123, %124), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
    %126 : Long() = aten::mul(%batch_size.30, %num_heads.22), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
    %127 : int = aten::Int(%126), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %128 : int[] = prim::ListConstruct(%127, %68, %77), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %hidden_states.81 : Float(204:64, 512:13056, 64:1) = aten::reshape(%125, %128), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
    %130 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
    %131 : int = aten::size(%hidden_states.79, %130), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
    %132 : Long() = prim::NumToTensor(%131), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %133 : int = aten::Int(%132), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %134 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
    %135 : int = aten::size(%hidden_states.79, %134), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
    %136 : Long() = prim::NumToTensor(%135), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %137 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %138 : Long() = aten::floor_divide(%136, %137), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %139 : int = aten::Int(%138), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %140 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
    %141 : int = aten::size(%hidden_states.79, %140), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
    %142 : Long() = prim::NumToTensor(%141), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %143 : int = aten::Int(%142), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %144 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
    %145 : int[] = prim::ListConstruct(%133, %139, %144, %143), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %hidden_states.80 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.79, %145), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
    %147 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %148 : int = aten::size(%hidden_states.80, %147), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %149 : Long() = prim::NumToTensor(%148), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %150 : int = aten::Int(%149), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %151 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %152 : int = aten::size(%hidden_states.80, %151), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %153 : Long() = prim::NumToTensor(%152), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %154 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %155 : int = aten::size(%hidden_states.80, %154), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %156 : Long() = prim::NumToTensor(%155), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %157 : int = aten::Int(%156), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %158 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %159 : int = aten::size(%hidden_states.80, %158), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %160 : Long() = prim::NumToTensor(%159), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %161 : int = aten::Int(%160), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %162 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %163 : Long() = aten::mul(%153, %162), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %164 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %165 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %166 : Long() = aten::sub(%163, %164, %165), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %167 : int = aten::Int(%166), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %168 : int[] = prim::ListConstruct(%150, %167, %157, %161), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %169 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %170 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %171 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %172 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %173 : int[] = prim::ListConstruct(%169, %170, %171, %172), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %174 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %175 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.80, %168, %173, %174), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %176 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
    %177 : int = aten::size(%hidden_states.81, %176), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %179 : int = aten::Int(%178), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
    %181 : int = aten::size(%hidden_states.81, %180), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
    %182 : Long() = prim::NumToTensor(%181), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %183 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %184 : Long() = aten::floor_divide(%182, %183), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %185 : int = aten::Int(%184), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %186 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
    %187 : int = aten::size(%hidden_states.81, %186), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
    %188 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %189 : int = aten::Int(%188), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %190 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
    %191 : int[] = prim::ListConstruct(%179, %185, %190, %189), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %hidden_states.82 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.81, %191), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
    %193 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %194 : int = aten::size(%hidden_states.82, %193), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %195 : Long() = prim::NumToTensor(%194), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %196 : int = aten::Int(%195), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %197 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %198 : int = aten::size(%hidden_states.82, %197), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %199 : Long() = prim::NumToTensor(%198), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %200 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %201 : int = aten::size(%hidden_states.82, %200), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %203 : int = aten::Int(%202), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %204 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %205 : int = aten::size(%hidden_states.82, %204), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %207 : int = aten::Int(%206), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %208 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %209 : Long() = aten::mul(%199, %208), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %210 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %211 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %212 : Long() = aten::sub(%209, %210, %211), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %213 : int = aten::Int(%212), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %214 : int[] = prim::ListConstruct(%196, %213, %203, %207), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %216 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %217 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %218 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %219 : int[] = prim::ListConstruct(%215, %216, %217, %218), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %220 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.82, %214, %219, %220), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %222 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/functional.py:327:0
    %223 : Tensor[] = prim::ListConstruct(%175, %221), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %input.92 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%222, %223), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/functional.py:327:0
    %225 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %226 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %227 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %229 : int[] = prim::ListConstruct(%225, %226, %227, %228), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %230 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.15 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.92, %229, %230), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %232 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %233 : int = aten::size(%hidden_states_padded.15, %232), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %234 : Long() = prim::NumToTensor(%233), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %235 : int = aten::Int(%234), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %236 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %237 : int = aten::size(%hidden_states_padded.15, %236), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %238 : Long() = prim::NumToTensor(%237), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %239 : int = aten::Int(%238), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %246 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %247 : int = aten::size(%hidden_states_padded.15, %246), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %248 : Long() = prim::NumToTensor(%247), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %249 : int = aten::Int(%248), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %250 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %251 : int = aten::size(%hidden_states_padded.15, %250), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %252 : Long() = prim::NumToTensor(%251), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %253 : int = aten::Int(%252), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %254 : int[] = prim::ListConstruct(%235, %239, %249, %253), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %diagonal_chunked_attention_scores.15 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.15, %254), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:400:0
    %256 : Long() = aten::mul(%batch_size.30, %num_heads.22), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
    %257 : int = aten::Int(%256), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %258 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
    %259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
    %260 : Long() = aten::add(%chunks_count.22, %258, %259), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
    %261 : int = aten::Int(%260), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %263 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %264 : int[] = prim::ListConstruct(%257, %261, %262, %263), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %265 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %266 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %268 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.15 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.15, %264, %265, %266, %267, %268), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %270 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %271 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %273 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %270, %271, %272, %273), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %275 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %276 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %277 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %279 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%274, %275, %276, %277, %278), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %280 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %281 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %282 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %284 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%279, %280, %281, %282, %283), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %285 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %286 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %287 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %289 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%284, %285, %286, %287, %288), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %290 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %291 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %292 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %294 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %290, %291, %292, %293), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %295 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %296 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %297 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %299 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%294, %295, %296, %297, %298), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %300 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %301 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %302 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %303 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %304 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%299, %300, %301, %302, %303), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %305 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %306 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %307 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %308 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %309 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%304, %305, %306, %307, %308), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %310 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %311 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %312 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %313 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %314 : int[] = prim::ListConstruct(%310, %311, %312, %313), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %315 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%289, %314), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %316 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %317 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%309, %315, %316), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %319 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %320 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %321 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %322 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %318, %319, %320, %321), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %323 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %324 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %325 : Float(204:262656, 512:513, 513:1) = aten::select(%322, %323, %324), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %327 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %328 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %329 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %330 : Float(204:262656, 256:513, 513:1) = aten::slice(%325, %326, %327, %328, %329), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %331 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %333 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %335 : Float(204:262656, 256:513, 257:1) = aten::slice(%330, %331, %332, %333, %334), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %336 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %340 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %336, %337, %338, %339), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %341 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %342 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %343 : Float(204:262656, 256:513, 513:1) = aten::select(%340, %341, %342), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %345 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %347 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %348 : Float(204:262656, 256:513, 513:1) = aten::slice(%343, %344, %345, %346, %347), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %349 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %350 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %352 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %353 : Float(204:262656, 256:513, 257:1) = aten::slice(%348, %349, %350, %351, %352), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %354 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %355 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %356 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %357 : int[] = prim::ListConstruct(%354, %355, %356), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %358 : Float(204:262656, 256:513, 257:1) = aten::view(%335, %357), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %359 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %360 : Float(204:262656, 256:513, 257:1) = aten::copy_(%353, %358, %359), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %361 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %362 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %363 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %364 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %365 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %361, %362, %363, %364), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %366 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %367 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %368 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %369 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %370 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%365, %366, %367, %368, %369), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %371 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %372 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %373 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %374 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %375 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%370, %371, %372, %373, %374), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %376 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %377 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %379 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %380 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%375, %376, %377, %378, %379), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %383 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %384 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %385 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %381, %382, %383, %384), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %386 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %387 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %388 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %389 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %390 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%385, %386, %387, %388, %389), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %391 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %392 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %393 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %394 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %395 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%390, %391, %392, %393, %394), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %397 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %398 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %399 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %400 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%395, %396, %397, %398, %399), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %401 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %402 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %403 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %404 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %405 : int[] = prim::ListConstruct(%401, %402, %403, %404), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %406 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%380, %405), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %407 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %408 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%400, %406, %407), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %409 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %410 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %411 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %412 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %413 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %409, %410, %411, %412), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %414 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %415 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %416 : Float(204:262656, 512:513, 513:1) = aten::select(%413, %414, %415), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %417 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %418 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %419 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %420 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %421 : Float(204:262656, 255:513, 513:1) = aten::slice(%416, %417, %418, %419, %420), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %422 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %423 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %425 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %426 : Float(204:262656, 255:513, 255:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %427 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %429 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %430 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %431 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %427, %428, %429, %430), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %432 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %433 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %434 : Float(204:262656, 256:513, 513:1) = aten::select(%431, %432, %433), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %435 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %436 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %437 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %438 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %439 : Float(204:262656, 255:513, 513:1) = aten::slice(%434, %435, %436, %437, %438), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %440 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %441 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %442 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %443 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %444 : Float(204:262656, 255:513, 255:1) = aten::slice(%439, %440, %441, %442, %443), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %445 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %446 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %447 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %448 : int[] = prim::ListConstruct(%445, %446, %447), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %449 : Float(204:262656, 255:513, 255:1) = aten::view(%426, %448), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %450 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %451 : Float(204:262656, 255:513, 255:1) = aten::copy_(%444, %449, %450), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %452 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
    %453 : int[] = prim::ListConstruct(%63, %73, %67, %452), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %454 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.15, %453), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
    %455 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
    %456 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.22 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%454, %455, %456), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
    %458 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %459 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %460 : int[] = prim::ListConstruct(%458, %459), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %461 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %462 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %463 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %464 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %465 : Float(256:257, 257:1) = aten::ones(%460, %461, %462, %463, %464), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %466 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %467 : Float(256:257, 257:1) = aten::tril(%465, %466), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %468 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %469 : int[] = prim::ListConstruct(%468), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %beginning_mask_2d.15 : Float(256:257, 257:1) = aten::flip(%467, %469), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %471 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %472 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.15, %471), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %473 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %474 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %475 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %476 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %477 : Float(1:65792, 256:257, 257:1) = aten::slice(%472, %473, %474, %475, %476), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %478 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %479 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%477, %478), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %480 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %481 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %482 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %483 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.15 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%479, %480, %481, %482, %483), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %485 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:460:0
    %486 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:460:0
    %487 : int[] = prim::ListConstruct(%485, %486), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %ending_mask.15 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.15, %487), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:460:0
    %489 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %490 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %491 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %493 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.22, %489, %490, %491, %492), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %494 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %495 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %496 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %497 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %498 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%493, %494, %495, %496, %497), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %499 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %500 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %501 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %502 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %503 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%498, %499, %500, %501, %502), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %504 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %505 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %506 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %507 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.15 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%503, %504, %505, %506, %507), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %509 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %510 : int = aten::size(%beginning_input.15, %509), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %511 : Long() = prim::NumToTensor(%510), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %512 : int = aten::Int(%511), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %513 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %514 : int = aten::size(%beginning_input.15, %513), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %515 : Long() = prim::NumToTensor(%514), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %516 : int = aten::Int(%515), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %517 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %518 : int = aten::size(%beginning_input.15, %517), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %519 : Long() = prim::NumToTensor(%518), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %520 : int = aten::Int(%519), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %521 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %522 : int = aten::size(%beginning_input.15, %521), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %523 : Long() = prim::NumToTensor(%522), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %524 : int = aten::Int(%523), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %525 : int[] = prim::ListConstruct(%512, %516, %520, %524), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %526 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %527 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.15, %525, %526), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %528 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:22:0
    %529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%527, %528), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:22:0
    %530 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:463:0
    %531 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.15, %529, %530), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:463:0
    %532 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %533 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %534 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %535 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %536 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.22, %532, %533, %534, %535), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %537 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %538 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %539 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %541 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%536, %537, %538, %539, %540), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %542 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %543 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %544 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %545 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %546 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%541, %542, %543, %544, %545), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %547 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %548 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %549 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.15 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%546, %547, %548, %549, %550), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %552 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %553 : int = aten::size(%ending_input.15, %552), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %554 : Long() = prim::NumToTensor(%553), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %555 : int = aten::Int(%554), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %556 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %557 : int = aten::size(%ending_input.15, %556), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %558 : Long() = prim::NumToTensor(%557), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %559 : int = aten::Int(%558), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %560 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %561 : int = aten::size(%ending_input.15, %560), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %562 : Long() = prim::NumToTensor(%561), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %563 : int = aten::Int(%562), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %564 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %565 : int = aten::size(%ending_input.15, %564), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %566 : Long() = prim::NumToTensor(%565), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %567 : int = aten::Int(%566), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %568 : int[] = prim::ListConstruct(%555, %559, %563, %567), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %569 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %570 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.15, %568, %569), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %571 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:22:0
    %572 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%570, %571), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:22:0
    %573 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:466:0
    %574 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.15, %572, %573), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:466:0
    %575 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:22:0
    %576 : Bool(17:512, 512:1) = aten::ne(%9, %575), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:22:0
    %577 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %578 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %579 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %580 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %581 : Bool(17:512, 512:1) = aten::slice(%576, %577, %578, %579, %580), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %582 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %583 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %584 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %585 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %586 : Bool(17:512, 512:1) = aten::slice(%581, %582, %583, %584, %585), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %587 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %588 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%586, %587), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %589 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %remove_from_windowed_attention_mask.8 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%588, %589), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
    %591 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.8, %query.15), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:275:0
    %592 : float = prim::Constant[value=-10000.](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:275:0
    %float_mask.8 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%591, %remove_from_windowed_attention_mask.8, %592), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:275:0
    %594 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %595 : int = aten::size(%float_mask.8, %594), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %596 : Long() = prim::NumToTensor(%595), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %597 : int = aten::Int(%596), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %598 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %599 : int = aten::size(%float_mask.8, %598), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %600 : Long() = prim::NumToTensor(%599), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %601 : int = aten::Int(%600), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %602 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %603 : int = aten::size(%float_mask.8, %602), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %604 : Long() = prim::NumToTensor(%603), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %605 : int = aten::Int(%604), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %606 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %607 : int = aten::size(%float_mask.8, %606), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %608 : Long() = prim::NumToTensor(%607), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %609 : int = aten::Int(%608), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %610 : int[] = prim::ListConstruct(%597, %601, %605, %609), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %611 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %612 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %613 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %614 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %query.16 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%610, %611, %612, %613, %614), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
    %616 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %617 : int = aten::size(%query.16, %616), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.31 : Long() = prim::NumToTensor(%617), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %619 : int = aten::Int(%batch_size.31), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %620 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %621 : int = aten::size(%query.16, %620), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.32 : Long() = prim::NumToTensor(%621), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %623 : int = aten::Int(%seq_len.32), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %624 : int = aten::Int(%seq_len.32), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %625 : int = aten::Int(%seq_len.32), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %626 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %627 : int = aten::size(%query.16, %626), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.23 : Long() = prim::NumToTensor(%627), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %629 : int = aten::Int(%num_heads.23), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %630 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %631 : int = aten::size(%query.16, %630), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.23 : Long() = prim::NumToTensor(%631), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %633 : int = aten::Int(%head_dim.23), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %634 : int = aten::Int(%head_dim.23), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %667 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %668 : Long() = aten::floor_divide(%seq_len.32, %667), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %669 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:478:0
    %670 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.23 : Long() = aten::sub(%668, %669, %670), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:478:0
    %672 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
    %673 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
    %674 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.16, %672, %673), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
    %675 : Long() = aten::mul(%batch_size.31, %num_heads.23), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
    %676 : int = aten::Int(%675), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %677 : int[] = prim::ListConstruct(%676, %625, %634), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %hidden_states.83 : Float(17:512, 512:1, 1:1) = aten::reshape(%674, %677), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
    %679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
    %680 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
    %681 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.8, %679, %680), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
    %682 : Long() = aten::mul(%batch_size.31, %num_heads.23), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
    %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %684 : int[] = prim::ListConstruct(%683, %624, %633), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %hidden_states.85 : Float(17:512, 512:1, 1:1) = aten::reshape(%681, %684), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
    %686 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
    %687 : int = aten::size(%hidden_states.83, %686), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
    %688 : Long() = prim::NumToTensor(%687), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %690 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
    %691 : int = aten::size(%hidden_states.83, %690), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
    %692 : Long() = prim::NumToTensor(%691), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %693 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %694 : Long() = aten::floor_divide(%692, %693), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %696 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
    %697 : int = aten::size(%hidden_states.83, %696), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
    %698 : Long() = prim::NumToTensor(%697), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %699 : int = aten::Int(%698), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %700 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
    %701 : int[] = prim::ListConstruct(%689, %695, %700, %699), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %hidden_states.84 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.83, %701), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
    %703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %704 : int = aten::size(%hidden_states.84, %703), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %705 : Long() = prim::NumToTensor(%704), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %706 : int = aten::Int(%705), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %708 : int = aten::size(%hidden_states.84, %707), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %709 : Long() = prim::NumToTensor(%708), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %710 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %711 : int = aten::size(%hidden_states.84, %710), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %712 : Long() = prim::NumToTensor(%711), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %713 : int = aten::Int(%712), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %714 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %715 : int = aten::size(%hidden_states.84, %714), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %716 : Long() = prim::NumToTensor(%715), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %717 : int = aten::Int(%716), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %718 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %719 : Long() = aten::mul(%709, %718), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %720 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %721 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %722 : Long() = aten::sub(%719, %720, %721), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %723 : int = aten::Int(%722), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %724 : int[] = prim::ListConstruct(%706, %723, %713, %717), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %725 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %726 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %727 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %728 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %729 : int[] = prim::ListConstruct(%725, %726, %727, %728), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %730 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %731 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.84, %724, %729, %730), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %732 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
    %733 : int = aten::size(%hidden_states.85, %732), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
    %734 : Long() = prim::NumToTensor(%733), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %735 : int = aten::Int(%734), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %736 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
    %737 : int = aten::size(%hidden_states.85, %736), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
    %738 : Long() = prim::NumToTensor(%737), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %739 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %740 : Long() = aten::floor_divide(%738, %739), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %741 : int = aten::Int(%740), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
    %743 : int = aten::size(%hidden_states.85, %742), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
    %744 : Long() = prim::NumToTensor(%743), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %745 : int = aten::Int(%744), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %746 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
    %747 : int[] = prim::ListConstruct(%735, %741, %746, %745), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %hidden_states.86 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.85, %747), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
    %749 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %750 : int = aten::size(%hidden_states.86, %749), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %751 : Long() = prim::NumToTensor(%750), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %752 : int = aten::Int(%751), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %754 : int = aten::size(%hidden_states.86, %753), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %755 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %756 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %757 : int = aten::size(%hidden_states.86, %756), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %758 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %759 : int = aten::Int(%758), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %760 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %761 : int = aten::size(%hidden_states.86, %760), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
    %762 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %764 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %765 : Long() = aten::mul(%755, %764), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %766 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %767 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %768 : Long() = aten::sub(%765, %766, %767), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
    %769 : int = aten::Int(%768), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %770 : int[] = prim::ListConstruct(%752, %769, %759, %763), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %771 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %772 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %773 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %776 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.86, %770, %775, %776), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
    %778 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/functional.py:327:0
    %779 : Tensor[] = prim::ListConstruct(%731, %777), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %input.93 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%778, %779), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/functional.py:327:0
    %781 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %782 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %783 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %786 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.16 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.93, %785, %786), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %788 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %789 : int = aten::size(%hidden_states_padded.16, %788), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %790 : Long() = prim::NumToTensor(%789), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %791 : int = aten::Int(%790), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %792 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %793 : int = aten::size(%hidden_states_padded.16, %792), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %794 : Long() = prim::NumToTensor(%793), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %795 : int = aten::Int(%794), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %802 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %803 : int = aten::size(%hidden_states_padded.16, %802), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %804 : Long() = prim::NumToTensor(%803), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %805 : int = aten::Int(%804), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %806 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %807 : int = aten::size(%hidden_states_padded.16, %806), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
    %808 : Long() = prim::NumToTensor(%807), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %809 : int = aten::Int(%808), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %810 : int[] = prim::ListConstruct(%791, %795, %805, %809), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %diagonal_chunked_attention_scores.16 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.16, %810), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:400:0
    %812 : Long() = aten::mul(%batch_size.31, %num_heads.23), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
    %813 : int = aten::Int(%812), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %814 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
    %815 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
    %816 : Long() = aten::add(%chunks_count.23, %814, %815), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
    %817 : int = aten::Int(%816), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %818 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %819 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %820 : int[] = prim::ListConstruct(%813, %817, %818, %819), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %821 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %822 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %823 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %824 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.16 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.16, %820, %821, %822, %823, %824), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
    %826 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %827 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %828 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %829 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %830 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %826, %827, %828, %829), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %831 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %832 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %833 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %834 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%830, %831, %832, %833, %834), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %836 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %837 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %838 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %839 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%835, %836, %837, %838, %839), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %841 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %842 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %843 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %845 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%840, %841, %842, %843, %844), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %846 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %847 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %848 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %849 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %846, %847, %848, %849), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %851 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %852 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %853 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %855 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%850, %851, %852, %853, %854), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %856 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %857 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %858 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %859 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %860 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%855, %856, %857, %858, %859), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %861 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %862 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %863 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %864 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %865 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%860, %861, %862, %863, %864), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %866 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %867 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %868 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %869 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %870 : int[] = prim::ListConstruct(%866, %867, %868, %869), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %871 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%845, %870), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %872 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %873 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%865, %871, %872), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
    %874 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %875 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %876 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %877 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %874, %875, %876, %877), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %879 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %880 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %881 : Float(17:262656, 512:513, 513:1) = aten::select(%878, %879, %880), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %883 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %884 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %885 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %886 : Float(17:262656, 256:513, 513:1) = aten::slice(%881, %882, %883, %884, %885), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %887 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %889 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%886, %887, %888, %889, %890), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %892 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %893 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %894 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %895 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %896 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %892, %893, %894, %895), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %897 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %898 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %899 : Float(17:262656, 256:513, 513:1) = aten::select(%896, %897, %898), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %901 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %902 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %903 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %904 : Float(17:262656, 256:513, 513:1) = aten::slice(%899, %900, %901, %902, %903), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %905 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %906 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %907 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %909 : Float(17:262656, 256:513, 257:1) = aten::slice(%904, %905, %906, %907, %908), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %910 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %911 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %912 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %913 : int[] = prim::ListConstruct(%910, %911, %912), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %914 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %913), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %915 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %916 : Float(17:262656, 256:513, 257:1) = aten::copy_(%909, %914, %915), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
    %917 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %918 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %919 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %920 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %921 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %917, %918, %919, %920), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %922 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %923 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %924 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %925 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %926 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%921, %922, %923, %924, %925), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %927 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %928 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %929 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %930 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %931 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%926, %927, %928, %929, %930), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %932 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %933 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %934 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %935 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %936 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%931, %932, %933, %934, %935), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %938 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %939 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %940 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %941 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %937, %938, %939, %940), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %942 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %943 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %944 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %945 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %946 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%941, %942, %943, %944, %945), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %947 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %948 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %949 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %950 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %951 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%946, %947, %948, %949, %950), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %952 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %953 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %954 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %955 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %956 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%951, %952, %953, %954, %955), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %957 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %959 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %960 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %962 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%936, %961), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %963 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %964 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%956, %962, %963), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
    %965 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %966 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %967 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %969 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %965, %966, %967, %968), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %970 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %971 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %972 : Float(17:262656, 512:513, 513:1) = aten::select(%969, %970, %971), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %973 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %974 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %975 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %976 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %977 : Float(17:262656, 255:513, 513:1) = aten::slice(%972, %973, %974, %975, %976), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %978 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %979 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %980 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %981 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %982 : Float(17:262656, 255:513, 255:1) = aten::slice(%977, %978, %979, %980, %981), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %983 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %984 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %985 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %986 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %987 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %983, %984, %985, %986), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %988 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %989 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %990 : Float(17:262656, 256:513, 513:1) = aten::select(%987, %988, %989), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %991 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %993 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %994 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %995 : Float(17:262656, 255:513, 513:1) = aten::slice(%990, %991, %992, %993, %994), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %996 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %998 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %999 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %1000 : Float(17:262656, 255:513, 255:1) = aten::slice(%995, %996, %997, %998, %999), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %1001 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %1002 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %1003 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %1004 : int[] = prim::ListConstruct(%1001, %1002, %1003), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1005 : Float(17:262656, 255:513, 255:1) = aten::view(%982, %1004), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %1006 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1007 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1000, %1005, %1006), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
    %1008 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
    %1009 : int[] = prim::ListConstruct(%619, %629, %623, %1008), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1010 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.16, %1009), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
    %1011 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
    %1012 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.23 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1010, %1011, %1012), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
    %1014 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %1015 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %1016 : int[] = prim::ListConstruct(%1014, %1015), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1017 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %1018 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %1019 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %1020 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %1021 : Float(256:257, 257:1) = aten::ones(%1016, %1017, %1018, %1019, %1020), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %1022 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %1023 : Float(256:257, 257:1) = aten::tril(%1021, %1022), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %1024 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %1025 : int[] = prim::ListConstruct(%1024), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %beginning_mask_2d.16 : Float(256:257, 257:1) = aten::flip(%1023, %1025), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
    %1027 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1028 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.16, %1027), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1029 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1030 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1031 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1032 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1033 : Float(1:65792, 256:257, 257:1) = aten::slice(%1028, %1029, %1030, %1031, %1032), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1034 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1035 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1033, %1034), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1036 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1037 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1038 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1039 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.16 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1035, %1036, %1037, %1038, %1039), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
    %1041 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:460:0
    %1042 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:460:0
    %1043 : int[] = prim::ListConstruct(%1041, %1042), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %ending_mask.16 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.16, %1043), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:460:0
    %1045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1046 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1047 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1048 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1049 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.23, %1045, %1046, %1047, %1048), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1050 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1052 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1053 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1054 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1049, %1050, %1051, %1052, %1053), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1055 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1056 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1057 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1058 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1059 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1054, %1055, %1056, %1057, %1058), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1060 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1061 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1062 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.16 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1059, %1060, %1061, %1062, %1063), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
    %1065 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %1066 : int = aten::size(%beginning_input.16, %1065), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %1067 : Long() = prim::NumToTensor(%1066), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1068 : int = aten::Int(%1067), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1069 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %1070 : int = aten::size(%beginning_input.16, %1069), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %1071 : Long() = prim::NumToTensor(%1070), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1072 : int = aten::Int(%1071), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1073 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %1074 : int = aten::size(%beginning_input.16, %1073), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %1075 : Long() = prim::NumToTensor(%1074), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1076 : int = aten::Int(%1075), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1077 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %1078 : int = aten::size(%beginning_input.16, %1077), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %1079 : Long() = prim::NumToTensor(%1078), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1080 : int = aten::Int(%1079), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1081 : int[] = prim::ListConstruct(%1068, %1072, %1076, %1080), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1082 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %1083 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.16, %1081, %1082), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
    %1084 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:22:0
    %1085 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1083, %1084), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:22:0
    %1086 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:463:0
    %1087 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.16, %1085, %1086), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:463:0
    %1088 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1089 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1090 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1091 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1092 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.23, %1088, %1089, %1090, %1091), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1094 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1095 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1096 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1097 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1092, %1093, %1094, %1095, %1096), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1098 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1099 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1100 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1101 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1102 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1097, %1098, %1099, %1100, %1101), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1103 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1104 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1105 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1106 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.16 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1102, %1103, %1104, %1105, %1106), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
    %1108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %1109 : int = aten::size(%ending_input.16, %1108), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %1110 : Long() = prim::NumToTensor(%1109), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1111 : int = aten::Int(%1110), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1112 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %1113 : int = aten::size(%ending_input.16, %1112), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %1114 : Long() = prim::NumToTensor(%1113), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1115 : int = aten::Int(%1114), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1116 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %1117 : int = aten::size(%ending_input.16, %1116), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %1118 : Long() = prim::NumToTensor(%1117), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1119 : int = aten::Int(%1118), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1120 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %1121 : int = aten::size(%ending_input.16, %1120), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %1122 : Long() = prim::NumToTensor(%1121), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1124 : int[] = prim::ListConstruct(%1111, %1115, %1119, %1123), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1125 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %1126 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.16, %1124, %1125), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
    %1127 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:22:0
    %1128 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1126, %1127), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:22:0
    %1129 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:466:0
    %1130 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.16, %1128, %1129), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:466:0
    %1131 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:284:0
    %attn_scores.8 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.22, %input_tensor.23, %1131), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:284:0
    %1151 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:1500:0
    %1152 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:1500:0
    %attn_probs_fp32.8 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.8, %1151, %1152), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:1500:0
    %attn_probs.15 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::type_as(%attn_probs_fp32.8, %attn_scores.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:320:0
    %1155 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1156 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1157 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1158 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1159 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.8, %1155, %1156, %1157, %1158), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1161 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1162 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1163 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1164 : Bool(17:512, 512:1) = aten::slice(%1159, %1160, %1161, %1162, %1163), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1165 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1166 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1164, %1165), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1167 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1168 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1166, %1167), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1169 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %input.94 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs.15, %1168, %1169), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
    %1171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:973:0
    %1172 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:973:0
    %attn_probs.16 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.94, %1171, %1172), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:973:0
    %1174 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:331:0
    %1175 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:331:0
    %1176 : int[] = prim::ListConstruct(%28, %35, %1174, %1175), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1177 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1389, %1176), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:331:0
    %1178 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:331:0
    %1179 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:331:0
    %value.8 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1177, %1178, %1179), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:331:0
    %1181 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
    %1182 : int = aten::size(%value.8, %1181), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
    %batch_size.32 : Long() = prim::NumToTensor(%1182), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1184 : int = aten::Int(%batch_size.32), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1185 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
    %1186 : int = aten::size(%value.8, %1185), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
    %seq_len.33 : Long() = prim::NumToTensor(%1186), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1188 : int = aten::Int(%seq_len.33), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1189 : int = aten::Int(%seq_len.33), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1190 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
    %1191 : int = aten::size(%value.8, %1190), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
    %num_heads.24 : Long() = prim::NumToTensor(%1191), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1193 : int = aten::Int(%num_heads.24), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1194 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
    %1195 : int = aten::size(%value.8, %1194), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
    %head_dim.24 : Long() = prim::NumToTensor(%1195), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1197 : int = aten::Int(%head_dim.24), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1198 : int = aten::Int(%head_dim.24), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1199 : int = aten::Int(%head_dim.24), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1236 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %1237 : Long() = aten::floor_divide(%seq_len.33, %1236), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %1238 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:542:0
    %1239 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:542:0
    %chunks_count.24 : Long() = aten::sub(%1237, %1238, %1239), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:542:0
    %1241 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:545:0
    %1242 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:545:0
    %1243 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.16, %1241, %1242), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:545:0
    %1244 : Long() = aten::mul(%batch_size.32, %num_heads.24), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:546:0
    %1245 : int = aten::Int(%1244), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1246 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %1247 : Long() = aten::floor_divide(%seq_len.33, %1246), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/tensor.py:424:0
    %1248 : int = aten::Int(%1247), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1249 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:545:0
    %1250 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:545:0
    %1251 : int[] = prim::ListConstruct(%1245, %1248, %1249, %1250), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %chunked_hidden_states.36 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1243, %1251), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:545:0
    %1253 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
    %1254 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
    %1255 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.8, %1253, %1254), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
    %1256 : Long() = aten::mul(%batch_size.32, %num_heads.24), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
    %1257 : int = aten::Int(%1256), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1258 : int[] = prim::ListConstruct(%1257, %1189, %1199), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %input.95 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1255, %1258), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
    %1260 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %1261 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %1262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %1263 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %1264 : int[] = prim::ListConstruct(%1260, %1261, %1262, %1263), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1265 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %padded_value.8 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.95, %1264, %1265), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %1267 : Long() = aten::mul(%batch_size.32, %num_heads.24), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:556:0
    %1268 : int = aten::Int(%1267), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1269 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:556:0
    %1270 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:556:0
    %1271 : Long() = aten::add(%chunks_count.24, %1269, %1270), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:556:0
    %1272 : int = aten::Int(%1271), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:564:0
    %1274 : int[] = prim::ListConstruct(%1268, %1272, %1273, %1198), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1275 : int = prim::Constant[value=65536](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:564:0
    %1276 : int = prim::Constant[value=16384](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:564:0
    %1277 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:564:0
    %1278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:564:0
    %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1280 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1281 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.8, %1274, %1279, %1280), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:564:0
    %1282 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
    %1283 : int = aten::size(%chunked_hidden_states.36, %1282), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
    %total_num_heads.8 : Long() = prim::NumToTensor(%1283), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1285 : int = aten::Int(%total_num_heads.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1286 : int = aten::Int(%total_num_heads.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1287 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
    %1288 : int = aten::size(%chunked_hidden_states.36, %1287), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
    %num_chunks.8 : Long() = prim::NumToTensor(%1288), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1290 : int = aten::Int(%num_chunks.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1291 : int = aten::Int(%num_chunks.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1292 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
    %1293 : int = aten::size(%chunked_hidden_states.36, %1292), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
    %window_overlap.8 : Long() = prim::NumToTensor(%1293), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1295 : int = aten::Int(%window_overlap.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1296 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
    %1297 : int = aten::size(%chunked_hidden_states.36, %1296), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
    %hidden_dim.8 : Long() = prim::NumToTensor(%1297), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1299 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:422:0
    %1300 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:422:0
    %1301 : Long() = aten::add(%window_overlap.8, %1299, %1300), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:422:0
    %1302 : int = aten::Int(%1301), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1303 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %1304 : int[] = prim::ListConstruct(%1303, %1302), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1305 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %chunked_hidden_states.37 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.36, %1304, %1305), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
    %1307 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:424:0
    %1308 : int[] = prim::ListConstruct(%1286, %1291, %1307), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %chunked_hidden_states.38 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.37, %1308), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:424:0
    %1310 : Long() = aten::neg(%window_overlap.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:428:0
    %1311 : int = aten::Int(%1310), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1312 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1313 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1314 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1315 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1316 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.38, %1312, %1313, %1314, %1315), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1319 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1320 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1316, %1317, %1318, %1319, %1320), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1322 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1323 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1324 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %chunked_hidden_states.39 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1321, %1322, %1323, %1311, %1324), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
    %1326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:431:0
    %1327 : Long() = aten::add(%window_overlap.8, %hidden_dim.8, %1326), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:431:0
    %1328 : int = aten::Int(%1327), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1329 : int[] = prim::ListConstruct(%1285, %1290, %1295, %1328), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %chunked_hidden_states.40 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.39, %1329), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:430:0
    %1331 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1333 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1335 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.40, %1331, %1332, %1333, %1334), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1336 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1340 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1335, %1336, %1337, %1338, %1339), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1341 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1342 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1343 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1345 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1340, %1341, %1342, %1343, %1344), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1346 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1347 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1348 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1350 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1345, %1346, %1347, %1348, %1349), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
    %1351 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/functional.py:327:0
    %1352 : Tensor[] = prim::ListConstruct(%1350, %1281), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %context.8 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%1351, %1352), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/functional.py:327:0
    %1354 : int[] = prim::ListConstruct(%1184, %1193, %1188, %1197), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1355 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.8, %1354), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:569:0
    %1356 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:569:0
    %1357 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:569:0
    %attn_output.15 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1355, %1356, %1357), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:569:0
    %1377 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
    %1378 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
    %1379 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.15, %1377, %1378), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
    %1380 : int[] = prim::ListConstruct(%27, %34, %41), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
    %1381 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1379, %1380), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
    %1382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
    %attn_output.16 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1381, %1382), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
    %1384 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:374:0
    %1385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:374:0
    %input.96 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.16, %1384, %1385), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_longformer.py:374:0
    return (%input.96)

LongformerSelfAttention.key
Linear._actual_script_module
  graph(%self.125 : __torch__.torch.nn.modules.linear.Linear,
        %input.91 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.125)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.125)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
    %output.44 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.91, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
    %key_vectors.8 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.44, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
    return (%key_vectors.8)

LongformerSelfAttention.query
Linear._actual_script_module
  graph(%self.124 : __torch__.torch.nn.modules.linear.Linear,
        %input.91 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.124)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.124)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
    %output.43 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.91, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
    %query_vectors.15 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.43, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
    return (%query_vectors.15)

LongformerSelfAttention.value
Linear._actual_script_module
  graph(%self.126 : __torch__.torch.nn.modules.linear.Linear,
        %input.91 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.126)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.126)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
    %output.45 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.91, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
    %value_vectors.8 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.45, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
    return (%value_vectors.8)

LongformerSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.130 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.98 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.130)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.130)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.24 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.98, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.24)

LongformerSelfOutput.dense
Linear._actual_script_module
  graph(%self.128 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:768, 512:13056, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.128)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.128)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
    %output.46 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
    %input.97 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.46, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.97)

LongformerSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.129 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.87 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.87)

LongformerIntermediate.dense
Linear._actual_script_module
  graph(%self.132 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.132)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.132)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
    %output.47 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
    %input.99 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.47, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.99)

LongformerOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.136 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.102 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.136)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.136)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
    %hidden_states.89 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.102, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%hidden_states.89)

LongformerOutput.dense
Linear._actual_script_module
  graph(%self.134 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1572864, 512:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.134)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.134)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
    %output.48 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
    %input.101 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.48, %2, %6), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
    return (%input.101)

LongformerOutput.dropout
Dropout._actual_script_module
  graph(%self.135 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.88 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.88)

LongformerLayer._actual_script_module
  graph(%self.137 : __torch__.transformers.modeling_longformer.LongformerLayer,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%self.137)
    %4 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%self.137)
    %5 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%self.137)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %attention_mask, %2)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

LongformerLayer.attention
LongformerAttention._actual_script_module
  graph(%self.138 : __torch__.transformers.modeling_longformer.LongformerAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%self.138)
    %4 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%self.138)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %2)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %2)
    return (%8)

LongformerLayer.intermediate
LongformerIntermediate._actual_script_module
  graph(%self.147 : __torch__.transformers.modeling_longformer.LongformerIntermediate,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.147)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.112 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%5), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
    return (%input.112)

LongformerLayer.output
LongformerOutput._actual_script_module
  graph(%self.149 : __torch__.transformers.modeling_longformer.LongformerOutput,
        %1 : Float(17:1572864, 512:3072, 3072:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.149)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.149)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.149)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output # transformers/modeling_longformer.py:830:0
    %input.114 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output # transformers/modeling_longformer.py:830:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.114)
    return (%13)

LongformerAttention.output
LongformerSelfOutput._actual_script_module
  graph(%self.143 : __torch__.transformers.modeling_longformer.LongformerSelfOutput,
        %1 : Float(17:768, 512:13056, 768:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.143)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.143)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.143)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output # transformers/modeling_longformer.py:758:0
    %input.110 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output # transformers/modeling_longformer.py:758:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.110)
    return (%13)

LongformerAttention.self
LongformerSelfAttention._actual_script_module
  graph(%self.139 : __torch__.transformers.modeling_longformer.LongformerSelfAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.139)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.139)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.139)
    %6 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:241:0
    %7 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:241:0
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:241:0
    %9 : Float(17:512, 512:1) = aten::squeeze(%7, %8), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:241:0
    %10 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:22:0
    %is_index_masked.9 : Bool(17:512, 512:1) = aten::lt(%9, %10), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:22:0
    %18 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:248:0
    %19 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:248:0
    %input.103 : Float(512:768, 17:393216, 768:1) = aten::transpose(%2, %18, %19), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:248:0
    %1387 : Tensor = prim::CallMethod[name="forward"](%5, %input.103)
    %1388 : Tensor = prim::CallMethod[name="forward"](%4, %input.103)
    %1389 : Tensor = prim::CallMethod[name="forward"](%3, %input.103)
    %24 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
    %25 : int = aten::size(%input.103, %24), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
    %seq_len.34 : Long() = prim::NumToTensor(%25), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %27 : int = aten::Int(%seq_len.34), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %28 : int = aten::Int(%seq_len.34), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %29 : int = aten::Int(%seq_len.34), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %30 : int = aten::Int(%seq_len.34), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %31 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
    %32 : int = aten::size(%input.103, %31), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
    %batch_size.33 : Long() = prim::NumToTensor(%32), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %34 : int = aten::Int(%batch_size.33), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %35 : int = aten::Int(%batch_size.33), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %36 : int = aten::Int(%batch_size.33), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %37 : int = aten::Int(%batch_size.33), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %38 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
    %39 : int = aten::size(%input.103, %38), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
    %embed_dim.9 : Long() = prim::NumToTensor(%39), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %41 : int = aten::Int(%embed_dim.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %44 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:261:0
    %query_vectors.18 : Float(512:13056, 17:768, 768:1) = aten::div_(%1387, %44), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:261:0
    %46 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:263:0
    %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:263:0
    %48 : int[] = prim::ListConstruct(%30, %37, %46, %47), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %49 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.18, %48), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:263:0
    %50 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:263:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:263:0
    %query.17 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%49, %50, %51), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:263:0
    %53 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:264:0
    %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:264:0
    %55 : int[] = prim::ListConstruct(%29, %36, %53, %54), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %56 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1388, %55), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:264:0
    %57 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:264:0
    %58 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:264:0
    %key.9 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%56, %57, %58), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:264:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %61 : int = aten::size(%query.17, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.34 : Long() = prim::NumToTensor(%61), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %63 : int = aten::Int(%batch_size.34), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %65 : int = aten::size(%query.17, %64), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.35 : Long() = prim::NumToTensor(%65), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %67 : int = aten::Int(%seq_len.35), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %68 : int = aten::Int(%seq_len.35), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %69 : int = aten::Int(%seq_len.35), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %70 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %71 : int = aten::size(%query.17, %70), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.25 : Long() = prim::NumToTensor(%71), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %73 : int = aten::Int(%num_heads.25), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %74 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %75 : int = aten::size(%query.17, %74), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.25 : Long() = prim::NumToTensor(%75), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %77 : int = aten::Int(%head_dim.25), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %78 : int = aten::Int(%head_dim.25), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %111 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %112 : Long() = aten::floor_divide(%seq_len.35, %111), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %113 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:478:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.25 : Long() = aten::sub(%112, %113, %114), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:478:0
    %116 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
    %117 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
    %118 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.17, %116, %117), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
    %119 : Long() = aten::mul(%batch_size.34, %num_heads.25), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
    %120 : int = aten::Int(%119), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %121 : int[] = prim::ListConstruct(%120, %69, %78), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %hidden_states.90 : Float(204:64, 512:13056, 64:1) = aten::reshape(%118, %121), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
    %123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
    %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
    %125 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.9, %123, %124), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
    %126 : Long() = aten::mul(%batch_size.34, %num_heads.25), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
    %127 : int = aten::Int(%126), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %128 : int[] = prim::ListConstruct(%127, %68, %77), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %hidden_states.92 : Float(204:64, 512:13056, 64:1) = aten::reshape(%125, %128), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
    %130 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
    %131 : int = aten::size(%hidden_states.90, %130), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
    %132 : Long() = prim::NumToTensor(%131), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %133 : int = aten::Int(%132), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %134 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
    %135 : int = aten::size(%hidden_states.90, %134), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
    %136 : Long() = prim::NumToTensor(%135), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %137 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %138 : Long() = aten::floor_divide(%136, %137), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %139 : int = aten::Int(%138), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %140 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
    %141 : int = aten::size(%hidden_states.90, %140), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
    %142 : Long() = prim::NumToTensor(%141), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %143 : int = aten::Int(%142), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %144 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
    %145 : int[] = prim::ListConstruct(%133, %139, %144, %143), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %hidden_states.91 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.90, %145), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
    %147 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %148 : int = aten::size(%hidden_states.91, %147), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %149 : Long() = prim::NumToTensor(%148), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %150 : int = aten::Int(%149), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %151 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %152 : int = aten::size(%hidden_states.91, %151), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %153 : Long() = prim::NumToTensor(%152), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %154 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %155 : int = aten::size(%hidden_states.91, %154), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %156 : Long() = prim::NumToTensor(%155), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %157 : int = aten::Int(%156), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %158 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %159 : int = aten::size(%hidden_states.91, %158), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %160 : Long() = prim::NumToTensor(%159), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %161 : int = aten::Int(%160), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %162 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %163 : Long() = aten::mul(%153, %162), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %164 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %165 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %166 : Long() = aten::sub(%163, %164, %165), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %167 : int = aten::Int(%166), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %168 : int[] = prim::ListConstruct(%150, %167, %157, %161), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %169 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %170 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %171 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %172 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %173 : int[] = prim::ListConstruct(%169, %170, %171, %172), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %174 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %175 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.91, %168, %173, %174), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %176 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
    %177 : int = aten::size(%hidden_states.92, %176), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %179 : int = aten::Int(%178), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
    %181 : int = aten::size(%hidden_states.92, %180), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
    %182 : Long() = prim::NumToTensor(%181), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %183 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %184 : Long() = aten::floor_divide(%182, %183), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %185 : int = aten::Int(%184), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %186 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
    %187 : int = aten::size(%hidden_states.92, %186), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
    %188 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %189 : int = aten::Int(%188), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %190 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
    %191 : int[] = prim::ListConstruct(%179, %185, %190, %189), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %hidden_states.93 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.92, %191), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
    %193 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %194 : int = aten::size(%hidden_states.93, %193), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %195 : Long() = prim::NumToTensor(%194), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %196 : int = aten::Int(%195), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %197 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %198 : int = aten::size(%hidden_states.93, %197), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %199 : Long() = prim::NumToTensor(%198), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %200 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %201 : int = aten::size(%hidden_states.93, %200), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %203 : int = aten::Int(%202), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %204 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %205 : int = aten::size(%hidden_states.93, %204), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %207 : int = aten::Int(%206), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %208 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %209 : Long() = aten::mul(%199, %208), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %210 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %211 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %212 : Long() = aten::sub(%209, %210, %211), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %213 : int = aten::Int(%212), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %214 : int[] = prim::ListConstruct(%196, %213, %203, %207), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %216 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %217 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %218 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %219 : int[] = prim::ListConstruct(%215, %216, %217, %218), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %220 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.93, %214, %219, %220), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %222 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/functional.py:327:0
    %223 : Tensor[] = prim::ListConstruct(%175, %221), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %input.104 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%222, %223), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/functional.py:327:0
    %225 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %226 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %227 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %229 : int[] = prim::ListConstruct(%225, %226, %227, %228), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %230 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.17 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.104, %229, %230), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %232 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %233 : int = aten::size(%hidden_states_padded.17, %232), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %234 : Long() = prim::NumToTensor(%233), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %235 : int = aten::Int(%234), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %236 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %237 : int = aten::size(%hidden_states_padded.17, %236), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %238 : Long() = prim::NumToTensor(%237), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %239 : int = aten::Int(%238), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %246 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %247 : int = aten::size(%hidden_states_padded.17, %246), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %248 : Long() = prim::NumToTensor(%247), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %249 : int = aten::Int(%248), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %250 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %251 : int = aten::size(%hidden_states_padded.17, %250), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %252 : Long() = prim::NumToTensor(%251), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %253 : int = aten::Int(%252), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %254 : int[] = prim::ListConstruct(%235, %239, %249, %253), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %diagonal_chunked_attention_scores.17 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.17, %254), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:400:0
    %256 : Long() = aten::mul(%batch_size.34, %num_heads.25), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
    %257 : int = aten::Int(%256), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %258 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
    %259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
    %260 : Long() = aten::add(%chunks_count.25, %258, %259), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
    %261 : int = aten::Int(%260), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %263 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %264 : int[] = prim::ListConstruct(%257, %261, %262, %263), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %265 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %266 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %268 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.17 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.17, %264, %265, %266, %267, %268), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %270 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %271 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %273 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %270, %271, %272, %273), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %275 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %276 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %277 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %279 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%274, %275, %276, %277, %278), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %280 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %281 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %282 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %284 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%279, %280, %281, %282, %283), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %285 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %286 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %287 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %289 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%284, %285, %286, %287, %288), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %290 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %291 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %292 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %294 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %290, %291, %292, %293), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %295 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %296 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %297 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %299 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%294, %295, %296, %297, %298), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %300 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %301 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %302 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %303 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %304 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%299, %300, %301, %302, %303), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %305 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %306 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %307 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %308 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %309 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%304, %305, %306, %307, %308), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %310 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %311 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %312 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %313 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %314 : int[] = prim::ListConstruct(%310, %311, %312, %313), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %315 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%289, %314), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %316 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %317 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%309, %315, %316), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %319 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %320 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %321 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %322 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %318, %319, %320, %321), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %323 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %324 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %325 : Float(204:262656, 512:513, 513:1) = aten::select(%322, %323, %324), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %327 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %328 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %329 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %330 : Float(204:262656, 256:513, 513:1) = aten::slice(%325, %326, %327, %328, %329), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %331 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %333 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %335 : Float(204:262656, 256:513, 257:1) = aten::slice(%330, %331, %332, %333, %334), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %336 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %340 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %336, %337, %338, %339), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %341 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %342 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %343 : Float(204:262656, 256:513, 513:1) = aten::select(%340, %341, %342), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %345 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %347 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %348 : Float(204:262656, 256:513, 513:1) = aten::slice(%343, %344, %345, %346, %347), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %349 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %350 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %352 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %353 : Float(204:262656, 256:513, 257:1) = aten::slice(%348, %349, %350, %351, %352), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %354 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %355 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %356 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %357 : int[] = prim::ListConstruct(%354, %355, %356), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %358 : Float(204:262656, 256:513, 257:1) = aten::view(%335, %357), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %359 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %360 : Float(204:262656, 256:513, 257:1) = aten::copy_(%353, %358, %359), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %361 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %362 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %363 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %364 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %365 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %361, %362, %363, %364), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %366 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %367 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %368 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %369 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %370 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%365, %366, %367, %368, %369), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %371 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %372 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %373 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %374 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %375 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%370, %371, %372, %373, %374), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %376 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %377 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %379 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %380 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%375, %376, %377, %378, %379), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %383 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %384 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %385 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %381, %382, %383, %384), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %386 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %387 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %388 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %389 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %390 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%385, %386, %387, %388, %389), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %391 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %392 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %393 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %394 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %395 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%390, %391, %392, %393, %394), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %397 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %398 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %399 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %400 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%395, %396, %397, %398, %399), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %401 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %402 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %403 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %404 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %405 : int[] = prim::ListConstruct(%401, %402, %403, %404), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %406 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%380, %405), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %407 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %408 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%400, %406, %407), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %409 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %410 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %411 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %412 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %413 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %409, %410, %411, %412), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %414 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %415 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %416 : Float(204:262656, 512:513, 513:1) = aten::select(%413, %414, %415), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %417 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %418 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %419 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %420 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %421 : Float(204:262656, 255:513, 513:1) = aten::slice(%416, %417, %418, %419, %420), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %422 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %423 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %425 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %426 : Float(204:262656, 255:513, 255:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %427 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %429 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %430 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %431 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %427, %428, %429, %430), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %432 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %433 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %434 : Float(204:262656, 256:513, 513:1) = aten::select(%431, %432, %433), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %435 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %436 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %437 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %438 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %439 : Float(204:262656, 255:513, 513:1) = aten::slice(%434, %435, %436, %437, %438), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %440 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %441 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %442 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %443 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %444 : Float(204:262656, 255:513, 255:1) = aten::slice(%439, %440, %441, %442, %443), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %445 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %446 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %447 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %448 : int[] = prim::ListConstruct(%445, %446, %447), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %449 : Float(204:262656, 255:513, 255:1) = aten::view(%426, %448), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %450 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %451 : Float(204:262656, 255:513, 255:1) = aten::copy_(%444, %449, %450), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %452 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
    %453 : int[] = prim::ListConstruct(%63, %73, %67, %452), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %454 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.17, %453), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
    %455 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
    %456 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.25 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%454, %455, %456), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
    %458 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %459 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %460 : int[] = prim::ListConstruct(%458, %459), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %461 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %462 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %463 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %464 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %465 : Float(256:257, 257:1) = aten::ones(%460, %461, %462, %463, %464), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %466 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %467 : Float(256:257, 257:1) = aten::tril(%465, %466), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %468 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %469 : int[] = prim::ListConstruct(%468), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %beginning_mask_2d.17 : Float(256:257, 257:1) = aten::flip(%467, %469), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %471 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %472 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.17, %471), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %473 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %474 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %475 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %476 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %477 : Float(1:65792, 256:257, 257:1) = aten::slice(%472, %473, %474, %475, %476), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %478 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %479 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%477, %478), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %480 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %481 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %482 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %483 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.17 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%479, %480, %481, %482, %483), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %485 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:460:0
    %486 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:460:0
    %487 : int[] = prim::ListConstruct(%485, %486), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %ending_mask.17 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.17, %487), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:460:0
    %489 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %490 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %491 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %493 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.25, %489, %490, %491, %492), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %494 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %495 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %496 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %497 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %498 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%493, %494, %495, %496, %497), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %499 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %500 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %501 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %502 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %503 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%498, %499, %500, %501, %502), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %504 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %505 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %506 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %507 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.17 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%503, %504, %505, %506, %507), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %509 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %510 : int = aten::size(%beginning_input.17, %509), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %511 : Long() = prim::NumToTensor(%510), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %512 : int = aten::Int(%511), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %513 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %514 : int = aten::size(%beginning_input.17, %513), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %515 : Long() = prim::NumToTensor(%514), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %516 : int = aten::Int(%515), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %517 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %518 : int = aten::size(%beginning_input.17, %517), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %519 : Long() = prim::NumToTensor(%518), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %520 : int = aten::Int(%519), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %521 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %522 : int = aten::size(%beginning_input.17, %521), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %523 : Long() = prim::NumToTensor(%522), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %524 : int = aten::Int(%523), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %525 : int[] = prim::ListConstruct(%512, %516, %520, %524), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %526 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %527 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.17, %525, %526), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %528 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:22:0
    %529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%527, %528), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:22:0
    %530 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:463:0
    %531 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.17, %529, %530), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:463:0
    %532 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %533 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %534 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %535 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %536 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.25, %532, %533, %534, %535), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %537 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %538 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %539 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %541 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%536, %537, %538, %539, %540), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %542 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %543 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %544 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %545 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %546 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%541, %542, %543, %544, %545), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %547 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %548 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %549 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.17 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%546, %547, %548, %549, %550), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %552 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %553 : int = aten::size(%ending_input.17, %552), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %554 : Long() = prim::NumToTensor(%553), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %555 : int = aten::Int(%554), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %556 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %557 : int = aten::size(%ending_input.17, %556), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %558 : Long() = prim::NumToTensor(%557), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %559 : int = aten::Int(%558), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %560 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %561 : int = aten::size(%ending_input.17, %560), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %562 : Long() = prim::NumToTensor(%561), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %563 : int = aten::Int(%562), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %564 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %565 : int = aten::size(%ending_input.17, %564), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %566 : Long() = prim::NumToTensor(%565), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %567 : int = aten::Int(%566), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %568 : int[] = prim::ListConstruct(%555, %559, %563, %567), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %569 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %570 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.17, %568, %569), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %571 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:22:0
    %572 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%570, %571), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:22:0
    %573 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:466:0
    %574 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.17, %572, %573), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:466:0
    %575 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:22:0
    %576 : Bool(17:512, 512:1) = aten::ne(%9, %575), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:22:0
    %577 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %578 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %579 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %580 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %581 : Bool(17:512, 512:1) = aten::slice(%576, %577, %578, %579, %580), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %582 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %583 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %584 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %585 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %586 : Bool(17:512, 512:1) = aten::slice(%581, %582, %583, %584, %585), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %587 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %588 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%586, %587), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %589 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %remove_from_windowed_attention_mask.9 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%588, %589), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
    %591 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.9, %query.17), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:275:0
    %592 : float = prim::Constant[value=-10000.](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:275:0
    %float_mask.9 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%591, %remove_from_windowed_attention_mask.9, %592), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:275:0
    %594 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %595 : int = aten::size(%float_mask.9, %594), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %596 : Long() = prim::NumToTensor(%595), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %597 : int = aten::Int(%596), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %598 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %599 : int = aten::size(%float_mask.9, %598), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %600 : Long() = prim::NumToTensor(%599), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %601 : int = aten::Int(%600), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %602 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %603 : int = aten::size(%float_mask.9, %602), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %604 : Long() = prim::NumToTensor(%603), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %605 : int = aten::Int(%604), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %606 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %607 : int = aten::size(%float_mask.9, %606), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %608 : Long() = prim::NumToTensor(%607), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %609 : int = aten::Int(%608), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %610 : int[] = prim::ListConstruct(%597, %601, %605, %609), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %611 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %612 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %613 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %614 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %query.18 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%610, %611, %612, %613, %614), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
    %616 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %617 : int = aten::size(%query.18, %616), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.35 : Long() = prim::NumToTensor(%617), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %619 : int = aten::Int(%batch_size.35), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %620 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %621 : int = aten::size(%query.18, %620), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.36 : Long() = prim::NumToTensor(%621), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %623 : int = aten::Int(%seq_len.36), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %624 : int = aten::Int(%seq_len.36), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %625 : int = aten::Int(%seq_len.36), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %626 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %627 : int = aten::size(%query.18, %626), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.26 : Long() = prim::NumToTensor(%627), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %629 : int = aten::Int(%num_heads.26), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %630 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %631 : int = aten::size(%query.18, %630), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.26 : Long() = prim::NumToTensor(%631), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %633 : int = aten::Int(%head_dim.26), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %634 : int = aten::Int(%head_dim.26), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %667 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %668 : Long() = aten::floor_divide(%seq_len.36, %667), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %669 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:478:0
    %670 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.26 : Long() = aten::sub(%668, %669, %670), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:478:0
    %672 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
    %673 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
    %674 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.18, %672, %673), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
    %675 : Long() = aten::mul(%batch_size.35, %num_heads.26), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
    %676 : int = aten::Int(%675), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %677 : int[] = prim::ListConstruct(%676, %625, %634), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %hidden_states.94 : Float(17:512, 512:1, 1:1) = aten::reshape(%674, %677), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
    %679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
    %680 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
    %681 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.9, %679, %680), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
    %682 : Long() = aten::mul(%batch_size.35, %num_heads.26), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
    %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %684 : int[] = prim::ListConstruct(%683, %624, %633), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %hidden_states.96 : Float(17:512, 512:1, 1:1) = aten::reshape(%681, %684), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
    %686 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
    %687 : int = aten::size(%hidden_states.94, %686), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
    %688 : Long() = prim::NumToTensor(%687), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %690 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
    %691 : int = aten::size(%hidden_states.94, %690), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
    %692 : Long() = prim::NumToTensor(%691), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %693 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %694 : Long() = aten::floor_divide(%692, %693), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %696 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
    %697 : int = aten::size(%hidden_states.94, %696), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
    %698 : Long() = prim::NumToTensor(%697), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %699 : int = aten::Int(%698), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %700 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
    %701 : int[] = prim::ListConstruct(%689, %695, %700, %699), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %hidden_states.95 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.94, %701), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
    %703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %704 : int = aten::size(%hidden_states.95, %703), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %705 : Long() = prim::NumToTensor(%704), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %706 : int = aten::Int(%705), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %708 : int = aten::size(%hidden_states.95, %707), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %709 : Long() = prim::NumToTensor(%708), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %710 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %711 : int = aten::size(%hidden_states.95, %710), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %712 : Long() = prim::NumToTensor(%711), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %713 : int = aten::Int(%712), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %714 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %715 : int = aten::size(%hidden_states.95, %714), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %716 : Long() = prim::NumToTensor(%715), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %717 : int = aten::Int(%716), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %718 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %719 : Long() = aten::mul(%709, %718), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %720 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %721 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %722 : Long() = aten::sub(%719, %720, %721), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %723 : int = aten::Int(%722), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %724 : int[] = prim::ListConstruct(%706, %723, %713, %717), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %725 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %726 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %727 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %728 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %729 : int[] = prim::ListConstruct(%725, %726, %727, %728), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %730 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %731 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.95, %724, %729, %730), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %732 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
    %733 : int = aten::size(%hidden_states.96, %732), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
    %734 : Long() = prim::NumToTensor(%733), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %735 : int = aten::Int(%734), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %736 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
    %737 : int = aten::size(%hidden_states.96, %736), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
    %738 : Long() = prim::NumToTensor(%737), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %739 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %740 : Long() = aten::floor_divide(%738, %739), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %741 : int = aten::Int(%740), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
    %743 : int = aten::size(%hidden_states.96, %742), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
    %744 : Long() = prim::NumToTensor(%743), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %745 : int = aten::Int(%744), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %746 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
    %747 : int[] = prim::ListConstruct(%735, %741, %746, %745), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %hidden_states.97 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.96, %747), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
    %749 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %750 : int = aten::size(%hidden_states.97, %749), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %751 : Long() = prim::NumToTensor(%750), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %752 : int = aten::Int(%751), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %754 : int = aten::size(%hidden_states.97, %753), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %755 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %756 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %757 : int = aten::size(%hidden_states.97, %756), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %758 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %759 : int = aten::Int(%758), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %760 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %761 : int = aten::size(%hidden_states.97, %760), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
    %762 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %764 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %765 : Long() = aten::mul(%755, %764), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %766 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %767 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %768 : Long() = aten::sub(%765, %766, %767), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
    %769 : int = aten::Int(%768), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %770 : int[] = prim::ListConstruct(%752, %769, %759, %763), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %771 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %772 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %773 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %776 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.97, %770, %775, %776), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
    %778 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/functional.py:327:0
    %779 : Tensor[] = prim::ListConstruct(%731, %777), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %input.105 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%778, %779), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/functional.py:327:0
    %781 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %782 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %783 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %786 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.18 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.105, %785, %786), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %788 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %789 : int = aten::size(%hidden_states_padded.18, %788), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %790 : Long() = prim::NumToTensor(%789), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %791 : int = aten::Int(%790), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %792 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %793 : int = aten::size(%hidden_states_padded.18, %792), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %794 : Long() = prim::NumToTensor(%793), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %795 : int = aten::Int(%794), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %802 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %803 : int = aten::size(%hidden_states_padded.18, %802), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %804 : Long() = prim::NumToTensor(%803), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %805 : int = aten::Int(%804), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %806 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %807 : int = aten::size(%hidden_states_padded.18, %806), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
    %808 : Long() = prim::NumToTensor(%807), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %809 : int = aten::Int(%808), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %810 : int[] = prim::ListConstruct(%791, %795, %805, %809), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %diagonal_chunked_attention_scores.18 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.18, %810), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:400:0
    %812 : Long() = aten::mul(%batch_size.35, %num_heads.26), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
    %813 : int = aten::Int(%812), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %814 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
    %815 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
    %816 : Long() = aten::add(%chunks_count.26, %814, %815), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
    %817 : int = aten::Int(%816), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %818 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %819 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %820 : int[] = prim::ListConstruct(%813, %817, %818, %819), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %821 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %822 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %823 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %824 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.18 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.18, %820, %821, %822, %823, %824), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
    %826 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %827 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %828 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %829 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %830 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %826, %827, %828, %829), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %831 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %832 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %833 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %834 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%830, %831, %832, %833, %834), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %836 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %837 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %838 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %839 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%835, %836, %837, %838, %839), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %841 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %842 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %843 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %845 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%840, %841, %842, %843, %844), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %846 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %847 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %848 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %849 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %846, %847, %848, %849), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %851 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %852 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %853 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %855 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%850, %851, %852, %853, %854), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %856 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %857 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %858 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %859 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %860 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%855, %856, %857, %858, %859), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %861 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %862 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %863 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %864 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %865 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%860, %861, %862, %863, %864), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %866 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %867 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %868 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %869 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %870 : int[] = prim::ListConstruct(%866, %867, %868, %869), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %871 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%845, %870), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %872 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %873 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%865, %871, %872), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
    %874 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %875 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %876 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %877 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %874, %875, %876, %877), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %879 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %880 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %881 : Float(17:262656, 512:513, 513:1) = aten::select(%878, %879, %880), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %883 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %884 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %885 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %886 : Float(17:262656, 256:513, 513:1) = aten::slice(%881, %882, %883, %884, %885), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %887 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %889 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%886, %887, %888, %889, %890), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %892 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %893 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %894 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %895 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %896 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %892, %893, %894, %895), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %897 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %898 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %899 : Float(17:262656, 256:513, 513:1) = aten::select(%896, %897, %898), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %901 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %902 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %903 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %904 : Float(17:262656, 256:513, 513:1) = aten::slice(%899, %900, %901, %902, %903), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %905 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %906 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %907 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %909 : Float(17:262656, 256:513, 257:1) = aten::slice(%904, %905, %906, %907, %908), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %910 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %911 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %912 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %913 : int[] = prim::ListConstruct(%910, %911, %912), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %914 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %913), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %915 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %916 : Float(17:262656, 256:513, 257:1) = aten::copy_(%909, %914, %915), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
    %917 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %918 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %919 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %920 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %921 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %917, %918, %919, %920), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %922 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %923 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %924 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %925 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %926 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%921, %922, %923, %924, %925), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %927 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %928 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %929 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %930 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %931 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%926, %927, %928, %929, %930), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %932 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %933 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %934 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %935 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %936 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%931, %932, %933, %934, %935), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %938 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %939 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %940 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %941 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %937, %938, %939, %940), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %942 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %943 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %944 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %945 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %946 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%941, %942, %943, %944, %945), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %947 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %948 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %949 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %950 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %951 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%946, %947, %948, %949, %950), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %952 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %953 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %954 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %955 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %956 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%951, %952, %953, %954, %955), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %957 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %959 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %960 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %962 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%936, %961), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %963 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %964 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%956, %962, %963), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
    %965 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %966 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %967 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %969 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %965, %966, %967, %968), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %970 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %971 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %972 : Float(17:262656, 512:513, 513:1) = aten::select(%969, %970, %971), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %973 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %974 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %975 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %976 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %977 : Float(17:262656, 255:513, 513:1) = aten::slice(%972, %973, %974, %975, %976), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %978 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %979 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %980 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %981 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %982 : Float(17:262656, 255:513, 255:1) = aten::slice(%977, %978, %979, %980, %981), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %983 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %984 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %985 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %986 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %987 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %983, %984, %985, %986), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %988 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %989 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %990 : Float(17:262656, 256:513, 513:1) = aten::select(%987, %988, %989), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %991 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %993 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %994 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %995 : Float(17:262656, 255:513, 513:1) = aten::slice(%990, %991, %992, %993, %994), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %996 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %998 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %999 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %1000 : Float(17:262656, 255:513, 255:1) = aten::slice(%995, %996, %997, %998, %999), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %1001 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %1002 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %1003 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %1004 : int[] = prim::ListConstruct(%1001, %1002, %1003), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1005 : Float(17:262656, 255:513, 255:1) = aten::view(%982, %1004), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %1006 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1007 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1000, %1005, %1006), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
    %1008 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
    %1009 : int[] = prim::ListConstruct(%619, %629, %623, %1008), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1010 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.18, %1009), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
    %1011 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
    %1012 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.26 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1010, %1011, %1012), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
    %1014 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %1015 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %1016 : int[] = prim::ListConstruct(%1014, %1015), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1017 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %1018 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %1019 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %1020 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %1021 : Float(256:257, 257:1) = aten::ones(%1016, %1017, %1018, %1019, %1020), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %1022 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %1023 : Float(256:257, 257:1) = aten::tril(%1021, %1022), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %1024 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %1025 : int[] = prim::ListConstruct(%1024), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %beginning_mask_2d.18 : Float(256:257, 257:1) = aten::flip(%1023, %1025), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
    %1027 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1028 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.18, %1027), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1029 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1030 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1031 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1032 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1033 : Float(1:65792, 256:257, 257:1) = aten::slice(%1028, %1029, %1030, %1031, %1032), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1034 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1035 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1033, %1034), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1036 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1037 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1038 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1039 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.18 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1035, %1036, %1037, %1038, %1039), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
    %1041 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:460:0
    %1042 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:460:0
    %1043 : int[] = prim::ListConstruct(%1041, %1042), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %ending_mask.18 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.18, %1043), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:460:0
    %1045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1046 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1047 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1048 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1049 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.26, %1045, %1046, %1047, %1048), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1050 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1052 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1053 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1054 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1049, %1050, %1051, %1052, %1053), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1055 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1056 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1057 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1058 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1059 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1054, %1055, %1056, %1057, %1058), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1060 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1061 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1062 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.18 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1059, %1060, %1061, %1062, %1063), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
    %1065 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %1066 : int = aten::size(%beginning_input.18, %1065), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %1067 : Long() = prim::NumToTensor(%1066), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1068 : int = aten::Int(%1067), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1069 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %1070 : int = aten::size(%beginning_input.18, %1069), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %1071 : Long() = prim::NumToTensor(%1070), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1072 : int = aten::Int(%1071), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1073 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %1074 : int = aten::size(%beginning_input.18, %1073), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %1075 : Long() = prim::NumToTensor(%1074), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1076 : int = aten::Int(%1075), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1077 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %1078 : int = aten::size(%beginning_input.18, %1077), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %1079 : Long() = prim::NumToTensor(%1078), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1080 : int = aten::Int(%1079), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1081 : int[] = prim::ListConstruct(%1068, %1072, %1076, %1080), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1082 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %1083 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.18, %1081, %1082), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
    %1084 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:22:0
    %1085 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1083, %1084), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:22:0
    %1086 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:463:0
    %1087 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.18, %1085, %1086), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:463:0
    %1088 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1089 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1090 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1091 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1092 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.26, %1088, %1089, %1090, %1091), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1094 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1095 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1096 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1097 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1092, %1093, %1094, %1095, %1096), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1098 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1099 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1100 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1101 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1102 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1097, %1098, %1099, %1100, %1101), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1103 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1104 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1105 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1106 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.18 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1102, %1103, %1104, %1105, %1106), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
    %1108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %1109 : int = aten::size(%ending_input.18, %1108), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %1110 : Long() = prim::NumToTensor(%1109), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1111 : int = aten::Int(%1110), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1112 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %1113 : int = aten::size(%ending_input.18, %1112), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %1114 : Long() = prim::NumToTensor(%1113), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1115 : int = aten::Int(%1114), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1116 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %1117 : int = aten::size(%ending_input.18, %1116), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %1118 : Long() = prim::NumToTensor(%1117), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1119 : int = aten::Int(%1118), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1120 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %1121 : int = aten::size(%ending_input.18, %1120), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %1122 : Long() = prim::NumToTensor(%1121), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1124 : int[] = prim::ListConstruct(%1111, %1115, %1119, %1123), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1125 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %1126 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.18, %1124, %1125), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
    %1127 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:22:0
    %1128 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1126, %1127), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:22:0
    %1129 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:466:0
    %1130 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.18, %1128, %1129), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:466:0
    %1131 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:284:0
    %attn_scores.9 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.25, %input_tensor.26, %1131), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:284:0
    %1151 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:1500:0
    %1152 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:1500:0
    %attn_probs_fp32.9 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.9, %1151, %1152), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:1500:0
    %attn_probs.17 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::type_as(%attn_probs_fp32.9, %attn_scores.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:320:0
    %1155 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1156 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1157 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1158 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1159 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.9, %1155, %1156, %1157, %1158), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1161 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1162 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1163 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1164 : Bool(17:512, 512:1) = aten::slice(%1159, %1160, %1161, %1162, %1163), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1165 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1166 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1164, %1165), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1167 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1168 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1166, %1167), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1169 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %input.106 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs.17, %1168, %1169), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
    %1171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:973:0
    %1172 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:973:0
    %attn_probs.18 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.106, %1171, %1172), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:973:0
    %1174 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:331:0
    %1175 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:331:0
    %1176 : int[] = prim::ListConstruct(%28, %35, %1174, %1175), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1177 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1389, %1176), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:331:0
    %1178 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:331:0
    %1179 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:331:0
    %value.9 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1177, %1178, %1179), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:331:0
    %1181 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
    %1182 : int = aten::size(%value.9, %1181), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
    %batch_size.36 : Long() = prim::NumToTensor(%1182), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1184 : int = aten::Int(%batch_size.36), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1185 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
    %1186 : int = aten::size(%value.9, %1185), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
    %seq_len.37 : Long() = prim::NumToTensor(%1186), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1188 : int = aten::Int(%seq_len.37), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1189 : int = aten::Int(%seq_len.37), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1190 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
    %1191 : int = aten::size(%value.9, %1190), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
    %num_heads.27 : Long() = prim::NumToTensor(%1191), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1193 : int = aten::Int(%num_heads.27), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1194 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
    %1195 : int = aten::size(%value.9, %1194), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
    %head_dim.27 : Long() = prim::NumToTensor(%1195), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1197 : int = aten::Int(%head_dim.27), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1198 : int = aten::Int(%head_dim.27), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1199 : int = aten::Int(%head_dim.27), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1236 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %1237 : Long() = aten::floor_divide(%seq_len.37, %1236), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %1238 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:542:0
    %1239 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:542:0
    %chunks_count.27 : Long() = aten::sub(%1237, %1238, %1239), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:542:0
    %1241 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:545:0
    %1242 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:545:0
    %1243 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.18, %1241, %1242), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:545:0
    %1244 : Long() = aten::mul(%batch_size.36, %num_heads.27), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:546:0
    %1245 : int = aten::Int(%1244), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1246 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %1247 : Long() = aten::floor_divide(%seq_len.37, %1246), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/tensor.py:424:0
    %1248 : int = aten::Int(%1247), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1249 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:545:0
    %1250 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:545:0
    %1251 : int[] = prim::ListConstruct(%1245, %1248, %1249, %1250), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %chunked_hidden_states.41 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1243, %1251), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:545:0
    %1253 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
    %1254 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
    %1255 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.9, %1253, %1254), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
    %1256 : Long() = aten::mul(%batch_size.36, %num_heads.27), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
    %1257 : int = aten::Int(%1256), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1258 : int[] = prim::ListConstruct(%1257, %1189, %1199), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %input.107 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1255, %1258), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
    %1260 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %1261 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %1262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %1263 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %1264 : int[] = prim::ListConstruct(%1260, %1261, %1262, %1263), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1265 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %padded_value.9 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.107, %1264, %1265), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %1267 : Long() = aten::mul(%batch_size.36, %num_heads.27), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:556:0
    %1268 : int = aten::Int(%1267), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1269 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:556:0
    %1270 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:556:0
    %1271 : Long() = aten::add(%chunks_count.27, %1269, %1270), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:556:0
    %1272 : int = aten::Int(%1271), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:564:0
    %1274 : int[] = prim::ListConstruct(%1268, %1272, %1273, %1198), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1275 : int = prim::Constant[value=65536](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:564:0
    %1276 : int = prim::Constant[value=16384](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:564:0
    %1277 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:564:0
    %1278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:564:0
    %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1280 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1281 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.9, %1274, %1279, %1280), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:564:0
    %1282 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
    %1283 : int = aten::size(%chunked_hidden_states.41, %1282), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
    %total_num_heads.9 : Long() = prim::NumToTensor(%1283), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1285 : int = aten::Int(%total_num_heads.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1286 : int = aten::Int(%total_num_heads.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1287 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
    %1288 : int = aten::size(%chunked_hidden_states.41, %1287), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
    %num_chunks.9 : Long() = prim::NumToTensor(%1288), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1290 : int = aten::Int(%num_chunks.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1291 : int = aten::Int(%num_chunks.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1292 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
    %1293 : int = aten::size(%chunked_hidden_states.41, %1292), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
    %window_overlap.9 : Long() = prim::NumToTensor(%1293), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1295 : int = aten::Int(%window_overlap.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1296 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
    %1297 : int = aten::size(%chunked_hidden_states.41, %1296), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
    %hidden_dim.9 : Long() = prim::NumToTensor(%1297), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1299 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:422:0
    %1300 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:422:0
    %1301 : Long() = aten::add(%window_overlap.9, %1299, %1300), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:422:0
    %1302 : int = aten::Int(%1301), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1303 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %1304 : int[] = prim::ListConstruct(%1303, %1302), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1305 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %chunked_hidden_states.42 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.41, %1304, %1305), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
    %1307 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:424:0
    %1308 : int[] = prim::ListConstruct(%1286, %1291, %1307), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %chunked_hidden_states.43 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.42, %1308), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:424:0
    %1310 : Long() = aten::neg(%window_overlap.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:428:0
    %1311 : int = aten::Int(%1310), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1312 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1313 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1314 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1315 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1316 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.43, %1312, %1313, %1314, %1315), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1319 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1320 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1316, %1317, %1318, %1319, %1320), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1322 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1323 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1324 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %chunked_hidden_states.44 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1321, %1322, %1323, %1311, %1324), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
    %1326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:431:0
    %1327 : Long() = aten::add(%window_overlap.9, %hidden_dim.9, %1326), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:431:0
    %1328 : int = aten::Int(%1327), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1329 : int[] = prim::ListConstruct(%1285, %1290, %1295, %1328), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %chunked_hidden_states.45 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.44, %1329), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:430:0
    %1331 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1333 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1335 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.45, %1331, %1332, %1333, %1334), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1336 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1340 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1335, %1336, %1337, %1338, %1339), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1341 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1342 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1343 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1345 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1340, %1341, %1342, %1343, %1344), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1346 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1347 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1348 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1350 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1345, %1346, %1347, %1348, %1349), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
    %1351 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/functional.py:327:0
    %1352 : Tensor[] = prim::ListConstruct(%1350, %1281), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %context.9 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%1351, %1352), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/functional.py:327:0
    %1354 : int[] = prim::ListConstruct(%1184, %1193, %1188, %1197), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1355 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.9, %1354), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:569:0
    %1356 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:569:0
    %1357 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:569:0
    %attn_output.17 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1355, %1356, %1357), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:569:0
    %1377 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
    %1378 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
    %1379 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.17, %1377, %1378), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
    %1380 : int[] = prim::ListConstruct(%27, %34, %41), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
    %1381 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1379, %1380), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
    %1382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
    %attn_output.18 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1381, %1382), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
    %1384 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:374:0
    %1385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:374:0
    %input.108 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.18, %1384, %1385), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_longformer.py:374:0
    return (%input.108)

LongformerSelfAttention.key
Linear._actual_script_module
  graph(%self.141 : __torch__.torch.nn.modules.linear.Linear,
        %input.103 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.141)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.141)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
    %output.50 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.103, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
    %key_vectors.9 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.50, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
    return (%key_vectors.9)

LongformerSelfAttention.query
Linear._actual_script_module
  graph(%self.140 : __torch__.torch.nn.modules.linear.Linear,
        %input.103 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.140)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.140)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
    %output.49 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.103, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
    %query_vectors.17 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.49, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
    return (%query_vectors.17)

LongformerSelfAttention.value
Linear._actual_script_module
  graph(%self.142 : __torch__.torch.nn.modules.linear.Linear,
        %input.103 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.142)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.142)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
    %output.51 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.103, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
    %value_vectors.9 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.51, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
    return (%value_vectors.9)

LongformerSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.146 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.110 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.146)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.146)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.27 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.110, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.27)

LongformerSelfOutput.dense
Linear._actual_script_module
  graph(%self.144 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:768, 512:13056, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.144)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.144)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
    %output.52 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
    %input.109 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.52, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.109)

LongformerSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.145 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.98 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.98)

LongformerIntermediate.dense
Linear._actual_script_module
  graph(%self.148 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.148)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.148)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
    %output.53 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
    %input.111 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.53, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.111)

LongformerOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.152 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.114 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.152)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.152)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
    %hidden_states.100 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.114, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%hidden_states.100)

LongformerOutput.dense
Linear._actual_script_module
  graph(%self.150 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1572864, 512:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.150)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.150)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
    %output.54 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
    %input.113 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.54, %2, %6), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
    return (%input.113)

LongformerOutput.dropout
Dropout._actual_script_module
  graph(%self.151 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.99 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.99)

LongformerLayer._actual_script_module
  graph(%self.153 : __torch__.transformers.modeling_longformer.LongformerLayer,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%self.153)
    %4 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%self.153)
    %5 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%self.153)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %attention_mask, %2)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

LongformerLayer.attention
LongformerAttention._actual_script_module
  graph(%self.154 : __torch__.transformers.modeling_longformer.LongformerAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%self.154)
    %4 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%self.154)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %2)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %2)
    return (%8)

LongformerLayer.intermediate
LongformerIntermediate._actual_script_module
  graph(%self.163 : __torch__.transformers.modeling_longformer.LongformerIntermediate,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.163)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.124 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%5), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
    return (%input.124)

LongformerLayer.output
LongformerOutput._actual_script_module
  graph(%self.165 : __torch__.transformers.modeling_longformer.LongformerOutput,
        %1 : Float(17:1572864, 512:3072, 3072:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.165)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.165)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.165)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output # transformers/modeling_longformer.py:830:0
    %input.126 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output # transformers/modeling_longformer.py:830:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.126)
    return (%13)

LongformerAttention.output
LongformerSelfOutput._actual_script_module
  graph(%self.159 : __torch__.transformers.modeling_longformer.LongformerSelfOutput,
        %1 : Float(17:768, 512:13056, 768:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.159)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.159)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.159)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output # transformers/modeling_longformer.py:758:0
    %input.122 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output # transformers/modeling_longformer.py:758:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.122)
    return (%13)

LongformerAttention.self
LongformerSelfAttention._actual_script_module
  graph(%self.155 : __torch__.transformers.modeling_longformer.LongformerSelfAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.155)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.155)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.155)
    %6 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:241:0
    %7 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:241:0
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:241:0
    %9 : Float(17:512, 512:1) = aten::squeeze(%7, %8), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:241:0
    %10 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:22:0
    %is_index_masked.10 : Bool(17:512, 512:1) = aten::lt(%9, %10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:22:0
    %18 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:248:0
    %19 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:248:0
    %input.115 : Float(512:768, 17:393216, 768:1) = aten::transpose(%2, %18, %19), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:248:0
    %1387 : Tensor = prim::CallMethod[name="forward"](%5, %input.115)
    %1388 : Tensor = prim::CallMethod[name="forward"](%4, %input.115)
    %1389 : Tensor = prim::CallMethod[name="forward"](%3, %input.115)
    %24 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
    %25 : int = aten::size(%input.115, %24), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
    %seq_len.38 : Long() = prim::NumToTensor(%25), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %27 : int = aten::Int(%seq_len.38), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %28 : int = aten::Int(%seq_len.38), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %29 : int = aten::Int(%seq_len.38), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %30 : int = aten::Int(%seq_len.38), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %31 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
    %32 : int = aten::size(%input.115, %31), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
    %batch_size.37 : Long() = prim::NumToTensor(%32), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %34 : int = aten::Int(%batch_size.37), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %35 : int = aten::Int(%batch_size.37), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %36 : int = aten::Int(%batch_size.37), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %37 : int = aten::Int(%batch_size.37), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %38 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
    %39 : int = aten::size(%input.115, %38), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
    %embed_dim.10 : Long() = prim::NumToTensor(%39), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %41 : int = aten::Int(%embed_dim.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %44 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:261:0
    %query_vectors.20 : Float(512:13056, 17:768, 768:1) = aten::div_(%1387, %44), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:261:0
    %46 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:263:0
    %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:263:0
    %48 : int[] = prim::ListConstruct(%30, %37, %46, %47), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %49 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.20, %48), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:263:0
    %50 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:263:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:263:0
    %query.19 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%49, %50, %51), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:263:0
    %53 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:264:0
    %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:264:0
    %55 : int[] = prim::ListConstruct(%29, %36, %53, %54), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %56 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1388, %55), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:264:0
    %57 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:264:0
    %58 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:264:0
    %key.10 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%56, %57, %58), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:264:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %61 : int = aten::size(%query.19, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.38 : Long() = prim::NumToTensor(%61), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %63 : int = aten::Int(%batch_size.38), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %65 : int = aten::size(%query.19, %64), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.39 : Long() = prim::NumToTensor(%65), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %67 : int = aten::Int(%seq_len.39), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %68 : int = aten::Int(%seq_len.39), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %69 : int = aten::Int(%seq_len.39), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %70 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %71 : int = aten::size(%query.19, %70), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.28 : Long() = prim::NumToTensor(%71), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %73 : int = aten::Int(%num_heads.28), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %74 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %75 : int = aten::size(%query.19, %74), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.28 : Long() = prim::NumToTensor(%75), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %77 : int = aten::Int(%head_dim.28), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %78 : int = aten::Int(%head_dim.28), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %111 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %112 : Long() = aten::floor_divide(%seq_len.39, %111), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %113 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:478:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.28 : Long() = aten::sub(%112, %113, %114), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:478:0
    %116 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
    %117 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
    %118 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.19, %116, %117), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
    %119 : Long() = aten::mul(%batch_size.38, %num_heads.28), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
    %120 : int = aten::Int(%119), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %121 : int[] = prim::ListConstruct(%120, %69, %78), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %hidden_states.101 : Float(204:64, 512:13056, 64:1) = aten::reshape(%118, %121), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
    %123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
    %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
    %125 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.10, %123, %124), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
    %126 : Long() = aten::mul(%batch_size.38, %num_heads.28), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
    %127 : int = aten::Int(%126), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %128 : int[] = prim::ListConstruct(%127, %68, %77), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %hidden_states.103 : Float(204:64, 512:13056, 64:1) = aten::reshape(%125, %128), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
    %130 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
    %131 : int = aten::size(%hidden_states.101, %130), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
    %132 : Long() = prim::NumToTensor(%131), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %133 : int = aten::Int(%132), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %134 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
    %135 : int = aten::size(%hidden_states.101, %134), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
    %136 : Long() = prim::NumToTensor(%135), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %137 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %138 : Long() = aten::floor_divide(%136, %137), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %139 : int = aten::Int(%138), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %140 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
    %141 : int = aten::size(%hidden_states.101, %140), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
    %142 : Long() = prim::NumToTensor(%141), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %143 : int = aten::Int(%142), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %144 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
    %145 : int[] = prim::ListConstruct(%133, %139, %144, %143), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %hidden_states.102 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.101, %145), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
    %147 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %148 : int = aten::size(%hidden_states.102, %147), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %149 : Long() = prim::NumToTensor(%148), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %150 : int = aten::Int(%149), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %151 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %152 : int = aten::size(%hidden_states.102, %151), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %153 : Long() = prim::NumToTensor(%152), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %154 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %155 : int = aten::size(%hidden_states.102, %154), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %156 : Long() = prim::NumToTensor(%155), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %157 : int = aten::Int(%156), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %158 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %159 : int = aten::size(%hidden_states.102, %158), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %160 : Long() = prim::NumToTensor(%159), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %161 : int = aten::Int(%160), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %162 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %163 : Long() = aten::mul(%153, %162), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %164 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %165 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %166 : Long() = aten::sub(%163, %164, %165), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %167 : int = aten::Int(%166), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %168 : int[] = prim::ListConstruct(%150, %167, %157, %161), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %169 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %170 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %171 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %172 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %173 : int[] = prim::ListConstruct(%169, %170, %171, %172), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %174 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %175 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.102, %168, %173, %174), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %176 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
    %177 : int = aten::size(%hidden_states.103, %176), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %179 : int = aten::Int(%178), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
    %181 : int = aten::size(%hidden_states.103, %180), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
    %182 : Long() = prim::NumToTensor(%181), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %183 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %184 : Long() = aten::floor_divide(%182, %183), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %185 : int = aten::Int(%184), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %186 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
    %187 : int = aten::size(%hidden_states.103, %186), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
    %188 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %189 : int = aten::Int(%188), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %190 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
    %191 : int[] = prim::ListConstruct(%179, %185, %190, %189), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %hidden_states.104 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.103, %191), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
    %193 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %194 : int = aten::size(%hidden_states.104, %193), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %195 : Long() = prim::NumToTensor(%194), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %196 : int = aten::Int(%195), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %197 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %198 : int = aten::size(%hidden_states.104, %197), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %199 : Long() = prim::NumToTensor(%198), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %200 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %201 : int = aten::size(%hidden_states.104, %200), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %203 : int = aten::Int(%202), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %204 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %205 : int = aten::size(%hidden_states.104, %204), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %207 : int = aten::Int(%206), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %208 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %209 : Long() = aten::mul(%199, %208), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %210 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %211 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %212 : Long() = aten::sub(%209, %210, %211), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %213 : int = aten::Int(%212), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %214 : int[] = prim::ListConstruct(%196, %213, %203, %207), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %216 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %217 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %218 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %219 : int[] = prim::ListConstruct(%215, %216, %217, %218), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %220 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.104, %214, %219, %220), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %222 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/functional.py:327:0
    %223 : Tensor[] = prim::ListConstruct(%175, %221), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %input.116 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%222, %223), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/functional.py:327:0
    %225 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %226 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %227 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %229 : int[] = prim::ListConstruct(%225, %226, %227, %228), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %230 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.19 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.116, %229, %230), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %232 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %233 : int = aten::size(%hidden_states_padded.19, %232), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %234 : Long() = prim::NumToTensor(%233), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %235 : int = aten::Int(%234), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %236 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %237 : int = aten::size(%hidden_states_padded.19, %236), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %238 : Long() = prim::NumToTensor(%237), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %239 : int = aten::Int(%238), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %246 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %247 : int = aten::size(%hidden_states_padded.19, %246), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %248 : Long() = prim::NumToTensor(%247), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %249 : int = aten::Int(%248), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %250 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %251 : int = aten::size(%hidden_states_padded.19, %250), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %252 : Long() = prim::NumToTensor(%251), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %253 : int = aten::Int(%252), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %254 : int[] = prim::ListConstruct(%235, %239, %249, %253), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %diagonal_chunked_attention_scores.19 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.19, %254), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:400:0
    %256 : Long() = aten::mul(%batch_size.38, %num_heads.28), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
    %257 : int = aten::Int(%256), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %258 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
    %259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
    %260 : Long() = aten::add(%chunks_count.28, %258, %259), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
    %261 : int = aten::Int(%260), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %263 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %264 : int[] = prim::ListConstruct(%257, %261, %262, %263), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %265 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %266 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %268 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.19 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.19, %264, %265, %266, %267, %268), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %270 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %271 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %273 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %270, %271, %272, %273), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %275 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %276 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %277 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %279 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%274, %275, %276, %277, %278), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %280 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %281 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %282 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %284 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%279, %280, %281, %282, %283), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %285 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %286 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %287 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %289 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%284, %285, %286, %287, %288), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %290 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %291 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %292 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %294 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %290, %291, %292, %293), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %295 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %296 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %297 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %299 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%294, %295, %296, %297, %298), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %300 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %301 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %302 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %303 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %304 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%299, %300, %301, %302, %303), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %305 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %306 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %307 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %308 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %309 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%304, %305, %306, %307, %308), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %310 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %311 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %312 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %313 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %314 : int[] = prim::ListConstruct(%310, %311, %312, %313), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %315 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%289, %314), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %316 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %317 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%309, %315, %316), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %319 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %320 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %321 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %322 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %318, %319, %320, %321), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %323 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %324 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %325 : Float(204:262656, 512:513, 513:1) = aten::select(%322, %323, %324), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %327 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %328 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %329 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %330 : Float(204:262656, 256:513, 513:1) = aten::slice(%325, %326, %327, %328, %329), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %331 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %333 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %335 : Float(204:262656, 256:513, 257:1) = aten::slice(%330, %331, %332, %333, %334), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %336 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %340 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %336, %337, %338, %339), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %341 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %342 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %343 : Float(204:262656, 256:513, 513:1) = aten::select(%340, %341, %342), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %345 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %347 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %348 : Float(204:262656, 256:513, 513:1) = aten::slice(%343, %344, %345, %346, %347), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %349 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %350 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %352 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %353 : Float(204:262656, 256:513, 257:1) = aten::slice(%348, %349, %350, %351, %352), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %354 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %355 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %356 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %357 : int[] = prim::ListConstruct(%354, %355, %356), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %358 : Float(204:262656, 256:513, 257:1) = aten::view(%335, %357), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %359 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %360 : Float(204:262656, 256:513, 257:1) = aten::copy_(%353, %358, %359), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %361 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %362 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %363 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %364 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %365 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %361, %362, %363, %364), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %366 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %367 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %368 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %369 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %370 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%365, %366, %367, %368, %369), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %371 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %372 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %373 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %374 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %375 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%370, %371, %372, %373, %374), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %376 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %377 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %379 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %380 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%375, %376, %377, %378, %379), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %383 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %384 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %385 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %381, %382, %383, %384), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %386 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %387 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %388 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %389 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %390 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%385, %386, %387, %388, %389), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %391 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %392 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %393 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %394 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %395 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%390, %391, %392, %393, %394), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %397 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %398 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %399 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %400 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%395, %396, %397, %398, %399), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %401 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %402 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %403 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %404 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %405 : int[] = prim::ListConstruct(%401, %402, %403, %404), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %406 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%380, %405), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %407 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %408 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%400, %406, %407), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %409 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %410 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %411 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %412 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %413 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %409, %410, %411, %412), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %414 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %415 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %416 : Float(204:262656, 512:513, 513:1) = aten::select(%413, %414, %415), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %417 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %418 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %419 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %420 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %421 : Float(204:262656, 255:513, 513:1) = aten::slice(%416, %417, %418, %419, %420), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %422 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %423 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %425 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %426 : Float(204:262656, 255:513, 255:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %427 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %429 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %430 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %431 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %427, %428, %429, %430), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %432 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %433 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %434 : Float(204:262656, 256:513, 513:1) = aten::select(%431, %432, %433), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %435 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %436 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %437 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %438 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %439 : Float(204:262656, 255:513, 513:1) = aten::slice(%434, %435, %436, %437, %438), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %440 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %441 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %442 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %443 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %444 : Float(204:262656, 255:513, 255:1) = aten::slice(%439, %440, %441, %442, %443), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %445 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %446 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %447 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %448 : int[] = prim::ListConstruct(%445, %446, %447), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %449 : Float(204:262656, 255:513, 255:1) = aten::view(%426, %448), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %450 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %451 : Float(204:262656, 255:513, 255:1) = aten::copy_(%444, %449, %450), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %452 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
    %453 : int[] = prim::ListConstruct(%63, %73, %67, %452), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %454 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.19, %453), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
    %455 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
    %456 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.28 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%454, %455, %456), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
    %458 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %459 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %460 : int[] = prim::ListConstruct(%458, %459), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %461 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %462 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %463 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %464 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %465 : Float(256:257, 257:1) = aten::ones(%460, %461, %462, %463, %464), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %466 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %467 : Float(256:257, 257:1) = aten::tril(%465, %466), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %468 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %469 : int[] = prim::ListConstruct(%468), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %beginning_mask_2d.19 : Float(256:257, 257:1) = aten::flip(%467, %469), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %471 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %472 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.19, %471), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %473 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %474 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %475 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %476 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %477 : Float(1:65792, 256:257, 257:1) = aten::slice(%472, %473, %474, %475, %476), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %478 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %479 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%477, %478), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %480 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %481 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %482 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %483 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.19 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%479, %480, %481, %482, %483), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %485 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:460:0
    %486 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:460:0
    %487 : int[] = prim::ListConstruct(%485, %486), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %ending_mask.19 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.19, %487), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:460:0
    %489 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %490 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %491 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %493 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.28, %489, %490, %491, %492), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %494 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %495 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %496 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %497 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %498 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%493, %494, %495, %496, %497), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %499 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %500 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %501 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %502 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %503 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%498, %499, %500, %501, %502), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %504 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %505 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %506 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %507 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.19 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%503, %504, %505, %506, %507), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %509 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %510 : int = aten::size(%beginning_input.19, %509), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %511 : Long() = prim::NumToTensor(%510), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %512 : int = aten::Int(%511), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %513 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %514 : int = aten::size(%beginning_input.19, %513), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %515 : Long() = prim::NumToTensor(%514), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %516 : int = aten::Int(%515), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %517 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %518 : int = aten::size(%beginning_input.19, %517), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %519 : Long() = prim::NumToTensor(%518), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %520 : int = aten::Int(%519), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %521 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %522 : int = aten::size(%beginning_input.19, %521), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %523 : Long() = prim::NumToTensor(%522), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %524 : int = aten::Int(%523), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %525 : int[] = prim::ListConstruct(%512, %516, %520, %524), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %526 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %527 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.19, %525, %526), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %528 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:22:0
    %529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%527, %528), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:22:0
    %530 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:463:0
    %531 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.19, %529, %530), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:463:0
    %532 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %533 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %534 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %535 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %536 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.28, %532, %533, %534, %535), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %537 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %538 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %539 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %541 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%536, %537, %538, %539, %540), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %542 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %543 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %544 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %545 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %546 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%541, %542, %543, %544, %545), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %547 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %548 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %549 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.19 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%546, %547, %548, %549, %550), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %552 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %553 : int = aten::size(%ending_input.19, %552), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %554 : Long() = prim::NumToTensor(%553), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %555 : int = aten::Int(%554), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %556 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %557 : int = aten::size(%ending_input.19, %556), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %558 : Long() = prim::NumToTensor(%557), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %559 : int = aten::Int(%558), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %560 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %561 : int = aten::size(%ending_input.19, %560), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %562 : Long() = prim::NumToTensor(%561), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %563 : int = aten::Int(%562), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %564 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %565 : int = aten::size(%ending_input.19, %564), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %566 : Long() = prim::NumToTensor(%565), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %567 : int = aten::Int(%566), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %568 : int[] = prim::ListConstruct(%555, %559, %563, %567), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %569 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %570 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.19, %568, %569), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %571 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:22:0
    %572 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%570, %571), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:22:0
    %573 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:466:0
    %574 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.19, %572, %573), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:466:0
    %575 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:22:0
    %576 : Bool(17:512, 512:1) = aten::ne(%9, %575), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:22:0
    %577 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %578 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %579 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %580 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %581 : Bool(17:512, 512:1) = aten::slice(%576, %577, %578, %579, %580), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %582 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %583 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %584 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %585 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %586 : Bool(17:512, 512:1) = aten::slice(%581, %582, %583, %584, %585), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %587 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %588 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%586, %587), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %589 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %remove_from_windowed_attention_mask.10 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%588, %589), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
    %591 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.10, %query.19), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:275:0
    %592 : float = prim::Constant[value=-10000.](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:275:0
    %float_mask.10 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%591, %remove_from_windowed_attention_mask.10, %592), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:275:0
    %594 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %595 : int = aten::size(%float_mask.10, %594), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %596 : Long() = prim::NumToTensor(%595), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %597 : int = aten::Int(%596), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %598 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %599 : int = aten::size(%float_mask.10, %598), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %600 : Long() = prim::NumToTensor(%599), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %601 : int = aten::Int(%600), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %602 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %603 : int = aten::size(%float_mask.10, %602), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %604 : Long() = prim::NumToTensor(%603), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %605 : int = aten::Int(%604), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %606 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %607 : int = aten::size(%float_mask.10, %606), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %608 : Long() = prim::NumToTensor(%607), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %609 : int = aten::Int(%608), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %610 : int[] = prim::ListConstruct(%597, %601, %605, %609), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %611 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %612 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %613 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %614 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %query.20 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%610, %611, %612, %613, %614), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
    %616 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %617 : int = aten::size(%query.20, %616), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.39 : Long() = prim::NumToTensor(%617), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %619 : int = aten::Int(%batch_size.39), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %620 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %621 : int = aten::size(%query.20, %620), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.40 : Long() = prim::NumToTensor(%621), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %623 : int = aten::Int(%seq_len.40), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %624 : int = aten::Int(%seq_len.40), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %625 : int = aten::Int(%seq_len.40), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %626 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %627 : int = aten::size(%query.20, %626), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.29 : Long() = prim::NumToTensor(%627), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %629 : int = aten::Int(%num_heads.29), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %630 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %631 : int = aten::size(%query.20, %630), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.29 : Long() = prim::NumToTensor(%631), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %633 : int = aten::Int(%head_dim.29), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %634 : int = aten::Int(%head_dim.29), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %667 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %668 : Long() = aten::floor_divide(%seq_len.40, %667), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %669 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:478:0
    %670 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.29 : Long() = aten::sub(%668, %669, %670), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:478:0
    %672 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
    %673 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
    %674 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.20, %672, %673), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
    %675 : Long() = aten::mul(%batch_size.39, %num_heads.29), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
    %676 : int = aten::Int(%675), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %677 : int[] = prim::ListConstruct(%676, %625, %634), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %hidden_states.105 : Float(17:512, 512:1, 1:1) = aten::reshape(%674, %677), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
    %679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
    %680 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
    %681 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.10, %679, %680), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
    %682 : Long() = aten::mul(%batch_size.39, %num_heads.29), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
    %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %684 : int[] = prim::ListConstruct(%683, %624, %633), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %hidden_states.107 : Float(17:512, 512:1, 1:1) = aten::reshape(%681, %684), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
    %686 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
    %687 : int = aten::size(%hidden_states.105, %686), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
    %688 : Long() = prim::NumToTensor(%687), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %690 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
    %691 : int = aten::size(%hidden_states.105, %690), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
    %692 : Long() = prim::NumToTensor(%691), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %693 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %694 : Long() = aten::floor_divide(%692, %693), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %696 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
    %697 : int = aten::size(%hidden_states.105, %696), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
    %698 : Long() = prim::NumToTensor(%697), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %699 : int = aten::Int(%698), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %700 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
    %701 : int[] = prim::ListConstruct(%689, %695, %700, %699), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %hidden_states.106 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.105, %701), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
    %703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %704 : int = aten::size(%hidden_states.106, %703), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %705 : Long() = prim::NumToTensor(%704), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %706 : int = aten::Int(%705), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %708 : int = aten::size(%hidden_states.106, %707), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %709 : Long() = prim::NumToTensor(%708), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %710 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %711 : int = aten::size(%hidden_states.106, %710), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %712 : Long() = prim::NumToTensor(%711), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %713 : int = aten::Int(%712), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %714 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %715 : int = aten::size(%hidden_states.106, %714), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %716 : Long() = prim::NumToTensor(%715), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %717 : int = aten::Int(%716), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %718 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %719 : Long() = aten::mul(%709, %718), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %720 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %721 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %722 : Long() = aten::sub(%719, %720, %721), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %723 : int = aten::Int(%722), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %724 : int[] = prim::ListConstruct(%706, %723, %713, %717), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %725 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %726 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %727 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %728 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %729 : int[] = prim::ListConstruct(%725, %726, %727, %728), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %730 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %731 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.106, %724, %729, %730), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %732 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
    %733 : int = aten::size(%hidden_states.107, %732), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
    %734 : Long() = prim::NumToTensor(%733), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %735 : int = aten::Int(%734), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %736 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
    %737 : int = aten::size(%hidden_states.107, %736), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
    %738 : Long() = prim::NumToTensor(%737), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %739 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %740 : Long() = aten::floor_divide(%738, %739), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %741 : int = aten::Int(%740), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
    %743 : int = aten::size(%hidden_states.107, %742), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
    %744 : Long() = prim::NumToTensor(%743), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %745 : int = aten::Int(%744), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %746 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
    %747 : int[] = prim::ListConstruct(%735, %741, %746, %745), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %hidden_states.108 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.107, %747), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
    %749 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %750 : int = aten::size(%hidden_states.108, %749), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %751 : Long() = prim::NumToTensor(%750), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %752 : int = aten::Int(%751), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %754 : int = aten::size(%hidden_states.108, %753), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %755 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %756 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %757 : int = aten::size(%hidden_states.108, %756), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %758 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %759 : int = aten::Int(%758), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %760 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %761 : int = aten::size(%hidden_states.108, %760), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
    %762 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %764 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %765 : Long() = aten::mul(%755, %764), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %766 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %767 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %768 : Long() = aten::sub(%765, %766, %767), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
    %769 : int = aten::Int(%768), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %770 : int[] = prim::ListConstruct(%752, %769, %759, %763), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %771 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %772 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %773 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %776 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.108, %770, %775, %776), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
    %778 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/functional.py:327:0
    %779 : Tensor[] = prim::ListConstruct(%731, %777), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %input.117 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%778, %779), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/functional.py:327:0
    %781 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %782 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %783 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %786 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.20 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.117, %785, %786), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %788 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %789 : int = aten::size(%hidden_states_padded.20, %788), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %790 : Long() = prim::NumToTensor(%789), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %791 : int = aten::Int(%790), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %792 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %793 : int = aten::size(%hidden_states_padded.20, %792), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %794 : Long() = prim::NumToTensor(%793), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %795 : int = aten::Int(%794), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %802 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %803 : int = aten::size(%hidden_states_padded.20, %802), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %804 : Long() = prim::NumToTensor(%803), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %805 : int = aten::Int(%804), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %806 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %807 : int = aten::size(%hidden_states_padded.20, %806), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
    %808 : Long() = prim::NumToTensor(%807), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %809 : int = aten::Int(%808), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %810 : int[] = prim::ListConstruct(%791, %795, %805, %809), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %diagonal_chunked_attention_scores.20 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.20, %810), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:400:0
    %812 : Long() = aten::mul(%batch_size.39, %num_heads.29), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
    %813 : int = aten::Int(%812), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %814 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
    %815 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
    %816 : Long() = aten::add(%chunks_count.29, %814, %815), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
    %817 : int = aten::Int(%816), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %818 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %819 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %820 : int[] = prim::ListConstruct(%813, %817, %818, %819), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %821 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %822 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %823 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %824 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.20 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.20, %820, %821, %822, %823, %824), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
    %826 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %827 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %828 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %829 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %830 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %826, %827, %828, %829), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %831 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %832 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %833 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %834 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%830, %831, %832, %833, %834), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %836 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %837 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %838 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %839 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%835, %836, %837, %838, %839), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %841 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %842 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %843 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %845 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%840, %841, %842, %843, %844), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %846 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %847 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %848 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %849 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %846, %847, %848, %849), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %851 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %852 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %853 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %855 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%850, %851, %852, %853, %854), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %856 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %857 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %858 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %859 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %860 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%855, %856, %857, %858, %859), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %861 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %862 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %863 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %864 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %865 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%860, %861, %862, %863, %864), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %866 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %867 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %868 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %869 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %870 : int[] = prim::ListConstruct(%866, %867, %868, %869), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %871 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%845, %870), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %872 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %873 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%865, %871, %872), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
    %874 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %875 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %876 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %877 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %874, %875, %876, %877), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %879 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %880 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %881 : Float(17:262656, 512:513, 513:1) = aten::select(%878, %879, %880), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %883 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %884 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %885 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %886 : Float(17:262656, 256:513, 513:1) = aten::slice(%881, %882, %883, %884, %885), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %887 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %889 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%886, %887, %888, %889, %890), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %892 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %893 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %894 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %895 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %896 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %892, %893, %894, %895), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %897 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %898 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %899 : Float(17:262656, 256:513, 513:1) = aten::select(%896, %897, %898), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %901 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %902 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %903 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %904 : Float(17:262656, 256:513, 513:1) = aten::slice(%899, %900, %901, %902, %903), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %905 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %906 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %907 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %909 : Float(17:262656, 256:513, 257:1) = aten::slice(%904, %905, %906, %907, %908), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %910 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %911 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %912 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %913 : int[] = prim::ListConstruct(%910, %911, %912), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %914 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %913), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %915 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %916 : Float(17:262656, 256:513, 257:1) = aten::copy_(%909, %914, %915), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
    %917 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %918 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %919 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %920 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %921 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %917, %918, %919, %920), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %922 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %923 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %924 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %925 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %926 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%921, %922, %923, %924, %925), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %927 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %928 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %929 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %930 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %931 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%926, %927, %928, %929, %930), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %932 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %933 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %934 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %935 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %936 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%931, %932, %933, %934, %935), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %938 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %939 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %940 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %941 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %937, %938, %939, %940), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %942 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %943 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %944 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %945 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %946 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%941, %942, %943, %944, %945), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %947 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %948 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %949 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %950 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %951 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%946, %947, %948, %949, %950), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %952 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %953 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %954 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %955 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %956 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%951, %952, %953, %954, %955), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %957 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %959 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %960 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %962 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%936, %961), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %963 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %964 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%956, %962, %963), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
    %965 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %966 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %967 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %969 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %965, %966, %967, %968), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %970 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %971 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %972 : Float(17:262656, 512:513, 513:1) = aten::select(%969, %970, %971), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %973 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %974 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %975 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %976 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %977 : Float(17:262656, 255:513, 513:1) = aten::slice(%972, %973, %974, %975, %976), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %978 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %979 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %980 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %981 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %982 : Float(17:262656, 255:513, 255:1) = aten::slice(%977, %978, %979, %980, %981), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %983 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %984 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %985 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %986 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %987 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %983, %984, %985, %986), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %988 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %989 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %990 : Float(17:262656, 256:513, 513:1) = aten::select(%987, %988, %989), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %991 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %993 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %994 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %995 : Float(17:262656, 255:513, 513:1) = aten::slice(%990, %991, %992, %993, %994), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %996 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %998 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %999 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %1000 : Float(17:262656, 255:513, 255:1) = aten::slice(%995, %996, %997, %998, %999), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %1001 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %1002 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %1003 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %1004 : int[] = prim::ListConstruct(%1001, %1002, %1003), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1005 : Float(17:262656, 255:513, 255:1) = aten::view(%982, %1004), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %1006 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1007 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1000, %1005, %1006), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
    %1008 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
    %1009 : int[] = prim::ListConstruct(%619, %629, %623, %1008), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1010 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.20, %1009), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
    %1011 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
    %1012 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.29 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1010, %1011, %1012), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
    %1014 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %1015 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %1016 : int[] = prim::ListConstruct(%1014, %1015), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1017 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %1018 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %1019 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %1020 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %1021 : Float(256:257, 257:1) = aten::ones(%1016, %1017, %1018, %1019, %1020), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %1022 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %1023 : Float(256:257, 257:1) = aten::tril(%1021, %1022), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %1024 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %1025 : int[] = prim::ListConstruct(%1024), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %beginning_mask_2d.20 : Float(256:257, 257:1) = aten::flip(%1023, %1025), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
    %1027 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1028 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.20, %1027), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1029 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1030 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1031 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1032 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1033 : Float(1:65792, 256:257, 257:1) = aten::slice(%1028, %1029, %1030, %1031, %1032), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1034 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1035 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1033, %1034), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1036 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1037 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1038 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1039 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.20 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1035, %1036, %1037, %1038, %1039), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
    %1041 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:460:0
    %1042 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:460:0
    %1043 : int[] = prim::ListConstruct(%1041, %1042), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %ending_mask.20 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.20, %1043), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:460:0
    %1045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1046 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1047 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1048 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1049 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.29, %1045, %1046, %1047, %1048), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1050 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1052 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1053 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1054 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1049, %1050, %1051, %1052, %1053), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1055 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1056 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1057 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1058 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1059 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1054, %1055, %1056, %1057, %1058), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1060 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1061 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1062 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.20 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1059, %1060, %1061, %1062, %1063), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
    %1065 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %1066 : int = aten::size(%beginning_input.20, %1065), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %1067 : Long() = prim::NumToTensor(%1066), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1068 : int = aten::Int(%1067), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1069 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %1070 : int = aten::size(%beginning_input.20, %1069), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %1071 : Long() = prim::NumToTensor(%1070), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1072 : int = aten::Int(%1071), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1073 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %1074 : int = aten::size(%beginning_input.20, %1073), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %1075 : Long() = prim::NumToTensor(%1074), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1076 : int = aten::Int(%1075), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1077 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %1078 : int = aten::size(%beginning_input.20, %1077), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %1079 : Long() = prim::NumToTensor(%1078), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1080 : int = aten::Int(%1079), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1081 : int[] = prim::ListConstruct(%1068, %1072, %1076, %1080), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1082 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %1083 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.20, %1081, %1082), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
    %1084 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:22:0
    %1085 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1083, %1084), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:22:0
    %1086 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:463:0
    %1087 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.20, %1085, %1086), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:463:0
    %1088 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1089 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1090 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1091 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1092 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.29, %1088, %1089, %1090, %1091), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1094 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1095 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1096 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1097 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1092, %1093, %1094, %1095, %1096), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1098 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1099 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1100 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1101 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1102 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1097, %1098, %1099, %1100, %1101), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1103 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1104 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1105 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1106 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.20 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1102, %1103, %1104, %1105, %1106), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
    %1108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %1109 : int = aten::size(%ending_input.20, %1108), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %1110 : Long() = prim::NumToTensor(%1109), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1111 : int = aten::Int(%1110), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1112 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %1113 : int = aten::size(%ending_input.20, %1112), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %1114 : Long() = prim::NumToTensor(%1113), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1115 : int = aten::Int(%1114), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1116 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %1117 : int = aten::size(%ending_input.20, %1116), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %1118 : Long() = prim::NumToTensor(%1117), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1119 : int = aten::Int(%1118), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1120 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %1121 : int = aten::size(%ending_input.20, %1120), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %1122 : Long() = prim::NumToTensor(%1121), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1124 : int[] = prim::ListConstruct(%1111, %1115, %1119, %1123), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1125 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %1126 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.20, %1124, %1125), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
    %1127 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:22:0
    %1128 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1126, %1127), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:22:0
    %1129 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:466:0
    %1130 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.20, %1128, %1129), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:466:0
    %1131 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:284:0
    %attn_scores.10 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.28, %input_tensor.29, %1131), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:284:0
    %1151 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:1500:0
    %1152 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:1500:0
    %attn_probs_fp32.10 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.10, %1151, %1152), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:1500:0
    %attn_probs.19 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::type_as(%attn_probs_fp32.10, %attn_scores.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:320:0
    %1155 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1156 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1157 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1158 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1159 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.10, %1155, %1156, %1157, %1158), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1161 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1162 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1163 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1164 : Bool(17:512, 512:1) = aten::slice(%1159, %1160, %1161, %1162, %1163), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1165 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1166 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1164, %1165), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1167 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1168 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1166, %1167), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1169 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %input.118 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs.19, %1168, %1169), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
    %1171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:973:0
    %1172 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:973:0
    %attn_probs.20 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.118, %1171, %1172), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:973:0
    %1174 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:331:0
    %1175 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:331:0
    %1176 : int[] = prim::ListConstruct(%28, %35, %1174, %1175), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1177 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1389, %1176), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:331:0
    %1178 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:331:0
    %1179 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:331:0
    %value.10 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1177, %1178, %1179), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:331:0
    %1181 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
    %1182 : int = aten::size(%value.10, %1181), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
    %batch_size.40 : Long() = prim::NumToTensor(%1182), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1184 : int = aten::Int(%batch_size.40), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1185 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
    %1186 : int = aten::size(%value.10, %1185), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
    %seq_len.41 : Long() = prim::NumToTensor(%1186), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1188 : int = aten::Int(%seq_len.41), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1189 : int = aten::Int(%seq_len.41), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1190 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
    %1191 : int = aten::size(%value.10, %1190), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
    %num_heads.30 : Long() = prim::NumToTensor(%1191), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1193 : int = aten::Int(%num_heads.30), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1194 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
    %1195 : int = aten::size(%value.10, %1194), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
    %head_dim.30 : Long() = prim::NumToTensor(%1195), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1197 : int = aten::Int(%head_dim.30), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1198 : int = aten::Int(%head_dim.30), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1199 : int = aten::Int(%head_dim.30), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1236 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %1237 : Long() = aten::floor_divide(%seq_len.41, %1236), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %1238 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:542:0
    %1239 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:542:0
    %chunks_count.30 : Long() = aten::sub(%1237, %1238, %1239), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:542:0
    %1241 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:545:0
    %1242 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:545:0
    %1243 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.20, %1241, %1242), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:545:0
    %1244 : Long() = aten::mul(%batch_size.40, %num_heads.30), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:546:0
    %1245 : int = aten::Int(%1244), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1246 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %1247 : Long() = aten::floor_divide(%seq_len.41, %1246), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/tensor.py:424:0
    %1248 : int = aten::Int(%1247), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1249 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:545:0
    %1250 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:545:0
    %1251 : int[] = prim::ListConstruct(%1245, %1248, %1249, %1250), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %chunked_hidden_states.46 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1243, %1251), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:545:0
    %1253 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
    %1254 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
    %1255 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.10, %1253, %1254), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
    %1256 : Long() = aten::mul(%batch_size.40, %num_heads.30), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
    %1257 : int = aten::Int(%1256), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1258 : int[] = prim::ListConstruct(%1257, %1189, %1199), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %input.119 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1255, %1258), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
    %1260 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %1261 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %1262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %1263 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %1264 : int[] = prim::ListConstruct(%1260, %1261, %1262, %1263), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1265 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %padded_value.10 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.119, %1264, %1265), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %1267 : Long() = aten::mul(%batch_size.40, %num_heads.30), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:556:0
    %1268 : int = aten::Int(%1267), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1269 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:556:0
    %1270 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:556:0
    %1271 : Long() = aten::add(%chunks_count.30, %1269, %1270), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:556:0
    %1272 : int = aten::Int(%1271), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:564:0
    %1274 : int[] = prim::ListConstruct(%1268, %1272, %1273, %1198), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1275 : int = prim::Constant[value=65536](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:564:0
    %1276 : int = prim::Constant[value=16384](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:564:0
    %1277 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:564:0
    %1278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:564:0
    %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1280 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1281 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.10, %1274, %1279, %1280), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:564:0
    %1282 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
    %1283 : int = aten::size(%chunked_hidden_states.46, %1282), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
    %total_num_heads.10 : Long() = prim::NumToTensor(%1283), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1285 : int = aten::Int(%total_num_heads.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1286 : int = aten::Int(%total_num_heads.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1287 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
    %1288 : int = aten::size(%chunked_hidden_states.46, %1287), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
    %num_chunks.10 : Long() = prim::NumToTensor(%1288), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1290 : int = aten::Int(%num_chunks.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1291 : int = aten::Int(%num_chunks.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1292 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
    %1293 : int = aten::size(%chunked_hidden_states.46, %1292), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
    %window_overlap.10 : Long() = prim::NumToTensor(%1293), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1295 : int = aten::Int(%window_overlap.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1296 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
    %1297 : int = aten::size(%chunked_hidden_states.46, %1296), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
    %hidden_dim.10 : Long() = prim::NumToTensor(%1297), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1299 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:422:0
    %1300 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:422:0
    %1301 : Long() = aten::add(%window_overlap.10, %1299, %1300), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:422:0
    %1302 : int = aten::Int(%1301), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1303 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %1304 : int[] = prim::ListConstruct(%1303, %1302), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1305 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %chunked_hidden_states.47 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.46, %1304, %1305), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
    %1307 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:424:0
    %1308 : int[] = prim::ListConstruct(%1286, %1291, %1307), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %chunked_hidden_states.48 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.47, %1308), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:424:0
    %1310 : Long() = aten::neg(%window_overlap.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:428:0
    %1311 : int = aten::Int(%1310), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1312 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1313 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1314 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1315 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1316 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.48, %1312, %1313, %1314, %1315), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1319 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1320 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1316, %1317, %1318, %1319, %1320), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1322 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1323 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1324 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %chunked_hidden_states.49 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1321, %1322, %1323, %1311, %1324), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
    %1326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:431:0
    %1327 : Long() = aten::add(%window_overlap.10, %hidden_dim.10, %1326), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:431:0
    %1328 : int = aten::Int(%1327), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1329 : int[] = prim::ListConstruct(%1285, %1290, %1295, %1328), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %chunked_hidden_states.50 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.49, %1329), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:430:0
    %1331 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1333 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1335 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.50, %1331, %1332, %1333, %1334), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1336 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1340 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1335, %1336, %1337, %1338, %1339), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1341 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1342 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1343 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1345 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1340, %1341, %1342, %1343, %1344), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1346 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1347 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1348 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1350 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1345, %1346, %1347, %1348, %1349), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
    %1351 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/functional.py:327:0
    %1352 : Tensor[] = prim::ListConstruct(%1350, %1281), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %context.10 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%1351, %1352), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/functional.py:327:0
    %1354 : int[] = prim::ListConstruct(%1184, %1193, %1188, %1197), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1355 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.10, %1354), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:569:0
    %1356 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:569:0
    %1357 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:569:0
    %attn_output.19 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1355, %1356, %1357), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:569:0
    %1377 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
    %1378 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
    %1379 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.19, %1377, %1378), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
    %1380 : int[] = prim::ListConstruct(%27, %34, %41), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
    %1381 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1379, %1380), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
    %1382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
    %attn_output.20 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1381, %1382), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
    %1384 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:374:0
    %1385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:374:0
    %input.120 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.20, %1384, %1385), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_longformer.py:374:0
    return (%input.120)

LongformerSelfAttention.key
Linear._actual_script_module
  graph(%self.157 : __torch__.torch.nn.modules.linear.Linear,
        %input.115 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.157)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.157)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
    %output.56 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.115, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
    %key_vectors.10 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.56, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
    return (%key_vectors.10)

LongformerSelfAttention.query
Linear._actual_script_module
  graph(%self.156 : __torch__.torch.nn.modules.linear.Linear,
        %input.115 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.156)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.156)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
    %output.55 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.115, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
    %query_vectors.19 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.55, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
    return (%query_vectors.19)

LongformerSelfAttention.value
Linear._actual_script_module
  graph(%self.158 : __torch__.torch.nn.modules.linear.Linear,
        %input.115 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.158)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.158)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
    %output.57 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.115, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
    %value_vectors.10 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.57, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
    return (%value_vectors.10)

LongformerSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.162 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.122 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.162)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.162)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.30 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.122, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.30)

LongformerSelfOutput.dense
Linear._actual_script_module
  graph(%self.160 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:768, 512:13056, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.160)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.160)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
    %output.58 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
    %input.121 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.58, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.121)

LongformerSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.161 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.109 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.109)

LongformerIntermediate.dense
Linear._actual_script_module
  graph(%self.164 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.164)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.164)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
    %output.59 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
    %input.123 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.59, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.123)

LongformerOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.168 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.126 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.168)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.168)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
    %hidden_states.111 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.126, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%hidden_states.111)

LongformerOutput.dense
Linear._actual_script_module
  graph(%self.166 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1572864, 512:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.166)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.166)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
    %output.60 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
    %input.125 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.60, %2, %6), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
    return (%input.125)

LongformerOutput.dropout
Dropout._actual_script_module
  graph(%self.167 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.110 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.110)

LongformerLayer._actual_script_module
  graph(%self.169 : __torch__.transformers.modeling_longformer.LongformerLayer,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%self.169)
    %4 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%self.169)
    %5 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%self.169)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %attention_mask, %2)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

LongformerLayer.attention
LongformerAttention._actual_script_module
  graph(%self.170 : __torch__.transformers.modeling_longformer.LongformerAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%self.170)
    %4 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%self.170)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %2)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %2)
    return (%8)

LongformerLayer.intermediate
LongformerIntermediate._actual_script_module
  graph(%self.179 : __torch__.transformers.modeling_longformer.LongformerIntermediate,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.179)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.136 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%5), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
    return (%input.136)

LongformerLayer.output
LongformerOutput._actual_script_module
  graph(%self.181 : __torch__.transformers.modeling_longformer.LongformerOutput,
        %1 : Float(17:1572864, 512:3072, 3072:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.181)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.181)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.181)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output # transformers/modeling_longformer.py:830:0
    %input.138 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output # transformers/modeling_longformer.py:830:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.138)
    return (%13)

LongformerAttention.output
LongformerSelfOutput._actual_script_module
  graph(%self.175 : __torch__.transformers.modeling_longformer.LongformerSelfOutput,
        %1 : Float(17:768, 512:13056, 768:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.175)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.175)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.175)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output # transformers/modeling_longformer.py:758:0
    %input.134 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output # transformers/modeling_longformer.py:758:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.134)
    return (%13)

LongformerAttention.self
LongformerSelfAttention._actual_script_module
  graph(%self.171 : __torch__.transformers.modeling_longformer.LongformerSelfAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.171)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.171)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.171)
    %6 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:241:0
    %7 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:241:0
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:241:0
    %9 : Float(17:512, 512:1) = aten::squeeze(%7, %8), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:241:0
    %10 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:22:0
    %is_index_masked.11 : Bool(17:512, 512:1) = aten::lt(%9, %10), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:22:0
    %18 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:248:0
    %19 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:248:0
    %input.127 : Float(512:768, 17:393216, 768:1) = aten::transpose(%2, %18, %19), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:248:0
    %1387 : Tensor = prim::CallMethod[name="forward"](%5, %input.127)
    %1388 : Tensor = prim::CallMethod[name="forward"](%4, %input.127)
    %1389 : Tensor = prim::CallMethod[name="forward"](%3, %input.127)
    %24 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
    %25 : int = aten::size(%input.127, %24), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
    %seq_len.42 : Long() = prim::NumToTensor(%25), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %27 : int = aten::Int(%seq_len.42), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %28 : int = aten::Int(%seq_len.42), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %29 : int = aten::Int(%seq_len.42), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %30 : int = aten::Int(%seq_len.42), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %31 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
    %32 : int = aten::size(%input.127, %31), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
    %batch_size.41 : Long() = prim::NumToTensor(%32), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %34 : int = aten::Int(%batch_size.41), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %35 : int = aten::Int(%batch_size.41), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %36 : int = aten::Int(%batch_size.41), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %37 : int = aten::Int(%batch_size.41), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %38 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
    %39 : int = aten::size(%input.127, %38), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
    %embed_dim.11 : Long() = prim::NumToTensor(%39), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %41 : int = aten::Int(%embed_dim.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %44 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:261:0
    %query_vectors.22 : Float(512:13056, 17:768, 768:1) = aten::div_(%1387, %44), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:261:0
    %46 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:263:0
    %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:263:0
    %48 : int[] = prim::ListConstruct(%30, %37, %46, %47), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %49 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.22, %48), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:263:0
    %50 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:263:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:263:0
    %query.21 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%49, %50, %51), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:263:0
    %53 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:264:0
    %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:264:0
    %55 : int[] = prim::ListConstruct(%29, %36, %53, %54), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %56 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1388, %55), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:264:0
    %57 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:264:0
    %58 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:264:0
    %key.11 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%56, %57, %58), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:264:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %61 : int = aten::size(%query.21, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.42 : Long() = prim::NumToTensor(%61), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %63 : int = aten::Int(%batch_size.42), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %65 : int = aten::size(%query.21, %64), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.43 : Long() = prim::NumToTensor(%65), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %67 : int = aten::Int(%seq_len.43), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %68 : int = aten::Int(%seq_len.43), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %69 : int = aten::Int(%seq_len.43), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %70 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %71 : int = aten::size(%query.21, %70), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.31 : Long() = prim::NumToTensor(%71), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %73 : int = aten::Int(%num_heads.31), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %74 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %75 : int = aten::size(%query.21, %74), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.31 : Long() = prim::NumToTensor(%75), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %77 : int = aten::Int(%head_dim.31), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %78 : int = aten::Int(%head_dim.31), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %111 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %112 : Long() = aten::floor_divide(%seq_len.43, %111), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %113 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:478:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.31 : Long() = aten::sub(%112, %113, %114), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:478:0
    %116 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
    %117 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
    %118 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.21, %116, %117), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
    %119 : Long() = aten::mul(%batch_size.42, %num_heads.31), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
    %120 : int = aten::Int(%119), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %121 : int[] = prim::ListConstruct(%120, %69, %78), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %hidden_states.112 : Float(204:64, 512:13056, 64:1) = aten::reshape(%118, %121), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
    %123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
    %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
    %125 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.11, %123, %124), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
    %126 : Long() = aten::mul(%batch_size.42, %num_heads.31), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
    %127 : int = aten::Int(%126), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %128 : int[] = prim::ListConstruct(%127, %68, %77), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %hidden_states.114 : Float(204:64, 512:13056, 64:1) = aten::reshape(%125, %128), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
    %130 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
    %131 : int = aten::size(%hidden_states.112, %130), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
    %132 : Long() = prim::NumToTensor(%131), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %133 : int = aten::Int(%132), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %134 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
    %135 : int = aten::size(%hidden_states.112, %134), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
    %136 : Long() = prim::NumToTensor(%135), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %137 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %138 : Long() = aten::floor_divide(%136, %137), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %139 : int = aten::Int(%138), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %140 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
    %141 : int = aten::size(%hidden_states.112, %140), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
    %142 : Long() = prim::NumToTensor(%141), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %143 : int = aten::Int(%142), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %144 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
    %145 : int[] = prim::ListConstruct(%133, %139, %144, %143), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %hidden_states.113 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.112, %145), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
    %147 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %148 : int = aten::size(%hidden_states.113, %147), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %149 : Long() = prim::NumToTensor(%148), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %150 : int = aten::Int(%149), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %151 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %152 : int = aten::size(%hidden_states.113, %151), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %153 : Long() = prim::NumToTensor(%152), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %154 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %155 : int = aten::size(%hidden_states.113, %154), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %156 : Long() = prim::NumToTensor(%155), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %157 : int = aten::Int(%156), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %158 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %159 : int = aten::size(%hidden_states.113, %158), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %160 : Long() = prim::NumToTensor(%159), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %161 : int = aten::Int(%160), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %162 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %163 : Long() = aten::mul(%153, %162), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %164 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %165 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %166 : Long() = aten::sub(%163, %164, %165), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %167 : int = aten::Int(%166), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %168 : int[] = prim::ListConstruct(%150, %167, %157, %161), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %169 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %170 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %171 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %172 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %173 : int[] = prim::ListConstruct(%169, %170, %171, %172), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %174 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %175 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.113, %168, %173, %174), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %176 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
    %177 : int = aten::size(%hidden_states.114, %176), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %179 : int = aten::Int(%178), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
    %181 : int = aten::size(%hidden_states.114, %180), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
    %182 : Long() = prim::NumToTensor(%181), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %183 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %184 : Long() = aten::floor_divide(%182, %183), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %185 : int = aten::Int(%184), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %186 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
    %187 : int = aten::size(%hidden_states.114, %186), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
    %188 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %189 : int = aten::Int(%188), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %190 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
    %191 : int[] = prim::ListConstruct(%179, %185, %190, %189), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %hidden_states.115 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.114, %191), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
    %193 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %194 : int = aten::size(%hidden_states.115, %193), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %195 : Long() = prim::NumToTensor(%194), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %196 : int = aten::Int(%195), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %197 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %198 : int = aten::size(%hidden_states.115, %197), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %199 : Long() = prim::NumToTensor(%198), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %200 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %201 : int = aten::size(%hidden_states.115, %200), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %203 : int = aten::Int(%202), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %204 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %205 : int = aten::size(%hidden_states.115, %204), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %207 : int = aten::Int(%206), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %208 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %209 : Long() = aten::mul(%199, %208), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %210 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %211 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %212 : Long() = aten::sub(%209, %210, %211), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %213 : int = aten::Int(%212), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %214 : int[] = prim::ListConstruct(%196, %213, %203, %207), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %216 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %217 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %218 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %219 : int[] = prim::ListConstruct(%215, %216, %217, %218), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %220 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.115, %214, %219, %220), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %222 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/functional.py:327:0
    %223 : Tensor[] = prim::ListConstruct(%175, %221), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %input.128 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%222, %223), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/functional.py:327:0
    %225 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %226 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %227 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %229 : int[] = prim::ListConstruct(%225, %226, %227, %228), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %230 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.21 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.128, %229, %230), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %232 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %233 : int = aten::size(%hidden_states_padded.21, %232), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %234 : Long() = prim::NumToTensor(%233), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %235 : int = aten::Int(%234), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %236 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %237 : int = aten::size(%hidden_states_padded.21, %236), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %238 : Long() = prim::NumToTensor(%237), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %239 : int = aten::Int(%238), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %246 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %247 : int = aten::size(%hidden_states_padded.21, %246), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %248 : Long() = prim::NumToTensor(%247), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %249 : int = aten::Int(%248), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %250 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %251 : int = aten::size(%hidden_states_padded.21, %250), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %252 : Long() = prim::NumToTensor(%251), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %253 : int = aten::Int(%252), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %254 : int[] = prim::ListConstruct(%235, %239, %249, %253), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %diagonal_chunked_attention_scores.21 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.21, %254), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:400:0
    %256 : Long() = aten::mul(%batch_size.42, %num_heads.31), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
    %257 : int = aten::Int(%256), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %258 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
    %259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
    %260 : Long() = aten::add(%chunks_count.31, %258, %259), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
    %261 : int = aten::Int(%260), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %263 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %264 : int[] = prim::ListConstruct(%257, %261, %262, %263), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %265 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %266 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %268 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.21 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.21, %264, %265, %266, %267, %268), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %270 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %271 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %273 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %270, %271, %272, %273), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %275 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %276 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %277 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %279 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%274, %275, %276, %277, %278), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %280 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %281 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %282 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %284 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%279, %280, %281, %282, %283), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %285 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %286 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %287 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %289 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%284, %285, %286, %287, %288), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %290 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %291 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %292 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %294 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %290, %291, %292, %293), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %295 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %296 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %297 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %299 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%294, %295, %296, %297, %298), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %300 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %301 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %302 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %303 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %304 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%299, %300, %301, %302, %303), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %305 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %306 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %307 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %308 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %309 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%304, %305, %306, %307, %308), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %310 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %311 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %312 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %313 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %314 : int[] = prim::ListConstruct(%310, %311, %312, %313), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %315 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%289, %314), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %316 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %317 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%309, %315, %316), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %319 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %320 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %321 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %322 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %318, %319, %320, %321), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %323 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %324 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %325 : Float(204:262656, 512:513, 513:1) = aten::select(%322, %323, %324), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %327 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %328 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %329 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %330 : Float(204:262656, 256:513, 513:1) = aten::slice(%325, %326, %327, %328, %329), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %331 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %333 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %335 : Float(204:262656, 256:513, 257:1) = aten::slice(%330, %331, %332, %333, %334), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %336 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %340 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %336, %337, %338, %339), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %341 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %342 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %343 : Float(204:262656, 256:513, 513:1) = aten::select(%340, %341, %342), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %345 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %347 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %348 : Float(204:262656, 256:513, 513:1) = aten::slice(%343, %344, %345, %346, %347), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %349 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %350 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %352 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %353 : Float(204:262656, 256:513, 257:1) = aten::slice(%348, %349, %350, %351, %352), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %354 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %355 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %356 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %357 : int[] = prim::ListConstruct(%354, %355, %356), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %358 : Float(204:262656, 256:513, 257:1) = aten::view(%335, %357), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %359 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %360 : Float(204:262656, 256:513, 257:1) = aten::copy_(%353, %358, %359), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %361 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %362 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %363 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %364 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %365 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %361, %362, %363, %364), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %366 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %367 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %368 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %369 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %370 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%365, %366, %367, %368, %369), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %371 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %372 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %373 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %374 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %375 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%370, %371, %372, %373, %374), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %376 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %377 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %379 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %380 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%375, %376, %377, %378, %379), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %383 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %384 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %385 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %381, %382, %383, %384), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %386 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %387 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %388 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %389 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %390 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%385, %386, %387, %388, %389), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %391 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %392 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %393 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %394 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %395 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%390, %391, %392, %393, %394), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %397 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %398 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %399 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %400 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%395, %396, %397, %398, %399), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %401 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %402 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %403 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %404 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %405 : int[] = prim::ListConstruct(%401, %402, %403, %404), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %406 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%380, %405), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %407 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %408 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%400, %406, %407), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %409 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %410 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %411 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %412 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %413 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %409, %410, %411, %412), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %414 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %415 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %416 : Float(204:262656, 512:513, 513:1) = aten::select(%413, %414, %415), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %417 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %418 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %419 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %420 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %421 : Float(204:262656, 255:513, 513:1) = aten::slice(%416, %417, %418, %419, %420), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %422 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %423 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %425 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %426 : Float(204:262656, 255:513, 255:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %427 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %429 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %430 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %431 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %427, %428, %429, %430), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %432 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %433 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %434 : Float(204:262656, 256:513, 513:1) = aten::select(%431, %432, %433), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %435 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %436 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %437 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %438 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %439 : Float(204:262656, 255:513, 513:1) = aten::slice(%434, %435, %436, %437, %438), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %440 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %441 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %442 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %443 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %444 : Float(204:262656, 255:513, 255:1) = aten::slice(%439, %440, %441, %442, %443), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %445 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %446 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %447 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %448 : int[] = prim::ListConstruct(%445, %446, %447), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %449 : Float(204:262656, 255:513, 255:1) = aten::view(%426, %448), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %450 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %451 : Float(204:262656, 255:513, 255:1) = aten::copy_(%444, %449, %450), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %452 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
    %453 : int[] = prim::ListConstruct(%63, %73, %67, %452), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %454 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.21, %453), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
    %455 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
    %456 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.31 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%454, %455, %456), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
    %458 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %459 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %460 : int[] = prim::ListConstruct(%458, %459), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %461 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %462 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %463 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %464 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %465 : Float(256:257, 257:1) = aten::ones(%460, %461, %462, %463, %464), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %466 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %467 : Float(256:257, 257:1) = aten::tril(%465, %466), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %468 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %469 : int[] = prim::ListConstruct(%468), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %beginning_mask_2d.21 : Float(256:257, 257:1) = aten::flip(%467, %469), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %471 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %472 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.21, %471), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %473 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %474 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %475 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %476 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %477 : Float(1:65792, 256:257, 257:1) = aten::slice(%472, %473, %474, %475, %476), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %478 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %479 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%477, %478), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %480 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %481 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %482 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %483 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.21 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%479, %480, %481, %482, %483), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %485 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:460:0
    %486 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:460:0
    %487 : int[] = prim::ListConstruct(%485, %486), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %ending_mask.21 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.21, %487), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:460:0
    %489 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %490 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %491 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %493 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.31, %489, %490, %491, %492), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %494 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %495 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %496 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %497 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %498 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%493, %494, %495, %496, %497), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %499 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %500 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %501 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %502 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %503 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%498, %499, %500, %501, %502), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %504 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %505 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %506 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %507 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.21 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%503, %504, %505, %506, %507), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %509 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %510 : int = aten::size(%beginning_input.21, %509), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %511 : Long() = prim::NumToTensor(%510), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %512 : int = aten::Int(%511), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %513 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %514 : int = aten::size(%beginning_input.21, %513), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %515 : Long() = prim::NumToTensor(%514), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %516 : int = aten::Int(%515), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %517 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %518 : int = aten::size(%beginning_input.21, %517), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %519 : Long() = prim::NumToTensor(%518), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %520 : int = aten::Int(%519), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %521 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %522 : int = aten::size(%beginning_input.21, %521), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %523 : Long() = prim::NumToTensor(%522), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %524 : int = aten::Int(%523), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %525 : int[] = prim::ListConstruct(%512, %516, %520, %524), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %526 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %527 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.21, %525, %526), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %528 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:22:0
    %529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%527, %528), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:22:0
    %530 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:463:0
    %531 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.21, %529, %530), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:463:0
    %532 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %533 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %534 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %535 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %536 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.31, %532, %533, %534, %535), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %537 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %538 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %539 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %541 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%536, %537, %538, %539, %540), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %542 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %543 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %544 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %545 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %546 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%541, %542, %543, %544, %545), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %547 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %548 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %549 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.21 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%546, %547, %548, %549, %550), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %552 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %553 : int = aten::size(%ending_input.21, %552), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %554 : Long() = prim::NumToTensor(%553), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %555 : int = aten::Int(%554), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %556 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %557 : int = aten::size(%ending_input.21, %556), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %558 : Long() = prim::NumToTensor(%557), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %559 : int = aten::Int(%558), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %560 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %561 : int = aten::size(%ending_input.21, %560), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %562 : Long() = prim::NumToTensor(%561), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %563 : int = aten::Int(%562), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %564 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %565 : int = aten::size(%ending_input.21, %564), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %566 : Long() = prim::NumToTensor(%565), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %567 : int = aten::Int(%566), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %568 : int[] = prim::ListConstruct(%555, %559, %563, %567), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %569 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %570 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.21, %568, %569), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %571 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:22:0
    %572 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%570, %571), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:22:0
    %573 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:466:0
    %574 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.21, %572, %573), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:466:0
    %575 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:22:0
    %576 : Bool(17:512, 512:1) = aten::ne(%9, %575), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:22:0
    %577 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %578 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %579 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %580 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %581 : Bool(17:512, 512:1) = aten::slice(%576, %577, %578, %579, %580), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %582 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %583 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %584 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %585 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %586 : Bool(17:512, 512:1) = aten::slice(%581, %582, %583, %584, %585), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %587 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %588 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%586, %587), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %589 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %remove_from_windowed_attention_mask.11 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%588, %589), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
    %591 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.11, %query.21), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:275:0
    %592 : float = prim::Constant[value=-10000.](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:275:0
    %float_mask.11 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%591, %remove_from_windowed_attention_mask.11, %592), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:275:0
    %594 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %595 : int = aten::size(%float_mask.11, %594), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %596 : Long() = prim::NumToTensor(%595), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %597 : int = aten::Int(%596), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %598 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %599 : int = aten::size(%float_mask.11, %598), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %600 : Long() = prim::NumToTensor(%599), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %601 : int = aten::Int(%600), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %602 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %603 : int = aten::size(%float_mask.11, %602), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %604 : Long() = prim::NumToTensor(%603), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %605 : int = aten::Int(%604), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %606 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %607 : int = aten::size(%float_mask.11, %606), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %608 : Long() = prim::NumToTensor(%607), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %609 : int = aten::Int(%608), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %610 : int[] = prim::ListConstruct(%597, %601, %605, %609), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %611 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %612 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %613 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %614 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %query.22 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%610, %611, %612, %613, %614), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
    %616 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %617 : int = aten::size(%query.22, %616), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.43 : Long() = prim::NumToTensor(%617), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %619 : int = aten::Int(%batch_size.43), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %620 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %621 : int = aten::size(%query.22, %620), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.44 : Long() = prim::NumToTensor(%621), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %623 : int = aten::Int(%seq_len.44), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %624 : int = aten::Int(%seq_len.44), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %625 : int = aten::Int(%seq_len.44), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %626 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %627 : int = aten::size(%query.22, %626), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.32 : Long() = prim::NumToTensor(%627), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %629 : int = aten::Int(%num_heads.32), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %630 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %631 : int = aten::size(%query.22, %630), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.32 : Long() = prim::NumToTensor(%631), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %633 : int = aten::Int(%head_dim.32), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %634 : int = aten::Int(%head_dim.32), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %667 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %668 : Long() = aten::floor_divide(%seq_len.44, %667), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %669 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:478:0
    %670 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.32 : Long() = aten::sub(%668, %669, %670), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:478:0
    %672 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
    %673 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
    %674 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.22, %672, %673), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
    %675 : Long() = aten::mul(%batch_size.43, %num_heads.32), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
    %676 : int = aten::Int(%675), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %677 : int[] = prim::ListConstruct(%676, %625, %634), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %hidden_states.116 : Float(17:512, 512:1, 1:1) = aten::reshape(%674, %677), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
    %679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
    %680 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
    %681 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.11, %679, %680), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
    %682 : Long() = aten::mul(%batch_size.43, %num_heads.32), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
    %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %684 : int[] = prim::ListConstruct(%683, %624, %633), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %hidden_states.118 : Float(17:512, 512:1, 1:1) = aten::reshape(%681, %684), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
    %686 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
    %687 : int = aten::size(%hidden_states.116, %686), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
    %688 : Long() = prim::NumToTensor(%687), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %690 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
    %691 : int = aten::size(%hidden_states.116, %690), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
    %692 : Long() = prim::NumToTensor(%691), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %693 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %694 : Long() = aten::floor_divide(%692, %693), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %696 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
    %697 : int = aten::size(%hidden_states.116, %696), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
    %698 : Long() = prim::NumToTensor(%697), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %699 : int = aten::Int(%698), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %700 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
    %701 : int[] = prim::ListConstruct(%689, %695, %700, %699), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %hidden_states.117 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.116, %701), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
    %703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %704 : int = aten::size(%hidden_states.117, %703), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %705 : Long() = prim::NumToTensor(%704), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %706 : int = aten::Int(%705), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %708 : int = aten::size(%hidden_states.117, %707), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %709 : Long() = prim::NumToTensor(%708), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %710 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %711 : int = aten::size(%hidden_states.117, %710), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %712 : Long() = prim::NumToTensor(%711), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %713 : int = aten::Int(%712), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %714 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %715 : int = aten::size(%hidden_states.117, %714), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %716 : Long() = prim::NumToTensor(%715), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %717 : int = aten::Int(%716), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %718 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %719 : Long() = aten::mul(%709, %718), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %720 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %721 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %722 : Long() = aten::sub(%719, %720, %721), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %723 : int = aten::Int(%722), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %724 : int[] = prim::ListConstruct(%706, %723, %713, %717), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %725 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %726 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %727 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %728 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %729 : int[] = prim::ListConstruct(%725, %726, %727, %728), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %730 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %731 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.117, %724, %729, %730), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %732 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
    %733 : int = aten::size(%hidden_states.118, %732), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
    %734 : Long() = prim::NumToTensor(%733), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %735 : int = aten::Int(%734), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %736 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
    %737 : int = aten::size(%hidden_states.118, %736), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
    %738 : Long() = prim::NumToTensor(%737), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %739 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %740 : Long() = aten::floor_divide(%738, %739), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %741 : int = aten::Int(%740), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
    %743 : int = aten::size(%hidden_states.118, %742), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
    %744 : Long() = prim::NumToTensor(%743), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %745 : int = aten::Int(%744), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %746 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
    %747 : int[] = prim::ListConstruct(%735, %741, %746, %745), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %hidden_states.119 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.118, %747), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
    %749 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %750 : int = aten::size(%hidden_states.119, %749), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %751 : Long() = prim::NumToTensor(%750), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %752 : int = aten::Int(%751), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %754 : int = aten::size(%hidden_states.119, %753), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %755 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %756 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %757 : int = aten::size(%hidden_states.119, %756), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %758 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %759 : int = aten::Int(%758), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %760 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %761 : int = aten::size(%hidden_states.119, %760), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
    %762 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %764 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %765 : Long() = aten::mul(%755, %764), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %766 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %767 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %768 : Long() = aten::sub(%765, %766, %767), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
    %769 : int = aten::Int(%768), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %770 : int[] = prim::ListConstruct(%752, %769, %759, %763), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %771 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %772 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %773 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %776 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.119, %770, %775, %776), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
    %778 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/functional.py:327:0
    %779 : Tensor[] = prim::ListConstruct(%731, %777), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %input.129 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%778, %779), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/functional.py:327:0
    %781 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %782 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %783 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %786 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.22 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.129, %785, %786), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %788 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %789 : int = aten::size(%hidden_states_padded.22, %788), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %790 : Long() = prim::NumToTensor(%789), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %791 : int = aten::Int(%790), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %792 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %793 : int = aten::size(%hidden_states_padded.22, %792), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %794 : Long() = prim::NumToTensor(%793), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %795 : int = aten::Int(%794), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %802 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %803 : int = aten::size(%hidden_states_padded.22, %802), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %804 : Long() = prim::NumToTensor(%803), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %805 : int = aten::Int(%804), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %806 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %807 : int = aten::size(%hidden_states_padded.22, %806), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
    %808 : Long() = prim::NumToTensor(%807), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %809 : int = aten::Int(%808), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %810 : int[] = prim::ListConstruct(%791, %795, %805, %809), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %diagonal_chunked_attention_scores.22 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.22, %810), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:400:0
    %812 : Long() = aten::mul(%batch_size.43, %num_heads.32), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
    %813 : int = aten::Int(%812), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %814 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
    %815 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
    %816 : Long() = aten::add(%chunks_count.32, %814, %815), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
    %817 : int = aten::Int(%816), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %818 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %819 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %820 : int[] = prim::ListConstruct(%813, %817, %818, %819), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %821 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %822 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %823 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %824 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.22 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.22, %820, %821, %822, %823, %824), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
    %826 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %827 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %828 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %829 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %830 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %826, %827, %828, %829), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %831 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %832 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %833 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %834 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%830, %831, %832, %833, %834), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %836 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %837 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %838 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %839 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%835, %836, %837, %838, %839), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %841 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %842 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %843 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %845 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%840, %841, %842, %843, %844), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %846 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %847 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %848 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %849 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %846, %847, %848, %849), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %851 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %852 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %853 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %855 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%850, %851, %852, %853, %854), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %856 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %857 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %858 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %859 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %860 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%855, %856, %857, %858, %859), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %861 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %862 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %863 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %864 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %865 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%860, %861, %862, %863, %864), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %866 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %867 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %868 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %869 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %870 : int[] = prim::ListConstruct(%866, %867, %868, %869), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %871 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%845, %870), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %872 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %873 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%865, %871, %872), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
    %874 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %875 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %876 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %877 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %874, %875, %876, %877), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %879 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %880 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %881 : Float(17:262656, 512:513, 513:1) = aten::select(%878, %879, %880), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %883 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %884 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %885 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %886 : Float(17:262656, 256:513, 513:1) = aten::slice(%881, %882, %883, %884, %885), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %887 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %889 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%886, %887, %888, %889, %890), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %892 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %893 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %894 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %895 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %896 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %892, %893, %894, %895), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %897 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %898 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %899 : Float(17:262656, 256:513, 513:1) = aten::select(%896, %897, %898), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %901 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %902 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %903 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %904 : Float(17:262656, 256:513, 513:1) = aten::slice(%899, %900, %901, %902, %903), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %905 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %906 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %907 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %909 : Float(17:262656, 256:513, 257:1) = aten::slice(%904, %905, %906, %907, %908), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %910 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %911 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %912 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %913 : int[] = prim::ListConstruct(%910, %911, %912), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %914 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %913), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %915 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %916 : Float(17:262656, 256:513, 257:1) = aten::copy_(%909, %914, %915), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
    %917 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %918 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %919 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %920 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %921 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %917, %918, %919, %920), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %922 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %923 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %924 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %925 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %926 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%921, %922, %923, %924, %925), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %927 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %928 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %929 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %930 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %931 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%926, %927, %928, %929, %930), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %932 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %933 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %934 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %935 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %936 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%931, %932, %933, %934, %935), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %938 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %939 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %940 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %941 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %937, %938, %939, %940), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %942 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %943 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %944 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %945 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %946 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%941, %942, %943, %944, %945), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %947 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %948 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %949 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %950 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %951 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%946, %947, %948, %949, %950), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %952 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %953 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %954 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %955 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %956 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%951, %952, %953, %954, %955), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %957 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %959 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %960 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %962 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%936, %961), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %963 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %964 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%956, %962, %963), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
    %965 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %966 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %967 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %969 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %965, %966, %967, %968), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %970 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %971 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %972 : Float(17:262656, 512:513, 513:1) = aten::select(%969, %970, %971), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %973 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %974 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %975 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %976 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %977 : Float(17:262656, 255:513, 513:1) = aten::slice(%972, %973, %974, %975, %976), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %978 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %979 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %980 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %981 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %982 : Float(17:262656, 255:513, 255:1) = aten::slice(%977, %978, %979, %980, %981), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %983 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %984 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %985 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %986 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %987 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %983, %984, %985, %986), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %988 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %989 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %990 : Float(17:262656, 256:513, 513:1) = aten::select(%987, %988, %989), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %991 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %993 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %994 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %995 : Float(17:262656, 255:513, 513:1) = aten::slice(%990, %991, %992, %993, %994), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %996 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %998 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %999 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %1000 : Float(17:262656, 255:513, 255:1) = aten::slice(%995, %996, %997, %998, %999), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %1001 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %1002 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %1003 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %1004 : int[] = prim::ListConstruct(%1001, %1002, %1003), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1005 : Float(17:262656, 255:513, 255:1) = aten::view(%982, %1004), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %1006 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1007 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1000, %1005, %1006), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
    %1008 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
    %1009 : int[] = prim::ListConstruct(%619, %629, %623, %1008), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1010 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.22, %1009), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
    %1011 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
    %1012 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.32 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1010, %1011, %1012), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
    %1014 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %1015 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %1016 : int[] = prim::ListConstruct(%1014, %1015), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1017 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %1018 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %1019 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %1020 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %1021 : Float(256:257, 257:1) = aten::ones(%1016, %1017, %1018, %1019, %1020), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %1022 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %1023 : Float(256:257, 257:1) = aten::tril(%1021, %1022), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %1024 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %1025 : int[] = prim::ListConstruct(%1024), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %beginning_mask_2d.22 : Float(256:257, 257:1) = aten::flip(%1023, %1025), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
    %1027 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1028 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.22, %1027), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1029 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1030 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1031 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1032 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1033 : Float(1:65792, 256:257, 257:1) = aten::slice(%1028, %1029, %1030, %1031, %1032), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1034 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1035 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1033, %1034), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1036 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1037 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1038 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1039 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.22 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1035, %1036, %1037, %1038, %1039), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
    %1041 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:460:0
    %1042 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:460:0
    %1043 : int[] = prim::ListConstruct(%1041, %1042), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %ending_mask.22 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.22, %1043), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:460:0
    %1045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1046 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1047 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1048 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1049 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.32, %1045, %1046, %1047, %1048), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1050 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1052 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1053 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1054 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1049, %1050, %1051, %1052, %1053), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1055 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1056 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1057 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1058 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1059 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1054, %1055, %1056, %1057, %1058), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1060 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1061 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1062 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.22 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1059, %1060, %1061, %1062, %1063), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
    %1065 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %1066 : int = aten::size(%beginning_input.22, %1065), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %1067 : Long() = prim::NumToTensor(%1066), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1068 : int = aten::Int(%1067), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1069 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %1070 : int = aten::size(%beginning_input.22, %1069), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %1071 : Long() = prim::NumToTensor(%1070), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1072 : int = aten::Int(%1071), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1073 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %1074 : int = aten::size(%beginning_input.22, %1073), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %1075 : Long() = prim::NumToTensor(%1074), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1076 : int = aten::Int(%1075), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1077 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %1078 : int = aten::size(%beginning_input.22, %1077), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %1079 : Long() = prim::NumToTensor(%1078), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1080 : int = aten::Int(%1079), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1081 : int[] = prim::ListConstruct(%1068, %1072, %1076, %1080), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1082 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %1083 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.22, %1081, %1082), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
    %1084 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:22:0
    %1085 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1083, %1084), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:22:0
    %1086 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:463:0
    %1087 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.22, %1085, %1086), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:463:0
    %1088 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1089 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1090 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1091 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1092 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.32, %1088, %1089, %1090, %1091), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1094 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1095 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1096 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1097 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1092, %1093, %1094, %1095, %1096), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1098 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1099 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1100 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1101 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1102 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1097, %1098, %1099, %1100, %1101), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1103 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1104 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1105 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1106 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.22 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1102, %1103, %1104, %1105, %1106), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
    %1108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %1109 : int = aten::size(%ending_input.22, %1108), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %1110 : Long() = prim::NumToTensor(%1109), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1111 : int = aten::Int(%1110), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1112 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %1113 : int = aten::size(%ending_input.22, %1112), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %1114 : Long() = prim::NumToTensor(%1113), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1115 : int = aten::Int(%1114), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1116 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %1117 : int = aten::size(%ending_input.22, %1116), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %1118 : Long() = prim::NumToTensor(%1117), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1119 : int = aten::Int(%1118), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1120 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %1121 : int = aten::size(%ending_input.22, %1120), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %1122 : Long() = prim::NumToTensor(%1121), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1124 : int[] = prim::ListConstruct(%1111, %1115, %1119, %1123), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1125 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %1126 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.22, %1124, %1125), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
    %1127 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:22:0
    %1128 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1126, %1127), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:22:0
    %1129 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:466:0
    %1130 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.22, %1128, %1129), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:466:0
    %1131 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:284:0
    %attn_scores.11 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.31, %input_tensor.32, %1131), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:284:0
    %1151 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:1500:0
    %1152 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:1500:0
    %attn_probs_fp32.11 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.11, %1151, %1152), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:1500:0
    %attn_probs.21 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::type_as(%attn_probs_fp32.11, %attn_scores.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:320:0
    %1155 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1156 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1157 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1158 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1159 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.11, %1155, %1156, %1157, %1158), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1161 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1162 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1163 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1164 : Bool(17:512, 512:1) = aten::slice(%1159, %1160, %1161, %1162, %1163), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1165 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1166 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1164, %1165), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1167 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1168 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1166, %1167), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1169 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %input.130 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs.21, %1168, %1169), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
    %1171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:973:0
    %1172 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:973:0
    %attn_probs.22 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.130, %1171, %1172), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:973:0
    %1174 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:331:0
    %1175 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:331:0
    %1176 : int[] = prim::ListConstruct(%28, %35, %1174, %1175), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1177 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1389, %1176), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:331:0
    %1178 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:331:0
    %1179 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:331:0
    %value.11 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1177, %1178, %1179), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:331:0
    %1181 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
    %1182 : int = aten::size(%value.11, %1181), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
    %batch_size.44 : Long() = prim::NumToTensor(%1182), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1184 : int = aten::Int(%batch_size.44), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1185 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
    %1186 : int = aten::size(%value.11, %1185), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
    %seq_len.45 : Long() = prim::NumToTensor(%1186), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1188 : int = aten::Int(%seq_len.45), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1189 : int = aten::Int(%seq_len.45), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1190 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
    %1191 : int = aten::size(%value.11, %1190), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
    %num_heads.33 : Long() = prim::NumToTensor(%1191), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1193 : int = aten::Int(%num_heads.33), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1194 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
    %1195 : int = aten::size(%value.11, %1194), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
    %head_dim.33 : Long() = prim::NumToTensor(%1195), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1197 : int = aten::Int(%head_dim.33), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1198 : int = aten::Int(%head_dim.33), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1199 : int = aten::Int(%head_dim.33), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1236 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %1237 : Long() = aten::floor_divide(%seq_len.45, %1236), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %1238 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:542:0
    %1239 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:542:0
    %chunks_count.33 : Long() = aten::sub(%1237, %1238, %1239), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:542:0
    %1241 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:545:0
    %1242 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:545:0
    %1243 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.22, %1241, %1242), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:545:0
    %1244 : Long() = aten::mul(%batch_size.44, %num_heads.33), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:546:0
    %1245 : int = aten::Int(%1244), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1246 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %1247 : Long() = aten::floor_divide(%seq_len.45, %1246), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/tensor.py:424:0
    %1248 : int = aten::Int(%1247), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1249 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:545:0
    %1250 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:545:0
    %1251 : int[] = prim::ListConstruct(%1245, %1248, %1249, %1250), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %chunked_hidden_states.51 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1243, %1251), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:545:0
    %1253 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
    %1254 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
    %1255 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.11, %1253, %1254), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
    %1256 : Long() = aten::mul(%batch_size.44, %num_heads.33), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
    %1257 : int = aten::Int(%1256), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1258 : int[] = prim::ListConstruct(%1257, %1189, %1199), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %input.131 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1255, %1258), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
    %1260 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %1261 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %1262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %1263 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %1264 : int[] = prim::ListConstruct(%1260, %1261, %1262, %1263), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1265 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %padded_value.11 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.131, %1264, %1265), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %1267 : Long() = aten::mul(%batch_size.44, %num_heads.33), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:556:0
    %1268 : int = aten::Int(%1267), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1269 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:556:0
    %1270 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:556:0
    %1271 : Long() = aten::add(%chunks_count.33, %1269, %1270), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:556:0
    %1272 : int = aten::Int(%1271), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:564:0
    %1274 : int[] = prim::ListConstruct(%1268, %1272, %1273, %1198), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1275 : int = prim::Constant[value=65536](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:564:0
    %1276 : int = prim::Constant[value=16384](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:564:0
    %1277 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:564:0
    %1278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:564:0
    %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1280 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1281 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.11, %1274, %1279, %1280), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:564:0
    %1282 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
    %1283 : int = aten::size(%chunked_hidden_states.51, %1282), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
    %total_num_heads.11 : Long() = prim::NumToTensor(%1283), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1285 : int = aten::Int(%total_num_heads.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1286 : int = aten::Int(%total_num_heads.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1287 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
    %1288 : int = aten::size(%chunked_hidden_states.51, %1287), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
    %num_chunks.11 : Long() = prim::NumToTensor(%1288), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1290 : int = aten::Int(%num_chunks.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1291 : int = aten::Int(%num_chunks.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1292 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
    %1293 : int = aten::size(%chunked_hidden_states.51, %1292), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
    %window_overlap.11 : Long() = prim::NumToTensor(%1293), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1295 : int = aten::Int(%window_overlap.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1296 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
    %1297 : int = aten::size(%chunked_hidden_states.51, %1296), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
    %hidden_dim.11 : Long() = prim::NumToTensor(%1297), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1299 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:422:0
    %1300 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:422:0
    %1301 : Long() = aten::add(%window_overlap.11, %1299, %1300), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:422:0
    %1302 : int = aten::Int(%1301), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1303 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %1304 : int[] = prim::ListConstruct(%1303, %1302), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1305 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %chunked_hidden_states.52 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.51, %1304, %1305), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
    %1307 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:424:0
    %1308 : int[] = prim::ListConstruct(%1286, %1291, %1307), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %chunked_hidden_states.53 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.52, %1308), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:424:0
    %1310 : Long() = aten::neg(%window_overlap.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:428:0
    %1311 : int = aten::Int(%1310), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1312 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1313 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1314 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1315 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1316 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.53, %1312, %1313, %1314, %1315), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1319 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1320 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1316, %1317, %1318, %1319, %1320), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1322 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1323 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1324 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %chunked_hidden_states.54 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1321, %1322, %1323, %1311, %1324), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
    %1326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:431:0
    %1327 : Long() = aten::add(%window_overlap.11, %hidden_dim.11, %1326), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:431:0
    %1328 : int = aten::Int(%1327), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1329 : int[] = prim::ListConstruct(%1285, %1290, %1295, %1328), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %chunked_hidden_states.55 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.54, %1329), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:430:0
    %1331 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1333 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1335 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.55, %1331, %1332, %1333, %1334), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1336 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1340 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1335, %1336, %1337, %1338, %1339), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1341 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1342 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1343 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1345 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1340, %1341, %1342, %1343, %1344), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1346 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1347 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1348 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1350 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1345, %1346, %1347, %1348, %1349), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
    %1351 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/functional.py:327:0
    %1352 : Tensor[] = prim::ListConstruct(%1350, %1281), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %context.11 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%1351, %1352), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/functional.py:327:0
    %1354 : int[] = prim::ListConstruct(%1184, %1193, %1188, %1197), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1355 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.11, %1354), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:569:0
    %1356 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:569:0
    %1357 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:569:0
    %attn_output.21 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1355, %1356, %1357), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:569:0
    %1377 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
    %1378 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
    %1379 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.21, %1377, %1378), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
    %1380 : int[] = prim::ListConstruct(%27, %34, %41), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
    %1381 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1379, %1380), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
    %1382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
    %attn_output.22 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1381, %1382), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
    %1384 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:374:0
    %1385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:374:0
    %input.132 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.22, %1384, %1385), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_longformer.py:374:0
    return (%input.132)

LongformerSelfAttention.key
Linear._actual_script_module
  graph(%self.173 : __torch__.torch.nn.modules.linear.Linear,
        %input.127 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.173)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.173)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
    %output.62 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.127, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
    %key_vectors.11 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.62, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
    return (%key_vectors.11)

LongformerSelfAttention.query
Linear._actual_script_module
  graph(%self.172 : __torch__.torch.nn.modules.linear.Linear,
        %input.127 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.172)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.172)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
    %output.61 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.127, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
    %query_vectors.21 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.61, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
    return (%query_vectors.21)

LongformerSelfAttention.value
Linear._actual_script_module
  graph(%self.174 : __torch__.torch.nn.modules.linear.Linear,
        %input.127 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.174)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.174)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
    %output.63 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.127, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
    %value_vectors.11 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.63, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
    return (%value_vectors.11)

LongformerSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.178 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.134 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.178)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.178)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.33 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.134, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.33)

LongformerSelfOutput.dense
Linear._actual_script_module
  graph(%self.176 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:768, 512:13056, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.176)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.176)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
    %output.64 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
    %input.133 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.64, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.133)

LongformerSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.177 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.120 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.120)

LongformerIntermediate.dense
Linear._actual_script_module
  graph(%self.180 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.180)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.180)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
    %output.65 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
    %input.135 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.65, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.135)

LongformerOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.184 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.138 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.184)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.184)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
    %hidden_states.122 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.138, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%hidden_states.122)

LongformerOutput.dense
Linear._actual_script_module
  graph(%self.182 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1572864, 512:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.182)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.182)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
    %output.66 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
    %input.137 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.66, %2, %6), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
    return (%input.137)

LongformerOutput.dropout
Dropout._actual_script_module
  graph(%self.183 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.121 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.121)

LongformerLayer._actual_script_module
  graph(%self.185 : __torch__.transformers.modeling_longformer.LongformerLayer,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerOutput = prim::GetAttr[name="output"](%self.185)
    %4 : __torch__.transformers.modeling_longformer.LongformerIntermediate = prim::GetAttr[name="intermediate"](%self.185)
    %5 : __torch__.transformers.modeling_longformer.LongformerAttention = prim::GetAttr[name="attention"](%self.185)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %attention_mask, %2)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

LongformerLayer.attention
LongformerAttention._actual_script_module
  graph(%self.186 : __torch__.transformers.modeling_longformer.LongformerAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.transformers.modeling_longformer.LongformerSelfOutput = prim::GetAttr[name="output"](%self.186)
    %4 : __torch__.transformers.modeling_longformer.LongformerSelfAttention = prim::GetAttr[name="self"](%self.186)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %attention_mask, %2)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %2)
    return (%8)

LongformerLayer.intermediate
LongformerIntermediate._actual_script_module
  graph(%self.195 : __torch__.transformers.modeling_longformer.LongformerIntermediate,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.195)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.148 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%5), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
    return (%input.148)

LongformerLayer.output
LongformerOutput._actual_script_module
  graph(%self.197 : __torch__.transformers.modeling_longformer.LongformerOutput,
        %1 : Float(17:1572864, 512:3072, 3072:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.197)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.197)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.197)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output # transformers/modeling_longformer.py:830:0
    %input.150 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output # transformers/modeling_longformer.py:830:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.150)
    return (%13)

LongformerAttention.output
LongformerSelfOutput._actual_script_module
  graph(%self.191 : __torch__.transformers.modeling_longformer.LongformerSelfOutput,
        %1 : Float(17:768, 512:13056, 768:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.191)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.191)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.191)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output # transformers/modeling_longformer.py:758:0
    %input.146 : Float(17:393216, 512:768, 768:1) = aten::add(%12, %2, %8), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output # transformers/modeling_longformer.py:758:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.146)
    return (%13)

LongformerAttention.self
LongformerSelfAttention._actual_script_module
  graph(%self.187 : __torch__.transformers.modeling_longformer.LongformerSelfAttention,
        %attention_mask : Float(17:512, 1:512, 1:512, 512:1),
        %2 : Float(17:393216, 512:768, 768:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.187)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.187)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.187)
    %6 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:241:0
    %7 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:241:0
    %8 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:241:0
    %9 : Float(17:512, 512:1) = aten::squeeze(%7, %8), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:241:0
    %10 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:22:0
    %is_index_masked : Bool(17:512, 512:1) = aten::lt(%9, %10), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:22:0
    %18 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:248:0
    %19 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:248:0
    %input.139 : Float(512:768, 17:393216, 768:1) = aten::transpose(%2, %18, %19), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:248:0
    %1387 : Tensor = prim::CallMethod[name="forward"](%5, %input.139)
    %1388 : Tensor = prim::CallMethod[name="forward"](%4, %input.139)
    %1389 : Tensor = prim::CallMethod[name="forward"](%3, %input.139)
    %24 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
    %25 : int = aten::size(%input.139, %24), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
    %seq_len.46 : Long() = prim::NumToTensor(%25), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %27 : int = aten::Int(%seq_len.46), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %28 : int = aten::Int(%seq_len.46), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %29 : int = aten::Int(%seq_len.46), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %30 : int = aten::Int(%seq_len.46), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %31 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
    %32 : int = aten::size(%input.139, %31), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
    %batch_size.45 : Long() = prim::NumToTensor(%32), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %34 : int = aten::Int(%batch_size.45), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %35 : int = aten::Int(%batch_size.45), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %36 : int = aten::Int(%batch_size.45), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %37 : int = aten::Int(%batch_size.45), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %38 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
    %39 : int = aten::size(%input.139, %38), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
    %embed_dim : Long() = prim::NumToTensor(%39), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %41 : int = aten::Int(%embed_dim), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %44 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:261:0
    %query_vectors : Float(512:13056, 17:768, 768:1) = aten::div_(%1387, %44), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:261:0
    %46 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:263:0
    %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:263:0
    %48 : int[] = prim::ListConstruct(%30, %37, %46, %47), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %49 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors, %48), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:263:0
    %50 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:263:0
    %51 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:263:0
    %query.23 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%49, %50, %51), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:263:0
    %53 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:264:0
    %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:264:0
    %55 : int[] = prim::ListConstruct(%29, %36, %53, %54), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %56 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1388, %55), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:264:0
    %57 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:264:0
    %58 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:264:0
    %key : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%56, %57, %58), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:264:0
    %60 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %61 : int = aten::size(%query.23, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.46 : Long() = prim::NumToTensor(%61), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %63 : int = aten::Int(%batch_size.46), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %64 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %65 : int = aten::size(%query.23, %64), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.47 : Long() = prim::NumToTensor(%65), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %67 : int = aten::Int(%seq_len.47), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %68 : int = aten::Int(%seq_len.47), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %69 : int = aten::Int(%seq_len.47), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %70 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %71 : int = aten::size(%query.23, %70), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.34 : Long() = prim::NumToTensor(%71), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %73 : int = aten::Int(%num_heads.34), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %74 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %75 : int = aten::size(%query.23, %74), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.34 : Long() = prim::NumToTensor(%75), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %77 : int = aten::Int(%head_dim.34), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %78 : int = aten::Int(%head_dim.34), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %111 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %112 : Long() = aten::floor_divide(%seq_len.47, %111), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %113 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:478:0
    %114 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.34 : Long() = aten::sub(%112, %113, %114), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:478:0
    %116 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
    %117 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
    %118 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.23, %116, %117), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
    %119 : Long() = aten::mul(%batch_size.46, %num_heads.34), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
    %120 : int = aten::Int(%119), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %121 : int[] = prim::ListConstruct(%120, %69, %78), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %hidden_states.123 : Float(204:64, 512:13056, 64:1) = aten::reshape(%118, %121), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
    %123 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
    %124 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
    %125 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key, %123, %124), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
    %126 : Long() = aten::mul(%batch_size.46, %num_heads.34), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
    %127 : int = aten::Int(%126), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %128 : int[] = prim::ListConstruct(%127, %68, %77), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %hidden_states.125 : Float(204:64, 512:13056, 64:1) = aten::reshape(%125, %128), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
    %130 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
    %131 : int = aten::size(%hidden_states.123, %130), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
    %132 : Long() = prim::NumToTensor(%131), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %133 : int = aten::Int(%132), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %134 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
    %135 : int = aten::size(%hidden_states.123, %134), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
    %136 : Long() = prim::NumToTensor(%135), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %137 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %138 : Long() = aten::floor_divide(%136, %137), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %139 : int = aten::Int(%138), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %140 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
    %141 : int = aten::size(%hidden_states.123, %140), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
    %142 : Long() = prim::NumToTensor(%141), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %143 : int = aten::Int(%142), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %144 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
    %145 : int[] = prim::ListConstruct(%133, %139, %144, %143), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %hidden_states.124 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.123, %145), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
    %147 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %148 : int = aten::size(%hidden_states.124, %147), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %149 : Long() = prim::NumToTensor(%148), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %150 : int = aten::Int(%149), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %151 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %152 : int = aten::size(%hidden_states.124, %151), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %153 : Long() = prim::NumToTensor(%152), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %154 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %155 : int = aten::size(%hidden_states.124, %154), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %156 : Long() = prim::NumToTensor(%155), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %157 : int = aten::Int(%156), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %158 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %159 : int = aten::size(%hidden_states.124, %158), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %160 : Long() = prim::NumToTensor(%159), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %161 : int = aten::Int(%160), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %162 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %163 : Long() = aten::mul(%153, %162), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %164 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %165 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %166 : Long() = aten::sub(%163, %164, %165), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %167 : int = aten::Int(%166), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %168 : int[] = prim::ListConstruct(%150, %167, %157, %161), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %169 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %170 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %171 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %172 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %173 : int[] = prim::ListConstruct(%169, %170, %171, %172), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %174 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %175 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.124, %168, %173, %174), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %176 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
    %177 : int = aten::size(%hidden_states.125, %176), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %179 : int = aten::Int(%178), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %180 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
    %181 : int = aten::size(%hidden_states.125, %180), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
    %182 : Long() = prim::NumToTensor(%181), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %183 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %184 : Long() = aten::floor_divide(%182, %183), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %185 : int = aten::Int(%184), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %186 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
    %187 : int = aten::size(%hidden_states.125, %186), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
    %188 : Long() = prim::NumToTensor(%187), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %189 : int = aten::Int(%188), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %190 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
    %191 : int[] = prim::ListConstruct(%179, %185, %190, %189), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %hidden_states.126 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.125, %191), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
    %193 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %194 : int = aten::size(%hidden_states.126, %193), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %195 : Long() = prim::NumToTensor(%194), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %196 : int = aten::Int(%195), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %197 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %198 : int = aten::size(%hidden_states.126, %197), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %199 : Long() = prim::NumToTensor(%198), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %200 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %201 : int = aten::size(%hidden_states.126, %200), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %203 : int = aten::Int(%202), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %204 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %205 : int = aten::size(%hidden_states.126, %204), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %207 : int = aten::Int(%206), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %208 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %209 : Long() = aten::mul(%199, %208), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %210 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %211 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %212 : Long() = aten::sub(%209, %210, %211), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %213 : int = aten::Int(%212), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %214 : int[] = prim::ListConstruct(%196, %213, %203, %207), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %215 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %216 : int = prim::Constant[value=3342336](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %217 : int = prim::Constant[value=13056](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %218 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %219 : int[] = prim::ListConstruct(%215, %216, %217, %218), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %220 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %221 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.126, %214, %219, %220), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %222 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/functional.py:327:0
    %223 : Tensor[] = prim::ListConstruct(%175, %221), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %input.140 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%222, %223), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/functional.py:327:0
    %225 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %226 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %227 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %228 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %229 : int[] = prim::ListConstruct(%225, %226, %227, %228), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %230 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded.23 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.140, %229, %230), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %232 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %233 : int = aten::size(%hidden_states_padded.23, %232), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %234 : Long() = prim::NumToTensor(%233), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %235 : int = aten::Int(%234), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %236 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %237 : int = aten::size(%hidden_states_padded.23, %236), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %238 : Long() = prim::NumToTensor(%237), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %239 : int = aten::Int(%238), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %246 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %247 : int = aten::size(%hidden_states_padded.23, %246), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %248 : Long() = prim::NumToTensor(%247), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %249 : int = aten::Int(%248), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %250 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %251 : int = aten::size(%hidden_states_padded.23, %250), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %252 : Long() = prim::NumToTensor(%251), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %253 : int = aten::Int(%252), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %254 : int[] = prim::ListConstruct(%235, %239, %249, %253), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %diagonal_chunked_attention_scores.23 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.23, %254), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:400:0
    %256 : Long() = aten::mul(%batch_size.46, %num_heads.34), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
    %257 : int = aten::Int(%256), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %258 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
    %259 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
    %260 : Long() = aten::add(%chunks_count.34, %258, %259), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
    %261 : int = aten::Int(%260), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %263 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %264 : int[] = prim::ListConstruct(%257, %261, %262, %263), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %265 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %266 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %268 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores.23 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.23, %264, %265, %266, %267, %268), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %270 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %271 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %273 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %270, %271, %272, %273), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %275 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %276 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %277 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %279 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%274, %275, %276, %277, %278), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %280 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %281 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %282 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %283 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %284 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%279, %280, %281, %282, %283), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %285 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %286 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %287 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %288 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %289 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%284, %285, %286, %287, %288), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %290 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %291 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %292 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %293 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %294 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %290, %291, %292, %293), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %295 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %296 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %297 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %298 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %299 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%294, %295, %296, %297, %298), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %300 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %301 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %302 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %303 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %304 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%299, %300, %301, %302, %303), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %305 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %306 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %307 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %308 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %309 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%304, %305, %306, %307, %308), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %310 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %311 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %312 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %313 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %314 : int[] = prim::ListConstruct(%310, %311, %312, %313), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %315 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%289, %314), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %316 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %317 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%309, %315, %316), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %319 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %320 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %321 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %322 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %318, %319, %320, %321), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %323 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %324 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %325 : Float(204:262656, 512:513, 513:1) = aten::select(%322, %323, %324), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %327 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %328 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %329 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %330 : Float(204:262656, 256:513, 513:1) = aten::slice(%325, %326, %327, %328, %329), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %331 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %333 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %335 : Float(204:262656, 256:513, 257:1) = aten::slice(%330, %331, %332, %333, %334), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %336 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %340 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %336, %337, %338, %339), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %341 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %342 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %343 : Float(204:262656, 256:513, 513:1) = aten::select(%340, %341, %342), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %345 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %347 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %348 : Float(204:262656, 256:513, 513:1) = aten::slice(%343, %344, %345, %346, %347), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %349 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %350 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %352 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %353 : Float(204:262656, 256:513, 257:1) = aten::slice(%348, %349, %350, %351, %352), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %354 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %355 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %356 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %357 : int[] = prim::ListConstruct(%354, %355, %356), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %358 : Float(204:262656, 256:513, 257:1) = aten::view(%335, %357), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %359 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %360 : Float(204:262656, 256:513, 257:1) = aten::copy_(%353, %358, %359), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %361 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %362 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %363 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %364 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %365 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %361, %362, %363, %364), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %366 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %367 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %368 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %369 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %370 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%365, %366, %367, %368, %369), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %371 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %372 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %373 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %374 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %375 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%370, %371, %372, %373, %374), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %376 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %377 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %379 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %380 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%375, %376, %377, %378, %379), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %381 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %383 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %384 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %385 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %381, %382, %383, %384), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %386 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %387 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %388 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %389 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %390 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%385, %386, %387, %388, %389), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %391 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %392 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %393 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %394 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %395 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%390, %391, %392, %393, %394), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %396 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %397 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %398 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %399 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %400 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%395, %396, %397, %398, %399), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %401 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %402 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %403 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %404 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %405 : int[] = prim::ListConstruct(%401, %402, %403, %404), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %406 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%380, %405), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %407 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %408 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%400, %406, %407), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %409 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %410 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %411 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %412 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %413 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %409, %410, %411, %412), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %414 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %415 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %416 : Float(204:262656, 512:513, 513:1) = aten::select(%413, %414, %415), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %417 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %418 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %419 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %420 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %421 : Float(204:262656, 255:513, 513:1) = aten::slice(%416, %417, %418, %419, %420), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %422 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %423 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %425 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %426 : Float(204:262656, 255:513, 255:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %427 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %428 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %429 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %430 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %431 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %427, %428, %429, %430), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %432 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %433 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %434 : Float(204:262656, 256:513, 513:1) = aten::select(%431, %432, %433), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %435 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %436 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %437 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %438 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %439 : Float(204:262656, 255:513, 513:1) = aten::slice(%434, %435, %436, %437, %438), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %440 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %441 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %442 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %443 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %444 : Float(204:262656, 255:513, 255:1) = aten::slice(%439, %440, %441, %442, %443), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %445 : int = prim::Constant[value=204](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %446 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %447 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %448 : int[] = prim::ListConstruct(%445, %446, %447), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %449 : Float(204:262656, 255:513, 255:1) = aten::view(%426, %448), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %450 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %451 : Float(204:262656, 255:513, 255:1) = aten::copy_(%444, %449, %450), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %452 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
    %453 : int[] = prim::ListConstruct(%63, %73, %67, %452), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %454 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.23, %453), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
    %455 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
    %456 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.34 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%454, %455, %456), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
    %458 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %459 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %460 : int[] = prim::ListConstruct(%458, %459), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %461 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %462 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %463 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %464 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %465 : Float(256:257, 257:1) = aten::ones(%460, %461, %462, %463, %464), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %466 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %467 : Float(256:257, 257:1) = aten::tril(%465, %466), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %468 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %469 : int[] = prim::ListConstruct(%468), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %beginning_mask_2d.23 : Float(256:257, 257:1) = aten::flip(%467, %469), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %471 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %472 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.23, %471), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %473 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %474 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %475 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %476 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %477 : Float(1:65792, 256:257, 257:1) = aten::slice(%472, %473, %474, %475, %476), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %478 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %479 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%477, %478), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %480 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %481 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %482 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %483 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask.23 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%479, %480, %481, %482, %483), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %485 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:460:0
    %486 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:460:0
    %487 : int[] = prim::ListConstruct(%485, %486), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %ending_mask.23 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.23, %487), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:460:0
    %489 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %490 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %491 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %492 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %493 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.34, %489, %490, %491, %492), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %494 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %495 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %496 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %497 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %498 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%493, %494, %495, %496, %497), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %499 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %500 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %501 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %502 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %503 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%498, %499, %500, %501, %502), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %504 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %505 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %506 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %507 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input.23 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%503, %504, %505, %506, %507), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %509 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %510 : int = aten::size(%beginning_input.23, %509), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %511 : Long() = prim::NumToTensor(%510), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %512 : int = aten::Int(%511), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %513 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %514 : int = aten::size(%beginning_input.23, %513), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %515 : Long() = prim::NumToTensor(%514), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %516 : int = aten::Int(%515), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %517 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %518 : int = aten::size(%beginning_input.23, %517), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %519 : Long() = prim::NumToTensor(%518), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %520 : int = aten::Int(%519), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %521 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %522 : int = aten::size(%beginning_input.23, %521), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %523 : Long() = prim::NumToTensor(%522), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %524 : int = aten::Int(%523), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %525 : int[] = prim::ListConstruct(%512, %516, %520, %524), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %526 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %527 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.23, %525, %526), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %528 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:22:0
    %529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%527, %528), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:22:0
    %530 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:463:0
    %531 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.23, %529, %530), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:463:0
    %532 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %533 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %534 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %535 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %536 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.34, %532, %533, %534, %535), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %537 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %538 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %539 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %540 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %541 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%536, %537, %538, %539, %540), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %542 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %543 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %544 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %545 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %546 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%541, %542, %543, %544, %545), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %547 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %548 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %549 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %550 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input.23 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%546, %547, %548, %549, %550), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %552 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %553 : int = aten::size(%ending_input.23, %552), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %554 : Long() = prim::NumToTensor(%553), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %555 : int = aten::Int(%554), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %556 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %557 : int = aten::size(%ending_input.23, %556), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %558 : Long() = prim::NumToTensor(%557), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %559 : int = aten::Int(%558), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %560 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %561 : int = aten::size(%ending_input.23, %560), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %562 : Long() = prim::NumToTensor(%561), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %563 : int = aten::Int(%562), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %564 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %565 : int = aten::size(%ending_input.23, %564), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %566 : Long() = prim::NumToTensor(%565), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %567 : int = aten::Int(%566), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %568 : int[] = prim::ListConstruct(%555, %559, %563, %567), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %569 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %570 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.23, %568, %569), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %571 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:22:0
    %572 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%570, %571), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:22:0
    %573 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:466:0
    %574 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.23, %572, %573), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:466:0
    %575 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:22:0
    %576 : Bool(17:512, 512:1) = aten::ne(%9, %575), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:22:0
    %577 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %578 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %579 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %580 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %581 : Bool(17:512, 512:1) = aten::slice(%576, %577, %578, %579, %580), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %582 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %583 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %584 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %585 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %586 : Bool(17:512, 512:1) = aten::slice(%581, %582, %583, %584, %585), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %587 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %588 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%586, %587), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %589 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %remove_from_windowed_attention_mask : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%588, %589), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
    %591 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask, %query.23), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:275:0
    %592 : float = prim::Constant[value=-10000.](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:275:0
    %float_mask : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%591, %remove_from_windowed_attention_mask, %592), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:275:0
    %594 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %595 : int = aten::size(%float_mask, %594), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %596 : Long() = prim::NumToTensor(%595), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %597 : int = aten::Int(%596), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %598 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %599 : int = aten::size(%float_mask, %598), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %600 : Long() = prim::NumToTensor(%599), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %601 : int = aten::Int(%600), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %602 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %603 : int = aten::size(%float_mask, %602), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %604 : Long() = prim::NumToTensor(%603), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %605 : int = aten::Int(%604), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %606 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %607 : int = aten::size(%float_mask, %606), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %608 : Long() = prim::NumToTensor(%607), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %609 : int = aten::Int(%608), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %610 : int[] = prim::ListConstruct(%597, %601, %605, %609), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %611 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %612 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %613 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %614 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %query : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%610, %611, %612, %613, %614), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
    %616 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %617 : int = aten::size(%query, %616), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %batch_size.47 : Long() = prim::NumToTensor(%617), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %619 : int = aten::Int(%batch_size.47), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %620 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %621 : int = aten::size(%query, %620), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %seq_len.48 : Long() = prim::NumToTensor(%621), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %623 : int = aten::Int(%seq_len.48), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %624 : int = aten::Int(%seq_len.48), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %625 : int = aten::Int(%seq_len.48), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %626 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %627 : int = aten::size(%query, %626), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %num_heads.35 : Long() = prim::NumToTensor(%627), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %629 : int = aten::Int(%num_heads.35), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %630 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %631 : int = aten::size(%query, %630), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
    %head_dim.35 : Long() = prim::NumToTensor(%631), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %633 : int = aten::Int(%head_dim.35), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %634 : int = aten::Int(%head_dim.35), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %667 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %668 : Long() = aten::floor_divide(%seq_len.48, %667), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %669 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:478:0
    %670 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:478:0
    %chunks_count.35 : Long() = aten::sub(%668, %669, %670), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:478:0
    %672 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
    %673 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
    %674 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query, %672, %673), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
    %675 : Long() = aten::mul(%batch_size.47, %num_heads.35), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
    %676 : int = aten::Int(%675), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %677 : int[] = prim::ListConstruct(%676, %625, %634), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %hidden_states.127 : Float(17:512, 512:1, 1:1) = aten::reshape(%674, %677), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
    %679 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
    %680 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
    %681 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask, %679, %680), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
    %682 : Long() = aten::mul(%batch_size.47, %num_heads.35), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
    %683 : int = aten::Int(%682), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %684 : int[] = prim::ListConstruct(%683, %624, %633), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %hidden_states.129 : Float(17:512, 512:1, 1:1) = aten::reshape(%681, %684), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
    %686 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
    %687 : int = aten::size(%hidden_states.127, %686), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
    %688 : Long() = prim::NumToTensor(%687), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %689 : int = aten::Int(%688), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %690 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
    %691 : int = aten::size(%hidden_states.127, %690), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
    %692 : Long() = prim::NumToTensor(%691), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %693 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %694 : Long() = aten::floor_divide(%692, %693), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %695 : int = aten::Int(%694), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %696 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
    %697 : int = aten::size(%hidden_states.127, %696), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
    %698 : Long() = prim::NumToTensor(%697), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %699 : int = aten::Int(%698), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %700 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
    %701 : int[] = prim::ListConstruct(%689, %695, %700, %699), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %hidden_states.128 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.127, %701), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
    %703 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %704 : int = aten::size(%hidden_states.128, %703), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %705 : Long() = prim::NumToTensor(%704), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %706 : int = aten::Int(%705), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %707 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %708 : int = aten::size(%hidden_states.128, %707), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %709 : Long() = prim::NumToTensor(%708), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %710 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %711 : int = aten::size(%hidden_states.128, %710), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %712 : Long() = prim::NumToTensor(%711), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %713 : int = aten::Int(%712), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %714 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %715 : int = aten::size(%hidden_states.128, %714), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %716 : Long() = prim::NumToTensor(%715), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %717 : int = aten::Int(%716), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %718 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %719 : Long() = aten::mul(%709, %718), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %720 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %721 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %722 : Long() = aten::sub(%719, %720, %721), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %723 : int = aten::Int(%722), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %724 : int[] = prim::ListConstruct(%706, %723, %713, %717), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %725 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %726 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %727 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %728 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %729 : int[] = prim::ListConstruct(%725, %726, %727, %728), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %730 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %731 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.128, %724, %729, %730), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %732 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
    %733 : int = aten::size(%hidden_states.129, %732), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
    %734 : Long() = prim::NumToTensor(%733), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %735 : int = aten::Int(%734), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %736 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
    %737 : int = aten::size(%hidden_states.129, %736), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
    %738 : Long() = prim::NumToTensor(%737), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %739 : Long() = prim::Constant[value={512}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %740 : Long() = aten::floor_divide(%738, %739), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %741 : int = aten::Int(%740), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %742 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
    %743 : int = aten::size(%hidden_states.129, %742), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
    %744 : Long() = prim::NumToTensor(%743), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %745 : int = aten::Int(%744), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %746 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
    %747 : int[] = prim::ListConstruct(%735, %741, %746, %745), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %hidden_states.130 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.129, %747), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
    %749 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %750 : int = aten::size(%hidden_states.130, %749), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %751 : Long() = prim::NumToTensor(%750), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %752 : int = aten::Int(%751), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %753 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %754 : int = aten::size(%hidden_states.130, %753), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %755 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %756 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %757 : int = aten::size(%hidden_states.130, %756), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %758 : Long() = prim::NumToTensor(%757), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %759 : int = aten::Int(%758), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %760 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %761 : int = aten::size(%hidden_states.130, %760), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
    %762 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %764 : Long() = prim::Constant[value={2}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %765 : Long() = aten::mul(%755, %764), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %766 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %767 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %768 : Long() = aten::sub(%765, %766, %767), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
    %769 : int = aten::Int(%768), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %770 : int[] = prim::ListConstruct(%752, %769, %759, %763), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %771 : int = prim::Constant[value=512](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %772 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %773 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %774 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %776 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %777 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.130, %770, %775, %776), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
    %778 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/functional.py:327:0
    %779 : Tensor[] = prim::ListConstruct(%731, %777), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %input.141 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%778, %779), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/functional.py:327:0
    %781 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %782 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %783 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %784 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %785 : int[] = prim::ListConstruct(%781, %782, %783, %784), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %786 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %hidden_states_padded : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.141, %785, %786), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %788 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %789 : int = aten::size(%hidden_states_padded, %788), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %790 : Long() = prim::NumToTensor(%789), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %791 : int = aten::Int(%790), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %792 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %793 : int = aten::size(%hidden_states_padded, %792), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %794 : Long() = prim::NumToTensor(%793), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %795 : int = aten::Int(%794), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %802 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %803 : int = aten::size(%hidden_states_padded, %802), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %804 : Long() = prim::NumToTensor(%803), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %805 : int = aten::Int(%804), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %806 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %807 : int = aten::size(%hidden_states_padded, %806), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
    %808 : Long() = prim::NumToTensor(%807), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %809 : int = aten::Int(%808), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %810 : int[] = prim::ListConstruct(%791, %795, %805, %809), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %diagonal_chunked_attention_scores : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded, %810), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:400:0
    %812 : Long() = aten::mul(%batch_size.47, %num_heads.35), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
    %813 : int = aten::Int(%812), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %814 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
    %815 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
    %816 : Long() = aten::add(%chunks_count.35, %814, %815), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
    %817 : int = aten::Int(%816), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %818 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %819 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %820 : int[] = prim::ListConstruct(%813, %817, %818, %819), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %821 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %822 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %823 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %824 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %diagonal_attention_scores : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores, %820, %821, %822, %823, %824), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
    %826 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %827 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %828 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %829 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %830 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %826, %827, %828, %829), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %831 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %832 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %833 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %834 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %835 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%830, %831, %832, %833, %834), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %836 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %837 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %838 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %839 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%835, %836, %837, %838, %839), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %841 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %842 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %843 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %844 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %845 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%840, %841, %842, %843, %844), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %846 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %847 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %848 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %849 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %850 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %846, %847, %848, %849), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %851 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %852 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %853 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %854 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %855 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%850, %851, %852, %853, %854), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %856 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %857 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %858 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %859 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %860 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%855, %856, %857, %858, %859), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %861 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %862 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %863 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %864 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %865 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%860, %861, %862, %863, %864), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %866 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %867 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %868 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %869 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %870 : int[] = prim::ListConstruct(%866, %867, %868, %869), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %871 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%845, %870), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %872 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %873 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%865, %871, %872), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
    %874 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %875 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %876 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %877 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %874, %875, %876, %877), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %879 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %880 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %881 : Float(17:262656, 512:513, 513:1) = aten::select(%878, %879, %880), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %882 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %883 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %884 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %885 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %886 : Float(17:262656, 256:513, 513:1) = aten::slice(%881, %882, %883, %884, %885), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %887 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %888 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %889 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %890 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%886, %887, %888, %889, %890), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %892 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %893 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %894 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %895 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %896 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %892, %893, %894, %895), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %897 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %898 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %899 : Float(17:262656, 256:513, 513:1) = aten::select(%896, %897, %898), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %900 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %901 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %902 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %903 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %904 : Float(17:262656, 256:513, 513:1) = aten::slice(%899, %900, %901, %902, %903), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %905 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %906 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %907 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %908 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %909 : Float(17:262656, 256:513, 257:1) = aten::slice(%904, %905, %906, %907, %908), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %910 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %911 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %912 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %913 : int[] = prim::ListConstruct(%910, %911, %912), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %914 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %913), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %915 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %916 : Float(17:262656, 256:513, 257:1) = aten::copy_(%909, %914, %915), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
    %917 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %918 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %919 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %920 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %921 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %917, %918, %919, %920), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %922 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %923 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %924 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %925 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %926 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%921, %922, %923, %924, %925), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %927 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %928 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %929 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %930 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %931 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%926, %927, %928, %929, %930), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %932 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %933 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %934 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %935 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %936 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%931, %932, %933, %934, %935), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %937 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %938 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %939 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %940 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %941 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %937, %938, %939, %940), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %942 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %943 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %944 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %945 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %946 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%941, %942, %943, %944, %945), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %947 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %948 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %949 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %950 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %951 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%946, %947, %948, %949, %950), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %952 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %953 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %954 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %955 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %956 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%951, %952, %953, %954, %955), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %957 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %958 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %959 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %960 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %962 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%936, %961), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %963 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %964 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%956, %962, %963), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
    %965 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %966 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %967 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %968 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %969 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %965, %966, %967, %968), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %970 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %971 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %972 : Float(17:262656, 512:513, 513:1) = aten::select(%969, %970, %971), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %973 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %974 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %975 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %976 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %977 : Float(17:262656, 255:513, 513:1) = aten::slice(%972, %973, %974, %975, %976), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %978 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %979 : int = prim::Constant[value=-255](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %980 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %981 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %982 : Float(17:262656, 255:513, 255:1) = aten::slice(%977, %978, %979, %980, %981), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %983 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %984 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %985 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %986 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %987 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %983, %984, %985, %986), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %988 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %989 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %990 : Float(17:262656, 256:513, 513:1) = aten::select(%987, %988, %989), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %991 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %992 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %993 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %994 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %995 : Float(17:262656, 255:513, 513:1) = aten::slice(%990, %991, %992, %993, %994), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %996 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %997 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %998 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %999 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %1000 : Float(17:262656, 255:513, 255:1) = aten::slice(%995, %996, %997, %998, %999), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %1001 : int = prim::Constant[value=17](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %1002 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %1003 : int = prim::Constant[value=255](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %1004 : int[] = prim::ListConstruct(%1001, %1002, %1003), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1005 : Float(17:262656, 255:513, 255:1) = aten::view(%982, %1004), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %1006 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1007 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1000, %1005, %1006), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
    %1008 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
    %1009 : int[] = prim::ListConstruct(%619, %629, %623, %1008), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1010 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores, %1009), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
    %1011 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
    %1012 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
    %input_tensor.35 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1010, %1011, %1012), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
    %1014 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %1015 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %1016 : int[] = prim::ListConstruct(%1014, %1015), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1017 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %1018 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %1019 : Device = prim::Constant[value="cpu"](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %1020 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %1021 : Float(256:257, 257:1) = aten::ones(%1016, %1017, %1018, %1019, %1020), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %1022 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %1023 : Float(256:257, 257:1) = aten::tril(%1021, %1022), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %1024 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %1025 : int[] = prim::ListConstruct(%1024), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %beginning_mask_2d : Float(256:257, 257:1) = aten::flip(%1023, %1025), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
    %1027 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1028 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d, %1027), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1029 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1030 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1031 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1032 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1033 : Float(1:65792, 256:257, 257:1) = aten::slice(%1028, %1029, %1030, %1031, %1032), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1034 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1035 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1033, %1034), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1036 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1037 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1038 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1039 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %beginning_mask : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1035, %1036, %1037, %1038, %1039), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
    %1041 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:460:0
    %1042 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:460:0
    %1043 : int[] = prim::ListConstruct(%1041, %1042), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %ending_mask : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask, %1043), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:460:0
    %1045 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1046 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1047 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1048 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1049 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.35, %1045, %1046, %1047, %1048), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1050 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1051 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1052 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1053 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1054 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1049, %1050, %1051, %1052, %1053), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1055 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1056 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1057 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1058 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1059 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1054, %1055, %1056, %1057, %1058), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1060 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1061 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1062 : int = prim::Constant[value=257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1063 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %beginning_input : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1059, %1060, %1061, %1062, %1063), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
    %1065 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %1066 : int = aten::size(%beginning_input, %1065), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %1067 : Long() = prim::NumToTensor(%1066), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1068 : int = aten::Int(%1067), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1069 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %1070 : int = aten::size(%beginning_input, %1069), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %1071 : Long() = prim::NumToTensor(%1070), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1072 : int = aten::Int(%1071), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1073 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %1074 : int = aten::size(%beginning_input, %1073), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %1075 : Long() = prim::NumToTensor(%1074), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1076 : int = aten::Int(%1075), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1077 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %1078 : int = aten::size(%beginning_input, %1077), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %1079 : Long() = prim::NumToTensor(%1078), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1080 : int = aten::Int(%1079), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1081 : int[] = prim::ListConstruct(%1068, %1072, %1076, %1080), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1082 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %1083 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask, %1081, %1082), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
    %1084 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:22:0
    %1085 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1083, %1084), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:22:0
    %1086 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:463:0
    %1087 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input, %1085, %1086), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:463:0
    %1088 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1089 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1090 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1091 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1092 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.35, %1088, %1089, %1090, %1091), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1093 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1094 : int = prim::Constant[value=-256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1095 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1096 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1097 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1092, %1093, %1094, %1095, %1096), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1098 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1099 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1100 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1101 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1102 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1097, %1098, %1099, %1100, %1101), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1103 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1104 : int = prim::Constant[value=-257](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1105 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1106 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %ending_input : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1102, %1103, %1104, %1105, %1106), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
    %1108 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %1109 : int = aten::size(%ending_input, %1108), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %1110 : Long() = prim::NumToTensor(%1109), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1111 : int = aten::Int(%1110), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1112 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %1113 : int = aten::size(%ending_input, %1112), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %1114 : Long() = prim::NumToTensor(%1113), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1115 : int = aten::Int(%1114), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1116 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %1117 : int = aten::size(%ending_input, %1116), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %1118 : Long() = prim::NumToTensor(%1117), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1119 : int = aten::Int(%1118), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1120 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %1121 : int = aten::size(%ending_input, %1120), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %1122 : Long() = prim::NumToTensor(%1121), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1124 : int[] = prim::ListConstruct(%1111, %1115, %1119, %1123), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1125 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %1126 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask, %1124, %1125), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
    %1127 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:22:0
    %1128 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1126, %1127), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:22:0
    %1129 : float = prim::Constant[value=-inf](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:466:0
    %1130 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input, %1128, %1129), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:466:0
    %1131 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:284:0
    %attn_scores : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.34, %input_tensor.35, %1131), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:284:0
    %1151 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:1500:0
    %1152 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:1500:0
    %attn_probs_fp32 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores, %1151, %1152), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:1500:0
    %attn_probs.23 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::type_as(%attn_probs_fp32, %attn_scores), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:320:0
    %1155 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1156 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1157 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1158 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1159 : Bool(17:512, 512:1) = aten::slice(%is_index_masked, %1155, %1156, %1157, %1158), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1160 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1161 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1162 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1163 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1164 : Bool(17:512, 512:1) = aten::slice(%1159, %1160, %1161, %1162, %1163), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1165 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1166 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1164, %1165), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1167 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1168 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1166, %1167), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1169 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %input.142 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs.23, %1168, %1169), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
    %1171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:973:0
    %1172 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:973:0
    %attn_probs : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.142, %1171, %1172), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:973:0
    %1174 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:331:0
    %1175 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:331:0
    %1176 : int[] = prim::ListConstruct(%28, %35, %1174, %1175), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1177 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%1389, %1176), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:331:0
    %1178 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:331:0
    %1179 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:331:0
    %value : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1177, %1178, %1179), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:331:0
    %1181 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
    %1182 : int = aten::size(%value, %1181), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
    %batch_size : Long() = prim::NumToTensor(%1182), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1184 : int = aten::Int(%batch_size), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1185 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
    %1186 : int = aten::size(%value, %1185), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
    %seq_len : Long() = prim::NumToTensor(%1186), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1188 : int = aten::Int(%seq_len), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1189 : int = aten::Int(%seq_len), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1190 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
    %1191 : int = aten::size(%value, %1190), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
    %num_heads : Long() = prim::NumToTensor(%1191), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1193 : int = aten::Int(%num_heads), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1194 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
    %1195 : int = aten::size(%value, %1194), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
    %head_dim : Long() = prim::NumToTensor(%1195), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1197 : int = aten::Int(%head_dim), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1198 : int = aten::Int(%head_dim), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1199 : int = aten::Int(%head_dim), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1236 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %1237 : Long() = aten::floor_divide(%seq_len, %1236), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %1238 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:542:0
    %1239 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:542:0
    %chunks_count : Long() = aten::sub(%1237, %1238, %1239), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:542:0
    %1241 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:545:0
    %1242 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:545:0
    %1243 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs, %1241, %1242), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:545:0
    %1244 : Long() = aten::mul(%batch_size, %num_heads), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:546:0
    %1245 : int = aten::Int(%1244), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1246 : Long() = prim::Constant[value={256}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %1247 : Long() = aten::floor_divide(%seq_len, %1246), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/tensor.py:424:0
    %1248 : int = aten::Int(%1247), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1249 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:545:0
    %1250 : int = prim::Constant[value=513](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:545:0
    %1251 : int[] = prim::ListConstruct(%1245, %1248, %1249, %1250), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %chunked_hidden_states.56 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1243, %1251), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:545:0
    %1253 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
    %1254 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
    %1255 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value, %1253, %1254), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
    %1256 : Long() = aten::mul(%batch_size, %num_heads), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
    %1257 : int = aten::Int(%1256), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1258 : int[] = prim::ListConstruct(%1257, %1189, %1199), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %input.143 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1255, %1258), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
    %1260 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %1261 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %1262 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %1263 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %1264 : int[] = prim::ListConstruct(%1260, %1261, %1262, %1263), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1265 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %padded_value : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.143, %1264, %1265), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %1267 : Long() = aten::mul(%batch_size, %num_heads), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:556:0
    %1268 : int = aten::Int(%1267), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1269 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:556:0
    %1270 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:556:0
    %1271 : Long() = aten::add(%chunks_count, %1269, %1270), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:556:0
    %1272 : int = aten::Int(%1271), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1273 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:564:0
    %1274 : int[] = prim::ListConstruct(%1268, %1272, %1273, %1198), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1275 : int = prim::Constant[value=65536](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:564:0
    %1276 : int = prim::Constant[value=16384](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:564:0
    %1277 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:564:0
    %1278 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:564:0
    %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1280 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1281 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value, %1274, %1279, %1280), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:564:0
    %1282 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
    %1283 : int = aten::size(%chunked_hidden_states.56, %1282), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
    %total_num_heads : Long() = prim::NumToTensor(%1283), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1285 : int = aten::Int(%total_num_heads), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1286 : int = aten::Int(%total_num_heads), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1287 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
    %1288 : int = aten::size(%chunked_hidden_states.56, %1287), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
    %num_chunks : Long() = prim::NumToTensor(%1288), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1290 : int = aten::Int(%num_chunks), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1291 : int = aten::Int(%num_chunks), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1292 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
    %1293 : int = aten::size(%chunked_hidden_states.56, %1292), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
    %window_overlap : Long() = prim::NumToTensor(%1293), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1295 : int = aten::Int(%window_overlap), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1296 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
    %1297 : int = aten::size(%chunked_hidden_states.56, %1296), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
    %hidden_dim : Long() = prim::NumToTensor(%1297), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1299 : Long() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:422:0
    %1300 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:422:0
    %1301 : Long() = aten::add(%window_overlap, %1299, %1300), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:422:0
    %1302 : int = aten::Int(%1301), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1303 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %1304 : int[] = prim::ListConstruct(%1303, %1302), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1305 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %chunked_hidden_states.57 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.56, %1304, %1305), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
    %1307 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:424:0
    %1308 : int[] = prim::ListConstruct(%1286, %1291, %1307), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %chunked_hidden_states.58 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.57, %1308), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:424:0
    %1310 : Long() = aten::neg(%window_overlap), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:428:0
    %1311 : int = aten::Int(%1310), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1312 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1313 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1314 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1315 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1316 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.58, %1312, %1313, %1314, %1315), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1317 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1318 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1319 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1320 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1316, %1317, %1318, %1319, %1320), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1322 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1323 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1324 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %chunked_hidden_states.59 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1321, %1322, %1323, %1311, %1324), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
    %1326 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:431:0
    %1327 : Long() = aten::add(%window_overlap, %hidden_dim, %1326), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:431:0
    %1328 : int = aten::Int(%1327), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1329 : int[] = prim::ListConstruct(%1285, %1290, %1295, %1328), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %chunked_hidden_states : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.59, %1329), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:430:0
    %1331 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1332 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1333 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1334 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1335 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states, %1331, %1332, %1333, %1334), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1336 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1337 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1338 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1339 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1340 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1335, %1336, %1337, %1338, %1339), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1341 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1342 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1343 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1344 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1345 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1340, %1341, %1342, %1343, %1344), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1346 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1347 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1348 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1349 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1350 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1345, %1346, %1347, %1348, %1349), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
    %1351 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/functional.py:327:0
    %1352 : Tensor[] = prim::ListConstruct(%1350, %1281), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %context : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%1351, %1352), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/functional.py:327:0
    %1354 : int[] = prim::ListConstruct(%1184, %1193, %1188, %1197), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1355 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context, %1354), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:569:0
    %1356 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:569:0
    %1357 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:569:0
    %attn_output.23 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1355, %1356, %1357), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:569:0
    %1377 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
    %1378 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
    %1379 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.23, %1377, %1378), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
    %1380 : int[] = prim::ListConstruct(%27, %34, %41), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
    %1381 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1379, %1380), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
    %1382 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
    %attn_output : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1381, %1382), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
    %1384 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:374:0
    %1385 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:374:0
    %input.144 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output, %1384, %1385), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_longformer.py:374:0
    return (%input.144)

LongformerSelfAttention.key
Linear._actual_script_module
  graph(%self.189 : __torch__.torch.nn.modules.linear.Linear,
        %input.139 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.189)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.189)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
    %output.68 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.139, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
    %key_vectors : Float(512:13056, 17:768, 768:1) = aten::add_(%output.68, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
    return (%key_vectors)

LongformerSelfAttention.query
Linear._actual_script_module
  graph(%self.188 : __torch__.torch.nn.modules.linear.Linear,
        %input.139 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.188)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.188)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
    %output.67 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.139, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
    %query_vectors.23 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.67, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
    return (%query_vectors.23)

LongformerSelfAttention.value
Linear._actual_script_module
  graph(%self.190 : __torch__.torch.nn.modules.linear.Linear,
        %input.139 : Float(512:768, 17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.190)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.190)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
    %output.69 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.139, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
    %value_vectors : Float(512:13056, 17:768, 768:1) = aten::add_(%output.69, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
    return (%value_vectors)

LongformerSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.194 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.146 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.194)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.194)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.146, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor)

LongformerSelfOutput.dense
Linear._actual_script_module
  graph(%self.192 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:768, 512:13056, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.192)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.192)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
    %output.70 : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
    %input.145 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.70, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.145)

LongformerSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.193 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.131 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.131)

LongformerIntermediate.dense
Linear._actual_script_module
  graph(%self.196 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.196)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.196)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
    %output.71 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
    %input.147 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.71, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.147)

LongformerOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.200 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.150 : Float(17:393216, 512:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.200)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.200)
    %4 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
    %hidden_states : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.150, %5, %3, %2, %6, %7), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%hidden_states)

LongformerOutput.dense
Linear._actual_script_module
  graph(%self.198 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1572864, 512:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.198)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.198)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
    %output : Float(17:393216, 512:768, 768:1) = aten::matmul(%1, %4), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
    %input.149 : Float(17:393216, 512:768, 768:1) = aten::add_(%output, %2, %6), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
    return (%input.149)

LongformerOutput.dropout
Dropout._actual_script_module
  graph(%self.199 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:393216, 512:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.132 : Float(17:393216, 512:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.132)

LongformerPooler.activation
Tanh._actual_script_module
  graph(%self : __torch__.torch.nn.modules.activation.Tanh,
        %1 : Float(17:768, 768:1)):
    %2 : Float(17:768, 768:1) = aten::tanh(%1), scope: __module.pooler/__module.pooler.activation # torch/nn/modules/activation.py:350:0
    return (%2)

LongformerPooler.dense
Linear._actual_script_module
  graph(%self.202 : __torch__.torch.nn.modules.linear.Linear,
        %input.151 : Float(17:393216, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.202)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.202)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.pooler/__module.pooler.dense # torch/nn/functional.py:1674:0
    %5 : int = prim::Constant[value=1](), scope: __module.pooler/__module.pooler.dense # torch/nn/functional.py:1674:0
    %6 : int = prim::Constant[value=1](), scope: __module.pooler/__module.pooler.dense # torch/nn/functional.py:1674:0
    %input : Float(17:768, 768:1) = aten::addmm(%2, %input.151, %4, %5, %6), scope: __module.pooler/__module.pooler.dense # torch/nn/functional.py:1674:0
    return (%input)

