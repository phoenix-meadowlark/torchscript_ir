graph(%self.1 : __torch__.transformers.modeling_xlm.XLMForTokenClassification,
      %input_ids : Long(17:13, 13:1),
      %padding_mask : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_36553.Linear = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.torch.nn.modules.dropout.___torch_mangle_36552.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %5 : __torch__.transformers.modeling_xlm.___torch_mangle_36551.XLMModel = prim::GetAttr[name="transformer"](%self.1)
  %10 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %11 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %12 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
  %13 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %14 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
  %15 : None = prim::Constant(), scope: __module.transformer
  %16 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlm.py:580:0
  %17 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
  %18 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %19 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %20 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %21 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
  %22 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %23 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %24 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlm.py:546:0
  %25 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlm.py:546:0
  %26 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlm.py:517:0
  %27 : __torch__.torch.nn.modules.container.___torch_mangle_36550.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %28 : __torch__.torch.nn.modules.normalization.___torch_mangle_36549.LayerNorm = prim::GetAttr[name="11"](%27)
  %29 : __torch__.torch.nn.modules.container.___torch_mangle_36537.ModuleList = prim::GetAttr[name="ffns"](%5)
  %30 : __torch__.transformers.modeling_xlm.___torch_mangle_36536.TransformerFFN = prim::GetAttr[name="11"](%29)
  %31 : __torch__.torch.nn.modules.container.___torch_mangle_36500.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %32 : __torch__.torch.nn.modules.normalization.___torch_mangle_36499.LayerNorm = prim::GetAttr[name="11"](%31)
  %33 : __torch__.torch.nn.modules.container.___torch_mangle_36487.ModuleList = prim::GetAttr[name="attentions"](%5)
  %34 : __torch__.transformers.modeling_xlm.___torch_mangle_36486.MultiHeadAttention = prim::GetAttr[name="11"](%33)
  %35 : __torch__.torch.nn.modules.container.___torch_mangle_36550.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %36 : __torch__.torch.nn.modules.normalization.___torch_mangle_36548.LayerNorm = prim::GetAttr[name="10"](%35)
  %37 : __torch__.torch.nn.modules.container.___torch_mangle_36537.ModuleList = prim::GetAttr[name="ffns"](%5)
  %38 : __torch__.transformers.modeling_xlm.___torch_mangle_36533.TransformerFFN = prim::GetAttr[name="10"](%37)
  %39 : __torch__.torch.nn.modules.container.___torch_mangle_36500.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %40 : __torch__.torch.nn.modules.normalization.___torch_mangle_36498.LayerNorm = prim::GetAttr[name="10"](%39)
  %41 : __torch__.torch.nn.modules.container.___torch_mangle_36487.ModuleList = prim::GetAttr[name="attentions"](%5)
  %42 : __torch__.transformers.modeling_xlm.___torch_mangle_36481.MultiHeadAttention = prim::GetAttr[name="10"](%41)
  %43 : __torch__.torch.nn.modules.container.___torch_mangle_36550.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %44 : __torch__.torch.nn.modules.normalization.___torch_mangle_36547.LayerNorm = prim::GetAttr[name="9"](%43)
  %45 : __torch__.torch.nn.modules.container.___torch_mangle_36537.ModuleList = prim::GetAttr[name="ffns"](%5)
  %46 : __torch__.transformers.modeling_xlm.___torch_mangle_36530.TransformerFFN = prim::GetAttr[name="9"](%45)
  %47 : __torch__.torch.nn.modules.container.___torch_mangle_36500.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %48 : __torch__.torch.nn.modules.normalization.___torch_mangle_36497.LayerNorm = prim::GetAttr[name="9"](%47)
  %49 : __torch__.torch.nn.modules.container.___torch_mangle_36487.ModuleList = prim::GetAttr[name="attentions"](%5)
  %50 : __torch__.transformers.modeling_xlm.___torch_mangle_36476.MultiHeadAttention = prim::GetAttr[name="9"](%49)
  %51 : __torch__.torch.nn.modules.container.___torch_mangle_36550.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %52 : __torch__.torch.nn.modules.normalization.___torch_mangle_36546.LayerNorm = prim::GetAttr[name="8"](%51)
  %53 : __torch__.torch.nn.modules.container.___torch_mangle_36537.ModuleList = prim::GetAttr[name="ffns"](%5)
  %54 : __torch__.transformers.modeling_xlm.___torch_mangle_36527.TransformerFFN = prim::GetAttr[name="8"](%53)
  %55 : __torch__.torch.nn.modules.container.___torch_mangle_36500.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %56 : __torch__.torch.nn.modules.normalization.___torch_mangle_36496.LayerNorm = prim::GetAttr[name="8"](%55)
  %57 : __torch__.torch.nn.modules.container.___torch_mangle_36487.ModuleList = prim::GetAttr[name="attentions"](%5)
  %58 : __torch__.transformers.modeling_xlm.___torch_mangle_36471.MultiHeadAttention = prim::GetAttr[name="8"](%57)
  %59 : __torch__.torch.nn.modules.container.___torch_mangle_36550.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %60 : __torch__.torch.nn.modules.normalization.___torch_mangle_36545.LayerNorm = prim::GetAttr[name="7"](%59)
  %61 : __torch__.torch.nn.modules.container.___torch_mangle_36537.ModuleList = prim::GetAttr[name="ffns"](%5)
  %62 : __torch__.transformers.modeling_xlm.___torch_mangle_36524.TransformerFFN = prim::GetAttr[name="7"](%61)
  %63 : __torch__.torch.nn.modules.container.___torch_mangle_36500.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %64 : __torch__.torch.nn.modules.normalization.___torch_mangle_36495.LayerNorm = prim::GetAttr[name="7"](%63)
  %65 : __torch__.torch.nn.modules.container.___torch_mangle_36487.ModuleList = prim::GetAttr[name="attentions"](%5)
  %66 : __torch__.transformers.modeling_xlm.___torch_mangle_36466.MultiHeadAttention = prim::GetAttr[name="7"](%65)
  %67 : __torch__.torch.nn.modules.container.___torch_mangle_36550.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %68 : __torch__.torch.nn.modules.normalization.___torch_mangle_36544.LayerNorm = prim::GetAttr[name="6"](%67)
  %69 : __torch__.torch.nn.modules.container.___torch_mangle_36537.ModuleList = prim::GetAttr[name="ffns"](%5)
  %70 : __torch__.transformers.modeling_xlm.___torch_mangle_36521.TransformerFFN = prim::GetAttr[name="6"](%69)
  %71 : __torch__.torch.nn.modules.container.___torch_mangle_36500.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %72 : __torch__.torch.nn.modules.normalization.___torch_mangle_36494.LayerNorm = prim::GetAttr[name="6"](%71)
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_36487.ModuleList = prim::GetAttr[name="attentions"](%5)
  %74 : __torch__.transformers.modeling_xlm.___torch_mangle_36461.MultiHeadAttention = prim::GetAttr[name="6"](%73)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_36550.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %76 : __torch__.torch.nn.modules.normalization.___torch_mangle_36543.LayerNorm = prim::GetAttr[name="5"](%75)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_36537.ModuleList = prim::GetAttr[name="ffns"](%5)
  %78 : __torch__.transformers.modeling_xlm.___torch_mangle_36518.TransformerFFN = prim::GetAttr[name="5"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_36500.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %80 : __torch__.torch.nn.modules.normalization.___torch_mangle_36493.LayerNorm = prim::GetAttr[name="5"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_36487.ModuleList = prim::GetAttr[name="attentions"](%5)
  %82 : __torch__.transformers.modeling_xlm.___torch_mangle_36456.MultiHeadAttention = prim::GetAttr[name="5"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_36550.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %84 : __torch__.torch.nn.modules.normalization.___torch_mangle_36542.LayerNorm = prim::GetAttr[name="4"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_36537.ModuleList = prim::GetAttr[name="ffns"](%5)
  %86 : __torch__.transformers.modeling_xlm.___torch_mangle_36515.TransformerFFN = prim::GetAttr[name="4"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_36500.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %88 : __torch__.torch.nn.modules.normalization.___torch_mangle_36492.LayerNorm = prim::GetAttr[name="4"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_36487.ModuleList = prim::GetAttr[name="attentions"](%5)
  %90 : __torch__.transformers.modeling_xlm.___torch_mangle_36451.MultiHeadAttention = prim::GetAttr[name="4"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_36550.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %92 : __torch__.torch.nn.modules.normalization.___torch_mangle_36541.LayerNorm = prim::GetAttr[name="3"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_36537.ModuleList = prim::GetAttr[name="ffns"](%5)
  %94 : __torch__.transformers.modeling_xlm.___torch_mangle_36512.TransformerFFN = prim::GetAttr[name="3"](%93)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_36500.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %96 : __torch__.torch.nn.modules.normalization.___torch_mangle_36491.LayerNorm = prim::GetAttr[name="3"](%95)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_36487.ModuleList = prim::GetAttr[name="attentions"](%5)
  %98 : __torch__.transformers.modeling_xlm.___torch_mangle_36446.MultiHeadAttention = prim::GetAttr[name="3"](%97)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_36550.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %100 : __torch__.torch.nn.modules.normalization.___torch_mangle_36540.LayerNorm = prim::GetAttr[name="2"](%99)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_36537.ModuleList = prim::GetAttr[name="ffns"](%5)
  %102 : __torch__.transformers.modeling_xlm.___torch_mangle_36509.TransformerFFN = prim::GetAttr[name="2"](%101)
  %103 : __torch__.torch.nn.modules.container.___torch_mangle_36500.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %104 : __torch__.torch.nn.modules.normalization.___torch_mangle_36490.LayerNorm = prim::GetAttr[name="2"](%103)
  %105 : __torch__.torch.nn.modules.container.___torch_mangle_36487.ModuleList = prim::GetAttr[name="attentions"](%5)
  %106 : __torch__.transformers.modeling_xlm.___torch_mangle_36441.MultiHeadAttention = prim::GetAttr[name="2"](%105)
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_36550.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %108 : __torch__.torch.nn.modules.normalization.___torch_mangle_36539.LayerNorm = prim::GetAttr[name="1"](%107)
  %109 : __torch__.torch.nn.modules.container.___torch_mangle_36537.ModuleList = prim::GetAttr[name="ffns"](%5)
  %110 : __torch__.transformers.modeling_xlm.___torch_mangle_36506.TransformerFFN = prim::GetAttr[name="1"](%109)
  %111 : __torch__.torch.nn.modules.container.___torch_mangle_36500.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %112 : __torch__.torch.nn.modules.normalization.___torch_mangle_36489.LayerNorm = prim::GetAttr[name="1"](%111)
  %113 : __torch__.torch.nn.modules.container.___torch_mangle_36487.ModuleList = prim::GetAttr[name="attentions"](%5)
  %114 : __torch__.transformers.modeling_xlm.___torch_mangle_36436.MultiHeadAttention = prim::GetAttr[name="1"](%113)
  %115 : __torch__.torch.nn.modules.container.___torch_mangle_36550.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %116 : __torch__.torch.nn.modules.normalization.___torch_mangle_36538.LayerNorm = prim::GetAttr[name="0"](%115)
  %117 : __torch__.torch.nn.modules.container.___torch_mangle_36537.ModuleList = prim::GetAttr[name="ffns"](%5)
  %118 : __torch__.transformers.modeling_xlm.___torch_mangle_36503.TransformerFFN = prim::GetAttr[name="0"](%117)
  %119 : __torch__.torch.nn.modules.container.___torch_mangle_36500.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %120 : __torch__.torch.nn.modules.normalization.___torch_mangle_36488.LayerNorm = prim::GetAttr[name="0"](%119)
  %121 : __torch__.torch.nn.modules.container.___torch_mangle_36487.ModuleList = prim::GetAttr[name="attentions"](%5)
  %122 : __torch__.transformers.modeling_xlm.___torch_mangle_36431.MultiHeadAttention = prim::GetAttr[name="0"](%121)
  %123 : __torch__.torch.nn.modules.normalization.___torch_mangle_36426.LayerNorm = prim::GetAttr[name="layer_norm_emb"](%5)
  %124 : __torch__.torch.nn.modules.sparse.___torch_mangle_36424.Embedding = prim::GetAttr[name="position_embeddings"](%5)
  %125 : __torch__.torch.nn.modules.sparse.___torch_mangle_36425.Embedding = prim::GetAttr[name="embeddings"](%5)
  %126 : Tensor = prim::GetAttr[name="position_ids"](%5)
  %127 : int = aten::size(%input_ids, %26), scope: __module.transformer # transformers/modeling_xlm.py:517:0
  %128 : Long(1:512, 512:1) = aten::slice(%126, %25, %25, %24, %26), scope: __module.transformer # transformers/modeling_xlm.py:546:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%128, %26, %25, %127, %26), scope: __module.transformer # transformers/modeling_xlm.py:546:0
  %130 : Tensor = prim::GetAttr[name="weight"](%125)
  %inputs_embeds : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%130, %input_ids, %22, %23, %23), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %132 : Tensor = prim::GetAttr[name="weight"](%124)
  %133 : Float(1:26624, 13:2048, 2048:1) = aten::embedding(%132, %input.1, %21, %23, %23), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
  %134 : Float(17:0, 13:2048, 2048:1) = aten::expand_as(%133, %inputs_embeds), scope: __module.transformer # transformers/modeling_xlm.py:573:0
  %input.2 : Float(17:26624, 13:2048, 2048:1) = aten::add(%inputs_embeds, %134, %26), scope: __module.transformer # transformers/modeling_xlm.py:573:0
  %136 : Tensor = prim::GetAttr[name="bias"](%123)
  %137 : Tensor = prim::GetAttr[name="weight"](%123)
  %138 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm_emb
  %input.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.2, %138, %137, %136, %19, %20), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.3, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %141 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:580:0
  %142 : Float(17:13, 13:1, 1:1) = aten::to(%141, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:580:0
  %input.4 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.1, %142), scope: __module.transformer # transformers/modeling_xlm.py:580:0
  %144 : __torch__.torch.nn.modules.linear.___torch_mangle_36430.Linear = prim::GetAttr[name="out_lin"](%122)
  %145 : __torch__.torch.nn.modules.linear.___torch_mangle_36429.Linear = prim::GetAttr[name="v_lin"](%122)
  %146 : __torch__.torch.nn.modules.linear.___torch_mangle_36428.Linear = prim::GetAttr[name="k_lin"](%122)
  %147 : __torch__.torch.nn.modules.linear.___torch_mangle_36427.Linear = prim::GetAttr[name="q_lin"](%122)
  %148 : int = aten::size(%input.4, %25), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
  %149 : int = aten::size(%input.4, %26), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
  %150 : Tensor = prim::GetAttr[name="bias"](%147)
  %151 : Tensor = prim::GetAttr[name="weight"](%147)
  %152 : Float(2048:1, 2048:2048) = aten::t(%151), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %output.1 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %152), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %x.1 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.1, %150, %26), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1678:0
  %155 : int[] = prim::ListConstruct(%148, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.0
  %156 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.1, %155), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%156, %26, %22), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %158 : Tensor = prim::GetAttr[name="bias"](%146)
  %159 : Tensor = prim::GetAttr[name="weight"](%146)
  %160 : Float(2048:1, 2048:2048) = aten::t(%159), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %output.2 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %160), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %x.2 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.2, %158, %26), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1678:0
  %163 : int[] = prim::ListConstruct(%148, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.0
  %164 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.2, %163), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %k.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%164, %26, %22), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %166 : Tensor = prim::GetAttr[name="bias"](%145)
  %167 : Tensor = prim::GetAttr[name="weight"](%145)
  %168 : Float(2048:1, 2048:2048) = aten::t(%167), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %output.3 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %168), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %x.3 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.3, %166, %26), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1678:0
  %171 : int[] = prim::ListConstruct(%148, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.0
  %172 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.3, %171), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %v.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%172, %26, %22), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.1, %12), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
  %175 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.1, %22, %13), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.2, %175), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %177 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.0 # torch/tensor.py:22:0
  %178 : int[] = prim::ListConstruct(%148, %26, %26, %149), scope: __module.transformer/__module.transformer.attentions.0
  %179 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%177, %178), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
  %mask.1 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%179, %scores.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
  %scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.1, %mask.1, %14), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
  %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.2, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:193:0
  %183 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %21, %15), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:1498:0
  %weights.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%183, %17, %23), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:973:0
  %x.4 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.1, %v.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:200:0
  %186 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.4, %26, %22), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %187 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%186, %25), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %188 : int[] = prim::ListConstruct(%148, %21, %18), scope: __module.transformer/__module.transformer.attentions.0
  %input.7 : Float(17:26624, 13:2048, 2048:1) = aten::view(%187, %188), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %190 : Tensor = prim::GetAttr[name="bias"](%144)
  %191 : Tensor = prim::GetAttr[name="weight"](%144)
  %192 : Float(2048:1, 2048:2048) = aten::t(%191), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %output.4 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.7, %192), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %input.8 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.4, %190, %26), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1678:0
  %attn.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.8, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.9 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.4, %attn.1, %26), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %197 : Tensor = prim::GetAttr[name="bias"](%120)
  %198 : Tensor = prim::GetAttr[name="weight"](%120)
  %199 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.0
  %input_tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.9, %199, %198, %197, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.0 # torch/nn/functional.py:2048:0
  %201 : __torch__.torch.nn.modules.linear.___torch_mangle_36502.Linear = prim::GetAttr[name="lin2"](%118)
  %202 : __torch__.torch.nn.modules.linear.___torch_mangle_36501.Linear = prim::GetAttr[name="lin1"](%118)
  %203 : Tensor = prim::GetAttr[name="bias"](%202)
  %204 : Tensor = prim::GetAttr[name="weight"](%202)
  %205 : Float(2048:1, 8192:2048) = aten::t(%204), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.1, %205), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %input.10 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.5, %203, %26), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1678:0
  %input.11 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.10), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:1369:0
  %209 : Tensor = prim::GetAttr[name="bias"](%201)
  %210 : Tensor = prim::GetAttr[name="weight"](%201)
  %211 : Float(8192:1, 2048:8192) = aten::t(%210), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %output.6 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.11, %211), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %input.12 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.6, %209, %26), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1678:0
  %214 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.12, %17, %23), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:973:0
  %input.13 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.1, %214, %26), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %216 : Tensor = prim::GetAttr[name="bias"](%116)
  %217 : Tensor = prim::GetAttr[name="weight"](%116)
  %218 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.0
  %tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.13, %218, %217, %216, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.0 # torch/nn/functional.py:2048:0
  %220 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %221 : Float(17:13, 13:1, 1:1) = aten::to(%220, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.14 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.2, %221), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %223 : __torch__.torch.nn.modules.linear.___torch_mangle_36435.Linear = prim::GetAttr[name="out_lin"](%114)
  %224 : __torch__.torch.nn.modules.linear.___torch_mangle_36434.Linear = prim::GetAttr[name="v_lin"](%114)
  %225 : __torch__.torch.nn.modules.linear.___torch_mangle_36433.Linear = prim::GetAttr[name="k_lin"](%114)
  %226 : __torch__.torch.nn.modules.linear.___torch_mangle_36432.Linear = prim::GetAttr[name="q_lin"](%114)
  %227 : int = aten::size(%input.14, %25), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
  %228 : int = aten::size(%input.14, %26), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
  %229 : Tensor = prim::GetAttr[name="bias"](%226)
  %230 : Tensor = prim::GetAttr[name="weight"](%226)
  %231 : Float(2048:1, 2048:2048) = aten::t(%230), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %output.7 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %231), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %x.5 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.7, %229, %26), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1678:0
  %234 : int[] = prim::ListConstruct(%227, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.1
  %235 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.5, %234), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%235, %26, %22), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %237 : Tensor = prim::GetAttr[name="bias"](%225)
  %238 : Tensor = prim::GetAttr[name="weight"](%225)
  %239 : Float(2048:1, 2048:2048) = aten::t(%238), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %output.8 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %239), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %x.6 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.8, %237, %26), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1678:0
  %242 : int[] = prim::ListConstruct(%227, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.1
  %243 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.6, %242), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %k.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%243, %26, %22), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %245 : Tensor = prim::GetAttr[name="bias"](%224)
  %246 : Tensor = prim::GetAttr[name="weight"](%224)
  %247 : Float(2048:1, 2048:2048) = aten::t(%246), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %output.9 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %247), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %x.7 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.9, %245, %26), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1678:0
  %250 : int[] = prim::ListConstruct(%227, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.1
  %251 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.7, %250), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %v.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%251, %26, %22), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.3, %12), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:188:0
  %254 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.2, %22, %13), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
  %scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.4, %254), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
  %256 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.1 # torch/tensor.py:22:0
  %257 : int[] = prim::ListConstruct(%227, %26, %26, %228), scope: __module.transformer/__module.transformer.attentions.1
  %258 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%256, %257), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
  %mask.2 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%258, %scores.3), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
  %scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.3, %mask.2, %14), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:191:0
  %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.4, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:193:0
  %262 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %21, %15), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:1498:0
  %weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%262, %17, %23), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:973:0
  %x.8 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.2, %v.2), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:200:0
  %265 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.8, %26, %22), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %266 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%265, %25), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %267 : int[] = prim::ListConstruct(%227, %21, %18), scope: __module.transformer/__module.transformer.attentions.1
  %input.17 : Float(17:26624, 13:2048, 2048:1) = aten::view(%266, %267), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %269 : Tensor = prim::GetAttr[name="bias"](%223)
  %270 : Tensor = prim::GetAttr[name="weight"](%223)
  %271 : Float(2048:1, 2048:2048) = aten::t(%270), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %output.10 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.17, %271), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %input.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.10, %269, %26), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1678:0
  %attn.2 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.18, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.19 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.14, %attn.2, %26), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %276 : Tensor = prim::GetAttr[name="bias"](%112)
  %277 : Tensor = prim::GetAttr[name="weight"](%112)
  %278 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.1
  %input_tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.19, %278, %277, %276, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.1 # torch/nn/functional.py:2048:0
  %280 : __torch__.torch.nn.modules.linear.___torch_mangle_36505.Linear = prim::GetAttr[name="lin2"](%110)
  %281 : __torch__.torch.nn.modules.linear.___torch_mangle_36504.Linear = prim::GetAttr[name="lin1"](%110)
  %282 : Tensor = prim::GetAttr[name="bias"](%281)
  %283 : Tensor = prim::GetAttr[name="weight"](%281)
  %284 : Float(2048:1, 8192:2048) = aten::t(%283), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %output.11 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.2, %284), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %input.20 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.11, %282, %26), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1678:0
  %input.21 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.20), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:1369:0
  %288 : Tensor = prim::GetAttr[name="bias"](%280)
  %289 : Tensor = prim::GetAttr[name="weight"](%280)
  %290 : Float(8192:1, 2048:8192) = aten::t(%289), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %output.12 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.21, %290), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.12, %288, %26), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1678:0
  %293 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.22, %17, %23), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:973:0
  %input.23 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.2, %293, %26), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %295 : Tensor = prim::GetAttr[name="bias"](%108)
  %296 : Tensor = prim::GetAttr[name="weight"](%108)
  %297 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.1
  %tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.23, %297, %296, %295, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.1 # torch/nn/functional.py:2048:0
  %299 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %300 : Float(17:13, 13:1, 1:1) = aten::to(%299, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.24 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.3, %300), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %302 : __torch__.torch.nn.modules.linear.___torch_mangle_36440.Linear = prim::GetAttr[name="out_lin"](%106)
  %303 : __torch__.torch.nn.modules.linear.___torch_mangle_36439.Linear = prim::GetAttr[name="v_lin"](%106)
  %304 : __torch__.torch.nn.modules.linear.___torch_mangle_36438.Linear = prim::GetAttr[name="k_lin"](%106)
  %305 : __torch__.torch.nn.modules.linear.___torch_mangle_36437.Linear = prim::GetAttr[name="q_lin"](%106)
  %306 : int = aten::size(%input.24, %25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
  %307 : int = aten::size(%input.24, %26), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
  %308 : Tensor = prim::GetAttr[name="bias"](%305)
  %309 : Tensor = prim::GetAttr[name="weight"](%305)
  %310 : Float(2048:1, 2048:2048) = aten::t(%309), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %output.13 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %310), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %x.9 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.13, %308, %26), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1678:0
  %313 : int[] = prim::ListConstruct(%306, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.2
  %314 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.9, %313), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%314, %26, %22), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %316 : Tensor = prim::GetAttr[name="bias"](%304)
  %317 : Tensor = prim::GetAttr[name="weight"](%304)
  %318 : Float(2048:1, 2048:2048) = aten::t(%317), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %output.14 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %318), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %x.10 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.14, %316, %26), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1678:0
  %321 : int[] = prim::ListConstruct(%306, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.2
  %322 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.10, %321), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %k.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%322, %26, %22), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %324 : Tensor = prim::GetAttr[name="bias"](%303)
  %325 : Tensor = prim::GetAttr[name="weight"](%303)
  %326 : Float(2048:1, 2048:2048) = aten::t(%325), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %output.15 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %326), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %x.11 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.15, %324, %26), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1678:0
  %329 : int[] = prim::ListConstruct(%306, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.2
  %330 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.11, %329), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %v.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%330, %26, %22), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.5, %12), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:188:0
  %333 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.3, %22, %13), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
  %scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.6, %333), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
  %335 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.2 # torch/tensor.py:22:0
  %336 : int[] = prim::ListConstruct(%306, %26, %26, %307), scope: __module.transformer/__module.transformer.attentions.2
  %337 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%335, %336), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
  %mask.3 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%337, %scores.5), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
  %scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.5, %mask.3, %14), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:191:0
  %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.6, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:193:0
  %341 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %21, %15), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:1498:0
  %weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%341, %17, %23), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:973:0
  %x.12 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.3, %v.3), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:200:0
  %344 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.12, %26, %22), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %345 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%344, %25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %346 : int[] = prim::ListConstruct(%306, %21, %18), scope: __module.transformer/__module.transformer.attentions.2
  %input.27 : Float(17:26624, 13:2048, 2048:1) = aten::view(%345, %346), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %348 : Tensor = prim::GetAttr[name="bias"](%302)
  %349 : Tensor = prim::GetAttr[name="weight"](%302)
  %350 : Float(2048:1, 2048:2048) = aten::t(%349), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %output.16 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.27, %350), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %input.28 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.16, %348, %26), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1678:0
  %attn.3 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.28, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.29 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.24, %attn.3, %26), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %355 : Tensor = prim::GetAttr[name="bias"](%104)
  %356 : Tensor = prim::GetAttr[name="weight"](%104)
  %357 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.2
  %input_tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.29, %357, %356, %355, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.2 # torch/nn/functional.py:2048:0
  %359 : __torch__.torch.nn.modules.linear.___torch_mangle_36508.Linear = prim::GetAttr[name="lin2"](%102)
  %360 : __torch__.torch.nn.modules.linear.___torch_mangle_36507.Linear = prim::GetAttr[name="lin1"](%102)
  %361 : Tensor = prim::GetAttr[name="bias"](%360)
  %362 : Tensor = prim::GetAttr[name="weight"](%360)
  %363 : Float(2048:1, 8192:2048) = aten::t(%362), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %output.17 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.3, %363), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %input.30 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.17, %361, %26), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1678:0
  %input.31 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.30), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:1369:0
  %367 : Tensor = prim::GetAttr[name="bias"](%359)
  %368 : Tensor = prim::GetAttr[name="weight"](%359)
  %369 : Float(8192:1, 2048:8192) = aten::t(%368), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %output.18 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.31, %369), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %input.32 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.18, %367, %26), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1678:0
  %372 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.32, %17, %23), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:973:0
  %input.33 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.3, %372, %26), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %374 : Tensor = prim::GetAttr[name="bias"](%100)
  %375 : Tensor = prim::GetAttr[name="weight"](%100)
  %376 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.2
  %tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.33, %376, %375, %374, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.2 # torch/nn/functional.py:2048:0
  %378 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %379 : Float(17:13, 13:1, 1:1) = aten::to(%378, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.34 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.4, %379), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %381 : __torch__.torch.nn.modules.linear.___torch_mangle_36445.Linear = prim::GetAttr[name="out_lin"](%98)
  %382 : __torch__.torch.nn.modules.linear.___torch_mangle_36444.Linear = prim::GetAttr[name="v_lin"](%98)
  %383 : __torch__.torch.nn.modules.linear.___torch_mangle_36443.Linear = prim::GetAttr[name="k_lin"](%98)
  %384 : __torch__.torch.nn.modules.linear.___torch_mangle_36442.Linear = prim::GetAttr[name="q_lin"](%98)
  %385 : int = aten::size(%input.34, %25), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
  %386 : int = aten::size(%input.34, %26), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
  %387 : Tensor = prim::GetAttr[name="bias"](%384)
  %388 : Tensor = prim::GetAttr[name="weight"](%384)
  %389 : Float(2048:1, 2048:2048) = aten::t(%388), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %output.19 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %389), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %x.13 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.19, %387, %26), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1678:0
  %392 : int[] = prim::ListConstruct(%385, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.3
  %393 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.13, %392), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%393, %26, %22), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %395 : Tensor = prim::GetAttr[name="bias"](%383)
  %396 : Tensor = prim::GetAttr[name="weight"](%383)
  %397 : Float(2048:1, 2048:2048) = aten::t(%396), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %output.20 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %397), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %x.14 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.20, %395, %26), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1678:0
  %400 : int[] = prim::ListConstruct(%385, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.3
  %401 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.14, %400), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %k.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%401, %26, %22), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %403 : Tensor = prim::GetAttr[name="bias"](%382)
  %404 : Tensor = prim::GetAttr[name="weight"](%382)
  %405 : Float(2048:1, 2048:2048) = aten::t(%404), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %output.21 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %405), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %x.15 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.21, %403, %26), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1678:0
  %408 : int[] = prim::ListConstruct(%385, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.3
  %409 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.15, %408), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %v.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%409, %26, %22), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.7, %12), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:188:0
  %412 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.4, %22, %13), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
  %scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.8, %412), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
  %414 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.3 # torch/tensor.py:22:0
  %415 : int[] = prim::ListConstruct(%385, %26, %26, %386), scope: __module.transformer/__module.transformer.attentions.3
  %416 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%414, %415), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
  %mask.4 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%416, %scores.7), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
  %scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.7, %mask.4, %14), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:191:0
  %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.8, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:193:0
  %420 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %21, %15), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:1498:0
  %weights.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%420, %17, %23), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:973:0
  %x.16 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.4, %v.4), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:200:0
  %423 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.16, %26, %22), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %424 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%423, %25), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %425 : int[] = prim::ListConstruct(%385, %21, %18), scope: __module.transformer/__module.transformer.attentions.3
  %input.37 : Float(17:26624, 13:2048, 2048:1) = aten::view(%424, %425), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %427 : Tensor = prim::GetAttr[name="bias"](%381)
  %428 : Tensor = prim::GetAttr[name="weight"](%381)
  %429 : Float(2048:1, 2048:2048) = aten::t(%428), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %output.22 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.37, %429), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %input.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.22, %427, %26), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1678:0
  %attn.4 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.38, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.39 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.34, %attn.4, %26), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %434 : Tensor = prim::GetAttr[name="bias"](%96)
  %435 : Tensor = prim::GetAttr[name="weight"](%96)
  %436 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.3
  %input_tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.39, %436, %435, %434, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.3 # torch/nn/functional.py:2048:0
  %438 : __torch__.torch.nn.modules.linear.___torch_mangle_36511.Linear = prim::GetAttr[name="lin2"](%94)
  %439 : __torch__.torch.nn.modules.linear.___torch_mangle_36510.Linear = prim::GetAttr[name="lin1"](%94)
  %440 : Tensor = prim::GetAttr[name="bias"](%439)
  %441 : Tensor = prim::GetAttr[name="weight"](%439)
  %442 : Float(2048:1, 8192:2048) = aten::t(%441), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %output.23 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.4, %442), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.23, %440, %26), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1678:0
  %input.41 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.40), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:1369:0
  %446 : Tensor = prim::GetAttr[name="bias"](%438)
  %447 : Tensor = prim::GetAttr[name="weight"](%438)
  %448 : Float(8192:1, 2048:8192) = aten::t(%447), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.41, %448), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %input.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.24, %446, %26), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1678:0
  %451 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.42, %17, %23), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:973:0
  %input.43 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.4, %451, %26), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %453 : Tensor = prim::GetAttr[name="bias"](%92)
  %454 : Tensor = prim::GetAttr[name="weight"](%92)
  %455 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.3
  %tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.43, %455, %454, %453, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.3 # torch/nn/functional.py:2048:0
  %457 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %458 : Float(17:13, 13:1, 1:1) = aten::to(%457, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.44 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.5, %458), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %460 : __torch__.torch.nn.modules.linear.___torch_mangle_36450.Linear = prim::GetAttr[name="out_lin"](%90)
  %461 : __torch__.torch.nn.modules.linear.___torch_mangle_36449.Linear = prim::GetAttr[name="v_lin"](%90)
  %462 : __torch__.torch.nn.modules.linear.___torch_mangle_36448.Linear = prim::GetAttr[name="k_lin"](%90)
  %463 : __torch__.torch.nn.modules.linear.___torch_mangle_36447.Linear = prim::GetAttr[name="q_lin"](%90)
  %464 : int = aten::size(%input.44, %25), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
  %465 : int = aten::size(%input.44, %26), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
  %466 : Tensor = prim::GetAttr[name="bias"](%463)
  %467 : Tensor = prim::GetAttr[name="weight"](%463)
  %468 : Float(2048:1, 2048:2048) = aten::t(%467), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %output.25 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %468), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %x.17 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.25, %466, %26), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1678:0
  %471 : int[] = prim::ListConstruct(%464, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.4
  %472 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.17, %471), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%472, %26, %22), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %474 : Tensor = prim::GetAttr[name="bias"](%462)
  %475 : Tensor = prim::GetAttr[name="weight"](%462)
  %476 : Float(2048:1, 2048:2048) = aten::t(%475), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %output.26 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %476), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %x.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.26, %474, %26), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1678:0
  %479 : int[] = prim::ListConstruct(%464, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.4
  %480 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.18, %479), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %k.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%480, %26, %22), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %482 : Tensor = prim::GetAttr[name="bias"](%461)
  %483 : Tensor = prim::GetAttr[name="weight"](%461)
  %484 : Float(2048:1, 2048:2048) = aten::t(%483), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %output.27 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %484), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %x.19 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.27, %482, %26), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1678:0
  %487 : int[] = prim::ListConstruct(%464, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.4
  %488 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.19, %487), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %v.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%488, %26, %22), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.9, %12), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:188:0
  %491 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.5, %22, %13), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
  %scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.10, %491), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
  %493 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.4 # torch/tensor.py:22:0
  %494 : int[] = prim::ListConstruct(%464, %26, %26, %465), scope: __module.transformer/__module.transformer.attentions.4
  %495 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%493, %494), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
  %mask.5 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%495, %scores.9), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
  %scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.9, %mask.5, %14), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:191:0
  %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.10, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:193:0
  %499 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %21, %15), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:1498:0
  %weights.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%499, %17, %23), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:973:0
  %x.20 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.5, %v.5), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:200:0
  %502 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.20, %26, %22), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %503 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%502, %25), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %504 : int[] = prim::ListConstruct(%464, %21, %18), scope: __module.transformer/__module.transformer.attentions.4
  %input.47 : Float(17:26624, 13:2048, 2048:1) = aten::view(%503, %504), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %506 : Tensor = prim::GetAttr[name="bias"](%460)
  %507 : Tensor = prim::GetAttr[name="weight"](%460)
  %508 : Float(2048:1, 2048:2048) = aten::t(%507), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %output.28 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.47, %508), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %input.48 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.28, %506, %26), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1678:0
  %attn.5 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.48, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.49 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.44, %attn.5, %26), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %513 : Tensor = prim::GetAttr[name="bias"](%88)
  %514 : Tensor = prim::GetAttr[name="weight"](%88)
  %515 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.4
  %input_tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.49, %515, %514, %513, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.4 # torch/nn/functional.py:2048:0
  %517 : __torch__.torch.nn.modules.linear.___torch_mangle_36514.Linear = prim::GetAttr[name="lin2"](%86)
  %518 : __torch__.torch.nn.modules.linear.___torch_mangle_36513.Linear = prim::GetAttr[name="lin1"](%86)
  %519 : Tensor = prim::GetAttr[name="bias"](%518)
  %520 : Tensor = prim::GetAttr[name="weight"](%518)
  %521 : Float(2048:1, 8192:2048) = aten::t(%520), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.5, %521), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %input.50 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.29, %519, %26), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1678:0
  %input.51 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.50), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:1369:0
  %525 : Tensor = prim::GetAttr[name="bias"](%517)
  %526 : Tensor = prim::GetAttr[name="weight"](%517)
  %527 : Float(8192:1, 2048:8192) = aten::t(%526), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.51, %527), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %input.52 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.30, %525, %26), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1678:0
  %530 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.52, %17, %23), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:973:0
  %input.53 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.5, %530, %26), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %532 : Tensor = prim::GetAttr[name="bias"](%84)
  %533 : Tensor = prim::GetAttr[name="weight"](%84)
  %534 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.4
  %tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.53, %534, %533, %532, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.4 # torch/nn/functional.py:2048:0
  %536 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %537 : Float(17:13, 13:1, 1:1) = aten::to(%536, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.54 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.6, %537), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %539 : __torch__.torch.nn.modules.linear.___torch_mangle_36455.Linear = prim::GetAttr[name="out_lin"](%82)
  %540 : __torch__.torch.nn.modules.linear.___torch_mangle_36454.Linear = prim::GetAttr[name="v_lin"](%82)
  %541 : __torch__.torch.nn.modules.linear.___torch_mangle_36453.Linear = prim::GetAttr[name="k_lin"](%82)
  %542 : __torch__.torch.nn.modules.linear.___torch_mangle_36452.Linear = prim::GetAttr[name="q_lin"](%82)
  %543 : int = aten::size(%input.54, %25), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
  %544 : int = aten::size(%input.54, %26), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
  %545 : Tensor = prim::GetAttr[name="bias"](%542)
  %546 : Tensor = prim::GetAttr[name="weight"](%542)
  %547 : Float(2048:1, 2048:2048) = aten::t(%546), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %output.31 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %547), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %x.21 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.31, %545, %26), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1678:0
  %550 : int[] = prim::ListConstruct(%543, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.5
  %551 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.21, %550), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%551, %26, %22), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %553 : Tensor = prim::GetAttr[name="bias"](%541)
  %554 : Tensor = prim::GetAttr[name="weight"](%541)
  %555 : Float(2048:1, 2048:2048) = aten::t(%554), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %output.32 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %555), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %x.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.32, %553, %26), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1678:0
  %558 : int[] = prim::ListConstruct(%543, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.5
  %559 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.22, %558), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %k.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%559, %26, %22), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %561 : Tensor = prim::GetAttr[name="bias"](%540)
  %562 : Tensor = prim::GetAttr[name="weight"](%540)
  %563 : Float(2048:1, 2048:2048) = aten::t(%562), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %output.33 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %563), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %x.23 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.33, %561, %26), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1678:0
  %566 : int[] = prim::ListConstruct(%543, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.5
  %567 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.23, %566), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %v.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%567, %26, %22), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.12 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.11, %12), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:188:0
  %570 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.6, %22, %13), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
  %scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.12, %570), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
  %572 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.5 # torch/tensor.py:22:0
  %573 : int[] = prim::ListConstruct(%543, %26, %26, %544), scope: __module.transformer/__module.transformer.attentions.5
  %574 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%572, %573), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
  %mask.6 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%574, %scores.11), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
  %scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.11, %mask.6, %14), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:191:0
  %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.12, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:193:0
  %578 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %21, %15), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:1498:0
  %weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%578, %17, %23), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:973:0
  %x.24 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.6, %v.6), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:200:0
  %581 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.24, %26, %22), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %582 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%581, %25), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %583 : int[] = prim::ListConstruct(%543, %21, %18), scope: __module.transformer/__module.transformer.attentions.5
  %input.57 : Float(17:26624, 13:2048, 2048:1) = aten::view(%582, %583), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %585 : Tensor = prim::GetAttr[name="bias"](%539)
  %586 : Tensor = prim::GetAttr[name="weight"](%539)
  %587 : Float(2048:1, 2048:2048) = aten::t(%586), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %output.34 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.57, %587), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %input.58 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.34, %585, %26), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1678:0
  %attn.6 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.58, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.59 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.54, %attn.6, %26), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %592 : Tensor = prim::GetAttr[name="bias"](%80)
  %593 : Tensor = prim::GetAttr[name="weight"](%80)
  %594 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.5
  %input_tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.59, %594, %593, %592, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.5 # torch/nn/functional.py:2048:0
  %596 : __torch__.torch.nn.modules.linear.___torch_mangle_36517.Linear = prim::GetAttr[name="lin2"](%78)
  %597 : __torch__.torch.nn.modules.linear.___torch_mangle_36516.Linear = prim::GetAttr[name="lin1"](%78)
  %598 : Tensor = prim::GetAttr[name="bias"](%597)
  %599 : Tensor = prim::GetAttr[name="weight"](%597)
  %600 : Float(2048:1, 8192:2048) = aten::t(%599), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.6, %600), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %input.60 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.35, %598, %26), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1678:0
  %input.61 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.60), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:1369:0
  %604 : Tensor = prim::GetAttr[name="bias"](%596)
  %605 : Tensor = prim::GetAttr[name="weight"](%596)
  %606 : Float(8192:1, 2048:8192) = aten::t(%605), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %output.36 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.61, %606), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %input.62 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.36, %604, %26), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1678:0
  %609 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.62, %17, %23), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:973:0
  %input.63 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.6, %609, %26), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %611 : Tensor = prim::GetAttr[name="bias"](%76)
  %612 : Tensor = prim::GetAttr[name="weight"](%76)
  %613 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.5
  %tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.63, %613, %612, %611, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.5 # torch/nn/functional.py:2048:0
  %615 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %616 : Float(17:13, 13:1, 1:1) = aten::to(%615, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.64 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.7, %616), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %618 : __torch__.torch.nn.modules.linear.___torch_mangle_36460.Linear = prim::GetAttr[name="out_lin"](%74)
  %619 : __torch__.torch.nn.modules.linear.___torch_mangle_36459.Linear = prim::GetAttr[name="v_lin"](%74)
  %620 : __torch__.torch.nn.modules.linear.___torch_mangle_36458.Linear = prim::GetAttr[name="k_lin"](%74)
  %621 : __torch__.torch.nn.modules.linear.___torch_mangle_36457.Linear = prim::GetAttr[name="q_lin"](%74)
  %622 : int = aten::size(%input.64, %25), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
  %623 : int = aten::size(%input.64, %26), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
  %624 : Tensor = prim::GetAttr[name="bias"](%621)
  %625 : Tensor = prim::GetAttr[name="weight"](%621)
  %626 : Float(2048:1, 2048:2048) = aten::t(%625), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %output.37 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %626), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %x.25 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.37, %624, %26), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1678:0
  %629 : int[] = prim::ListConstruct(%622, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.6
  %630 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.25, %629), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.13 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%630, %26, %22), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %632 : Tensor = prim::GetAttr[name="bias"](%620)
  %633 : Tensor = prim::GetAttr[name="weight"](%620)
  %634 : Float(2048:1, 2048:2048) = aten::t(%633), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %output.38 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %634), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %x.26 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.38, %632, %26), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1678:0
  %637 : int[] = prim::ListConstruct(%622, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.6
  %638 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.26, %637), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %k.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%638, %26, %22), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %640 : Tensor = prim::GetAttr[name="bias"](%619)
  %641 : Tensor = prim::GetAttr[name="weight"](%619)
  %642 : Float(2048:1, 2048:2048) = aten::t(%641), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %output.39 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %642), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %x.27 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.39, %640, %26), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1678:0
  %645 : int[] = prim::ListConstruct(%622, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.6
  %646 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.27, %645), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %v.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%646, %26, %22), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.14 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.13, %12), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:188:0
  %649 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.7, %22, %13), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
  %scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.14, %649), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
  %651 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.6 # torch/tensor.py:22:0
  %652 : int[] = prim::ListConstruct(%622, %26, %26, %623), scope: __module.transformer/__module.transformer.attentions.6
  %653 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%651, %652), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
  %mask.7 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%653, %scores.13), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
  %scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.13, %mask.7, %14), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:191:0
  %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.14, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:193:0
  %657 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %21, %15), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:1498:0
  %weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%657, %17, %23), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:973:0
  %x.28 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.7, %v.7), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:200:0
  %660 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.28, %26, %22), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %661 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%660, %25), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %662 : int[] = prim::ListConstruct(%622, %21, %18), scope: __module.transformer/__module.transformer.attentions.6
  %input.67 : Float(17:26624, 13:2048, 2048:1) = aten::view(%661, %662), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %664 : Tensor = prim::GetAttr[name="bias"](%618)
  %665 : Tensor = prim::GetAttr[name="weight"](%618)
  %666 : Float(2048:1, 2048:2048) = aten::t(%665), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %output.40 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.67, %666), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %input.68 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.40, %664, %26), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1678:0
  %attn.7 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.68, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.69 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.64, %attn.7, %26), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %671 : Tensor = prim::GetAttr[name="bias"](%72)
  %672 : Tensor = prim::GetAttr[name="weight"](%72)
  %673 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.6
  %input_tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.69, %673, %672, %671, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.6 # torch/nn/functional.py:2048:0
  %675 : __torch__.torch.nn.modules.linear.___torch_mangle_36520.Linear = prim::GetAttr[name="lin2"](%70)
  %676 : __torch__.torch.nn.modules.linear.___torch_mangle_36519.Linear = prim::GetAttr[name="lin1"](%70)
  %677 : Tensor = prim::GetAttr[name="bias"](%676)
  %678 : Tensor = prim::GetAttr[name="weight"](%676)
  %679 : Float(2048:1, 8192:2048) = aten::t(%678), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %output.41 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.7, %679), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %input.70 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.41, %677, %26), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1678:0
  %input.71 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.70), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:1369:0
  %683 : Tensor = prim::GetAttr[name="bias"](%675)
  %684 : Tensor = prim::GetAttr[name="weight"](%675)
  %685 : Float(8192:1, 2048:8192) = aten::t(%684), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %output.42 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.71, %685), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %input.72 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.42, %683, %26), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1678:0
  %688 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.72, %17, %23), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:973:0
  %input.73 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.7, %688, %26), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %690 : Tensor = prim::GetAttr[name="bias"](%68)
  %691 : Tensor = prim::GetAttr[name="weight"](%68)
  %692 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.6
  %tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.73, %692, %691, %690, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.6 # torch/nn/functional.py:2048:0
  %694 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %695 : Float(17:13, 13:1, 1:1) = aten::to(%694, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.74 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.8, %695), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %697 : __torch__.torch.nn.modules.linear.___torch_mangle_36465.Linear = prim::GetAttr[name="out_lin"](%66)
  %698 : __torch__.torch.nn.modules.linear.___torch_mangle_36464.Linear = prim::GetAttr[name="v_lin"](%66)
  %699 : __torch__.torch.nn.modules.linear.___torch_mangle_36463.Linear = prim::GetAttr[name="k_lin"](%66)
  %700 : __torch__.torch.nn.modules.linear.___torch_mangle_36462.Linear = prim::GetAttr[name="q_lin"](%66)
  %701 : int = aten::size(%input.74, %25), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
  %702 : int = aten::size(%input.74, %26), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
  %703 : Tensor = prim::GetAttr[name="bias"](%700)
  %704 : Tensor = prim::GetAttr[name="weight"](%700)
  %705 : Float(2048:1, 2048:2048) = aten::t(%704), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %output.43 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %705), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %x.29 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.43, %703, %26), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1678:0
  %708 : int[] = prim::ListConstruct(%701, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.7
  %709 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.29, %708), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.15 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%709, %26, %22), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %711 : Tensor = prim::GetAttr[name="bias"](%699)
  %712 : Tensor = prim::GetAttr[name="weight"](%699)
  %713 : Float(2048:1, 2048:2048) = aten::t(%712), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %output.44 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %713), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %x.30 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.44, %711, %26), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1678:0
  %716 : int[] = prim::ListConstruct(%701, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.7
  %717 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.30, %716), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %k.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%717, %26, %22), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %719 : Tensor = prim::GetAttr[name="bias"](%698)
  %720 : Tensor = prim::GetAttr[name="weight"](%698)
  %721 : Float(2048:1, 2048:2048) = aten::t(%720), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %output.45 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %721), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %x.31 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.45, %719, %26), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1678:0
  %724 : int[] = prim::ListConstruct(%701, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.7
  %725 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.31, %724), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %v.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%725, %26, %22), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.16 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.15, %12), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:188:0
  %728 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.8, %22, %13), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
  %scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.16, %728), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
  %730 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.7 # torch/tensor.py:22:0
  %731 : int[] = prim::ListConstruct(%701, %26, %26, %702), scope: __module.transformer/__module.transformer.attentions.7
  %732 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%730, %731), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
  %mask.8 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%732, %scores.15), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
  %scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.15, %mask.8, %14), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:191:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.16, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:193:0
  %736 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %21, %15), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:1498:0
  %weights.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%736, %17, %23), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:973:0
  %x.32 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.8, %v.8), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:200:0
  %739 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.32, %26, %22), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %740 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%739, %25), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %741 : int[] = prim::ListConstruct(%701, %21, %18), scope: __module.transformer/__module.transformer.attentions.7
  %input.77 : Float(17:26624, 13:2048, 2048:1) = aten::view(%740, %741), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %743 : Tensor = prim::GetAttr[name="bias"](%697)
  %744 : Tensor = prim::GetAttr[name="weight"](%697)
  %745 : Float(2048:1, 2048:2048) = aten::t(%744), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %output.46 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.77, %745), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %input.78 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.46, %743, %26), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1678:0
  %attn.8 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.78, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.79 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.74, %attn.8, %26), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %750 : Tensor = prim::GetAttr[name="bias"](%64)
  %751 : Tensor = prim::GetAttr[name="weight"](%64)
  %752 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.7
  %input_tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.79, %752, %751, %750, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.7 # torch/nn/functional.py:2048:0
  %754 : __torch__.torch.nn.modules.linear.___torch_mangle_36523.Linear = prim::GetAttr[name="lin2"](%62)
  %755 : __torch__.torch.nn.modules.linear.___torch_mangle_36522.Linear = prim::GetAttr[name="lin1"](%62)
  %756 : Tensor = prim::GetAttr[name="bias"](%755)
  %757 : Tensor = prim::GetAttr[name="weight"](%755)
  %758 : Float(2048:1, 8192:2048) = aten::t(%757), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %output.47 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.8, %758), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %input.80 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.47, %756, %26), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1678:0
  %input.81 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.80), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:1369:0
  %762 : Tensor = prim::GetAttr[name="bias"](%754)
  %763 : Tensor = prim::GetAttr[name="weight"](%754)
  %764 : Float(8192:1, 2048:8192) = aten::t(%763), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %output.48 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.81, %764), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %input.82 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.48, %762, %26), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1678:0
  %767 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.82, %17, %23), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:973:0
  %input.83 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.8, %767, %26), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %769 : Tensor = prim::GetAttr[name="bias"](%60)
  %770 : Tensor = prim::GetAttr[name="weight"](%60)
  %771 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.7
  %tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.83, %771, %770, %769, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.7 # torch/nn/functional.py:2048:0
  %773 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %774 : Float(17:13, 13:1, 1:1) = aten::to(%773, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.84 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.9, %774), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %776 : __torch__.torch.nn.modules.linear.___torch_mangle_36470.Linear = prim::GetAttr[name="out_lin"](%58)
  %777 : __torch__.torch.nn.modules.linear.___torch_mangle_36469.Linear = prim::GetAttr[name="v_lin"](%58)
  %778 : __torch__.torch.nn.modules.linear.___torch_mangle_36468.Linear = prim::GetAttr[name="k_lin"](%58)
  %779 : __torch__.torch.nn.modules.linear.___torch_mangle_36467.Linear = prim::GetAttr[name="q_lin"](%58)
  %780 : int = aten::size(%input.84, %25), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
  %781 : int = aten::size(%input.84, %26), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
  %782 : Tensor = prim::GetAttr[name="bias"](%779)
  %783 : Tensor = prim::GetAttr[name="weight"](%779)
  %784 : Float(2048:1, 2048:2048) = aten::t(%783), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %output.49 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %784), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %x.33 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.49, %782, %26), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1678:0
  %787 : int[] = prim::ListConstruct(%780, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.8
  %788 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.33, %787), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.17 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%788, %26, %22), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %790 : Tensor = prim::GetAttr[name="bias"](%778)
  %791 : Tensor = prim::GetAttr[name="weight"](%778)
  %792 : Float(2048:1, 2048:2048) = aten::t(%791), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %output.50 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %792), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %x.34 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.50, %790, %26), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1678:0
  %795 : int[] = prim::ListConstruct(%780, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.8
  %796 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.34, %795), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %k.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%796, %26, %22), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %798 : Tensor = prim::GetAttr[name="bias"](%777)
  %799 : Tensor = prim::GetAttr[name="weight"](%777)
  %800 : Float(2048:1, 2048:2048) = aten::t(%799), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %output.51 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %800), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %x.35 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.51, %798, %26), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1678:0
  %803 : int[] = prim::ListConstruct(%780, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.8
  %804 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.35, %803), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %v.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%804, %26, %22), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.18 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.17, %12), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:188:0
  %807 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.9, %22, %13), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
  %scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.18, %807), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
  %809 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.8 # torch/tensor.py:22:0
  %810 : int[] = prim::ListConstruct(%780, %26, %26, %781), scope: __module.transformer/__module.transformer.attentions.8
  %811 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%809, %810), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
  %mask.9 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%811, %scores.17), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
  %scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.17, %mask.9, %14), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:191:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.18, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:193:0
  %815 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %21, %15), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:1498:0
  %weights.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%815, %17, %23), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:973:0
  %x.36 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.9, %v.9), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:200:0
  %818 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.36, %26, %22), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %819 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%818, %25), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %820 : int[] = prim::ListConstruct(%780, %21, %18), scope: __module.transformer/__module.transformer.attentions.8
  %input.87 : Float(17:26624, 13:2048, 2048:1) = aten::view(%819, %820), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %822 : Tensor = prim::GetAttr[name="bias"](%776)
  %823 : Tensor = prim::GetAttr[name="weight"](%776)
  %824 : Float(2048:1, 2048:2048) = aten::t(%823), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %output.52 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.87, %824), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %input.88 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.52, %822, %26), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1678:0
  %attn.9 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.88, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.89 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.84, %attn.9, %26), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %829 : Tensor = prim::GetAttr[name="bias"](%56)
  %830 : Tensor = prim::GetAttr[name="weight"](%56)
  %831 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.8
  %input_tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.89, %831, %830, %829, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.8 # torch/nn/functional.py:2048:0
  %833 : __torch__.torch.nn.modules.linear.___torch_mangle_36526.Linear = prim::GetAttr[name="lin2"](%54)
  %834 : __torch__.torch.nn.modules.linear.___torch_mangle_36525.Linear = prim::GetAttr[name="lin1"](%54)
  %835 : Tensor = prim::GetAttr[name="bias"](%834)
  %836 : Tensor = prim::GetAttr[name="weight"](%834)
  %837 : Float(2048:1, 8192:2048) = aten::t(%836), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %output.53 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.9, %837), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %input.90 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.53, %835, %26), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1678:0
  %input.91 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.90), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:1369:0
  %841 : Tensor = prim::GetAttr[name="bias"](%833)
  %842 : Tensor = prim::GetAttr[name="weight"](%833)
  %843 : Float(8192:1, 2048:8192) = aten::t(%842), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.91, %843), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %input.92 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.54, %841, %26), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1678:0
  %846 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.92, %17, %23), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:973:0
  %input.93 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.9, %846, %26), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %848 : Tensor = prim::GetAttr[name="bias"](%52)
  %849 : Tensor = prim::GetAttr[name="weight"](%52)
  %850 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.8
  %tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.93, %850, %849, %848, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.8 # torch/nn/functional.py:2048:0
  %852 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %853 : Float(17:13, 13:1, 1:1) = aten::to(%852, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.94 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.10, %853), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %855 : __torch__.torch.nn.modules.linear.___torch_mangle_36475.Linear = prim::GetAttr[name="out_lin"](%50)
  %856 : __torch__.torch.nn.modules.linear.___torch_mangle_36474.Linear = prim::GetAttr[name="v_lin"](%50)
  %857 : __torch__.torch.nn.modules.linear.___torch_mangle_36473.Linear = prim::GetAttr[name="k_lin"](%50)
  %858 : __torch__.torch.nn.modules.linear.___torch_mangle_36472.Linear = prim::GetAttr[name="q_lin"](%50)
  %859 : int = aten::size(%input.94, %25), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
  %860 : int = aten::size(%input.94, %26), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
  %861 : Tensor = prim::GetAttr[name="bias"](%858)
  %862 : Tensor = prim::GetAttr[name="weight"](%858)
  %863 : Float(2048:1, 2048:2048) = aten::t(%862), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %output.55 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %863), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %x.37 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.55, %861, %26), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1678:0
  %866 : int[] = prim::ListConstruct(%859, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.9
  %867 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.37, %866), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.19 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%867, %26, %22), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %869 : Tensor = prim::GetAttr[name="bias"](%857)
  %870 : Tensor = prim::GetAttr[name="weight"](%857)
  %871 : Float(2048:1, 2048:2048) = aten::t(%870), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %output.56 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %871), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %x.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.56, %869, %26), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1678:0
  %874 : int[] = prim::ListConstruct(%859, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.9
  %875 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.38, %874), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %k.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%875, %26, %22), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %877 : Tensor = prim::GetAttr[name="bias"](%856)
  %878 : Tensor = prim::GetAttr[name="weight"](%856)
  %879 : Float(2048:1, 2048:2048) = aten::t(%878), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %output.57 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %879), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %x.39 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.57, %877, %26), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1678:0
  %882 : int[] = prim::ListConstruct(%859, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.9
  %883 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.39, %882), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %v.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%883, %26, %22), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.20 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.19, %12), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:188:0
  %886 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.10, %22, %13), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
  %scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.20, %886), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
  %888 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.9 # torch/tensor.py:22:0
  %889 : int[] = prim::ListConstruct(%859, %26, %26, %860), scope: __module.transformer/__module.transformer.attentions.9
  %890 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%888, %889), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
  %mask.10 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%890, %scores.19), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
  %scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.19, %mask.10, %14), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:191:0
  %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.20, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:193:0
  %894 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %21, %15), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:1498:0
  %weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%894, %17, %23), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:973:0
  %x.40 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.10, %v.10), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:200:0
  %897 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.40, %26, %22), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %898 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%897, %25), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %899 : int[] = prim::ListConstruct(%859, %21, %18), scope: __module.transformer/__module.transformer.attentions.9
  %input.97 : Float(17:26624, 13:2048, 2048:1) = aten::view(%898, %899), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %901 : Tensor = prim::GetAttr[name="bias"](%855)
  %902 : Tensor = prim::GetAttr[name="weight"](%855)
  %903 : Float(2048:1, 2048:2048) = aten::t(%902), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %output.58 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.97, %903), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %input.98 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.58, %901, %26), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1678:0
  %attn.10 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.98, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.99 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.94, %attn.10, %26), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %908 : Tensor = prim::GetAttr[name="bias"](%48)
  %909 : Tensor = prim::GetAttr[name="weight"](%48)
  %910 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.9
  %input_tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.99, %910, %909, %908, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.9 # torch/nn/functional.py:2048:0
  %912 : __torch__.torch.nn.modules.linear.___torch_mangle_36529.Linear = prim::GetAttr[name="lin2"](%46)
  %913 : __torch__.torch.nn.modules.linear.___torch_mangle_36528.Linear = prim::GetAttr[name="lin1"](%46)
  %914 : Tensor = prim::GetAttr[name="bias"](%913)
  %915 : Tensor = prim::GetAttr[name="weight"](%913)
  %916 : Float(2048:1, 8192:2048) = aten::t(%915), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.10, %916), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %input.100 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.59, %914, %26), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1678:0
  %input.101 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.100), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:1369:0
  %920 : Tensor = prim::GetAttr[name="bias"](%912)
  %921 : Tensor = prim::GetAttr[name="weight"](%912)
  %922 : Float(8192:1, 2048:8192) = aten::t(%921), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %output.60 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.101, %922), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %input.102 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.60, %920, %26), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1678:0
  %925 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.102, %17, %23), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:973:0
  %input.103 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.10, %925, %26), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %927 : Tensor = prim::GetAttr[name="bias"](%44)
  %928 : Tensor = prim::GetAttr[name="weight"](%44)
  %929 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.9
  %tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.103, %929, %928, %927, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.9 # torch/nn/functional.py:2048:0
  %931 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %932 : Float(17:13, 13:1, 1:1) = aten::to(%931, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.104 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.11, %932), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %934 : __torch__.torch.nn.modules.linear.___torch_mangle_36480.Linear = prim::GetAttr[name="out_lin"](%42)
  %935 : __torch__.torch.nn.modules.linear.___torch_mangle_36479.Linear = prim::GetAttr[name="v_lin"](%42)
  %936 : __torch__.torch.nn.modules.linear.___torch_mangle_36478.Linear = prim::GetAttr[name="k_lin"](%42)
  %937 : __torch__.torch.nn.modules.linear.___torch_mangle_36477.Linear = prim::GetAttr[name="q_lin"](%42)
  %938 : int = aten::size(%input.104, %25), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
  %939 : int = aten::size(%input.104, %26), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
  %940 : Tensor = prim::GetAttr[name="bias"](%937)
  %941 : Tensor = prim::GetAttr[name="weight"](%937)
  %942 : Float(2048:1, 2048:2048) = aten::t(%941), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %output.61 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %942), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %x.41 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.61, %940, %26), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1678:0
  %945 : int[] = prim::ListConstruct(%938, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.10
  %946 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.41, %945), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.21 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%946, %26, %22), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %948 : Tensor = prim::GetAttr[name="bias"](%936)
  %949 : Tensor = prim::GetAttr[name="weight"](%936)
  %950 : Float(2048:1, 2048:2048) = aten::t(%949), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %output.62 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %950), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %x.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.62, %948, %26), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1678:0
  %953 : int[] = prim::ListConstruct(%938, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.10
  %954 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.42, %953), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %k.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%954, %26, %22), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %956 : Tensor = prim::GetAttr[name="bias"](%935)
  %957 : Tensor = prim::GetAttr[name="weight"](%935)
  %958 : Float(2048:1, 2048:2048) = aten::t(%957), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %output.63 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %958), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %x.43 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.63, %956, %26), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1678:0
  %961 : int[] = prim::ListConstruct(%938, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.10
  %962 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.43, %961), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %v.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%962, %26, %22), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.22 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.21, %12), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:188:0
  %965 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.11, %22, %13), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
  %scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.22, %965), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
  %967 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.10 # torch/tensor.py:22:0
  %968 : int[] = prim::ListConstruct(%938, %26, %26, %939), scope: __module.transformer/__module.transformer.attentions.10
  %969 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%967, %968), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
  %mask.11 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%969, %scores.21), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
  %scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.21, %mask.11, %14), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:191:0
  %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.22, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:193:0
  %973 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %21, %15), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:1498:0
  %weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%973, %17, %23), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:973:0
  %x.44 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.11, %v.11), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:200:0
  %976 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.44, %26, %22), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %977 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%976, %25), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %978 : int[] = prim::ListConstruct(%938, %21, %18), scope: __module.transformer/__module.transformer.attentions.10
  %input.107 : Float(17:26624, 13:2048, 2048:1) = aten::view(%977, %978), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %980 : Tensor = prim::GetAttr[name="bias"](%934)
  %981 : Tensor = prim::GetAttr[name="weight"](%934)
  %982 : Float(2048:1, 2048:2048) = aten::t(%981), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %output.64 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.107, %982), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %input.108 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.64, %980, %26), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1678:0
  %attn.11 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.108, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.109 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.104, %attn.11, %26), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %987 : Tensor = prim::GetAttr[name="bias"](%40)
  %988 : Tensor = prim::GetAttr[name="weight"](%40)
  %989 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.10
  %input_tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.109, %989, %988, %987, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.10 # torch/nn/functional.py:2048:0
  %991 : __torch__.torch.nn.modules.linear.___torch_mangle_36532.Linear = prim::GetAttr[name="lin2"](%38)
  %992 : __torch__.torch.nn.modules.linear.___torch_mangle_36531.Linear = prim::GetAttr[name="lin1"](%38)
  %993 : Tensor = prim::GetAttr[name="bias"](%992)
  %994 : Tensor = prim::GetAttr[name="weight"](%992)
  %995 : Float(2048:1, 8192:2048) = aten::t(%994), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %output.65 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.11, %995), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %input.110 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.65, %993, %26), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1678:0
  %input.111 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.110), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:1369:0
  %999 : Tensor = prim::GetAttr[name="bias"](%991)
  %1000 : Tensor = prim::GetAttr[name="weight"](%991)
  %1001 : Float(8192:1, 2048:8192) = aten::t(%1000), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %output.66 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.111, %1001), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %input.112 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.66, %999, %26), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1678:0
  %1004 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.112, %17, %23), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:973:0
  %input.113 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.11, %1004, %26), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %1006 : Tensor = prim::GetAttr[name="bias"](%36)
  %1007 : Tensor = prim::GetAttr[name="weight"](%36)
  %1008 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.10
  %tensor.12 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.113, %1008, %1007, %1006, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.10 # torch/nn/functional.py:2048:0
  %1010 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %1011 : Float(17:13, 13:1, 1:1) = aten::to(%1010, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.114 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.12, %1011), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %1013 : __torch__.torch.nn.modules.linear.___torch_mangle_36485.Linear = prim::GetAttr[name="out_lin"](%34)
  %1014 : __torch__.torch.nn.modules.linear.___torch_mangle_36484.Linear = prim::GetAttr[name="v_lin"](%34)
  %1015 : __torch__.torch.nn.modules.linear.___torch_mangle_36483.Linear = prim::GetAttr[name="k_lin"](%34)
  %1016 : __torch__.torch.nn.modules.linear.___torch_mangle_36482.Linear = prim::GetAttr[name="q_lin"](%34)
  %1017 : int = aten::size(%input.114, %25), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
  %1018 : int = aten::size(%input.114, %26), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
  %1019 : Tensor = prim::GetAttr[name="bias"](%1016)
  %1020 : Tensor = prim::GetAttr[name="weight"](%1016)
  %1021 : Float(2048:1, 2048:2048) = aten::t(%1020), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %output.67 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1021), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %x.45 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.67, %1019, %26), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1678:0
  %1024 : int[] = prim::ListConstruct(%1017, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.11
  %1025 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.45, %1024), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %q.23 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1025, %26, %22), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %1027 : Tensor = prim::GetAttr[name="bias"](%1015)
  %1028 : Tensor = prim::GetAttr[name="weight"](%1015)
  %1029 : Float(2048:1, 2048:2048) = aten::t(%1028), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %output.68 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1029), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %x.46 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.68, %1027, %26), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1678:0
  %1032 : int[] = prim::ListConstruct(%1017, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.11
  %1033 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.46, %1032), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %k : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1033, %26, %22), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %1035 : Tensor = prim::GetAttr[name="bias"](%1014)
  %1036 : Tensor = prim::GetAttr[name="weight"](%1014)
  %1037 : Float(2048:1, 2048:2048) = aten::t(%1036), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %output.69 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1037), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %x.47 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.69, %1035, %26), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1678:0
  %1040 : int[] = prim::ListConstruct(%1017, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.11
  %1041 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.47, %1040), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %v : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1041, %26, %22), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %q : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.23, %12), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:188:0
  %1044 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k, %22, %13), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
  %scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q, %1044), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
  %1046 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.11 # torch/tensor.py:22:0
  %1047 : int[] = prim::ListConstruct(%1017, %26, %26, %1018), scope: __module.transformer/__module.transformer.attentions.11
  %1048 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%1046, %1047), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
  %mask : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%1048, %scores.23), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
  %scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.23, %mask, %14), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:191:0
  %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:193:0
  %1052 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %21, %15), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:1498:0
  %weights : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%1052, %17, %23), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:973:0
  %x : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights, %v), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:200:0
  %1055 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x, %26, %22), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1056 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%1055, %25), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1057 : int[] = prim::ListConstruct(%1017, %21, %18), scope: __module.transformer/__module.transformer.attentions.11
  %input.117 : Float(17:26624, 13:2048, 2048:1) = aten::view(%1056, %1057), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1059 : Tensor = prim::GetAttr[name="bias"](%1013)
  %1060 : Tensor = prim::GetAttr[name="weight"](%1013)
  %1061 : Float(2048:1, 2048:2048) = aten::t(%1060), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %output.70 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.117, %1061), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %input.118 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.70, %1059, %26), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1678:0
  %attn : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.118, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.119 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.114, %attn, %26), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %1066 : Tensor = prim::GetAttr[name="bias"](%32)
  %1067 : Tensor = prim::GetAttr[name="weight"](%32)
  %1068 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.11
  %input_tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.119, %1068, %1067, %1066, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.11 # torch/nn/functional.py:2048:0
  %1070 : __torch__.torch.nn.modules.linear.___torch_mangle_36535.Linear = prim::GetAttr[name="lin2"](%30)
  %1071 : __torch__.torch.nn.modules.linear.___torch_mangle_36534.Linear = prim::GetAttr[name="lin1"](%30)
  %1072 : Tensor = prim::GetAttr[name="bias"](%1071)
  %1073 : Tensor = prim::GetAttr[name="weight"](%1071)
  %1074 : Float(2048:1, 8192:2048) = aten::t(%1073), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %output.71 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor, %1074), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %input.120 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.71, %1072, %26), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1678:0
  %input.121 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.120), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:1369:0
  %1078 : Tensor = prim::GetAttr[name="bias"](%1070)
  %1079 : Tensor = prim::GetAttr[name="weight"](%1070)
  %1080 : Float(8192:1, 2048:8192) = aten::t(%1079), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %output.72 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.121, %1080), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %input.122 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.72, %1078, %26), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1678:0
  %1083 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.122, %17, %23), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:973:0
  %input.123 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor, %1083, %26), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %1085 : Tensor = prim::GetAttr[name="bias"](%28)
  %1086 : Tensor = prim::GetAttr[name="weight"](%28)
  %1087 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.11
  %tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.123, %1087, %1086, %1085, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.11 # torch/nn/functional.py:2048:0
  %1089 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %1090 : Float(17:13, 13:1, 1:1) = aten::to(%1089, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.124 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor, %1090), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %1092 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %1093 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %input : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.124, %1093, %1092), scope: __module.dropout # torch/nn/functional.py:973:0
  %1095 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1678:0
  %1096 : Tensor = prim::GetAttr[name="bias"](%3)
  %1097 : Tensor = prim::GetAttr[name="weight"](%3)
  %1098 : Float(2048:1, 2:2048) = aten::t(%1097), scope: __module.classifier # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %1098), scope: __module.classifier # torch/nn/functional.py:1676:0
  %1100 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %1096, %1095), scope: __module.classifier # torch/nn/functional.py:1678:0
  %9 : (Float(17:26, 13:2, 2:1)) = prim::TupleConstruct(%1100)
  return (%9)
