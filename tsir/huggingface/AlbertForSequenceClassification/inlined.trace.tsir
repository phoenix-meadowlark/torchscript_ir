graph(%self.1 : __torch__.transformers.modeling_albert.AlbertForSequenceClassification,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_254.Linear = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.torch.nn.modules.dropout.___torch_mangle_253.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %5 : __torch__.transformers.modeling_albert.___torch_mangle_252.AlbertModel = prim::GetAttr[name="albert"](%self.1)
  %10 : str = prim::Constant[value="bfnd,ndh->bfh"](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %11 : int = prim::Constant[value=4096](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %12 : Double() = prim::Constant[value={8}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %13 : int = prim::Constant[value=-2](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %14 : int = prim::Constant[value=3](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %15 : int = prim::Constant[value=64](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %16 : Double() = prim::Constant[value={0.5}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %17 : float = prim::Constant[value=3.](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %18 : Double() = prim::Constant[value={0.044715}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %19 : Double() = prim::Constant[value={0.797885}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %20 : Double() = prim::Constant[value={1}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %21 : int = prim::Constant[value=9223372036854775807](), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %22 : int = prim::Constant[value=-1](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %23 : bool = prim::Constant[value=1](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %24 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %25 : int = prim::Constant[value=128](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %26 : float = prim::Constant[value=0.](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.dropout # torch/nn/functional.py:973:0
  %27 : Double() = prim::Constant[value={-10000}](), scope: __module.albert # transformers/modeling_albert.py:678:0
  %28 : float = prim::Constant[value=1.](), scope: __module.albert # torch/tensor.py:396:0
  %29 : None = prim::Constant(), scope: __module.albert
  %30 : int = prim::Constant[value=6](), scope: __module.albert # transformers/modeling_albert.py:677:0
  %31 : int = prim::Constant[value=2](), scope: __module.albert # transformers/modeling_albert.py:676:0
  %32 : bool = prim::Constant[value=0](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %33 : Device = prim::Constant[value="cpu"](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %34 : int = prim::Constant[value=4](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %35 : int = prim::Constant[value=1](), scope: __module.albert # transformers/modeling_albert.py:663:0
  %36 : int = prim::Constant[value=0](), scope: __module.albert # transformers/modeling_albert.py:663:0
  %37 : __torch__.torch.nn.modules.linear.___torch_mangle_250.Linear = prim::GetAttr[name="pooler"](%5)
  %38 : __torch__.transformers.modeling_albert.___torch_mangle_249.AlbertTransformer = prim::GetAttr[name="encoder"](%5)
  %39 : __torch__.transformers.modeling_albert.___torch_mangle_231.AlbertEmbeddings = prim::GetAttr[name="embeddings"](%5)
  %40 : int = aten::size(%input_ids, %36), scope: __module.albert # transformers/modeling_albert.py:663:0
  %41 : int = aten::size(%input_ids, %35), scope: __module.albert # transformers/modeling_albert.py:663:0
  %42 : int[] = prim::ListConstruct(%40, %41), scope: __module.albert
  %input.2 : Long(17:13, 13:1) = aten::zeros(%42, %34, %36, %33, %32), scope: __module.albert # transformers/modeling_albert.py:674:0
  %44 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%attention_mask.1, %35), scope: __module.albert # transformers/modeling_albert.py:676:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%44, %31), scope: __module.albert # transformers/modeling_albert.py:676:0
  %46 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %30, %32, %32, %29), scope: __module.albert # transformers/modeling_albert.py:677:0
  %47 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%46, %28, %35), scope: __module.albert # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%47, %27), scope: __module.albert # transformers/modeling_albert.py:678:0
  %49 : __torch__.torch.nn.modules.normalization.___torch_mangle_229.LayerNorm = prim::GetAttr[name="LayerNorm"](%39)
  %50 : __torch__.torch.nn.modules.sparse.___torch_mangle_228.Embedding = prim::GetAttr[name="token_type_embeddings"](%39)
  %51 : __torch__.torch.nn.modules.sparse.___torch_mangle_227.Embedding = prim::GetAttr[name="position_embeddings"](%39)
  %52 : __torch__.torch.nn.modules.sparse.___torch_mangle_226.Embedding = prim::GetAttr[name="word_embeddings"](%39)
  %53 : Tensor = prim::GetAttr[name="position_ids"](%39)
  %54 : int = aten::size(%input_ids, %35), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:222:0
  %55 : Long(1:512, 512:1) = aten::slice(%53, %36, %36, %21, %35), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%55, %35, %36, %54, %35), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %57 : Tensor = prim::GetAttr[name="weight"](%52)
  %inputs_embeds : Float(17:1664, 13:128, 128:1) = aten::embedding(%57, %input_ids, %36, %32, %32), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %59 : Tensor = prim::GetAttr[name="weight"](%51)
  %position_embeddings : Float(1:1664, 13:128, 128:1) = aten::embedding(%59, %input.1, %22, %32, %32), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %61 : Tensor = prim::GetAttr[name="weight"](%50)
  %token_type_embeddings : Float(17:1664, 13:128, 128:1) = aten::embedding(%61, %input.2, %22, %32, %32), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %63 : Float(17:1664, 13:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %35), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:239:0
  %input.3 : Float(17:1664, 13:128, 128:1) = aten::add(%63, %token_type_embeddings, %35), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:239:0
  %65 : Tensor = prim::GetAttr[name="bias"](%49)
  %66 : Tensor = prim::GetAttr[name="weight"](%49)
  %67 : int[] = prim::ListConstruct(%25), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm
  %input.4 : Float(17:1664, 13:128, 128:1) = aten::layer_norm(%input.3, %67, %66, %65, %24, %23), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:1664, 13:128, 128:1) = aten::dropout(%input.4, %26, %32), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.dropout # torch/nn/functional.py:973:0
  %70 : __torch__.torch.nn.modules.container.___torch_mangle_248.ModuleList = prim::GetAttr[name="albert_layer_groups"](%38)
  %71 : __torch__.transformers.modeling_albert.___torch_mangle_247.AlbertLayerGroup = prim::GetAttr[name="0"](%70)
  %72 : __torch__.torch.nn.modules.linear.___torch_mangle_232.Linear = prim::GetAttr[name="embedding_hidden_mapping_in"](%38)
  %73 : Tensor = prim::GetAttr[name="bias"](%72)
  %74 : Tensor = prim::GetAttr[name="weight"](%72)
  %75 : Float(128:1, 4096:128) = aten::t(%74), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1676:0
  %output.1 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.5, %75), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1676:0
  %input.6 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.1, %73, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1678:0
  %78 : __torch__.torch.nn.modules.container.___torch_mangle_246.ModuleList = prim::GetAttr[name="albert_layers"](%71)
  %79 : __torch__.transformers.modeling_albert.___torch_mangle_245.AlbertLayer = prim::GetAttr[name="0"](%78)
  %80 : __torch__.torch.nn.modules.normalization.___torch_mangle_233.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%79)
  %81 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="ffn_output"](%79)
  %82 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="ffn"](%79)
  %83 : __torch__.transformers.modeling_albert.___torch_mangle_241.AlbertAttention = prim::GetAttr[name="attention"](%79)
  %84 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="LayerNorm"](%83)
  %85 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%83)
  %86 : Tensor = prim::GetAttr[name="bias"](%85)
  %87 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%83)
  %88 : Tensor = prim::GetAttr[name="weight"](%87)
  %89 : __torch__.torch.nn.modules.linear.___torch_mangle_236.Linear = prim::GetAttr[name="value"](%83)
  %90 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="key"](%83)
  %91 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="query"](%83)
  %92 : Tensor = prim::GetAttr[name="bias"](%91)
  %93 : Tensor = prim::GetAttr[name="weight"](%91)
  %94 : Float(4096:1, 4096:4096) = aten::t(%93), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.2 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %94), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.2, %92, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %97 : Tensor = prim::GetAttr[name="bias"](%90)
  %98 : Tensor = prim::GetAttr[name="weight"](%90)
  %99 : Float(4096:1, 4096:4096) = aten::t(%98), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.3 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %99), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.3, %97, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %102 : Tensor = prim::GetAttr[name="bias"](%89)
  %103 : Tensor = prim::GetAttr[name="weight"](%89)
  %104 : Float(4096:1, 4096:4096) = aten::t(%103), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.4 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %104), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.4, %102, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %107 : int = aten::size(%x.1, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %108 : int = aten::size(%x.1, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %109 : int[] = prim::ListConstruct(%107, %108, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.2 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.1, %109), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %111 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.2, %111), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %113 : int = aten::size(%x.3, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %114 : int = aten::size(%x.3, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %115 : int[] = prim::ListConstruct(%113, %114, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.4 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.3, %115), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %117 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.4, %117), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %119 : int = aten::size(%x.5, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %120 : int = aten::size(%x.5, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %121 : int[] = prim::ListConstruct(%119, %120, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.6 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.5, %121), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %123 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.6, %123), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %125 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.1, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.1 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %125), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.2 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.1, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.7, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.8, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %132 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %133 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.1, %132), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %134 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%133, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %135 : Float(4096:1, 4096:4096) = aten::t(%88), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %136 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %137 : Float(64:64, 64:1, 4096:4096) = aten::view(%135, %136), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %138 : Float(64:64, 64:1, 4096:4096) = aten::to(%137, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.1 : Float(4096:1) = aten::to(%86, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %140 : Tensor[] = prim::ListConstruct(%134, %138), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %141 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %140), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.9 : Float(17:53248, 13:4096, 4096:1) = aten::add(%141, %b.1, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.1 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.9, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.10 : Float(17:53248, 13:4096, 4096:1) = aten::add(%input.6, %projected_context_layer.1, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %145 : Tensor = prim::GetAttr[name="bias"](%84)
  %146 : Tensor = prim::GetAttr[name="weight"](%84)
  %147 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.1 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.10, %147, %146, %145, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %149 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.1, %b.1)
  %150 : Float(17:53248, 13:4096, 4096:1), %151 : Float(4096:1) = prim::TupleUnpack(%149)
  %152 : Tensor = prim::GetAttr[name="bias"](%82)
  %153 : Tensor = prim::GetAttr[name="weight"](%82)
  %154 : Float(4096:1, 16384:4096) = aten::t(%153), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.5 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%150, %154), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.7 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.5, %152, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %157 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.7, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %158 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.7, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %159 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%158, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %160 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.7, %159, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %161 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%160, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %162 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%161), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %163 : Float(17:212992, 13:16384, 16384:1) = aten::add(%162, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.11 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%157, %163), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %165 : Tensor = prim::GetAttr[name="bias"](%81)
  %166 : Tensor = prim::GetAttr[name="weight"](%81)
  %167 : Float(16384:1, 4096:16384) = aten::t(%166), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.6 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.11, %167), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.1 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.6, %165, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.12 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.1, %150, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %171 : Tensor = prim::GetAttr[name="bias"](%80)
  %172 : Tensor = prim::GetAttr[name="weight"](%80)
  %173 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.13 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.12, %173, %172, %171, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %175 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.13, %151)
  %176 : Float(17:53248, 13:4096, 4096:1), %177 : Float(4096:1) = prim::TupleUnpack(%175)
  %178 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%176, %177)
  %179 : Float(17:53248, 13:4096, 4096:1), %180 : Float(4096:1) = prim::TupleUnpack(%178)
  %181 : __torch__.torch.nn.modules.container.___torch_mangle_246.ModuleList = prim::GetAttr[name="albert_layers"](%71)
  %182 : __torch__.transformers.modeling_albert.___torch_mangle_245.AlbertLayer = prim::GetAttr[name="0"](%181)
  %183 : __torch__.torch.nn.modules.normalization.___torch_mangle_233.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%182)
  %184 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="ffn_output"](%182)
  %185 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="ffn"](%182)
  %186 : __torch__.transformers.modeling_albert.___torch_mangle_241.AlbertAttention = prim::GetAttr[name="attention"](%182)
  %187 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="LayerNorm"](%186)
  %188 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%186)
  %189 : Tensor = prim::GetAttr[name="weight"](%188)
  %190 : __torch__.torch.nn.modules.linear.___torch_mangle_236.Linear = prim::GetAttr[name="value"](%186)
  %191 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="key"](%186)
  %192 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="query"](%186)
  %193 : Tensor = prim::GetAttr[name="bias"](%192)
  %194 : Tensor = prim::GetAttr[name="weight"](%192)
  %195 : Float(4096:1, 4096:4096) = aten::t(%194), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%179, %195), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.8 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.7, %193, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %198 : Tensor = prim::GetAttr[name="bias"](%191)
  %199 : Tensor = prim::GetAttr[name="weight"](%191)
  %200 : Float(4096:1, 4096:4096) = aten::t(%199), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%179, %200), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.8, %198, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %203 : Tensor = prim::GetAttr[name="bias"](%190)
  %204 : Tensor = prim::GetAttr[name="weight"](%190)
  %205 : Float(4096:1, 4096:4096) = aten::t(%204), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%179, %205), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.12 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.9, %203, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %208 : int = aten::size(%x.8, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %209 : int = aten::size(%x.8, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %210 : int[] = prim::ListConstruct(%208, %209, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.9 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.8, %210), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %212 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.9, %212), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %214 : int = aten::size(%x.10, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %215 : int = aten::size(%x.10, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %216 : int[] = prim::ListConstruct(%214, %215, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.11 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.10, %216), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %218 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.11, %218), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %220 : int = aten::size(%x.12, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %221 : int = aten::size(%x.12, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %222 : int[] = prim::ListConstruct(%220, %221, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.13 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.12, %222), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %224 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.13, %224), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %226 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.2, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.3 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %226), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.4 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.3, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.14 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.15 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.14, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.15, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.2 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %233 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %234 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.2, %233), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %235 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%234, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %236 : Float(4096:1, 4096:4096) = aten::t(%189), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %237 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %238 : Float(64:64, 64:1, 4096:4096) = aten::view(%236, %237), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %239 : Float(64:64, 64:1, 4096:4096) = aten::to(%238, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.2 : Float(4096:1) = aten::to(%180, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %241 : Tensor[] = prim::ListConstruct(%235, %239), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %242 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %241), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.16 : Float(17:53248, 13:4096, 4096:1) = aten::add(%242, %b.2, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.2 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.16, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.17 : Float(17:53248, 13:4096, 4096:1) = aten::add(%179, %projected_context_layer.2, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %246 : Tensor = prim::GetAttr[name="bias"](%187)
  %247 : Tensor = prim::GetAttr[name="weight"](%187)
  %248 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.2 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.17, %248, %247, %246, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %250 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.2, %b.2)
  %251 : Float(17:53248, 13:4096, 4096:1), %252 : Float(4096:1) = prim::TupleUnpack(%250)
  %253 : Tensor = prim::GetAttr[name="bias"](%185)
  %254 : Tensor = prim::GetAttr[name="weight"](%185)
  %255 : Float(4096:1, 16384:4096) = aten::t(%254), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.10 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%251, %255), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.14 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.10, %253, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %258 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.14, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %259 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.14, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %260 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%259, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %261 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.14, %260, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %262 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%261, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %263 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%262), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %264 : Float(17:212992, 13:16384, 16384:1) = aten::add(%263, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.18 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%258, %264), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %266 : Tensor = prim::GetAttr[name="bias"](%184)
  %267 : Tensor = prim::GetAttr[name="weight"](%184)
  %268 : Float(16384:1, 4096:16384) = aten::t(%267), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.11 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.18, %268), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.2 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.11, %266, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.19 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.2, %251, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %272 : Tensor = prim::GetAttr[name="bias"](%183)
  %273 : Tensor = prim::GetAttr[name="weight"](%183)
  %274 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.20 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.19, %274, %273, %272, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %276 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.20, %252)
  %277 : Float(17:53248, 13:4096, 4096:1), %278 : Float(4096:1) = prim::TupleUnpack(%276)
  %279 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%277, %278)
  %280 : Float(17:53248, 13:4096, 4096:1), %281 : Float(4096:1) = prim::TupleUnpack(%279)
  %282 : __torch__.torch.nn.modules.container.___torch_mangle_246.ModuleList = prim::GetAttr[name="albert_layers"](%71)
  %283 : __torch__.transformers.modeling_albert.___torch_mangle_245.AlbertLayer = prim::GetAttr[name="0"](%282)
  %284 : __torch__.torch.nn.modules.normalization.___torch_mangle_233.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%283)
  %285 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="ffn_output"](%283)
  %286 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="ffn"](%283)
  %287 : __torch__.transformers.modeling_albert.___torch_mangle_241.AlbertAttention = prim::GetAttr[name="attention"](%283)
  %288 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="LayerNorm"](%287)
  %289 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%287)
  %290 : Tensor = prim::GetAttr[name="weight"](%289)
  %291 : __torch__.torch.nn.modules.linear.___torch_mangle_236.Linear = prim::GetAttr[name="value"](%287)
  %292 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="key"](%287)
  %293 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="query"](%287)
  %294 : Tensor = prim::GetAttr[name="bias"](%293)
  %295 : Tensor = prim::GetAttr[name="weight"](%293)
  %296 : Float(4096:1, 4096:4096) = aten::t(%295), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.12 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%280, %296), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.15 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.12, %294, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %299 : Tensor = prim::GetAttr[name="bias"](%292)
  %300 : Tensor = prim::GetAttr[name="weight"](%292)
  %301 : Float(4096:1, 4096:4096) = aten::t(%300), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.13 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%280, %301), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.17 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.13, %299, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %304 : Tensor = prim::GetAttr[name="bias"](%291)
  %305 : Tensor = prim::GetAttr[name="weight"](%291)
  %306 : Float(4096:1, 4096:4096) = aten::t(%305), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.14 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%280, %306), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.19 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.14, %304, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %309 : int = aten::size(%x.15, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %310 : int = aten::size(%x.15, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %311 : int[] = prim::ListConstruct(%309, %310, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.16 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.15, %311), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %313 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.16, %313), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %315 : int = aten::size(%x.17, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %316 : int = aten::size(%x.17, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %317 : int[] = prim::ListConstruct(%315, %316, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.18 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.17, %317), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %319 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.18, %319), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %321 : int = aten::size(%x.19, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %322 : int = aten::size(%x.19, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %323 : int[] = prim::ListConstruct(%321, %322, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.20 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.19, %323), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %325 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.20, %325), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %327 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.3, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.5 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %327), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.6 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.5, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.21 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.22 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.21, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.22, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %334 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %335 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.3, %334), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %336 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%335, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %337 : Float(4096:1, 4096:4096) = aten::t(%290), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %338 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %339 : Float(64:64, 64:1, 4096:4096) = aten::view(%337, %338), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %340 : Float(64:64, 64:1, 4096:4096) = aten::to(%339, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.3 : Float(4096:1) = aten::to(%281, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %342 : Tensor[] = prim::ListConstruct(%336, %340), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %343 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %342), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.23 : Float(17:53248, 13:4096, 4096:1) = aten::add(%343, %b.3, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.3 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.23, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.24 : Float(17:53248, 13:4096, 4096:1) = aten::add(%280, %projected_context_layer.3, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %347 : Tensor = prim::GetAttr[name="bias"](%288)
  %348 : Tensor = prim::GetAttr[name="weight"](%288)
  %349 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.3 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.24, %349, %348, %347, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %351 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.3, %b.3)
  %352 : Float(17:53248, 13:4096, 4096:1), %353 : Float(4096:1) = prim::TupleUnpack(%351)
  %354 : Tensor = prim::GetAttr[name="bias"](%286)
  %355 : Tensor = prim::GetAttr[name="weight"](%286)
  %356 : Float(4096:1, 16384:4096) = aten::t(%355), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.15 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%352, %356), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.21 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.15, %354, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %359 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.21, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %360 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.21, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %361 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%360, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %362 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.21, %361, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %363 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%362, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %364 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%363), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %365 : Float(17:212992, 13:16384, 16384:1) = aten::add(%364, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.25 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%359, %365), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %367 : Tensor = prim::GetAttr[name="bias"](%285)
  %368 : Tensor = prim::GetAttr[name="weight"](%285)
  %369 : Float(16384:1, 4096:16384) = aten::t(%368), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.16 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.25, %369), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.3 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.16, %367, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.26 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.3, %352, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %373 : Tensor = prim::GetAttr[name="bias"](%284)
  %374 : Tensor = prim::GetAttr[name="weight"](%284)
  %375 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.27 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.26, %375, %374, %373, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %377 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.27, %353)
  %378 : Float(17:53248, 13:4096, 4096:1), %379 : Float(4096:1) = prim::TupleUnpack(%377)
  %380 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%378, %379)
  %381 : Float(17:53248, 13:4096, 4096:1), %382 : Float(4096:1) = prim::TupleUnpack(%380)
  %383 : __torch__.torch.nn.modules.container.___torch_mangle_246.ModuleList = prim::GetAttr[name="albert_layers"](%71)
  %384 : __torch__.transformers.modeling_albert.___torch_mangle_245.AlbertLayer = prim::GetAttr[name="0"](%383)
  %385 : __torch__.torch.nn.modules.normalization.___torch_mangle_233.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%384)
  %386 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="ffn_output"](%384)
  %387 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="ffn"](%384)
  %388 : __torch__.transformers.modeling_albert.___torch_mangle_241.AlbertAttention = prim::GetAttr[name="attention"](%384)
  %389 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="LayerNorm"](%388)
  %390 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%388)
  %391 : Tensor = prim::GetAttr[name="weight"](%390)
  %392 : __torch__.torch.nn.modules.linear.___torch_mangle_236.Linear = prim::GetAttr[name="value"](%388)
  %393 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="key"](%388)
  %394 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="query"](%388)
  %395 : Tensor = prim::GetAttr[name="bias"](%394)
  %396 : Tensor = prim::GetAttr[name="weight"](%394)
  %397 : Float(4096:1, 4096:4096) = aten::t(%396), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.17 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%381, %397), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.22 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.17, %395, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %400 : Tensor = prim::GetAttr[name="bias"](%393)
  %401 : Tensor = prim::GetAttr[name="weight"](%393)
  %402 : Float(4096:1, 4096:4096) = aten::t(%401), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.18 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%381, %402), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.24 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.18, %400, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %405 : Tensor = prim::GetAttr[name="bias"](%392)
  %406 : Tensor = prim::GetAttr[name="weight"](%392)
  %407 : Float(4096:1, 4096:4096) = aten::t(%406), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.19 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%381, %407), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.26 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.19, %405, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %410 : int = aten::size(%x.22, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %411 : int = aten::size(%x.22, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %412 : int[] = prim::ListConstruct(%410, %411, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.23 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.22, %412), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %414 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.23, %414), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %416 : int = aten::size(%x.24, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %417 : int = aten::size(%x.24, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %418 : int[] = prim::ListConstruct(%416, %417, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.25 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.24, %418), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %420 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.25, %420), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %422 : int = aten::size(%x.26, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %423 : int = aten::size(%x.26, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %424 : int[] = prim::ListConstruct(%422, %423, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.27 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.26, %424), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %426 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.27, %426), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %428 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.4, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %428), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.7, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.28 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.29 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.28, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.29, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.4 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %435 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %436 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.4, %435), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %437 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%436, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %438 : Float(4096:1, 4096:4096) = aten::t(%391), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %439 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %440 : Float(64:64, 64:1, 4096:4096) = aten::view(%438, %439), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %441 : Float(64:64, 64:1, 4096:4096) = aten::to(%440, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.4 : Float(4096:1) = aten::to(%382, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %443 : Tensor[] = prim::ListConstruct(%437, %441), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %444 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %443), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.30 : Float(17:53248, 13:4096, 4096:1) = aten::add(%444, %b.4, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.4 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.30, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.31 : Float(17:53248, 13:4096, 4096:1) = aten::add(%381, %projected_context_layer.4, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %448 : Tensor = prim::GetAttr[name="bias"](%389)
  %449 : Tensor = prim::GetAttr[name="weight"](%389)
  %450 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.4 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.31, %450, %449, %448, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %452 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.4, %b.4)
  %453 : Float(17:53248, 13:4096, 4096:1), %454 : Float(4096:1) = prim::TupleUnpack(%452)
  %455 : Tensor = prim::GetAttr[name="bias"](%387)
  %456 : Tensor = prim::GetAttr[name="weight"](%387)
  %457 : Float(4096:1, 16384:4096) = aten::t(%456), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.20 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%453, %457), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.28 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.20, %455, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %460 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.28, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %461 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.28, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %462 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%461, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %463 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.28, %462, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %464 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%463, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %465 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%464), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %466 : Float(17:212992, 13:16384, 16384:1) = aten::add(%465, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.32 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%460, %466), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %468 : Tensor = prim::GetAttr[name="bias"](%386)
  %469 : Tensor = prim::GetAttr[name="weight"](%386)
  %470 : Float(16384:1, 4096:16384) = aten::t(%469), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.21 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.32, %470), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.4 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.21, %468, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.33 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.4, %453, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %474 : Tensor = prim::GetAttr[name="bias"](%385)
  %475 : Tensor = prim::GetAttr[name="weight"](%385)
  %476 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.34 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.33, %476, %475, %474, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %478 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.34, %454)
  %479 : Float(17:53248, 13:4096, 4096:1), %480 : Float(4096:1) = prim::TupleUnpack(%478)
  %481 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%479, %480)
  %482 : Float(17:53248, 13:4096, 4096:1), %483 : Float(4096:1) = prim::TupleUnpack(%481)
  %484 : __torch__.torch.nn.modules.container.___torch_mangle_246.ModuleList = prim::GetAttr[name="albert_layers"](%71)
  %485 : __torch__.transformers.modeling_albert.___torch_mangle_245.AlbertLayer = prim::GetAttr[name="0"](%484)
  %486 : __torch__.torch.nn.modules.normalization.___torch_mangle_233.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%485)
  %487 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="ffn_output"](%485)
  %488 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="ffn"](%485)
  %489 : __torch__.transformers.modeling_albert.___torch_mangle_241.AlbertAttention = prim::GetAttr[name="attention"](%485)
  %490 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="LayerNorm"](%489)
  %491 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%489)
  %492 : Tensor = prim::GetAttr[name="weight"](%491)
  %493 : __torch__.torch.nn.modules.linear.___torch_mangle_236.Linear = prim::GetAttr[name="value"](%489)
  %494 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="key"](%489)
  %495 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="query"](%489)
  %496 : Tensor = prim::GetAttr[name="bias"](%495)
  %497 : Tensor = prim::GetAttr[name="weight"](%495)
  %498 : Float(4096:1, 4096:4096) = aten::t(%497), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.22 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%482, %498), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.29 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.22, %496, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %501 : Tensor = prim::GetAttr[name="bias"](%494)
  %502 : Tensor = prim::GetAttr[name="weight"](%494)
  %503 : Float(4096:1, 4096:4096) = aten::t(%502), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.23 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%482, %503), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.31 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.23, %501, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %506 : Tensor = prim::GetAttr[name="bias"](%493)
  %507 : Tensor = prim::GetAttr[name="weight"](%493)
  %508 : Float(4096:1, 4096:4096) = aten::t(%507), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.24 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%482, %508), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.33 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.24, %506, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %511 : int = aten::size(%x.29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %512 : int = aten::size(%x.29, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %513 : int[] = prim::ListConstruct(%511, %512, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.30 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.29, %513), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %515 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.30, %515), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %517 : int = aten::size(%x.31, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %518 : int = aten::size(%x.31, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %519 : int[] = prim::ListConstruct(%517, %518, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.32 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.31, %519), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %521 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.32, %521), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %523 : int = aten::size(%x.33, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %524 : int = aten::size(%x.33, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %525 : int[] = prim::ListConstruct(%523, %524, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.34 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.33, %525), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %527 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.34, %527), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %529 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.5, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.9 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %529), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.10 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.9, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.35 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.36 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.35, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.36, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %536 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %537 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.5, %536), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %538 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%537, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %539 : Float(4096:1, 4096:4096) = aten::t(%492), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %540 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %541 : Float(64:64, 64:1, 4096:4096) = aten::view(%539, %540), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %542 : Float(64:64, 64:1, 4096:4096) = aten::to(%541, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.5 : Float(4096:1) = aten::to(%483, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %544 : Tensor[] = prim::ListConstruct(%538, %542), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %545 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %544), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.37 : Float(17:53248, 13:4096, 4096:1) = aten::add(%545, %b.5, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.5 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.37, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.38 : Float(17:53248, 13:4096, 4096:1) = aten::add(%482, %projected_context_layer.5, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %549 : Tensor = prim::GetAttr[name="bias"](%490)
  %550 : Tensor = prim::GetAttr[name="weight"](%490)
  %551 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.5 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.38, %551, %550, %549, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %553 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.5, %b.5)
  %554 : Float(17:53248, 13:4096, 4096:1), %555 : Float(4096:1) = prim::TupleUnpack(%553)
  %556 : Tensor = prim::GetAttr[name="bias"](%488)
  %557 : Tensor = prim::GetAttr[name="weight"](%488)
  %558 : Float(4096:1, 16384:4096) = aten::t(%557), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.25 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%554, %558), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.35 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.25, %556, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %561 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.35, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %562 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.35, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %563 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%562, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %564 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.35, %563, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %565 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%564, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %566 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%565), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %567 : Float(17:212992, 13:16384, 16384:1) = aten::add(%566, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.39 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%561, %567), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %569 : Tensor = prim::GetAttr[name="bias"](%487)
  %570 : Tensor = prim::GetAttr[name="weight"](%487)
  %571 : Float(16384:1, 4096:16384) = aten::t(%570), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.26 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.39, %571), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.5 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.26, %569, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.40 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.5, %554, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %575 : Tensor = prim::GetAttr[name="bias"](%486)
  %576 : Tensor = prim::GetAttr[name="weight"](%486)
  %577 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.41 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.40, %577, %576, %575, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %579 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.41, %555)
  %580 : Float(17:53248, 13:4096, 4096:1), %581 : Float(4096:1) = prim::TupleUnpack(%579)
  %582 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%580, %581)
  %583 : Float(17:53248, 13:4096, 4096:1), %584 : Float(4096:1) = prim::TupleUnpack(%582)
  %585 : __torch__.torch.nn.modules.container.___torch_mangle_246.ModuleList = prim::GetAttr[name="albert_layers"](%71)
  %586 : __torch__.transformers.modeling_albert.___torch_mangle_245.AlbertLayer = prim::GetAttr[name="0"](%585)
  %587 : __torch__.torch.nn.modules.normalization.___torch_mangle_233.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%586)
  %588 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="ffn_output"](%586)
  %589 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="ffn"](%586)
  %590 : __torch__.transformers.modeling_albert.___torch_mangle_241.AlbertAttention = prim::GetAttr[name="attention"](%586)
  %591 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="LayerNorm"](%590)
  %592 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%590)
  %593 : Tensor = prim::GetAttr[name="weight"](%592)
  %594 : __torch__.torch.nn.modules.linear.___torch_mangle_236.Linear = prim::GetAttr[name="value"](%590)
  %595 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="key"](%590)
  %596 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="query"](%590)
  %597 : Tensor = prim::GetAttr[name="bias"](%596)
  %598 : Tensor = prim::GetAttr[name="weight"](%596)
  %599 : Float(4096:1, 4096:4096) = aten::t(%598), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.27 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%583, %599), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.36 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.27, %597, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %602 : Tensor = prim::GetAttr[name="bias"](%595)
  %603 : Tensor = prim::GetAttr[name="weight"](%595)
  %604 : Float(4096:1, 4096:4096) = aten::t(%603), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.28 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%583, %604), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.38 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.28, %602, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %607 : Tensor = prim::GetAttr[name="bias"](%594)
  %608 : Tensor = prim::GetAttr[name="weight"](%594)
  %609 : Float(4096:1, 4096:4096) = aten::t(%608), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.29 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%583, %609), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.40 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.29, %607, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %612 : int = aten::size(%x.36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %613 : int = aten::size(%x.36, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %614 : int[] = prim::ListConstruct(%612, %613, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.37 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.36, %614), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %616 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.37, %616), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %618 : int = aten::size(%x.38, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %619 : int = aten::size(%x.38, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %620 : int[] = prim::ListConstruct(%618, %619, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.39 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.38, %620), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %622 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.39, %622), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %624 : int = aten::size(%x.40, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %625 : int = aten::size(%x.40, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %626 : int[] = prim::ListConstruct(%624, %625, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.41 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.40, %626), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %628 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.41, %628), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %630 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.6, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.11 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %630), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.12 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.11, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.42 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.43 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.42, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.43, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.6 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %637 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %638 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.6, %637), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %639 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%638, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %640 : Float(4096:1, 4096:4096) = aten::t(%593), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %641 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %642 : Float(64:64, 64:1, 4096:4096) = aten::view(%640, %641), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %643 : Float(64:64, 64:1, 4096:4096) = aten::to(%642, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.6 : Float(4096:1) = aten::to(%584, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %645 : Tensor[] = prim::ListConstruct(%639, %643), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %646 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %645), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.44 : Float(17:53248, 13:4096, 4096:1) = aten::add(%646, %b.6, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.6 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.44, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:53248, 13:4096, 4096:1) = aten::add(%583, %projected_context_layer.6, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %650 : Tensor = prim::GetAttr[name="bias"](%591)
  %651 : Tensor = prim::GetAttr[name="weight"](%591)
  %652 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.6 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.45, %652, %651, %650, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %654 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.6, %b.6)
  %655 : Float(17:53248, 13:4096, 4096:1), %656 : Float(4096:1) = prim::TupleUnpack(%654)
  %657 : Tensor = prim::GetAttr[name="bias"](%589)
  %658 : Tensor = prim::GetAttr[name="weight"](%589)
  %659 : Float(4096:1, 16384:4096) = aten::t(%658), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.30 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%655, %659), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.42 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.30, %657, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %662 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.42, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %663 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.42, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %664 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%663, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %665 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.42, %664, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %666 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%665, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %667 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%666), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %668 : Float(17:212992, 13:16384, 16384:1) = aten::add(%667, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.46 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%662, %668), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %670 : Tensor = prim::GetAttr[name="bias"](%588)
  %671 : Tensor = prim::GetAttr[name="weight"](%588)
  %672 : Float(16384:1, 4096:16384) = aten::t(%671), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.31 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.46, %672), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.6 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.31, %670, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.47 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.6, %655, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %676 : Tensor = prim::GetAttr[name="bias"](%587)
  %677 : Tensor = prim::GetAttr[name="weight"](%587)
  %678 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.48 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.47, %678, %677, %676, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %680 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.48, %656)
  %681 : Float(17:53248, 13:4096, 4096:1), %682 : Float(4096:1) = prim::TupleUnpack(%680)
  %683 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%681, %682)
  %684 : Float(17:53248, 13:4096, 4096:1), %685 : Float(4096:1) = prim::TupleUnpack(%683)
  %686 : __torch__.torch.nn.modules.container.___torch_mangle_246.ModuleList = prim::GetAttr[name="albert_layers"](%71)
  %687 : __torch__.transformers.modeling_albert.___torch_mangle_245.AlbertLayer = prim::GetAttr[name="0"](%686)
  %688 : __torch__.torch.nn.modules.normalization.___torch_mangle_233.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%687)
  %689 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="ffn_output"](%687)
  %690 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="ffn"](%687)
  %691 : __torch__.transformers.modeling_albert.___torch_mangle_241.AlbertAttention = prim::GetAttr[name="attention"](%687)
  %692 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="LayerNorm"](%691)
  %693 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%691)
  %694 : Tensor = prim::GetAttr[name="weight"](%693)
  %695 : __torch__.torch.nn.modules.linear.___torch_mangle_236.Linear = prim::GetAttr[name="value"](%691)
  %696 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="key"](%691)
  %697 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="query"](%691)
  %698 : Tensor = prim::GetAttr[name="bias"](%697)
  %699 : Tensor = prim::GetAttr[name="weight"](%697)
  %700 : Float(4096:1, 4096:4096) = aten::t(%699), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.32 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%684, %700), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.32, %698, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %703 : Tensor = prim::GetAttr[name="bias"](%696)
  %704 : Tensor = prim::GetAttr[name="weight"](%696)
  %705 : Float(4096:1, 4096:4096) = aten::t(%704), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.33 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%684, %705), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.33, %703, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %708 : Tensor = prim::GetAttr[name="bias"](%695)
  %709 : Tensor = prim::GetAttr[name="weight"](%695)
  %710 : Float(4096:1, 4096:4096) = aten::t(%709), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.34 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%684, %710), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.34, %708, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %713 : int = aten::size(%x.43, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %714 : int = aten::size(%x.43, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %715 : int[] = prim::ListConstruct(%713, %714, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.44 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.43, %715), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %717 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.44, %717), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %719 : int = aten::size(%x.45, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %720 : int = aten::size(%x.45, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %721 : int[] = prim::ListConstruct(%719, %720, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.46 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.45, %721), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %723 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.46, %723), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %725 : int = aten::size(%x.47, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %726 : int = aten::size(%x.47, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %727 : int[] = prim::ListConstruct(%725, %726, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.48 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.47, %727), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %729 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.48, %729), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %731 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.7, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.13 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %731), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.14 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.13, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.49 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.50 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.49, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.50, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %738 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %739 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.7, %738), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %740 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%739, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %741 : Float(4096:1, 4096:4096) = aten::t(%694), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %742 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %743 : Float(64:64, 64:1, 4096:4096) = aten::view(%741, %742), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %744 : Float(64:64, 64:1, 4096:4096) = aten::to(%743, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.7 : Float(4096:1) = aten::to(%685, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %746 : Tensor[] = prim::ListConstruct(%740, %744), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %747 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %746), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.51 : Float(17:53248, 13:4096, 4096:1) = aten::add(%747, %b.7, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.7 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.51, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.52 : Float(17:53248, 13:4096, 4096:1) = aten::add(%684, %projected_context_layer.7, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %751 : Tensor = prim::GetAttr[name="bias"](%692)
  %752 : Tensor = prim::GetAttr[name="weight"](%692)
  %753 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.7 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.52, %753, %752, %751, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %755 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.7, %b.7)
  %756 : Float(17:53248, 13:4096, 4096:1), %757 : Float(4096:1) = prim::TupleUnpack(%755)
  %758 : Tensor = prim::GetAttr[name="bias"](%690)
  %759 : Tensor = prim::GetAttr[name="weight"](%690)
  %760 : Float(4096:1, 16384:4096) = aten::t(%759), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.35 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%756, %760), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.49 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.35, %758, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %763 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.49, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %764 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.49, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %765 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%764, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %766 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.49, %765, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %767 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%766, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %768 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%767), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %769 : Float(17:212992, 13:16384, 16384:1) = aten::add(%768, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.53 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%763, %769), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %771 : Tensor = prim::GetAttr[name="bias"](%689)
  %772 : Tensor = prim::GetAttr[name="weight"](%689)
  %773 : Float(16384:1, 4096:16384) = aten::t(%772), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.36 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.53, %773), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.7 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.36, %771, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.54 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.7, %756, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %777 : Tensor = prim::GetAttr[name="bias"](%688)
  %778 : Tensor = prim::GetAttr[name="weight"](%688)
  %779 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.55 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.54, %779, %778, %777, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %781 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.55, %757)
  %782 : Float(17:53248, 13:4096, 4096:1), %783 : Float(4096:1) = prim::TupleUnpack(%781)
  %784 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%782, %783)
  %785 : Float(17:53248, 13:4096, 4096:1), %786 : Float(4096:1) = prim::TupleUnpack(%784)
  %787 : __torch__.torch.nn.modules.container.___torch_mangle_246.ModuleList = prim::GetAttr[name="albert_layers"](%71)
  %788 : __torch__.transformers.modeling_albert.___torch_mangle_245.AlbertLayer = prim::GetAttr[name="0"](%787)
  %789 : __torch__.torch.nn.modules.normalization.___torch_mangle_233.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%788)
  %790 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="ffn_output"](%788)
  %791 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="ffn"](%788)
  %792 : __torch__.transformers.modeling_albert.___torch_mangle_241.AlbertAttention = prim::GetAttr[name="attention"](%788)
  %793 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="LayerNorm"](%792)
  %794 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%792)
  %795 : Tensor = prim::GetAttr[name="weight"](%794)
  %796 : __torch__.torch.nn.modules.linear.___torch_mangle_236.Linear = prim::GetAttr[name="value"](%792)
  %797 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="key"](%792)
  %798 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="query"](%792)
  %799 : Tensor = prim::GetAttr[name="bias"](%798)
  %800 : Tensor = prim::GetAttr[name="weight"](%798)
  %801 : Float(4096:1, 4096:4096) = aten::t(%800), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%785, %801), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.50 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.37, %799, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %804 : Tensor = prim::GetAttr[name="bias"](%797)
  %805 : Tensor = prim::GetAttr[name="weight"](%797)
  %806 : Float(4096:1, 4096:4096) = aten::t(%805), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%785, %806), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.52 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.38, %804, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %809 : Tensor = prim::GetAttr[name="bias"](%796)
  %810 : Tensor = prim::GetAttr[name="weight"](%796)
  %811 : Float(4096:1, 4096:4096) = aten::t(%810), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%785, %811), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.54 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.39, %809, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %814 : int = aten::size(%x.50, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %815 : int = aten::size(%x.50, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %816 : int[] = prim::ListConstruct(%814, %815, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.51 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.50, %816), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %818 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.51, %818), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %820 : int = aten::size(%x.52, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %821 : int = aten::size(%x.52, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %822 : int[] = prim::ListConstruct(%820, %821, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.53 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.52, %822), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %824 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.53, %824), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %826 : int = aten::size(%x.54, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %827 : int = aten::size(%x.54, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %828 : int[] = prim::ListConstruct(%826, %827, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.55 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.54, %828), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %830 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.55, %830), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %832 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.8, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.15 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %832), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.16 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.15, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.56 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.57 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.56, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.57, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.8 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %839 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %840 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.8, %839), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %841 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%840, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %842 : Float(4096:1, 4096:4096) = aten::t(%795), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %843 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %844 : Float(64:64, 64:1, 4096:4096) = aten::view(%842, %843), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %845 : Float(64:64, 64:1, 4096:4096) = aten::to(%844, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.8 : Float(4096:1) = aten::to(%786, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %847 : Tensor[] = prim::ListConstruct(%841, %845), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %848 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %847), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.58 : Float(17:53248, 13:4096, 4096:1) = aten::add(%848, %b.8, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.8 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.58, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.59 : Float(17:53248, 13:4096, 4096:1) = aten::add(%785, %projected_context_layer.8, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %852 : Tensor = prim::GetAttr[name="bias"](%793)
  %853 : Tensor = prim::GetAttr[name="weight"](%793)
  %854 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.8 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.59, %854, %853, %852, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %856 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.8, %b.8)
  %857 : Float(17:53248, 13:4096, 4096:1), %858 : Float(4096:1) = prim::TupleUnpack(%856)
  %859 : Tensor = prim::GetAttr[name="bias"](%791)
  %860 : Tensor = prim::GetAttr[name="weight"](%791)
  %861 : Float(4096:1, 16384:4096) = aten::t(%860), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.40 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%857, %861), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.56 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.40, %859, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %864 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.56, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %865 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.56, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %866 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%865, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %867 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.56, %866, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %868 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%867, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %869 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%868), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %870 : Float(17:212992, 13:16384, 16384:1) = aten::add(%869, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.60 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%864, %870), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %872 : Tensor = prim::GetAttr[name="bias"](%790)
  %873 : Tensor = prim::GetAttr[name="weight"](%790)
  %874 : Float(16384:1, 4096:16384) = aten::t(%873), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.41 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.60, %874), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.8 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.41, %872, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.61 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.8, %857, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %878 : Tensor = prim::GetAttr[name="bias"](%789)
  %879 : Tensor = prim::GetAttr[name="weight"](%789)
  %880 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.62 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.61, %880, %879, %878, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %882 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.62, %858)
  %883 : Float(17:53248, 13:4096, 4096:1), %884 : Float(4096:1) = prim::TupleUnpack(%882)
  %885 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%883, %884)
  %886 : Float(17:53248, 13:4096, 4096:1), %887 : Float(4096:1) = prim::TupleUnpack(%885)
  %888 : __torch__.torch.nn.modules.container.___torch_mangle_246.ModuleList = prim::GetAttr[name="albert_layers"](%71)
  %889 : __torch__.transformers.modeling_albert.___torch_mangle_245.AlbertLayer = prim::GetAttr[name="0"](%888)
  %890 : __torch__.torch.nn.modules.normalization.___torch_mangle_233.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%889)
  %891 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="ffn_output"](%889)
  %892 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="ffn"](%889)
  %893 : __torch__.transformers.modeling_albert.___torch_mangle_241.AlbertAttention = prim::GetAttr[name="attention"](%889)
  %894 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="LayerNorm"](%893)
  %895 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%893)
  %896 : Tensor = prim::GetAttr[name="weight"](%895)
  %897 : __torch__.torch.nn.modules.linear.___torch_mangle_236.Linear = prim::GetAttr[name="value"](%893)
  %898 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="key"](%893)
  %899 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="query"](%893)
  %900 : Tensor = prim::GetAttr[name="bias"](%899)
  %901 : Tensor = prim::GetAttr[name="weight"](%899)
  %902 : Float(4096:1, 4096:4096) = aten::t(%901), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.42 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%886, %902), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.57 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.42, %900, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %905 : Tensor = prim::GetAttr[name="bias"](%898)
  %906 : Tensor = prim::GetAttr[name="weight"](%898)
  %907 : Float(4096:1, 4096:4096) = aten::t(%906), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.43 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%886, %907), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.59 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.43, %905, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %910 : Tensor = prim::GetAttr[name="bias"](%897)
  %911 : Tensor = prim::GetAttr[name="weight"](%897)
  %912 : Float(4096:1, 4096:4096) = aten::t(%911), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.44 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%886, %912), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.61 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.44, %910, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %915 : int = aten::size(%x.57, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %916 : int = aten::size(%x.57, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %917 : int[] = prim::ListConstruct(%915, %916, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.58 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.57, %917), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %919 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.58, %919), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %921 : int = aten::size(%x.59, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %922 : int = aten::size(%x.59, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %923 : int[] = prim::ListConstruct(%921, %922, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.60 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.59, %923), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %925 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.60, %925), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %927 : int = aten::size(%x.61, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %928 : int = aten::size(%x.61, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %929 : int[] = prim::ListConstruct(%927, %928, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.62 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.61, %929), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %931 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.62, %931), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %933 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.9, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.17 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %933), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.18 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.17, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.63 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.64 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.63, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.64, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %940 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %941 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.9, %940), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %942 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%941, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %943 : Float(4096:1, 4096:4096) = aten::t(%896), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %944 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %945 : Float(64:64, 64:1, 4096:4096) = aten::view(%943, %944), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %946 : Float(64:64, 64:1, 4096:4096) = aten::to(%945, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.9 : Float(4096:1) = aten::to(%887, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %948 : Tensor[] = prim::ListConstruct(%942, %946), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %949 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %948), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.65 : Float(17:53248, 13:4096, 4096:1) = aten::add(%949, %b.9, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.9 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.65, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.66 : Float(17:53248, 13:4096, 4096:1) = aten::add(%886, %projected_context_layer.9, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %953 : Tensor = prim::GetAttr[name="bias"](%894)
  %954 : Tensor = prim::GetAttr[name="weight"](%894)
  %955 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.9 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.66, %955, %954, %953, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %957 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.9, %b.9)
  %958 : Float(17:53248, 13:4096, 4096:1), %959 : Float(4096:1) = prim::TupleUnpack(%957)
  %960 : Tensor = prim::GetAttr[name="bias"](%892)
  %961 : Tensor = prim::GetAttr[name="weight"](%892)
  %962 : Float(4096:1, 16384:4096) = aten::t(%961), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.45 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%958, %962), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.63 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.45, %960, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %965 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.63, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %966 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.63, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %967 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%966, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %968 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.63, %967, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %969 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%968, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %970 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%969), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %971 : Float(17:212992, 13:16384, 16384:1) = aten::add(%970, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.67 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%965, %971), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %973 : Tensor = prim::GetAttr[name="bias"](%891)
  %974 : Tensor = prim::GetAttr[name="weight"](%891)
  %975 : Float(16384:1, 4096:16384) = aten::t(%974), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.46 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.67, %975), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.9 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.46, %973, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.68 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.9, %958, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %979 : Tensor = prim::GetAttr[name="bias"](%890)
  %980 : Tensor = prim::GetAttr[name="weight"](%890)
  %981 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.69 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.68, %981, %980, %979, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %983 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.69, %959)
  %984 : Float(17:53248, 13:4096, 4096:1), %985 : Float(4096:1) = prim::TupleUnpack(%983)
  %986 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%984, %985)
  %987 : Float(17:53248, 13:4096, 4096:1), %988 : Float(4096:1) = prim::TupleUnpack(%986)
  %989 : __torch__.torch.nn.modules.container.___torch_mangle_246.ModuleList = prim::GetAttr[name="albert_layers"](%71)
  %990 : __torch__.transformers.modeling_albert.___torch_mangle_245.AlbertLayer = prim::GetAttr[name="0"](%989)
  %991 : __torch__.torch.nn.modules.normalization.___torch_mangle_233.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%990)
  %992 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="ffn_output"](%990)
  %993 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="ffn"](%990)
  %994 : __torch__.transformers.modeling_albert.___torch_mangle_241.AlbertAttention = prim::GetAttr[name="attention"](%990)
  %995 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="LayerNorm"](%994)
  %996 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%994)
  %997 : Tensor = prim::GetAttr[name="weight"](%996)
  %998 : __torch__.torch.nn.modules.linear.___torch_mangle_236.Linear = prim::GetAttr[name="value"](%994)
  %999 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="key"](%994)
  %1000 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="query"](%994)
  %1001 : Tensor = prim::GetAttr[name="bias"](%1000)
  %1002 : Tensor = prim::GetAttr[name="weight"](%1000)
  %1003 : Float(4096:1, 4096:4096) = aten::t(%1002), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.47 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%987, %1003), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.64 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.47, %1001, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1006 : Tensor = prim::GetAttr[name="bias"](%999)
  %1007 : Tensor = prim::GetAttr[name="weight"](%999)
  %1008 : Float(4096:1, 4096:4096) = aten::t(%1007), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.48 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%987, %1008), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.66 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.48, %1006, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1011 : Tensor = prim::GetAttr[name="bias"](%998)
  %1012 : Tensor = prim::GetAttr[name="weight"](%998)
  %1013 : Float(4096:1, 4096:4096) = aten::t(%1012), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.49 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%987, %1013), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.68 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.49, %1011, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1016 : int = aten::size(%x.64, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1017 : int = aten::size(%x.64, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1018 : int[] = prim::ListConstruct(%1016, %1017, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.65 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.64, %1018), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1020 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.65, %1020), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1022 : int = aten::size(%x.66, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1023 : int = aten::size(%x.66, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1024 : int[] = prim::ListConstruct(%1022, %1023, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.67 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.66, %1024), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1026 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.67, %1026), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1028 : int = aten::size(%x.68, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1029 : int = aten::size(%x.68, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1030 : int[] = prim::ListConstruct(%1028, %1029, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.69 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.68, %1030), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1032 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.69, %1032), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1034 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.10, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.19 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %1034), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.20 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.19, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.70 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.71 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.70, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.71, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.10 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1041 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1042 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.10, %1041), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1043 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1042, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1044 : Float(4096:1, 4096:4096) = aten::t(%997), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1045 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1046 : Float(64:64, 64:1, 4096:4096) = aten::view(%1044, %1045), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1047 : Float(64:64, 64:1, 4096:4096) = aten::to(%1046, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.10 : Float(4096:1) = aten::to(%988, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1049 : Tensor[] = prim::ListConstruct(%1043, %1047), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1050 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %1049), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.72 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1050, %b.10, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.10 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.72, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:53248, 13:4096, 4096:1) = aten::add(%987, %projected_context_layer.10, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1054 : Tensor = prim::GetAttr[name="bias"](%995)
  %1055 : Tensor = prim::GetAttr[name="weight"](%995)
  %1056 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.10 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.73, %1056, %1055, %1054, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1058 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.10, %b.10)
  %1059 : Float(17:53248, 13:4096, 4096:1), %1060 : Float(4096:1) = prim::TupleUnpack(%1058)
  %1061 : Tensor = prim::GetAttr[name="bias"](%993)
  %1062 : Tensor = prim::GetAttr[name="weight"](%993)
  %1063 : Float(4096:1, 16384:4096) = aten::t(%1062), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.50 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%1059, %1063), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.70 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.50, %1061, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1066 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.70, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1067 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.70, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1068 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1067, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1069 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.70, %1068, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1070 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1069, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1071 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1070), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1072 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1071, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.74 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1066, %1072), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1074 : Tensor = prim::GetAttr[name="bias"](%992)
  %1075 : Tensor = prim::GetAttr[name="weight"](%992)
  %1076 : Float(16384:1, 4096:16384) = aten::t(%1075), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.51 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.74, %1076), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.51, %1074, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.75 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.10, %1059, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1080 : Tensor = prim::GetAttr[name="bias"](%991)
  %1081 : Tensor = prim::GetAttr[name="weight"](%991)
  %1082 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.76 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.75, %1082, %1081, %1080, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1084 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.76, %1060)
  %1085 : Float(17:53248, 13:4096, 4096:1), %1086 : Float(4096:1) = prim::TupleUnpack(%1084)
  %1087 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1085, %1086)
  %1088 : Float(17:53248, 13:4096, 4096:1), %1089 : Float(4096:1) = prim::TupleUnpack(%1087)
  %1090 : __torch__.torch.nn.modules.container.___torch_mangle_246.ModuleList = prim::GetAttr[name="albert_layers"](%71)
  %1091 : __torch__.transformers.modeling_albert.___torch_mangle_245.AlbertLayer = prim::GetAttr[name="0"](%1090)
  %1092 : __torch__.torch.nn.modules.normalization.___torch_mangle_233.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1091)
  %1093 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="ffn_output"](%1091)
  %1094 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="ffn"](%1091)
  %1095 : __torch__.transformers.modeling_albert.___torch_mangle_241.AlbertAttention = prim::GetAttr[name="attention"](%1091)
  %1096 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="LayerNorm"](%1095)
  %1097 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%1095)
  %1098 : Tensor = prim::GetAttr[name="weight"](%1097)
  %1099 : __torch__.torch.nn.modules.linear.___torch_mangle_236.Linear = prim::GetAttr[name="value"](%1095)
  %1100 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="key"](%1095)
  %1101 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="query"](%1095)
  %1102 : Tensor = prim::GetAttr[name="bias"](%1101)
  %1103 : Tensor = prim::GetAttr[name="weight"](%1101)
  %1104 : Float(4096:1, 4096:4096) = aten::t(%1103), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.52 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1088, %1104), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.71 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.52, %1102, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1107 : Tensor = prim::GetAttr[name="bias"](%1100)
  %1108 : Tensor = prim::GetAttr[name="weight"](%1100)
  %1109 : Float(4096:1, 4096:4096) = aten::t(%1108), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.53 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1088, %1109), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.73 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.53, %1107, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1112 : Tensor = prim::GetAttr[name="bias"](%1099)
  %1113 : Tensor = prim::GetAttr[name="weight"](%1099)
  %1114 : Float(4096:1, 4096:4096) = aten::t(%1113), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.54 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1088, %1114), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.75 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.54, %1112, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1117 : int = aten::size(%x.71, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1118 : int = aten::size(%x.71, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1119 : int[] = prim::ListConstruct(%1117, %1118, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.72 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.71, %1119), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1121 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.72, %1121), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1123 : int = aten::size(%x.73, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1124 : int = aten::size(%x.73, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1125 : int[] = prim::ListConstruct(%1123, %1124, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.74 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.73, %1125), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1127 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.74, %1127), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1129 : int = aten::size(%x.75, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1130 : int = aten::size(%x.75, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1131 : int[] = prim::ListConstruct(%1129, %1130, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.76 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.75, %1131), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1133 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.76, %1133), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1135 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.11, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.21 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1135), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.22 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.21, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.77 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.78 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.77, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.78, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1142 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1143 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.11, %1142), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1144 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1143, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1145 : Float(4096:1, 4096:4096) = aten::t(%1098), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1146 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1147 : Float(64:64, 64:1, 4096:4096) = aten::view(%1145, %1146), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1148 : Float(64:64, 64:1, 4096:4096) = aten::to(%1147, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.11 : Float(4096:1) = aten::to(%1089, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1150 : Tensor[] = prim::ListConstruct(%1144, %1148), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1151 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %1150), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.79 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1151, %b.11, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.11 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.79, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.80 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1088, %projected_context_layer.11, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1155 : Tensor = prim::GetAttr[name="bias"](%1096)
  %1156 : Tensor = prim::GetAttr[name="weight"](%1096)
  %1157 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.11 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.80, %1157, %1156, %1155, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1159 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.11, %b.11)
  %1160 : Float(17:53248, 13:4096, 4096:1), %1161 : Float(4096:1) = prim::TupleUnpack(%1159)
  %1162 : Tensor = prim::GetAttr[name="bias"](%1094)
  %1163 : Tensor = prim::GetAttr[name="weight"](%1094)
  %1164 : Float(4096:1, 16384:4096) = aten::t(%1163), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.55 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%1160, %1164), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.77 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.55, %1162, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1167 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.77, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1168 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.77, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1169 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1168, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1170 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.77, %1169, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1171 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1170, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1172 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1171), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1173 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1172, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.81 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1167, %1173), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1175 : Tensor = prim::GetAttr[name="bias"](%1093)
  %1176 : Tensor = prim::GetAttr[name="weight"](%1093)
  %1177 : Float(16384:1, 4096:16384) = aten::t(%1176), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.56 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.81, %1177), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.11 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.56, %1175, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.82 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.11, %1160, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1181 : Tensor = prim::GetAttr[name="bias"](%1092)
  %1182 : Tensor = prim::GetAttr[name="weight"](%1092)
  %1183 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.83 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.82, %1183, %1182, %1181, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1185 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.83, %1161)
  %1186 : Float(17:53248, 13:4096, 4096:1), %1187 : Float(4096:1) = prim::TupleUnpack(%1185)
  %1188 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1186, %1187)
  %1189 : Float(17:53248, 13:4096, 4096:1), %1190 : Float(4096:1) = prim::TupleUnpack(%1188)
  %1191 : __torch__.torch.nn.modules.container.___torch_mangle_246.ModuleList = prim::GetAttr[name="albert_layers"](%71)
  %1192 : __torch__.transformers.modeling_albert.___torch_mangle_245.AlbertLayer = prim::GetAttr[name="0"](%1191)
  %1193 : __torch__.torch.nn.modules.normalization.___torch_mangle_233.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1192)
  %1194 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="ffn_output"](%1192)
  %1195 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="ffn"](%1192)
  %1196 : __torch__.transformers.modeling_albert.___torch_mangle_241.AlbertAttention = prim::GetAttr[name="attention"](%1192)
  %1197 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="LayerNorm"](%1196)
  %1198 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name="dense"](%1196)
  %1199 : Tensor = prim::GetAttr[name="weight"](%1198)
  %1200 : __torch__.torch.nn.modules.linear.___torch_mangle_236.Linear = prim::GetAttr[name="value"](%1196)
  %1201 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="key"](%1196)
  %1202 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="query"](%1196)
  %1203 : Tensor = prim::GetAttr[name="bias"](%1202)
  %1204 : Tensor = prim::GetAttr[name="weight"](%1202)
  %1205 : Float(4096:1, 4096:4096) = aten::t(%1204), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.57 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1189, %1205), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.78 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.57, %1203, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1208 : Tensor = prim::GetAttr[name="bias"](%1201)
  %1209 : Tensor = prim::GetAttr[name="weight"](%1201)
  %1210 : Float(4096:1, 4096:4096) = aten::t(%1209), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.58 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1189, %1210), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.80 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.58, %1208, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1213 : Tensor = prim::GetAttr[name="bias"](%1200)
  %1214 : Tensor = prim::GetAttr[name="weight"](%1200)
  %1215 : Float(4096:1, 4096:4096) = aten::t(%1214), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.59 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1189, %1215), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.82 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.59, %1213, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1218 : int = aten::size(%x.78, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1219 : int = aten::size(%x.78, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1220 : int[] = prim::ListConstruct(%1218, %1219, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.79 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.78, %1220), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1222 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.79, %1222), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1224 : int = aten::size(%x.80, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1225 : int = aten::size(%x.80, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1226 : int[] = prim::ListConstruct(%1224, %1225, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.81 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.80, %1226), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1228 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.81, %1228), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1230 : int = aten::size(%x.82, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1231 : int = aten::size(%x.82, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1232 : int[] = prim::ListConstruct(%1230, %1231, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.83 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.82, %1232), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1234 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.83, %1234), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1236 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.23 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer, %1236), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.23, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.84 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.85 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.84, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.85, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1243 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1244 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer, %1243), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1245 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1244, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1246 : Float(4096:1, 4096:4096) = aten::t(%1199), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1247 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1248 : Float(64:64, 64:1, 4096:4096) = aten::view(%1246, %1247), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1249 : Float(64:64, 64:1, 4096:4096) = aten::to(%1248, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b : Float(4096:1) = aten::to(%1190, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1251 : Tensor[] = prim::ListConstruct(%1245, %1249), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1252 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %1251), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.86 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1252, %b, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.86, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.87 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1189, %projected_context_layer, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1256 : Tensor = prim::GetAttr[name="bias"](%1197)
  %1257 : Tensor = prim::GetAttr[name="weight"](%1197)
  %1258 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.87, %1258, %1257, %1256, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1260 : Tensor = prim::GetAttr[name="bias"](%1195)
  %1261 : Tensor = prim::GetAttr[name="weight"](%1195)
  %1262 : Float(4096:1, 16384:4096) = aten::t(%1261), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.60 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%input_tensor, %1262), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.60, %1260, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1265 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1266 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1267 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1266, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1268 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x, %1267, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1269 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1268, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1270 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1269), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1271 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1270, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.88 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1265, %1271), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1273 : Tensor = prim::GetAttr[name="bias"](%1194)
  %1274 : Tensor = prim::GetAttr[name="weight"](%1194)
  %1275 : Float(16384:1, 4096:16384) = aten::t(%1274), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.88, %1275), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output, %1273, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.89 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output, %input_tensor, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1279 : Tensor = prim::GetAttr[name="bias"](%1193)
  %1280 : Tensor = prim::GetAttr[name="weight"](%1193)
  %1281 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %sequence_output : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.89, %1281, %1280, %1279, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1283 : Float(17:53248, 13:4096, 4096:1) = aten::slice(%sequence_output, %36, %36, %21, %35), scope: __module.albert # transformers/modeling_albert.py:695:0
  %input.90 : Float(17:53248, 4096:1) = aten::select(%1283, %35, %36), scope: __module.albert # transformers/modeling_albert.py:695:0
  %1285 : Tensor = prim::GetAttr[name="bias"](%37)
  %1286 : Tensor = prim::GetAttr[name="weight"](%37)
  %1287 : Float(4096:1, 4096:4096) = aten::t(%1286), scope: __module.albert/__module.albert.pooler # torch/nn/functional.py:1674:0
  %input.91 : Float(17:4096, 4096:1) = aten::addmm(%1285, %input.90, %1287, %35, %35), scope: __module.albert/__module.albert.pooler # torch/nn/functional.py:1674:0
  %input.92 : Float(17:4096, 4096:1) = aten::tanh(%input.91), scope: __module.albert/__module.albert.pooler_activation # torch/nn/modules/activation.py:350:0
  %1290 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %1291 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %input : Float(17:4096, 4096:1) = aten::dropout(%input.92, %1291, %1290), scope: __module.dropout # torch/nn/functional.py:973:0
  %1293 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1674:0
  %1294 : Tensor = prim::GetAttr[name="bias"](%3)
  %1295 : Tensor = prim::GetAttr[name="weight"](%3)
  %1296 : Float(4096:1, 2:4096) = aten::t(%1295), scope: __module.classifier # torch/nn/functional.py:1674:0
  %1297 : Float(17:2, 2:1) = aten::addmm(%1294, %input, %1296, %1293, %1293), scope: __module.classifier # torch/nn/functional.py:1674:0
  %9 : (Float(17:2, 2:1)) = prim::TupleConstruct(%1297)
  return (%9)
