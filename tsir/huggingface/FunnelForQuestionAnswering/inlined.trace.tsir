graph(%self.1 : __torch__.transformers.modeling_funnel.FunnelForQuestionAnswering,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_3482.Linear = prim::GetAttr[name="qa_outputs"](%self.1)
  %4 : __torch__.transformers.modeling_funnel.___torch_mangle_3481.FunnelModel = prim::GetAttr[name="funnel"](%self.1)
  %17 : int = prim::Constant[value=384](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %18 : float = prim::Constant[value=1.](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %19 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %20 : Long() = prim::Constant[value={384}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:248:0
  %21 : Float() = prim::Constant[value={10000}](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %22 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder
  %23 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %24 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %25 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %26 : Long() = prim::Constant[value={13}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %27 : Float(1:1) = prim::Constant[value={-1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %28 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %29 : Long() = prim::Constant[value={14}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %30 : int = prim::Constant[value=-2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %31 : Float(1:1) = prim::Constant[value={-3}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %32 : Long() = prim::Constant[value={16}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %33 : int = prim::Constant[value=-4](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %34 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %35 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %36 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %37 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %38 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %39 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %40 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %41 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %42 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %43 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %44 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %45 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %46 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %47 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %48 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %49 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %50 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %51 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %52 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %53 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %54 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
  %55 : bool = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
  %56 : Device = prim::Constant[value="cpu"](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
  %57 : int = prim::Constant[value=4](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
  %58 : int = prim::Constant[value=1](), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
  %59 : int = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
  %60 : __torch__.transformers.modeling_funnel.___torch_mangle_3480.FunnelDecoder = prim::GetAttr[name="decoder"](%4)
  %61 : __torch__.transformers.modeling_funnel.___torch_mangle_3445.FunnelEncoder = prim::GetAttr[name="encoder"](%4)
  %62 : __torch__.transformers.modeling_funnel.___torch_mangle_3257.FunnelEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %63 : int = aten::size(%input_ids, %59), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
  %64 : int = aten::size(%input_ids, %58), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
  %65 : int[] = prim::ListConstruct(%63, %64), scope: __module.funnel
  %token_type_ids : Long(17:13, 13:1) = aten::zeros(%65, %57, %59, %56, %55), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
  %67 : __torch__.torch.nn.modules.normalization.___torch_mangle_3255.LayerNorm = prim::GetAttr[name="layer_norm"](%62)
  %68 : __torch__.torch.nn.modules.sparse.___torch_mangle_3254.Embedding = prim::GetAttr[name="word_embeddings"](%62)
  %69 : Tensor = prim::GetAttr[name="weight"](%68)
  %input.1 : Float(17:9984, 13:768, 768:1) = aten::embedding(%69, %input_ids, %50, %55, %55), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %71 : Tensor = prim::GetAttr[name="bias"](%67)
  %72 : Tensor = prim::GetAttr[name="weight"](%67)
  %73 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm
  %input.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.1, %73, %72, %71, %52, %51), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.2, %54, %55), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
  %76 : __torch__.torch.nn.modules.container.___torch_mangle_3444.ModuleList = prim::GetAttr[name="blocks"](%61)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_3443.ModuleList = prim::GetAttr[name="2"](%76)
  %78 : __torch__.transformers.modeling_funnel.___torch_mangle_3442.FunnelLayer = prim::GetAttr[name="3"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_3444.ModuleList = prim::GetAttr[name="blocks"](%61)
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_3443.ModuleList = prim::GetAttr[name="2"](%79)
  %81 : __torch__.transformers.modeling_funnel.___torch_mangle_3427.FunnelLayer = prim::GetAttr[name="2"](%80)
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_3444.ModuleList = prim::GetAttr[name="blocks"](%61)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_3443.ModuleList = prim::GetAttr[name="2"](%82)
  %84 : __torch__.transformers.modeling_funnel.___torch_mangle_3412.FunnelLayer = prim::GetAttr[name="1"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_3444.ModuleList = prim::GetAttr[name="blocks"](%61)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_3443.ModuleList = prim::GetAttr[name="2"](%85)
  %87 : __torch__.transformers.modeling_funnel.___torch_mangle_3397.FunnelLayer = prim::GetAttr[name="0"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_3444.ModuleList = prim::GetAttr[name="blocks"](%61)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_3382.ModuleList = prim::GetAttr[name="1"](%88)
  %90 : __torch__.transformers.modeling_funnel.___torch_mangle_3381.FunnelLayer = prim::GetAttr[name="3"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_3444.ModuleList = prim::GetAttr[name="blocks"](%61)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_3382.ModuleList = prim::GetAttr[name="1"](%91)
  %93 : __torch__.transformers.modeling_funnel.___torch_mangle_3366.FunnelLayer = prim::GetAttr[name="2"](%92)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_3444.ModuleList = prim::GetAttr[name="blocks"](%61)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_3382.ModuleList = prim::GetAttr[name="1"](%94)
  %96 : __torch__.transformers.modeling_funnel.___torch_mangle_3351.FunnelLayer = prim::GetAttr[name="1"](%95)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_3444.ModuleList = prim::GetAttr[name="blocks"](%61)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_3382.ModuleList = prim::GetAttr[name="1"](%97)
  %99 : __torch__.transformers.modeling_funnel.___torch_mangle_3336.FunnelLayer = prim::GetAttr[name="0"](%98)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_3444.ModuleList = prim::GetAttr[name="blocks"](%61)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_3321.ModuleList = prim::GetAttr[name="0"](%100)
  %102 : __torch__.transformers.modeling_funnel.___torch_mangle_3320.FunnelLayer = prim::GetAttr[name="3"](%101)
  %103 : __torch__.torch.nn.modules.container.___torch_mangle_3444.ModuleList = prim::GetAttr[name="blocks"](%61)
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_3321.ModuleList = prim::GetAttr[name="0"](%103)
  %105 : __torch__.transformers.modeling_funnel.___torch_mangle_3305.FunnelLayer = prim::GetAttr[name="2"](%104)
  %106 : __torch__.torch.nn.modules.container.___torch_mangle_3444.ModuleList = prim::GetAttr[name="blocks"](%61)
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_3321.ModuleList = prim::GetAttr[name="0"](%106)
  %108 : __torch__.transformers.modeling_funnel.___torch_mangle_3290.FunnelLayer = prim::GetAttr[name="1"](%107)
  %109 : __torch__.torch.nn.modules.container.___torch_mangle_3444.ModuleList = prim::GetAttr[name="blocks"](%61)
  %110 : __torch__.torch.nn.modules.container.___torch_mangle_3321.ModuleList = prim::GetAttr[name="0"](%109)
  %111 : __torch__.transformers.modeling_funnel.___torch_mangle_3275.FunnelLayer = prim::GetAttr[name="0"](%110)
  %attention_mask.2 : Float(17:13, 13:1) = aten::type_as(%attention_mask.1, %inputs_embeds), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:625:0
  %113 : int = aten::size(%inputs_embeds, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:195:0
  %seq_len.1 : Long() = prim::NumToTensor(%113), scope: __module.funnel/__module.funnel.encoder
  %freq_seq.1 : Float(384:1) = aten::arange(%59, %17, %18, %19, %59, %56, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %116 : Float(384:1) = aten::div(%freq_seq.1, %20), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:248:0
  %117 : Float() = aten::to(%21, %56, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %118 : Float() = aten::detach(%117), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %119 : Float(384:1) = aten::pow(%118, %116), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %120 : Float(384:1) = aten::reciprocal(%119), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %inv_freq.1 : Float(384:1) = aten::mul(%120, %23), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %122 : Long() = aten::neg(%seq_len.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %123 : Long() = aten::mul(%122, %24), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %124 : Scalar = aten::ScalarImplicit(%123), scope: __module.funnel/__module.funnel.encoder
  %125 : Long() = aten::mul(%seq_len.1, %24), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %126 : Scalar = aten::ScalarImplicit(%125), scope: __module.funnel/__module.funnel.encoder
  %rel_pos_id.1 : Float(52:1) = aten::arange(%124, %126, %18, %19, %59, %56, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %zero_offset.1 : Long() = aten::mul(%seq_len.1, %24), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:251:0
  %129 : Float(52:1) = aten::slice(%rel_pos_id.1, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %130 : Float(52:1, 1:1) = aten::unsqueeze(%129, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %131 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq.1, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %sinusoid.1 : Float(52:384, 384:1) = aten::mul(%130, %131), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %input.3 : Float(52:384, 384:1) = aten::sin(%sinusoid.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:253:0
  %sin_embed.1 : Float(52:384, 384:1) = aten::dropout(%input.3, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %input.4 : Float(52:384, 384:1) = aten::cos(%sinusoid.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:254:0
  %cos_embed.1 : Float(52:384, 384:1) = aten::dropout(%input.4, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
  %137 : Tensor[] = prim::ListConstruct(%sin_embed.1, %cos_embed.1), scope: __module.funnel/__module.funnel.encoder
  %pos_embed.1 : Float(52:768, 768:1) = aten::cat(%137, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:255:0
  %pos.1 : Float(13:1) = aten::arange(%59, %113, %58, %19, %59, %56, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:257:0
  %140 : Float() = aten::select(%pos.1, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %141 : Float() = aten::select(%pos.1, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.1 : Float() = aten::sub(%140, %141, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.1 : Float() = aten::add(%ref_point.1, %26, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %144 : Scalar = aten::ScalarImplicit(%max_dist.1), scope: __module.funnel/__module.funnel.encoder
  %145 : Float() = aten::select(%pos.1, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %146 : Float() = aten::select(%pos.1, %59, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.1 : Float() = aten::sub(%145, %146, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %148 : Float() = aten::sub(%min_dist.1, %23, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %149 : Scalar = aten::ScalarImplicit(%148), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.1 : Long(26:1) = aten::arange(%144, %149, %50, %57, %59, %56, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %151 : Long(26:1) = aten::slice(%rel_pos.1, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %152 : Long(26:1, 1:1) = aten::unsqueeze(%151, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.2 : Long(26:1, 1:1) = aten::add(%152, %zero_offset.1, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %154 : int = aten::size(%rel_pos.2, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %155 : int[] = prim::ListConstruct(%154, %53), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.3 : Long(26:1, 768:0) = aten::expand(%rel_pos.2, %155, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %157 : Float(26:768, 768:1) = aten::gather(%pos_embed.1, %59, %rel_pos.3, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %158 : Float(1:1) = aten::to(%27, %56, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos.1 : Float(1:1) = aten::detach(%158), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id.1 : Float(11:1) = aten::slice(%pos.1, %59, %58, %50, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
  %161 : Float(6:2) = aten::slice(%pooled_pos_id.1, %59, %59, %25, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %162 : Tensor[] = prim::ListConstruct(%cls_pos.1, %161), scope: __module.funnel/__module.funnel.encoder
  %pooled_pos.1 : Float(7:1) = aten::cat(%162, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %164 : Float() = aten::select(%pooled_pos.1, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %165 : Float() = aten::select(%pos.1, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.2 : Float() = aten::sub(%164, %165, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.2 : Float() = aten::add(%ref_point.2, %29, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %168 : Scalar = aten::ScalarImplicit(%max_dist.2), scope: __module.funnel/__module.funnel.encoder
  %169 : Float() = aten::select(%pooled_pos.1, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %170 : Float() = aten::select(%pos.1, %59, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.2 : Float() = aten::sub(%169, %170, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %172 : Float() = aten::sub(%min_dist.2, %23, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %173 : Scalar = aten::ScalarImplicit(%172), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.4 : Long(27:1) = aten::arange(%168, %173, %50, %57, %59, %56, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %175 : Long(27:1) = aten::slice(%rel_pos.4, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %176 : Long(27:1, 1:1) = aten::unsqueeze(%175, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.5 : Long(27:1, 1:1) = aten::add(%176, %zero_offset.1, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %178 : int = aten::size(%rel_pos.5, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %179 : int[] = prim::ListConstruct(%178, %53), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.6 : Long(27:1, 768:0) = aten::expand(%rel_pos.5, %179, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %181 : Float(27:768, 768:1) = aten::gather(%pos_embed.1, %59, %rel_pos.6, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
  %182 : Float() = aten::select(%pooled_pos.1, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %183 : Float() = aten::select(%pooled_pos.1, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.3 : Float() = aten::sub(%182, %183, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.3 : Float() = aten::add(%ref_point.3, %29, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %186 : Scalar = aten::ScalarImplicit(%max_dist.3), scope: __module.funnel/__module.funnel.encoder
  %187 : Float() = aten::select(%pooled_pos.1, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %188 : Float() = aten::select(%pooled_pos.1, %59, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.3 : Float() = aten::sub(%187, %188, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %190 : Float() = aten::sub(%min_dist.3, %23, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %191 : Scalar = aten::ScalarImplicit(%190), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.7 : Long(14:1) = aten::arange(%186, %191, %30, %57, %59, %56, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %193 : Long(14:1) = aten::slice(%rel_pos.7, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %194 : Long(14:1, 1:1) = aten::unsqueeze(%193, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.8 : Long(14:1, 1:1) = aten::add(%194, %zero_offset.1, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %196 : int = aten::size(%rel_pos.8, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %197 : int[] = prim::ListConstruct(%196, %53), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.9 : Long(14:1, 768:0) = aten::expand(%rel_pos.8, %197, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %199 : Float(14:768, 768:1) = aten::gather(%pos_embed.1, %59, %rel_pos.9, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %200 : Float(1:1) = aten::to(%31, %56, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos.2 : Float(1:1) = aten::detach(%200), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id.2 : Float(5:1) = aten::slice(%pooled_pos.1, %59, %58, %50, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
  %203 : Float(3:2) = aten::slice(%pooled_pos_id.2, %59, %59, %25, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %204 : Tensor[] = prim::ListConstruct(%cls_pos.2, %203), scope: __module.funnel/__module.funnel.encoder
  %pooled_pos.2 : Float(4:1) = aten::cat(%204, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %206 : Float() = aten::select(%pooled_pos.2, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %207 : Float() = aten::select(%pooled_pos.1, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.4 : Float() = aten::sub(%206, %207, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.4 : Float() = aten::add(%ref_point.4, %32, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %210 : Scalar = aten::ScalarImplicit(%max_dist.4), scope: __module.funnel/__module.funnel.encoder
  %211 : Float() = aten::select(%pooled_pos.2, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %212 : Float() = aten::select(%pooled_pos.1, %59, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.4 : Float() = aten::sub(%211, %212, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %214 : Float() = aten::sub(%min_dist.4, %23, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %215 : Scalar = aten::ScalarImplicit(%214), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.10 : Long(15:1) = aten::arange(%210, %215, %30, %57, %59, %56, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %217 : Long(15:1) = aten::slice(%rel_pos.10, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %218 : Long(15:1, 1:1) = aten::unsqueeze(%217, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.11 : Long(15:1, 1:1) = aten::add(%218, %zero_offset.1, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %220 : int = aten::size(%rel_pos.11, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %221 : int[] = prim::ListConstruct(%220, %53), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.12 : Long(15:1, 768:0) = aten::expand(%rel_pos.11, %221, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %223 : Float(15:768, 768:1) = aten::gather(%pos_embed.1, %59, %rel_pos.12, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
  %224 : Float() = aten::select(%pooled_pos.2, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %225 : Float() = aten::select(%pooled_pos.2, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.5 : Float() = aten::sub(%224, %225, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.5 : Float() = aten::add(%ref_point.5, %32, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %228 : Scalar = aten::ScalarImplicit(%max_dist.5), scope: __module.funnel/__module.funnel.encoder
  %229 : Float() = aten::select(%pooled_pos.2, %59, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %230 : Float() = aten::select(%pooled_pos.2, %59, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.5 : Float() = aten::sub(%229, %230, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %232 : Float() = aten::sub(%min_dist.5, %23, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %233 : Scalar = aten::ScalarImplicit(%232), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.13 : Long(8:1) = aten::arange(%228, %233, %33, %57, %59, %56, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %235 : Long(8:1) = aten::slice(%rel_pos.13, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %236 : Long(8:1, 1:1) = aten::unsqueeze(%235, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.14 : Long(8:1, 1:1) = aten::add(%236, %zero_offset.1, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %238 : int = aten::size(%rel_pos.14, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %239 : int[] = prim::ListConstruct(%238, %53), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.15 : Long(8:1, 768:0) = aten::expand(%rel_pos.14, %239, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %241 : Float(8:768, 768:1) = aten::gather(%pos_embed.1, %59, %rel_pos.15, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %242 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %243 : Long(17:13, 13:1) = aten::slice(%242, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %244 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%243, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %245 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %246 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%245, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %token_type_mat.1 : Bool(17:169, 13:13, 13:1) = aten::eq(%244, %246), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
  %cls_ids.1 : Bool(17:13, 13:1) = aten::eq(%token_type_ids, %28), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
  %249 : Bool(17:13, 13:1) = aten::slice(%cls_ids.1, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %250 : Bool(17:13, 13:1) = aten::slice(%249, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %251 : Bool(17:13, 13:1, 1:1) = aten::unsqueeze(%250, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %252 : Bool(17:13, 13:1) = aten::slice(%cls_ids.1, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %253 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%252, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %cls_mat.1 : Bool(17:169, 13:13, 13:1) = aten::__or__(%251, %253), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %token_type_mat.2 : Bool(17:169, 13:13, 13:1) = aten::__or__(%cls_mat.1, %token_type_mat.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:211:0
  %256 : Long() = aten::sub(%seq_len.1, %23, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %257 : int = aten::Int(%256), scope: __module.funnel/__module.funnel.encoder
  %258 : Long() = aten::sub(%seq_len.1, %23, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %259 : int = aten::Int(%258), scope: __module.funnel/__module.funnel.encoder
  %260 : int[] = prim::ListConstruct(%257, %259), scope: __module.funnel/__module.funnel.encoder
  %input.5 : Float(12:12, 12:1) = aten::ones(%260, %19, %59, %56, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %262 : int[] = prim::ListConstruct(%58, %59, %58, %59), scope: __module.funnel/__module.funnel.encoder
  %cls_mask.1 : Float(13:13, 13:1) = aten::constant_pad_nd(%input.5, %262, %59), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:3552:0
  %264 : __torch__.transformers.modeling_funnel.___torch_mangle_3274.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%111)
  %265 : __torch__.transformers.modeling_funnel.___torch_mangle_3268.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%111)
  %266 : __torch__.torch.nn.modules.normalization.___torch_mangle_3267.LayerNorm = prim::GetAttr[name="layer_norm"](%265)
  %267 : __torch__.torch.nn.modules.linear.___torch_mangle_3266.Linear = prim::GetAttr[name="post_proj"](%265)
  %268 : Tensor = prim::GetAttr[name="seg_embed"](%265)
  %269 : Tensor = prim::GetAttr[name="r_s_bias"](%265)
  %270 : Tensor = prim::GetAttr[name="r_kernel"](%265)
  %271 : Tensor = prim::GetAttr[name="r_r_bias"](%265)
  %272 : Tensor = prim::GetAttr[name="r_w_bias"](%265)
  %273 : __torch__.torch.nn.modules.linear.___torch_mangle_3265.Linear = prim::GetAttr[name="v_head"](%265)
  %274 : __torch__.torch.nn.modules.linear.___torch_mangle_3264.Linear = prim::GetAttr[name="k_head"](%265)
  %275 : __torch__.torch.nn.modules.linear.___torch_mangle_3263.Linear = prim::GetAttr[name="q_head"](%265)
  %276 : int = aten::size(%inputs_embeds, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %277 : int = aten::size(%inputs_embeds, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %278 : int = aten::size(%inputs_embeds, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:531:0
  %279 : Tensor = prim::GetAttr[name="weight"](%275)
  %280 : Float(768:1, 768:768) = aten::t(%279), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %281 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %280), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %282 : int[] = prim::ListConstruct(%276, %277, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %q_head.1 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%281, %282), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %284 : Tensor = prim::GetAttr[name="bias"](%274)
  %285 : Tensor = prim::GetAttr[name="weight"](%274)
  %286 : Float(768:1, 768:768) = aten::t(%285), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %286), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %288 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %284, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1678:0
  %289 : int[] = prim::ListConstruct(%276, %278, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %290 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%288, %289), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:537:0
  %291 : Tensor = prim::GetAttr[name="bias"](%273)
  %292 : Tensor = prim::GetAttr[name="weight"](%273)
  %293 : Float(768:1, 768:768) = aten::t(%292), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %293), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %295 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %291, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1678:0
  %296 : int[] = prim::ListConstruct(%276, %278, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %297 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%295, %296), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.1, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.1 : Float(12:64, 64:1) = aten::mul(%272, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:542:0
  %300 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_w_bias.1, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:544:0
  %301 : Tensor[] = prim::ListConstruct(%300, %290), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %content_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%43, %301), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %v.1 : Float(12:64, 64:1) = aten::mul(%271, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:486:0
  %304 : Tensor[] = prim::ListConstruct(%157, %270), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %305 : Float(26:768, 12:64, 64:1) = aten::einsum(%44, %304), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %306 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %v.1, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:493:0
  %307 : Tensor[] = prim::ListConstruct(%306, %305), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.1 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%45, %307), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %309 : int = aten::size(%positional_attn.1, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %310 : int = aten::size(%positional_attn.1, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %311 : int = aten::size(%positional_attn.1, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %312 : int = aten::size(%positional_attn.1, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.1 : Long() = prim::NumToTensor(%312), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %314 : int[] = prim::ListConstruct(%309, %310, %312, %311), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.2 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.1, %314), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:428:0
  %316 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %317 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%316, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %318 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%317, %28, %58, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.3 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%318, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %320 : Long() = aten::sub(%max_rel_len.1, %23, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %321 : int = aten::Int(%320), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %322 : int[] = prim::ListConstruct(%309, %310, %311, %321), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.4 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.3, %322), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.5 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.4, %46, %59, %278, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.6 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.5, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:498:0
  %326 : int = aten::size(%token_type_mat.2, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %327 : int = aten::size(%token_type_mat.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %328 : int = aten::size(%token_type_mat.2, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.1 : Float(12:64, 64:1) = aten::mul(%269, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:508:0
  %330 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_s_bias.1, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:511:0
  %331 : Tensor[] = prim::ListConstruct(%330, %268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %332 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%47, %331), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %333 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %334 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%333, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %335 : int = aten::size(%q_head.2, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %336 : int[] = prim::ListConstruct(%326, %335, %327, %328), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %token_type_mat.3 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%334, %336, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %338 : Tensor[] = aten::split(%332, %58, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:371:0
  %diff_token_type.1 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.1 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%338), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %341 : int = aten::size(%token_type_mat.3, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %342 : int = aten::size(%token_type_mat.3, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %343 : int = aten::size(%token_type_mat.3, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %344 : int = aten::size(%token_type_mat.3, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %345 : int[] = prim::ListConstruct(%341, %342, %343, %344), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %346 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.1, %345, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %347 : int = aten::size(%token_type_mat.3, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %348 : int = aten::size(%token_type_mat.3, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %349 : int = aten::size(%token_type_mat.3, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %350 : int = aten::size(%token_type_mat.3, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %351 : int[] = prim::ListConstruct(%347, %348, %349, %350), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %352 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.1, %351, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.3, %346, %352), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.1, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:522:0
  %355 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.1, %positional_attn.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%355, %token_type_attn.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.1, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:553:0
  %358 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %359 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%358, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %360 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%359, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %361 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%360, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %362 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%361, %58, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:396:0
  %363 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%362, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.2, %363, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.3, %50, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:558:0
  %366 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.6, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %367 : Tensor[] = prim::ListConstruct(%366, %297), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %attn_vec.1 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%49, %367), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %369 : int[] = prim::ListConstruct(%276, %277, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %input.7 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.1, %369), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:565:0
  %371 : Tensor = prim::GetAttr[name="bias"](%267)
  %372 : Tensor = prim::GetAttr[name="weight"](%267)
  %373 : Float(768:1, 768:768) = aten::t(%372), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.7, %373), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %371, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.8, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %attn_out.1, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:568:0
  %378 : Tensor = prim::GetAttr[name="bias"](%266)
  %379 : Tensor = prim::GetAttr[name="weight"](%266)
  %380 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.9, %380, %379, %378, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %382 : __torch__.torch.nn.modules.normalization.___torch_mangle_3273.LayerNorm = prim::GetAttr[name="layer_norm"](%264)
  %383 : __torch__.torch.nn.modules.linear.___torch_mangle_3271.Linear = prim::GetAttr[name="linear_2"](%264)
  %384 : __torch__.torch.nn.modules.linear.___torch_mangle_3269.Linear = prim::GetAttr[name="linear_1"](%264)
  %385 : Tensor = prim::GetAttr[name="bias"](%384)
  %386 : Tensor = prim::GetAttr[name="weight"](%384)
  %387 : Float(768:1, 3072:768) = aten::t(%386), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.10, %387), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.1 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.4, %385, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %390 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.1, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %391 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.1, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %392 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%391, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %393 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.1, %392, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %394 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%393, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %395 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%394), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %396 : Float(17:39936, 13:3072, 3072:1) = aten::add(%395, %38, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%390, %396), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.11, %39, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %399 : Tensor = prim::GetAttr[name="bias"](%383)
  %400 : Tensor = prim::GetAttr[name="weight"](%383)
  %401 : Float(3072:1, 768:3072) = aten::t(%400), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %401), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.5, %399, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%input.10, %h.1, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/modeling_funnel.py:588:0
  %406 : Tensor = prim::GetAttr[name="bias"](%382)
  %407 : Tensor = prim::GetAttr[name="weight"](%382)
  %408 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm
  %query.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %408, %407, %406, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %410 : __torch__.transformers.modeling_funnel.___torch_mangle_3289.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%108)
  %411 : __torch__.transformers.modeling_funnel.___torch_mangle_3283.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%108)
  %412 : __torch__.torch.nn.modules.normalization.___torch_mangle_3282.LayerNorm = prim::GetAttr[name="layer_norm"](%411)
  %413 : __torch__.torch.nn.modules.linear.___torch_mangle_3281.Linear = prim::GetAttr[name="post_proj"](%411)
  %414 : Tensor = prim::GetAttr[name="seg_embed"](%411)
  %415 : Tensor = prim::GetAttr[name="r_s_bias"](%411)
  %416 : Tensor = prim::GetAttr[name="r_kernel"](%411)
  %417 : Tensor = prim::GetAttr[name="r_r_bias"](%411)
  %418 : Tensor = prim::GetAttr[name="r_w_bias"](%411)
  %419 : __torch__.torch.nn.modules.linear.___torch_mangle_3280.Linear = prim::GetAttr[name="v_head"](%411)
  %420 : __torch__.torch.nn.modules.linear.___torch_mangle_3279.Linear = prim::GetAttr[name="k_head"](%411)
  %421 : __torch__.torch.nn.modules.linear.___torch_mangle_3278.Linear = prim::GetAttr[name="q_head"](%411)
  %422 : int = aten::size(%query.1, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %423 : int = aten::size(%query.1, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %424 : int = aten::size(%query.1, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:531:0
  %425 : Tensor = prim::GetAttr[name="weight"](%421)
  %426 : Float(768:1, 768:768) = aten::t(%425), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %427 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %426), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %428 : int[] = prim::ListConstruct(%422, %423, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %q_head.3 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%427, %428), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:535:0
  %430 : Tensor = prim::GetAttr[name="bias"](%420)
  %431 : Tensor = prim::GetAttr[name="weight"](%420)
  %432 : Float(768:1, 768:768) = aten::t(%431), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %432), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %434 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %430, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1678:0
  %435 : int[] = prim::ListConstruct(%422, %424, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %436 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%434, %435), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:537:0
  %437 : Tensor = prim::GetAttr[name="bias"](%419)
  %438 : Tensor = prim::GetAttr[name="weight"](%419)
  %439 : Float(768:1, 768:768) = aten::t(%438), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %439), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %441 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %437, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1678:0
  %442 : int[] = prim::ListConstruct(%422, %424, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %443 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%441, %442), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.3, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.2 : Float(12:64, 64:1) = aten::mul(%418, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:542:0
  %446 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_w_bias.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:544:0
  %447 : Tensor[] = prim::ListConstruct(%446, %436), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %content_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%43, %447), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %v.2 : Float(12:64, 64:1) = aten::mul(%417, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:486:0
  %450 : Tensor[] = prim::ListConstruct(%157, %416), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %451 : Float(26:768, 12:64, 64:1) = aten::einsum(%44, %450), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %452 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %v.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:493:0
  %453 : Tensor[] = prim::ListConstruct(%452, %451), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.7 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%45, %453), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %455 : int = aten::size(%positional_attn.7, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %456 : int = aten::size(%positional_attn.7, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %457 : int = aten::size(%positional_attn.7, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %458 : int = aten::size(%positional_attn.7, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.2 : Long() = prim::NumToTensor(%458), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %460 : int[] = prim::ListConstruct(%455, %456, %458, %457), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.8 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.7, %460), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:428:0
  %462 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.8, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %463 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%462, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %464 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%463, %28, %58, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.9 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%464, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %466 : Long() = aten::sub(%max_rel_len.2, %23, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %467 : int = aten::Int(%466), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %468 : int[] = prim::ListConstruct(%455, %456, %457, %467), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.10 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.9, %468), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.11 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.10, %46, %59, %424, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.12 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.11, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:498:0
  %472 : int = aten::size(%token_type_mat.2, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %473 : int = aten::size(%token_type_mat.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %474 : int = aten::size(%token_type_mat.2, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.2 : Float(12:64, 64:1) = aten::mul(%415, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:508:0
  %476 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_s_bias.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:511:0
  %477 : Tensor[] = prim::ListConstruct(%476, %414), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %478 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%47, %477), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %479 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %480 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%479, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %481 : int = aten::size(%q_head.4, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %482 : int[] = prim::ListConstruct(%472, %481, %473, %474), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %token_type_mat.4 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%480, %482, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %484 : Tensor[] = aten::split(%478, %58, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:371:0
  %diff_token_type.2 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.2 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%484), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %487 : int = aten::size(%token_type_mat.4, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %488 : int = aten::size(%token_type_mat.4, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %489 : int = aten::size(%token_type_mat.4, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %490 : int = aten::size(%token_type_mat.4, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %491 : int[] = prim::ListConstruct(%487, %488, %489, %490), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %492 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.2, %491, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %493 : int = aten::size(%token_type_mat.4, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %494 : int = aten::size(%token_type_mat.4, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %495 : int = aten::size(%token_type_mat.4, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %496 : int = aten::size(%token_type_mat.4, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %497 : int[] = prim::ListConstruct(%493, %494, %495, %496), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %498 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.2, %497, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.4, %492, %498), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.3, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:522:0
  %501 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.2, %positional_attn.12, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%501, %token_type_attn.4, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.4, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:553:0
  %504 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %505 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%504, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %506 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%505, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %507 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%506, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %508 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%507, %58, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:396:0
  %509 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%508, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.5, %509, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %input.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.6, %50, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:558:0
  %512 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.15, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %513 : Tensor[] = prim::ListConstruct(%512, %443), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %attn_vec.2 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%49, %513), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %515 : int[] = prim::ListConstruct(%422, %423, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %input.16 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.2, %515), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:565:0
  %517 : Tensor = prim::GetAttr[name="bias"](%413)
  %518 : Tensor = prim::GetAttr[name="weight"](%413)
  %519 : Float(768:1, 768:768) = aten::t(%518), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.16, %519), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %517, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.17, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::add(%query.1, %attn_out.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:568:0
  %524 : Tensor = prim::GetAttr[name="bias"](%412)
  %525 : Tensor = prim::GetAttr[name="weight"](%412)
  %526 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.18, %526, %525, %524, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %528 : __torch__.torch.nn.modules.normalization.___torch_mangle_3288.LayerNorm = prim::GetAttr[name="layer_norm"](%410)
  %529 : __torch__.torch.nn.modules.linear.___torch_mangle_3286.Linear = prim::GetAttr[name="linear_2"](%410)
  %530 : __torch__.torch.nn.modules.linear.___torch_mangle_3284.Linear = prim::GetAttr[name="linear_1"](%410)
  %531 : Tensor = prim::GetAttr[name="bias"](%530)
  %532 : Tensor = prim::GetAttr[name="weight"](%530)
  %533 : Float(768:1, 3072:768) = aten::t(%532), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.9 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.19, %533), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.2 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.9, %531, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %536 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.2, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %537 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.2, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %538 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%537, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %539 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.2, %538, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %540 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%539, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %541 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%540), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %542 : Float(17:39936, 13:3072, 3072:1) = aten::add(%541, %38, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.20 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%536, %542), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.20, %39, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %545 : Tensor = prim::GetAttr[name="bias"](%529)
  %546 : Tensor = prim::GetAttr[name="weight"](%529)
  %547 : Float(3072:1, 768:3072) = aten::t(%546), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.21, %547), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %545, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.22, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add(%input.19, %h.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/modeling_funnel.py:588:0
  %552 : Tensor = prim::GetAttr[name="bias"](%528)
  %553 : Tensor = prim::GetAttr[name="weight"](%528)
  %554 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm
  %query.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.23, %554, %553, %552, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %556 : __torch__.transformers.modeling_funnel.___torch_mangle_3304.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%105)
  %557 : __torch__.transformers.modeling_funnel.___torch_mangle_3298.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%105)
  %558 : __torch__.torch.nn.modules.normalization.___torch_mangle_3297.LayerNorm = prim::GetAttr[name="layer_norm"](%557)
  %559 : __torch__.torch.nn.modules.linear.___torch_mangle_3296.Linear = prim::GetAttr[name="post_proj"](%557)
  %560 : Tensor = prim::GetAttr[name="seg_embed"](%557)
  %561 : Tensor = prim::GetAttr[name="r_s_bias"](%557)
  %562 : Tensor = prim::GetAttr[name="r_kernel"](%557)
  %563 : Tensor = prim::GetAttr[name="r_r_bias"](%557)
  %564 : Tensor = prim::GetAttr[name="r_w_bias"](%557)
  %565 : __torch__.torch.nn.modules.linear.___torch_mangle_3295.Linear = prim::GetAttr[name="v_head"](%557)
  %566 : __torch__.torch.nn.modules.linear.___torch_mangle_3294.Linear = prim::GetAttr[name="k_head"](%557)
  %567 : __torch__.torch.nn.modules.linear.___torch_mangle_3293.Linear = prim::GetAttr[name="q_head"](%557)
  %568 : int = aten::size(%query.2, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %569 : int = aten::size(%query.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %570 : int = aten::size(%query.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:531:0
  %571 : Tensor = prim::GetAttr[name="weight"](%567)
  %572 : Float(768:1, 768:768) = aten::t(%571), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %573 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %572), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %574 : int[] = prim::ListConstruct(%568, %569, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %q_head.5 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%573, %574), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:535:0
  %576 : Tensor = prim::GetAttr[name="bias"](%566)
  %577 : Tensor = prim::GetAttr[name="weight"](%566)
  %578 : Float(768:1, 768:768) = aten::t(%577), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.11 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %578), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %580 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.11, %576, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1678:0
  %581 : int[] = prim::ListConstruct(%568, %570, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %582 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%580, %581), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:537:0
  %583 : Tensor = prim::GetAttr[name="bias"](%565)
  %584 : Tensor = prim::GetAttr[name="weight"](%565)
  %585 : Float(768:1, 768:768) = aten::t(%584), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %585), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %587 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %583, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1678:0
  %588 : int[] = prim::ListConstruct(%568, %570, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %589 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%587, %588), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.5, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.3 : Float(12:64, 64:1) = aten::mul(%564, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:542:0
  %592 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_w_bias.3, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:544:0
  %593 : Tensor[] = prim::ListConstruct(%592, %582), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %content_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%43, %593), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %v.3 : Float(12:64, 64:1) = aten::mul(%563, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:486:0
  %596 : Tensor[] = prim::ListConstruct(%157, %562), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %597 : Float(26:768, 12:64, 64:1) = aten::einsum(%44, %596), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %598 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %v.3, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:493:0
  %599 : Tensor[] = prim::ListConstruct(%598, %597), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.13 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%45, %599), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %601 : int = aten::size(%positional_attn.13, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %602 : int = aten::size(%positional_attn.13, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %603 : int = aten::size(%positional_attn.13, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %604 : int = aten::size(%positional_attn.13, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.3 : Long() = prim::NumToTensor(%604), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %606 : int[] = prim::ListConstruct(%601, %602, %604, %603), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.14 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.13, %606), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:428:0
  %608 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.14, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %609 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%608, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %610 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%609, %28, %58, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.15 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%610, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %612 : Long() = aten::sub(%max_rel_len.3, %23, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %613 : int = aten::Int(%612), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %614 : int[] = prim::ListConstruct(%601, %602, %603, %613), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.16 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.15, %614), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.17 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.16, %46, %59, %570, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.18 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.17, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:498:0
  %618 : int = aten::size(%token_type_mat.2, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %619 : int = aten::size(%token_type_mat.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %620 : int = aten::size(%token_type_mat.2, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.3 : Float(12:64, 64:1) = aten::mul(%561, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:508:0
  %622 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_s_bias.3, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:511:0
  %623 : Tensor[] = prim::ListConstruct(%622, %560), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %624 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%47, %623), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %625 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %626 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%625, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %627 : int = aten::size(%q_head.6, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %628 : int[] = prim::ListConstruct(%618, %627, %619, %620), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %token_type_mat.5 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%626, %628, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %630 : Tensor[] = aten::split(%624, %58, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:371:0
  %diff_token_type.3 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.3 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%630), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %633 : int = aten::size(%token_type_mat.5, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %634 : int = aten::size(%token_type_mat.5, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %635 : int = aten::size(%token_type_mat.5, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %636 : int = aten::size(%token_type_mat.5, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %637 : int[] = prim::ListConstruct(%633, %634, %635, %636), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %638 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.3, %637, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %639 : int = aten::size(%token_type_mat.5, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %640 : int = aten::size(%token_type_mat.5, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %641 : int = aten::size(%token_type_mat.5, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %642 : int = aten::size(%token_type_mat.5, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %643 : int[] = prim::ListConstruct(%639, %640, %641, %642), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %644 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.3, %643, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.5, %638, %644), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.5, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:522:0
  %647 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.3, %positional_attn.18, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%647, %token_type_attn.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.7, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:553:0
  %650 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %651 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%650, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %652 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%651, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %653 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%652, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %654 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%653, %58, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:396:0
  %655 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%654, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.8, %655, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %input.24 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.9, %50, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:558:0
  %658 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.24, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %659 : Tensor[] = prim::ListConstruct(%658, %589), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %attn_vec.3 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%49, %659), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %661 : int[] = prim::ListConstruct(%568, %569, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.3, %661), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:565:0
  %663 : Tensor = prim::GetAttr[name="bias"](%559)
  %664 : Tensor = prim::GetAttr[name="weight"](%559)
  %665 : Float(768:1, 768:768) = aten::t(%664), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %665), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.26 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %663, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.26, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.27 : Float(17:9984, 13:768, 768:1) = aten::add(%query.2, %attn_out.3, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:568:0
  %670 : Tensor = prim::GetAttr[name="bias"](%558)
  %671 : Tensor = prim::GetAttr[name="weight"](%558)
  %672 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.27, %672, %671, %670, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %674 : __torch__.torch.nn.modules.normalization.___torch_mangle_3303.LayerNorm = prim::GetAttr[name="layer_norm"](%556)
  %675 : __torch__.torch.nn.modules.linear.___torch_mangle_3301.Linear = prim::GetAttr[name="linear_2"](%556)
  %676 : __torch__.torch.nn.modules.linear.___torch_mangle_3299.Linear = prim::GetAttr[name="linear_1"](%556)
  %677 : Tensor = prim::GetAttr[name="bias"](%676)
  %678 : Tensor = prim::GetAttr[name="weight"](%676)
  %679 : Float(768:1, 3072:768) = aten::t(%678), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.14 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.28, %679), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.3 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.14, %677, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %682 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.3, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %683 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.3, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %684 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%683, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %685 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.3, %684, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %686 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%685, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %687 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%686), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %688 : Float(17:39936, 13:3072, 3072:1) = aten::add(%687, %38, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.29 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%682, %688), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.30 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.29, %39, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %691 : Tensor = prim::GetAttr[name="bias"](%675)
  %692 : Tensor = prim::GetAttr[name="weight"](%675)
  %693 : Float(3072:1, 768:3072) = aten::t(%692), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.30, %693), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %691, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.31, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.32 : Float(17:9984, 13:768, 768:1) = aten::add(%input.28, %h.3, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/modeling_funnel.py:588:0
  %698 : Tensor = prim::GetAttr[name="bias"](%674)
  %699 : Tensor = prim::GetAttr[name="weight"](%674)
  %700 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm
  %query.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.32, %700, %699, %698, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %702 : __torch__.transformers.modeling_funnel.___torch_mangle_3319.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%102)
  %703 : __torch__.transformers.modeling_funnel.___torch_mangle_3313.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%102)
  %704 : __torch__.torch.nn.modules.normalization.___torch_mangle_3312.LayerNorm = prim::GetAttr[name="layer_norm"](%703)
  %705 : __torch__.torch.nn.modules.linear.___torch_mangle_3311.Linear = prim::GetAttr[name="post_proj"](%703)
  %706 : Tensor = prim::GetAttr[name="seg_embed"](%703)
  %707 : Tensor = prim::GetAttr[name="r_s_bias"](%703)
  %708 : Tensor = prim::GetAttr[name="r_kernel"](%703)
  %709 : Tensor = prim::GetAttr[name="r_r_bias"](%703)
  %710 : Tensor = prim::GetAttr[name="r_w_bias"](%703)
  %711 : __torch__.torch.nn.modules.linear.___torch_mangle_3310.Linear = prim::GetAttr[name="v_head"](%703)
  %712 : __torch__.torch.nn.modules.linear.___torch_mangle_3309.Linear = prim::GetAttr[name="k_head"](%703)
  %713 : __torch__.torch.nn.modules.linear.___torch_mangle_3308.Linear = prim::GetAttr[name="q_head"](%703)
  %714 : int = aten::size(%query.3, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %715 : int = aten::size(%query.3, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %716 : int = aten::size(%query.3, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:531:0
  %717 : Tensor = prim::GetAttr[name="weight"](%713)
  %718 : Float(768:1, 768:768) = aten::t(%717), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %719 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %718), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %720 : int[] = prim::ListConstruct(%714, %715, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %q_head.7 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%719, %720), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:535:0
  %722 : Tensor = prim::GetAttr[name="bias"](%712)
  %723 : Tensor = prim::GetAttr[name="weight"](%712)
  %724 : Float(768:1, 768:768) = aten::t(%723), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %724), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %726 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %722, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1678:0
  %727 : int[] = prim::ListConstruct(%714, %716, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %728 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%726, %727), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:537:0
  %729 : Tensor = prim::GetAttr[name="bias"](%711)
  %730 : Tensor = prim::GetAttr[name="weight"](%711)
  %731 : Float(768:1, 768:768) = aten::t(%730), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.17 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %731), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %733 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.17, %729, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1678:0
  %734 : int[] = prim::ListConstruct(%714, %716, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %735 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%733, %734), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.7, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.4 : Float(12:64, 64:1) = aten::mul(%710, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:542:0
  %738 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_w_bias.4, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:544:0
  %739 : Tensor[] = prim::ListConstruct(%738, %728), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %content_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%43, %739), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %v.4 : Float(12:64, 64:1) = aten::mul(%709, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:486:0
  %742 : Tensor[] = prim::ListConstruct(%157, %708), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %743 : Float(26:768, 12:64, 64:1) = aten::einsum(%44, %742), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %744 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %v.4, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:493:0
  %745 : Tensor[] = prim::ListConstruct(%744, %743), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.19 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%45, %745), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %747 : int = aten::size(%positional_attn.19, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %748 : int = aten::size(%positional_attn.19, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %749 : int = aten::size(%positional_attn.19, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %750 : int = aten::size(%positional_attn.19, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.4 : Long() = prim::NumToTensor(%750), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %752 : int[] = prim::ListConstruct(%747, %748, %750, %749), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.20 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.19, %752), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:428:0
  %754 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.20, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %755 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%754, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %756 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%755, %28, %58, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.21 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%756, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %758 : Long() = aten::sub(%max_rel_len.4, %23, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %759 : int = aten::Int(%758), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %760 : int[] = prim::ListConstruct(%747, %748, %749, %759), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.22 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.21, %760), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.23 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.22, %46, %59, %716, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.24 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.23, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:498:0
  %764 : int = aten::size(%token_type_mat.2, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %765 : int = aten::size(%token_type_mat.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %766 : int = aten::size(%token_type_mat.2, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.4 : Float(12:64, 64:1) = aten::mul(%707, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:508:0
  %768 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_s_bias.4, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:511:0
  %769 : Tensor[] = prim::ListConstruct(%768, %706), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %770 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%47, %769), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %771 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %772 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%771, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %773 : int = aten::size(%q_head.8, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %774 : int[] = prim::ListConstruct(%764, %773, %765, %766), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %token_type_mat.6 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%772, %774, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %776 : Tensor[] = aten::split(%770, %58, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:371:0
  %diff_token_type.4 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.4 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%776), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %779 : int = aten::size(%token_type_mat.6, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %780 : int = aten::size(%token_type_mat.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %781 : int = aten::size(%token_type_mat.6, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %782 : int = aten::size(%token_type_mat.6, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %783 : int[] = prim::ListConstruct(%779, %780, %781, %782), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %784 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.4, %783, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %785 : int = aten::size(%token_type_mat.6, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %786 : int = aten::size(%token_type_mat.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %787 : int = aten::size(%token_type_mat.6, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %788 : int = aten::size(%token_type_mat.6, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %789 : int[] = prim::ListConstruct(%785, %786, %787, %788), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %790 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.4, %789, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.6, %784, %790), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.7, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:522:0
  %793 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.4, %positional_attn.24, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%793, %token_type_attn.8, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.10, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:553:0
  %796 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %797 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%796, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %798 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%797, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %799 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%798, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %800 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%799, %58, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:396:0
  %801 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%800, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.11, %801, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %input.33 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.12, %50, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:558:0
  %804 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.33, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %805 : Tensor[] = prim::ListConstruct(%804, %735), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %attn_vec.4 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%49, %805), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %807 : int[] = prim::ListConstruct(%714, %715, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.4, %807), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:565:0
  %809 : Tensor = prim::GetAttr[name="bias"](%705)
  %810 : Tensor = prim::GetAttr[name="weight"](%705)
  %811 : Float(768:1, 768:768) = aten::t(%810), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.34, %811), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %809, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.35, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.36 : Float(17:9984, 13:768, 768:1) = aten::add(%query.3, %attn_out.4, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:568:0
  %816 : Tensor = prim::GetAttr[name="bias"](%704)
  %817 : Tensor = prim::GetAttr[name="weight"](%704)
  %818 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm
  %input.37 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.36, %818, %817, %816, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %820 : __torch__.torch.nn.modules.normalization.___torch_mangle_3318.LayerNorm = prim::GetAttr[name="layer_norm"](%702)
  %821 : __torch__.torch.nn.modules.linear.___torch_mangle_3316.Linear = prim::GetAttr[name="linear_2"](%702)
  %822 : __torch__.torch.nn.modules.linear.___torch_mangle_3314.Linear = prim::GetAttr[name="linear_1"](%702)
  %823 : Tensor = prim::GetAttr[name="bias"](%822)
  %824 : Tensor = prim::GetAttr[name="weight"](%822)
  %825 : Float(768:1, 3072:768) = aten::t(%824), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.37, %825), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.4 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.19, %823, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %828 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.4, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %829 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.4, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %830 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%829, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %831 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.4, %830, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %832 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%831, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %833 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%832), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %834 : Float(17:39936, 13:3072, 3072:1) = aten::add(%833, %38, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.38 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%828, %834), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.39 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.38, %39, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %837 : Tensor = prim::GetAttr[name="bias"](%821)
  %838 : Tensor = prim::GetAttr[name="weight"](%821)
  %839 : Float(3072:1, 768:3072) = aten::t(%838), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.39, %839), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %837, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.40, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.41 : Float(17:9984, 13:768, 768:1) = aten::add(%input.37, %h.4, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/modeling_funnel.py:588:0
  %844 : Tensor = prim::GetAttr[name="bias"](%820)
  %845 : Tensor = prim::GetAttr[name="weight"](%820)
  %846 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm
  %hidden.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.41, %846, %845, %844, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %848 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %849 : Bool(17:169, 1:13, 13:1) = aten::slice(%848, %58, %59, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %850 : Tensor[] = prim::ListConstruct(%849, %token_type_mat.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.1 : Bool(17:182, 14:13, 13:1) = aten::cat(%850, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %852 : Bool(17:182, 14:13, 13:1) = aten::slice(%tensor.1, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.7 : Bool(17:182, 7:26, 13:1) = aten::slice(%852, %58, %59, %50, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %854 : Float(1:13, 13:1) = aten::slice(%cls_mask.1, %59, %59, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %855 : Tensor[] = prim::ListConstruct(%854, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder
  %tensor.2 : Float(14:13, 13:1) = aten::cat(%855, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.2 : Float(7:26, 13:1) = aten::slice(%tensor.2, %59, %59, %50, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %858 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden.1, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.1 : Float(17:9984, 12:768, 768:1) = aten::slice(%858, %58, %59, %50, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %860 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden.1, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %861 : Float(17:9984, 1:768, 768:1) = aten::slice(%860, %58, %59, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %862 : Tensor[] = prim::ListConstruct(%861, %suffix.1), scope: __module.funnel/__module.funnel.encoder
  %tensor.3 : Float(17:9984, 13:768, 768:1) = aten::cat(%862, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %864 : Float(17:9984, 13:768, 768:1) = aten::slice(%tensor.3, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %865 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::unsqueeze(%864, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %866 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%865, %28, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %tensor.4 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%866, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %868 : int[] = prim::ListConstruct(%28, %58), scope: __module.funnel/__module.funnel.encoder
  %869 : int[] = prim::ListConstruct(%28, %58), scope: __module.funnel/__module.funnel.encoder
  %870 : int[] = prim::ListConstruct(%59, %59), scope: __module.funnel/__module.funnel.encoder
  %tensor.5 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::avg_pool2d(%tensor.4, %868, %869, %870, %51, %51, %22), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
  %872 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%tensor.5, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %query.4 : Float(17:5376, 7:768, 768:1) = aten::select(%872, %58, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %874 : __torch__.transformers.modeling_funnel.___torch_mangle_3335.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%99)
  %875 : __torch__.transformers.modeling_funnel.___torch_mangle_3329.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%99)
  %876 : __torch__.torch.nn.modules.normalization.___torch_mangle_3328.LayerNorm = prim::GetAttr[name="layer_norm"](%875)
  %877 : __torch__.torch.nn.modules.linear.___torch_mangle_3327.Linear = prim::GetAttr[name="post_proj"](%875)
  %878 : Tensor = prim::GetAttr[name="seg_embed"](%875)
  %879 : Tensor = prim::GetAttr[name="r_s_bias"](%875)
  %880 : Tensor = prim::GetAttr[name="r_kernel"](%875)
  %881 : Tensor = prim::GetAttr[name="r_r_bias"](%875)
  %882 : Tensor = prim::GetAttr[name="r_w_bias"](%875)
  %883 : __torch__.torch.nn.modules.linear.___torch_mangle_3326.Linear = prim::GetAttr[name="v_head"](%875)
  %884 : __torch__.torch.nn.modules.linear.___torch_mangle_3325.Linear = prim::GetAttr[name="k_head"](%875)
  %885 : __torch__.torch.nn.modules.linear.___torch_mangle_3324.Linear = prim::GetAttr[name="q_head"](%875)
  %886 : int = aten::size(%query.4, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %887 : int = aten::size(%query.4, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %888 : int = aten::size(%hidden.1, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:531:0
  %889 : Tensor = prim::GetAttr[name="weight"](%885)
  %890 : Float(768:1, 768:768) = aten::t(%889), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %891 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.4, %890), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %892 : int[] = prim::ListConstruct(%886, %887, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %q_head.9 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%891, %892), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:535:0
  %894 : Tensor = prim::GetAttr[name="bias"](%884)
  %895 : Tensor = prim::GetAttr[name="weight"](%884)
  %896 : Float(768:1, 768:768) = aten::t(%895), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %896), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %898 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %894, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1678:0
  %899 : int[] = prim::ListConstruct(%886, %888, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %900 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%898, %899), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:537:0
  %901 : Tensor = prim::GetAttr[name="bias"](%883)
  %902 : Tensor = prim::GetAttr[name="weight"](%883)
  %903 : Float(768:1, 768:768) = aten::t(%902), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %903), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %905 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %901, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1678:0
  %906 : int[] = prim::ListConstruct(%886, %888, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %907 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%905, %906), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.10 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.9, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.5 : Float(12:64, 64:1) = aten::mul(%882, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:542:0
  %910 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_w_bias.5, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:544:0
  %911 : Tensor[] = prim::ListConstruct(%910, %900), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %content_score.5 : Float(17:1092, 12:91, 7:13, 13:1) = aten::einsum(%43, %911), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %v.5 : Float(12:64, 64:1) = aten::mul(%881, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:486:0
  %914 : Tensor[] = prim::ListConstruct(%181, %880), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %915 : Float(27:768, 12:64, 64:1) = aten::einsum(%44, %914), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %916 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %v.5, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:493:0
  %917 : Tensor[] = prim::ListConstruct(%916, %915), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.25 : Float(17:189, 12:3213, 7:27, 27:1) = aten::einsum(%45, %917), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %919 : int = aten::size(%positional_attn.25, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %920 : int = aten::size(%positional_attn.25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %921 : int = aten::size(%positional_attn.25, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %922 : int = aten::size(%positional_attn.25, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.5 : Long() = prim::NumToTensor(%922), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %924 : int[] = prim::ListConstruct(%919, %920, %922, %921), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.26 : Float(17:189, 12:3213, 27:7, 7:1) = aten::reshape(%positional_attn.25, %924), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:428:0
  %926 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%positional_attn.26, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %927 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%926, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %928 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%927, %28, %28, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.27 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%928, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %930 : Long() = aten::sub(%max_rel_len.5, %24, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %931 : int = aten::Int(%930), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %932 : int[] = prim::ListConstruct(%919, %920, %921, %931), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.28 : Float(17:189, 12:3213, 7:25, 25:1) = aten::reshape(%positional_attn.27, %932), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.29 : Float(17:189, 12:3213, 7:25, 13:1) = aten::slice(%positional_attn.28, %46, %59, %888, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.30 : Float(17:189, 12:3213, 7:25, 13:1) = aten::mul_(%positional_attn.29, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:498:0
  %936 : int = aten::size(%token_type_mat.7, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %937 : int = aten::size(%token_type_mat.7, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %938 : int = aten::size(%token_type_mat.7, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.5 : Float(12:64, 64:1) = aten::mul(%879, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:508:0
  %940 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_s_bias.5, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:511:0
  %941 : Tensor[] = prim::ListConstruct(%940, %878), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %942 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%47, %941), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %943 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %944 : Bool(17:182, 1:182, 7:26, 13:1) = aten::unsqueeze(%943, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %945 : int = aten::size(%q_head.10, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %946 : int[] = prim::ListConstruct(%936, %945, %937, %938), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %token_type_mat.8 : Bool(17:182, 12:0, 7:26, 13:1) = aten::expand(%944, %946, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %948 : Tensor[] = aten::split(%942, %58, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:371:0
  %diff_token_type.5 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.5 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%948), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %951 : int = aten::size(%token_type_mat.8, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %952 : int = aten::size(%token_type_mat.8, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %953 : int = aten::size(%token_type_mat.8, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %954 : int = aten::size(%token_type_mat.8, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %955 : int[] = prim::ListConstruct(%951, %952, %953, %954), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %956 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%same_token_type.5, %955, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %957 : int = aten::size(%token_type_mat.8, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %958 : int = aten::size(%token_type_mat.8, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %959 : int = aten::size(%token_type_mat.8, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %960 : int = aten::size(%token_type_mat.8, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %961 : int[] = prim::ListConstruct(%957, %958, %959, %960), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %962 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%diff_token_type.5, %961, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.9 : Float(17:1092, 12:91, 7:13, 13:1) = aten::where(%token_type_mat.8, %956, %962), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.10 : Float(17:1092, 12:91, 7:13, 13:1) = aten::mul_(%token_type_attn.9, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:522:0
  %965 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%content_score.5, %positional_attn.30, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.13 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%965, %token_type_attn.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.14 : Float(17:1092, 12:91, 7:13, 13:1) = aten::to(%attn_score.13, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:553:0
  %968 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %969 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%968, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %970 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%969, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %971 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%970, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %972 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%971, %58, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:396:0
  %973 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%972, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.15 : Float(17:1092, 12:91, 7:13, 13:1) = aten::sub(%attn_score.14, %973, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %input.42 : Float(17:1092, 12:91, 7:13, 13:1) = aten::softmax(%attn_score.15, %50, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:558:0
  %976 : Float(17:1092, 12:91, 7:13, 13:1) = aten::dropout(%input.42, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %977 : Tensor[] = prim::ListConstruct(%976, %907), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %attn_vec.5 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%49, %977), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %979 : int[] = prim::ListConstruct(%886, %887, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %input.43 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.5, %979), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:565:0
  %981 : Tensor = prim::GetAttr[name="bias"](%877)
  %982 : Tensor = prim::GetAttr[name="weight"](%877)
  %983 : Float(768:1, 768:768) = aten::t(%982), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.23 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.43, %983), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.44 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.23, %981, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.44, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:5376, 7:768, 768:1) = aten::add(%query.4, %attn_out.5, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:568:0
  %988 : Tensor = prim::GetAttr[name="bias"](%876)
  %989 : Tensor = prim::GetAttr[name="weight"](%876)
  %990 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm
  %input.46 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.45, %990, %989, %988, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %992 : __torch__.torch.nn.modules.normalization.___torch_mangle_3334.LayerNorm = prim::GetAttr[name="layer_norm"](%874)
  %993 : __torch__.torch.nn.modules.linear.___torch_mangle_3332.Linear = prim::GetAttr[name="linear_2"](%874)
  %994 : __torch__.torch.nn.modules.linear.___torch_mangle_3330.Linear = prim::GetAttr[name="linear_1"](%874)
  %995 : Tensor = prim::GetAttr[name="bias"](%994)
  %996 : Tensor = prim::GetAttr[name="weight"](%994)
  %997 : Float(768:1, 3072:768) = aten::t(%996), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.46, %997), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.5 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.24, %995, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1000 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.5, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1001 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.5, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1002 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1001, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1003 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.5, %1002, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1004 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1003, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1005 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1004), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1006 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1005, %38, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.47 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1000, %1006), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.48 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.47, %39, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1009 : Tensor = prim::GetAttr[name="bias"](%993)
  %1010 : Tensor = prim::GetAttr[name="weight"](%993)
  %1011 : Float(3072:1, 768:3072) = aten::t(%1010), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.25 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.48, %1011), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.49 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.25, %1009, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.49, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:5376, 7:768, 768:1) = aten::add(%input.46, %h.5, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/modeling_funnel.py:588:0
  %1016 : Tensor = prim::GetAttr[name="bias"](%992)
  %1017 : Tensor = prim::GetAttr[name="weight"](%992)
  %1018 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm
  %query.5 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.50, %1018, %1017, %1016, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1020 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1021 : Bool(17:182, 7:26, 13:1) = aten::slice(%1020, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1022 : Bool(17:182, 7:26, 1:1) = aten::slice(%1021, %28, %59, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1023 : Tensor[] = prim::ListConstruct(%1022, %token_type_mat.7), scope: __module.funnel/__module.funnel.encoder
  %tensor.6 : Bool(17:98, 7:14, 14:1) = aten::cat(%1023, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1025 : Bool(17:98, 7:14, 14:1) = aten::slice(%tensor.6, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1026 : Bool(17:98, 7:14, 14:1) = aten::slice(%1025, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.9 : Bool(17:98, 7:14, 7:2) = aten::slice(%1026, %28, %59, %50, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1028 : Float(7:26, 13:1) = aten::slice(%cls_mask.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1029 : Float(7:26, 1:1) = aten::slice(%1028, %58, %59, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1030 : Tensor[] = prim::ListConstruct(%1029, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.7 : Float(7:14, 14:1) = aten::cat(%1030, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1032 : Float(7:14, 14:1) = aten::slice(%tensor.7, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask.3 : Float(7:14, 7:2) = aten::slice(%1032, %58, %59, %50, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1034 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.2 : Float(17:13, 12:1) = aten::slice(%1034, %58, %59, %50, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1036 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1037 : Float(17:13, 1:1) = aten::slice(%1036, %58, %59, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1038 : Tensor[] = prim::ListConstruct(%1037, %suffix.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.8 : Float(17:13, 13:1) = aten::cat(%1038, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1040 : Float(17:13, 13:1) = aten::slice(%tensor.8, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1041 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%1040, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1042 : Float(17:13, 1:13, 13:1) = aten::slice(%1041, %28, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %tensor.9 : Float(17:13, 1:13, 13:1, 1:1) = aten::unsqueeze(%1042, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %input.51 : Float(17:13, 1:13, 13:1, 1:1) = aten::neg(%tensor.9), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1045 : int[] = prim::ListConstruct(%28, %58), scope: __module.funnel/__module.funnel.encoder
  %1046 : int[] = prim::ListConstruct(%28, %58), scope: __module.funnel/__module.funnel.encoder
  %1047 : int[] = prim::ListConstruct(%59, %59), scope: __module.funnel/__module.funnel.encoder
  %1048 : int[] = prim::ListConstruct(%58, %58), scope: __module.funnel/__module.funnel.encoder
  %1049 : Float(17:7, 1:7, 7:1, 1:1) = aten::max_pool2d(%input.51, %1045, %1046, %1047, %1048, %51), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
  %tensor.10 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%1049), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1051 : Float(17:7, 1:7, 7:1, 1:1) = aten::slice(%tensor.10, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1052 : Float(17:7, 7:1, 1:1) = aten::select(%1051, %58, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1053 : Float(17:7, 7:1, 1:1) = aten::slice(%1052, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask.3 : Float(17:7, 7:1) = aten::select(%1053, %28, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1055 : __torch__.transformers.modeling_funnel.___torch_mangle_3350.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%96)
  %1056 : __torch__.transformers.modeling_funnel.___torch_mangle_3344.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%96)
  %1057 : __torch__.torch.nn.modules.normalization.___torch_mangle_3343.LayerNorm = prim::GetAttr[name="layer_norm"](%1056)
  %1058 : __torch__.torch.nn.modules.linear.___torch_mangle_3342.Linear = prim::GetAttr[name="post_proj"](%1056)
  %1059 : Tensor = prim::GetAttr[name="seg_embed"](%1056)
  %1060 : Tensor = prim::GetAttr[name="r_s_bias"](%1056)
  %1061 : Tensor = prim::GetAttr[name="r_kernel"](%1056)
  %1062 : Tensor = prim::GetAttr[name="r_r_bias"](%1056)
  %1063 : Tensor = prim::GetAttr[name="r_w_bias"](%1056)
  %1064 : __torch__.torch.nn.modules.linear.___torch_mangle_3341.Linear = prim::GetAttr[name="v_head"](%1056)
  %1065 : __torch__.torch.nn.modules.linear.___torch_mangle_3340.Linear = prim::GetAttr[name="k_head"](%1056)
  %1066 : __torch__.torch.nn.modules.linear.___torch_mangle_3339.Linear = prim::GetAttr[name="q_head"](%1056)
  %1067 : int = aten::size(%query.5, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1068 : int = aten::size(%query.5, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1069 : int = aten::size(%query.5, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:531:0
  %1070 : Tensor = prim::GetAttr[name="weight"](%1066)
  %1071 : Float(768:1, 768:768) = aten::t(%1070), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1072 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1071), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1073 : int[] = prim::ListConstruct(%1067, %1068, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %q_head.11 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1072, %1073), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:535:0
  %1075 : Tensor = prim::GetAttr[name="bias"](%1065)
  %1076 : Tensor = prim::GetAttr[name="weight"](%1065)
  %1077 : Float(768:1, 768:768) = aten::t(%1076), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.26 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1077), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %1079 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.26, %1075, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1678:0
  %1080 : int[] = prim::ListConstruct(%1067, %1069, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1081 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1079, %1080), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:537:0
  %1082 : Tensor = prim::GetAttr[name="bias"](%1064)
  %1083 : Tensor = prim::GetAttr[name="weight"](%1064)
  %1084 : Float(768:1, 768:768) = aten::t(%1083), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.27 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1084), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %1086 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.27, %1082, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1678:0
  %1087 : int[] = prim::ListConstruct(%1067, %1069, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1088 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1086, %1087), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.12 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.11, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.6 : Float(12:64, 64:1) = aten::mul(%1063, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:542:0
  %1091 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_w_bias.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:544:0
  %1092 : Tensor[] = prim::ListConstruct(%1091, %1081), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %content_score.6 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%43, %1092), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %v.6 : Float(12:64, 64:1) = aten::mul(%1062, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:486:0
  %1095 : Tensor[] = prim::ListConstruct(%199, %1061), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1096 : Float(14:768, 12:64, 64:1) = aten::einsum(%44, %1095), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1097 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %v.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:493:0
  %1098 : Tensor[] = prim::ListConstruct(%1097, %1096), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.31 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%45, %1098), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1100 : int = aten::size(%positional_attn.31, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1101 : int = aten::size(%positional_attn.31, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1102 : int = aten::size(%positional_attn.31, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1103 : int = aten::size(%positional_attn.31, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.6 : Long() = prim::NumToTensor(%1103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1105 : int[] = prim::ListConstruct(%1100, %1101, %1103, %1102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.32 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.31, %1105), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:428:0
  %1107 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.32, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1108 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1107, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1109 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1108, %28, %58, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.33 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1109, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1111 : Long() = aten::sub(%max_rel_len.6, %23, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %1112 : int = aten::Int(%1111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1113 : int[] = prim::ListConstruct(%1100, %1101, %1102, %1112), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.34 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.33, %1113), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.35 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.34, %46, %59, %1069, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.36 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.35, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:498:0
  %1117 : int = aten::size(%token_type_mat.9, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1118 : int = aten::size(%token_type_mat.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1119 : int = aten::size(%token_type_mat.9, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.6 : Float(12:64, 64:1) = aten::mul(%1060, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:508:0
  %1121 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_s_bias.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:511:0
  %1122 : Tensor[] = prim::ListConstruct(%1121, %1059), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1123 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%47, %1122), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1124 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1125 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1124, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1126 : int = aten::size(%q_head.12, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1127 : int[] = prim::ListConstruct(%1117, %1126, %1118, %1119), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %token_type_mat.10 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1125, %1127, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1129 : Tensor[] = aten::split(%1123, %58, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:371:0
  %diff_token_type.6 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.6 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1129), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1132 : int = aten::size(%token_type_mat.10, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1133 : int = aten::size(%token_type_mat.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1134 : int = aten::size(%token_type_mat.10, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1135 : int = aten::size(%token_type_mat.10, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1136 : int[] = prim::ListConstruct(%1132, %1133, %1134, %1135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1137 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.6, %1136, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1138 : int = aten::size(%token_type_mat.10, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1139 : int = aten::size(%token_type_mat.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1140 : int = aten::size(%token_type_mat.10, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1141 : int = aten::size(%token_type_mat.10, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1142 : int[] = prim::ListConstruct(%1138, %1139, %1140, %1141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1143 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.6, %1142, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.11 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.10, %1137, %1143), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.12 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.11, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:522:0
  %1146 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.6, %positional_attn.36, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1146, %token_type_attn.12, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.17 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.16, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:553:0
  %1149 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1150 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1149, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1151 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1150, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1152 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1151, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1153 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1152, %58, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:396:0
  %1154 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1153, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.18 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.17, %1154, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %input.52 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.18, %50, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:558:0
  %1157 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.52, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1158 : Tensor[] = prim::ListConstruct(%1157, %1088), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %attn_vec.6 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%49, %1158), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1160 : int[] = prim::ListConstruct(%1067, %1068, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %input.53 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.6, %1160), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:565:0
  %1162 : Tensor = prim::GetAttr[name="bias"](%1058)
  %1163 : Tensor = prim::GetAttr[name="weight"](%1058)
  %1164 : Float(768:1, 768:768) = aten::t(%1163), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.28 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.53, %1164), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.54 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.28, %1162, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.54, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.55 : Float(17:5376, 7:768, 768:1) = aten::add(%query.5, %attn_out.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:568:0
  %1169 : Tensor = prim::GetAttr[name="bias"](%1057)
  %1170 : Tensor = prim::GetAttr[name="weight"](%1057)
  %1171 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm
  %input.56 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.55, %1171, %1170, %1169, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1173 : __torch__.torch.nn.modules.normalization.___torch_mangle_3349.LayerNorm = prim::GetAttr[name="layer_norm"](%1055)
  %1174 : __torch__.torch.nn.modules.linear.___torch_mangle_3347.Linear = prim::GetAttr[name="linear_2"](%1055)
  %1175 : __torch__.torch.nn.modules.linear.___torch_mangle_3345.Linear = prim::GetAttr[name="linear_1"](%1055)
  %1176 : Tensor = prim::GetAttr[name="bias"](%1175)
  %1177 : Tensor = prim::GetAttr[name="weight"](%1175)
  %1178 : Float(768:1, 3072:768) = aten::t(%1177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.56, %1178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.6 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.29, %1176, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1181 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.6, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1182 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.6, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1183 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1182, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1184 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.6, %1183, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1185 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1184, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1186 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1185), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1187 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1186, %38, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.57 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1181, %1187), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.58 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.57, %39, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1190 : Tensor = prim::GetAttr[name="bias"](%1174)
  %1191 : Tensor = prim::GetAttr[name="weight"](%1174)
  %1192 : Float(3072:1, 768:3072) = aten::t(%1191), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.58, %1192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.59 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.30, %1190, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.59, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:5376, 7:768, 768:1) = aten::add(%input.56, %h.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/modeling_funnel.py:588:0
  %1197 : Tensor = prim::GetAttr[name="bias"](%1173)
  %1198 : Tensor = prim::GetAttr[name="weight"](%1173)
  %1199 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm
  %query.6 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.60, %1199, %1198, %1197, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1201 : __torch__.transformers.modeling_funnel.___torch_mangle_3365.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%93)
  %1202 : __torch__.transformers.modeling_funnel.___torch_mangle_3359.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%93)
  %1203 : __torch__.torch.nn.modules.normalization.___torch_mangle_3358.LayerNorm = prim::GetAttr[name="layer_norm"](%1202)
  %1204 : __torch__.torch.nn.modules.linear.___torch_mangle_3357.Linear = prim::GetAttr[name="post_proj"](%1202)
  %1205 : Tensor = prim::GetAttr[name="seg_embed"](%1202)
  %1206 : Tensor = prim::GetAttr[name="r_s_bias"](%1202)
  %1207 : Tensor = prim::GetAttr[name="r_kernel"](%1202)
  %1208 : Tensor = prim::GetAttr[name="r_r_bias"](%1202)
  %1209 : Tensor = prim::GetAttr[name="r_w_bias"](%1202)
  %1210 : __torch__.torch.nn.modules.linear.___torch_mangle_3356.Linear = prim::GetAttr[name="v_head"](%1202)
  %1211 : __torch__.torch.nn.modules.linear.___torch_mangle_3355.Linear = prim::GetAttr[name="k_head"](%1202)
  %1212 : __torch__.torch.nn.modules.linear.___torch_mangle_3354.Linear = prim::GetAttr[name="q_head"](%1202)
  %1213 : int = aten::size(%query.6, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1214 : int = aten::size(%query.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1215 : int = aten::size(%query.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:531:0
  %1216 : Tensor = prim::GetAttr[name="weight"](%1212)
  %1217 : Float(768:1, 768:768) = aten::t(%1216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1218 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1217), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1219 : int[] = prim::ListConstruct(%1213, %1214, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %q_head.13 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1218, %1219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:535:0
  %1221 : Tensor = prim::GetAttr[name="bias"](%1211)
  %1222 : Tensor = prim::GetAttr[name="weight"](%1211)
  %1223 : Float(768:1, 768:768) = aten::t(%1222), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.31 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %1225 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.31, %1221, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1678:0
  %1226 : int[] = prim::ListConstruct(%1213, %1215, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1227 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1225, %1226), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:537:0
  %1228 : Tensor = prim::GetAttr[name="bias"](%1210)
  %1229 : Tensor = prim::GetAttr[name="weight"](%1210)
  %1230 : Float(768:1, 768:768) = aten::t(%1229), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.32 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1230), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %1232 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.32, %1228, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1678:0
  %1233 : int[] = prim::ListConstruct(%1213, %1215, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1234 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1232, %1233), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.14 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.13, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.7 : Float(12:64, 64:1) = aten::mul(%1209, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:542:0
  %1237 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_w_bias.7, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:544:0
  %1238 : Tensor[] = prim::ListConstruct(%1237, %1227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %content_score.7 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%43, %1238), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %v.7 : Float(12:64, 64:1) = aten::mul(%1208, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:486:0
  %1241 : Tensor[] = prim::ListConstruct(%199, %1207), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1242 : Float(14:768, 12:64, 64:1) = aten::einsum(%44, %1241), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1243 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %v.7, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:493:0
  %1244 : Tensor[] = prim::ListConstruct(%1243, %1242), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.37 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%45, %1244), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1246 : int = aten::size(%positional_attn.37, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1247 : int = aten::size(%positional_attn.37, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1248 : int = aten::size(%positional_attn.37, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1249 : int = aten::size(%positional_attn.37, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.7 : Long() = prim::NumToTensor(%1249), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1251 : int[] = prim::ListConstruct(%1246, %1247, %1249, %1248), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.38 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.37, %1251), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:428:0
  %1253 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.38, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1254 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1253, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1255 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1254, %28, %58, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.39 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1255, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1257 : Long() = aten::sub(%max_rel_len.7, %23, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %1258 : int = aten::Int(%1257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1259 : int[] = prim::ListConstruct(%1246, %1247, %1248, %1258), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.40 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.39, %1259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.41 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.40, %46, %59, %1215, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.42 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.41, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:498:0
  %1263 : int = aten::size(%token_type_mat.9, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1264 : int = aten::size(%token_type_mat.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1265 : int = aten::size(%token_type_mat.9, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.7 : Float(12:64, 64:1) = aten::mul(%1206, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:508:0
  %1267 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_s_bias.7, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:511:0
  %1268 : Tensor[] = prim::ListConstruct(%1267, %1205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1269 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%47, %1268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1270 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1271 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1270, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1272 : int = aten::size(%q_head.14, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1273 : int[] = prim::ListConstruct(%1263, %1272, %1264, %1265), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %token_type_mat.11 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1271, %1273, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1275 : Tensor[] = aten::split(%1269, %58, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:371:0
  %diff_token_type.7 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.7 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1275), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1278 : int = aten::size(%token_type_mat.11, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1279 : int = aten::size(%token_type_mat.11, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1280 : int = aten::size(%token_type_mat.11, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1281 : int = aten::size(%token_type_mat.11, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1282 : int[] = prim::ListConstruct(%1278, %1279, %1280, %1281), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1283 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.7, %1282, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1284 : int = aten::size(%token_type_mat.11, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1285 : int = aten::size(%token_type_mat.11, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1286 : int = aten::size(%token_type_mat.11, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1287 : int = aten::size(%token_type_mat.11, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1288 : int[] = prim::ListConstruct(%1284, %1285, %1286, %1287), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1289 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.7, %1288, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.13 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.11, %1283, %1289), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.14 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.13, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:522:0
  %1292 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.7, %positional_attn.42, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.19 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1292, %token_type_attn.14, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.20 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.19, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:553:0
  %1295 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1296 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1295, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1297 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1296, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1298 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1297, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1299 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1298, %58, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:396:0
  %1300 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1299, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.21 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.20, %1300, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %input.61 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.21, %50, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:558:0
  %1303 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.61, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1304 : Tensor[] = prim::ListConstruct(%1303, %1234), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %attn_vec.7 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%49, %1304), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1306 : int[] = prim::ListConstruct(%1213, %1214, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %input.62 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.7, %1306), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:565:0
  %1308 : Tensor = prim::GetAttr[name="bias"](%1204)
  %1309 : Tensor = prim::GetAttr[name="weight"](%1204)
  %1310 : Float(768:1, 768:768) = aten::t(%1309), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.33 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.62, %1310), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.63 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.33, %1308, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.63, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:5376, 7:768, 768:1) = aten::add(%query.6, %attn_out.7, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:568:0
  %1315 : Tensor = prim::GetAttr[name="bias"](%1203)
  %1316 : Tensor = prim::GetAttr[name="weight"](%1203)
  %1317 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm
  %input.65 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.64, %1317, %1316, %1315, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1319 : __torch__.torch.nn.modules.normalization.___torch_mangle_3364.LayerNorm = prim::GetAttr[name="layer_norm"](%1201)
  %1320 : __torch__.torch.nn.modules.linear.___torch_mangle_3362.Linear = prim::GetAttr[name="linear_2"](%1201)
  %1321 : __torch__.torch.nn.modules.linear.___torch_mangle_3360.Linear = prim::GetAttr[name="linear_1"](%1201)
  %1322 : Tensor = prim::GetAttr[name="bias"](%1321)
  %1323 : Tensor = prim::GetAttr[name="weight"](%1321)
  %1324 : Float(768:1, 3072:768) = aten::t(%1323), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.65, %1324), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.7 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.34, %1322, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1327 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.7, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1328 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.7, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1329 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1328, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1330 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.7, %1329, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1331 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1330, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1332 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1331), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1333 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1332, %38, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.66 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1327, %1333), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.67 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.66, %39, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1336 : Tensor = prim::GetAttr[name="bias"](%1320)
  %1337 : Tensor = prim::GetAttr[name="weight"](%1320)
  %1338 : Float(3072:1, 768:3072) = aten::t(%1337), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.67, %1338), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.68 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.35, %1336, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.68, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(17:5376, 7:768, 768:1) = aten::add(%input.65, %h.7, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/modeling_funnel.py:588:0
  %1343 : Tensor = prim::GetAttr[name="bias"](%1319)
  %1344 : Tensor = prim::GetAttr[name="weight"](%1319)
  %1345 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm
  %query.7 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.69, %1345, %1344, %1343, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1347 : __torch__.transformers.modeling_funnel.___torch_mangle_3380.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%90)
  %1348 : __torch__.transformers.modeling_funnel.___torch_mangle_3374.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%90)
  %1349 : __torch__.torch.nn.modules.normalization.___torch_mangle_3373.LayerNorm = prim::GetAttr[name="layer_norm"](%1348)
  %1350 : __torch__.torch.nn.modules.linear.___torch_mangle_3372.Linear = prim::GetAttr[name="post_proj"](%1348)
  %1351 : Tensor = prim::GetAttr[name="seg_embed"](%1348)
  %1352 : Tensor = prim::GetAttr[name="r_s_bias"](%1348)
  %1353 : Tensor = prim::GetAttr[name="r_kernel"](%1348)
  %1354 : Tensor = prim::GetAttr[name="r_r_bias"](%1348)
  %1355 : Tensor = prim::GetAttr[name="r_w_bias"](%1348)
  %1356 : __torch__.torch.nn.modules.linear.___torch_mangle_3371.Linear = prim::GetAttr[name="v_head"](%1348)
  %1357 : __torch__.torch.nn.modules.linear.___torch_mangle_3370.Linear = prim::GetAttr[name="k_head"](%1348)
  %1358 : __torch__.torch.nn.modules.linear.___torch_mangle_3369.Linear = prim::GetAttr[name="q_head"](%1348)
  %1359 : int = aten::size(%query.7, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1360 : int = aten::size(%query.7, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1361 : int = aten::size(%query.7, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:531:0
  %1362 : Tensor = prim::GetAttr[name="weight"](%1358)
  %1363 : Float(768:1, 768:768) = aten::t(%1362), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1364 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1363), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1365 : int[] = prim::ListConstruct(%1359, %1360, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %q_head.15 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1364, %1365), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:535:0
  %1367 : Tensor = prim::GetAttr[name="bias"](%1357)
  %1368 : Tensor = prim::GetAttr[name="weight"](%1357)
  %1369 : Float(768:1, 768:768) = aten::t(%1368), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.36 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1369), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %1371 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.36, %1367, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1678:0
  %1372 : int[] = prim::ListConstruct(%1359, %1361, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1373 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1371, %1372), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:537:0
  %1374 : Tensor = prim::GetAttr[name="bias"](%1356)
  %1375 : Tensor = prim::GetAttr[name="weight"](%1356)
  %1376 : Float(768:1, 768:768) = aten::t(%1375), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.37 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1376), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %1378 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.37, %1374, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1678:0
  %1379 : int[] = prim::ListConstruct(%1359, %1361, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1380 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1378, %1379), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.16 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.15, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.8 : Float(12:64, 64:1) = aten::mul(%1355, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:542:0
  %1383 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_w_bias.8, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:544:0
  %1384 : Tensor[] = prim::ListConstruct(%1383, %1373), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %content_score.8 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%43, %1384), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %v.8 : Float(12:64, 64:1) = aten::mul(%1354, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:486:0
  %1387 : Tensor[] = prim::ListConstruct(%199, %1353), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1388 : Float(14:768, 12:64, 64:1) = aten::einsum(%44, %1387), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1389 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %v.8, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:493:0
  %1390 : Tensor[] = prim::ListConstruct(%1389, %1388), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.43 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%45, %1390), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1392 : int = aten::size(%positional_attn.43, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1393 : int = aten::size(%positional_attn.43, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1394 : int = aten::size(%positional_attn.43, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1395 : int = aten::size(%positional_attn.43, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.8 : Long() = prim::NumToTensor(%1395), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1397 : int[] = prim::ListConstruct(%1392, %1393, %1395, %1394), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.44 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.43, %1397), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:428:0
  %1399 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.44, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1400 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1399, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1401 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1400, %28, %58, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.45 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1401, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1403 : Long() = aten::sub(%max_rel_len.8, %23, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %1404 : int = aten::Int(%1403), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1405 : int[] = prim::ListConstruct(%1392, %1393, %1394, %1404), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.46 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.45, %1405), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.47 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.46, %46, %59, %1361, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.48 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.47, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:498:0
  %1409 : int = aten::size(%token_type_mat.9, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1410 : int = aten::size(%token_type_mat.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1411 : int = aten::size(%token_type_mat.9, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.8 : Float(12:64, 64:1) = aten::mul(%1352, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:508:0
  %1413 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_s_bias.8, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:511:0
  %1414 : Tensor[] = prim::ListConstruct(%1413, %1351), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1415 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%47, %1414), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1416 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1417 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1416, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1418 : int = aten::size(%q_head.16, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1419 : int[] = prim::ListConstruct(%1409, %1418, %1410, %1411), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %token_type_mat.12 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1417, %1419, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1421 : Tensor[] = aten::split(%1415, %58, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:371:0
  %diff_token_type.8 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.8 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1421), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1424 : int = aten::size(%token_type_mat.12, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1425 : int = aten::size(%token_type_mat.12, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1426 : int = aten::size(%token_type_mat.12, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1427 : int = aten::size(%token_type_mat.12, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1428 : int[] = prim::ListConstruct(%1424, %1425, %1426, %1427), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1429 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.8, %1428, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1430 : int = aten::size(%token_type_mat.12, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1431 : int = aten::size(%token_type_mat.12, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1432 : int = aten::size(%token_type_mat.12, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1433 : int = aten::size(%token_type_mat.12, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1434 : int[] = prim::ListConstruct(%1430, %1431, %1432, %1433), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1435 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.8, %1434, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.15 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.12, %1429, %1435), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.15, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:522:0
  %1438 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.8, %positional_attn.48, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.22 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1438, %token_type_attn.16, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.23 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.22, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:553:0
  %1441 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1442 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1441, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1443 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1442, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1444 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1443, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1445 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1444, %58, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:396:0
  %1446 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1445, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.24 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.23, %1446, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %input.70 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.24, %50, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:558:0
  %1449 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.70, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %1450 : Tensor[] = prim::ListConstruct(%1449, %1380), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %attn_vec.8 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%49, %1450), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1452 : int[] = prim::ListConstruct(%1359, %1360, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %input.71 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.8, %1452), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:565:0
  %1454 : Tensor = prim::GetAttr[name="bias"](%1350)
  %1455 : Tensor = prim::GetAttr[name="weight"](%1350)
  %1456 : Float(768:1, 768:768) = aten::t(%1455), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.38 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.71, %1456), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.72 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.38, %1454, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.72, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:5376, 7:768, 768:1) = aten::add(%query.7, %attn_out.8, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:568:0
  %1461 : Tensor = prim::GetAttr[name="bias"](%1349)
  %1462 : Tensor = prim::GetAttr[name="weight"](%1349)
  %1463 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm
  %input.74 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.73, %1463, %1462, %1461, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %1465 : __torch__.torch.nn.modules.normalization.___torch_mangle_3379.LayerNorm = prim::GetAttr[name="layer_norm"](%1347)
  %1466 : __torch__.torch.nn.modules.linear.___torch_mangle_3377.Linear = prim::GetAttr[name="linear_2"](%1347)
  %1467 : __torch__.torch.nn.modules.linear.___torch_mangle_3375.Linear = prim::GetAttr[name="linear_1"](%1347)
  %1468 : Tensor = prim::GetAttr[name="bias"](%1467)
  %1469 : Tensor = prim::GetAttr[name="weight"](%1467)
  %1470 : Float(768:1, 3072:768) = aten::t(%1469), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.39 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.74, %1470), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.8 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.39, %1468, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1473 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.8, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1474 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.8, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1475 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1474, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1476 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.8, %1475, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1477 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1476, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1478 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1477), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1479 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1478, %38, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.75 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1473, %1479), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.76 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.75, %39, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1482 : Tensor = prim::GetAttr[name="bias"](%1466)
  %1483 : Tensor = prim::GetAttr[name="weight"](%1466)
  %1484 : Float(3072:1, 768:3072) = aten::t(%1483), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.40 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.76, %1484), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.77 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.40, %1482, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.77, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(17:5376, 7:768, 768:1) = aten::add(%input.74, %h.8, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/modeling_funnel.py:588:0
  %1489 : Tensor = prim::GetAttr[name="bias"](%1465)
  %1490 : Tensor = prim::GetAttr[name="weight"](%1465)
  %1491 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm
  %hidden : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.78, %1491, %1490, %1489, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1493 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1494 : Bool(17:98, 1:14, 7:2) = aten::slice(%1493, %58, %59, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1495 : Tensor[] = prim::ListConstruct(%1494, %token_type_mat.9), scope: __module.funnel/__module.funnel.encoder
  %tensor.11 : Bool(17:56, 8:7, 7:1) = aten::cat(%1495, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1497 : Bool(17:56, 8:7, 7:1) = aten::slice(%tensor.11, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.13 : Bool(17:56, 4:14, 7:1) = aten::slice(%1497, %58, %59, %50, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1499 : Float(1:14, 7:2) = aten::slice(%cls_mask.3, %59, %59, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1500 : Tensor[] = prim::ListConstruct(%1499, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder
  %tensor.12 : Float(8:7, 7:1) = aten::cat(%1500, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.4 : Float(4:14, 7:1) = aten::slice(%tensor.12, %59, %59, %50, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1503 : Float(17:5376, 7:768, 768:1) = aten::slice(%hidden, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.3 : Float(17:5376, 6:768, 768:1) = aten::slice(%1503, %58, %59, %50, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1505 : Float(17:5376, 7:768, 768:1) = aten::slice(%hidden, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1506 : Float(17:5376, 1:768, 768:1) = aten::slice(%1505, %58, %59, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1507 : Tensor[] = prim::ListConstruct(%1506, %suffix.3), scope: __module.funnel/__module.funnel.encoder
  %tensor.13 : Float(17:5376, 7:768, 768:1) = aten::cat(%1507, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1509 : Float(17:5376, 7:768, 768:1) = aten::slice(%tensor.13, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1510 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::unsqueeze(%1509, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1511 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%1510, %28, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %tensor.14 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%1511, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1513 : int[] = prim::ListConstruct(%28, %58), scope: __module.funnel/__module.funnel.encoder
  %1514 : int[] = prim::ListConstruct(%28, %58), scope: __module.funnel/__module.funnel.encoder
  %1515 : int[] = prim::ListConstruct(%59, %59), scope: __module.funnel/__module.funnel.encoder
  %tensor.15 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::avg_pool2d(%tensor.14, %1513, %1514, %1515, %51, %51, %22), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
  %1517 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::slice(%tensor.15, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %query.8 : Float(17:3072, 4:768, 768:1) = aten::select(%1517, %58, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %1519 : __torch__.transformers.modeling_funnel.___torch_mangle_3396.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%87)
  %1520 : __torch__.transformers.modeling_funnel.___torch_mangle_3390.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%87)
  %1521 : __torch__.torch.nn.modules.normalization.___torch_mangle_3389.LayerNorm = prim::GetAttr[name="layer_norm"](%1520)
  %1522 : __torch__.torch.nn.modules.linear.___torch_mangle_3388.Linear = prim::GetAttr[name="post_proj"](%1520)
  %1523 : Tensor = prim::GetAttr[name="seg_embed"](%1520)
  %1524 : Tensor = prim::GetAttr[name="r_s_bias"](%1520)
  %1525 : Tensor = prim::GetAttr[name="r_kernel"](%1520)
  %1526 : Tensor = prim::GetAttr[name="r_r_bias"](%1520)
  %1527 : Tensor = prim::GetAttr[name="r_w_bias"](%1520)
  %1528 : __torch__.torch.nn.modules.linear.___torch_mangle_3387.Linear = prim::GetAttr[name="v_head"](%1520)
  %1529 : __torch__.torch.nn.modules.linear.___torch_mangle_3386.Linear = prim::GetAttr[name="k_head"](%1520)
  %1530 : __torch__.torch.nn.modules.linear.___torch_mangle_3385.Linear = prim::GetAttr[name="q_head"](%1520)
  %1531 : int = aten::size(%query.8, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1532 : int = aten::size(%query.8, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1533 : int = aten::size(%hidden, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:531:0
  %1534 : Tensor = prim::GetAttr[name="weight"](%1530)
  %1535 : Float(768:1, 768:768) = aten::t(%1534), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1536 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.8, %1535), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1537 : int[] = prim::ListConstruct(%1531, %1532, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %q_head.17 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1536, %1537), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:535:0
  %1539 : Tensor = prim::GetAttr[name="bias"](%1529)
  %1540 : Tensor = prim::GetAttr[name="weight"](%1529)
  %1541 : Float(768:1, 768:768) = aten::t(%1540), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.41 : Float(17:5376, 7:768, 768:1) = aten::matmul(%hidden, %1541), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %1543 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.41, %1539, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1678:0
  %1544 : int[] = prim::ListConstruct(%1531, %1533, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1545 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1543, %1544), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:537:0
  %1546 : Tensor = prim::GetAttr[name="bias"](%1528)
  %1547 : Tensor = prim::GetAttr[name="weight"](%1528)
  %1548 : Float(768:1, 768:768) = aten::t(%1547), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.42 : Float(17:5376, 7:768, 768:1) = aten::matmul(%hidden, %1548), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %1550 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.42, %1546, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1678:0
  %1551 : int[] = prim::ListConstruct(%1531, %1533, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1552 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1550, %1551), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.18 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.17, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.9 : Float(12:64, 64:1) = aten::mul(%1527, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:542:0
  %1555 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_w_bias.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:544:0
  %1556 : Tensor[] = prim::ListConstruct(%1555, %1545), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %content_score.9 : Float(17:336, 12:28, 4:7, 7:1) = aten::einsum(%43, %1556), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %v.9 : Float(12:64, 64:1) = aten::mul(%1526, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:486:0
  %1559 : Tensor[] = prim::ListConstruct(%223, %1525), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1560 : Float(15:768, 12:64, 64:1) = aten::einsum(%44, %1559), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1561 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %v.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:493:0
  %1562 : Tensor[] = prim::ListConstruct(%1561, %1560), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.49 : Float(17:60, 12:1020, 4:15, 15:1) = aten::einsum(%45, %1562), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1564 : int = aten::size(%positional_attn.49, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1565 : int = aten::size(%positional_attn.49, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1566 : int = aten::size(%positional_attn.49, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1567 : int = aten::size(%positional_attn.49, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.9 : Long() = prim::NumToTensor(%1567), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1569 : int[] = prim::ListConstruct(%1564, %1565, %1567, %1566), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.50 : Float(17:60, 12:1020, 15:4, 4:1) = aten::reshape(%positional_attn.49, %1569), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:428:0
  %1571 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%positional_attn.50, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1572 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%1571, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1573 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%1572, %28, %28, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.51 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%1573, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1575 : Long() = aten::sub(%max_rel_len.9, %24, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %1576 : int = aten::Int(%1575), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1577 : int[] = prim::ListConstruct(%1564, %1565, %1566, %1576), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.52 : Float(17:60, 12:1020, 4:13, 13:1) = aten::reshape(%positional_attn.51, %1577), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.53 : Float(17:60, 12:1020, 4:13, 7:1) = aten::slice(%positional_attn.52, %46, %59, %1533, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.54 : Float(17:60, 12:1020, 4:13, 7:1) = aten::mul_(%positional_attn.53, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:498:0
  %1581 : int = aten::size(%token_type_mat.13, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1582 : int = aten::size(%token_type_mat.13, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1583 : int = aten::size(%token_type_mat.13, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.9 : Float(12:64, 64:1) = aten::mul(%1524, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:508:0
  %1585 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_s_bias.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:511:0
  %1586 : Tensor[] = prim::ListConstruct(%1585, %1523), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1587 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%47, %1586), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1588 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1589 : Bool(17:56, 1:56, 4:14, 7:1) = aten::unsqueeze(%1588, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1590 : int = aten::size(%q_head.18, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1591 : int[] = prim::ListConstruct(%1581, %1590, %1582, %1583), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %token_type_mat.14 : Bool(17:56, 12:0, 4:14, 7:1) = aten::expand(%1589, %1591, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1593 : Tensor[] = aten::split(%1587, %58, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:371:0
  %diff_token_type.9 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.9 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1593), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1596 : int = aten::size(%token_type_mat.14, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1597 : int = aten::size(%token_type_mat.14, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1598 : int = aten::size(%token_type_mat.14, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1599 : int = aten::size(%token_type_mat.14, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1600 : int[] = prim::ListConstruct(%1596, %1597, %1598, %1599), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1601 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%same_token_type.9, %1600, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1602 : int = aten::size(%token_type_mat.14, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1603 : int = aten::size(%token_type_mat.14, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1604 : int = aten::size(%token_type_mat.14, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1605 : int = aten::size(%token_type_mat.14, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1606 : int[] = prim::ListConstruct(%1602, %1603, %1604, %1605), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1607 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%diff_token_type.9, %1606, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.17 : Float(17:336, 12:28, 4:7, 7:1) = aten::where(%token_type_mat.14, %1601, %1607), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.18 : Float(17:336, 12:28, 4:7, 7:1) = aten::mul_(%token_type_attn.17, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:522:0
  %1610 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%content_score.9, %positional_attn.54, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.25 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%1610, %token_type_attn.18, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.26 : Float(17:336, 12:28, 4:7, 7:1) = aten::to(%attn_score.25, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:553:0
  %1613 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1614 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1613, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1615 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1614, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1616 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1615, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1617 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1616, %58, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:396:0
  %1618 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1617, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.27 : Float(17:336, 12:28, 4:7, 7:1) = aten::sub(%attn_score.26, %1618, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %input.79 : Float(17:336, 12:28, 4:7, 7:1) = aten::softmax(%attn_score.27, %50, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:558:0
  %1621 : Float(17:336, 12:28, 4:7, 7:1) = aten::dropout(%input.79, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %1622 : Tensor[] = prim::ListConstruct(%1621, %1552), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %attn_vec.9 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%49, %1622), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1624 : int[] = prim::ListConstruct(%1531, %1532, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %input.80 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.9, %1624), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:565:0
  %1626 : Tensor = prim::GetAttr[name="bias"](%1522)
  %1627 : Tensor = prim::GetAttr[name="weight"](%1522)
  %1628 : Float(768:1, 768:768) = aten::t(%1627), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.43 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.80, %1628), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.81 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.43, %1626, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.81, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.82 : Float(17:3072, 4:768, 768:1) = aten::add(%query.8, %attn_out.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:568:0
  %1633 : Tensor = prim::GetAttr[name="bias"](%1521)
  %1634 : Tensor = prim::GetAttr[name="weight"](%1521)
  %1635 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm
  %input.83 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.82, %1635, %1634, %1633, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %1637 : __torch__.torch.nn.modules.normalization.___torch_mangle_3395.LayerNorm = prim::GetAttr[name="layer_norm"](%1519)
  %1638 : __torch__.torch.nn.modules.linear.___torch_mangle_3393.Linear = prim::GetAttr[name="linear_2"](%1519)
  %1639 : __torch__.torch.nn.modules.linear.___torch_mangle_3391.Linear = prim::GetAttr[name="linear_1"](%1519)
  %1640 : Tensor = prim::GetAttr[name="bias"](%1639)
  %1641 : Tensor = prim::GetAttr[name="weight"](%1639)
  %1642 : Float(768:1, 3072:768) = aten::t(%1641), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.44 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.83, %1642), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.9 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.44, %1640, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1645 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.9, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1646 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.9, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1647 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1646, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1648 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.9, %1647, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1649 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1648, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1650 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1649), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1651 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1650, %38, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.84 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1645, %1651), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.85 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.84, %39, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1654 : Tensor = prim::GetAttr[name="bias"](%1638)
  %1655 : Tensor = prim::GetAttr[name="weight"](%1638)
  %1656 : Float(3072:1, 768:3072) = aten::t(%1655), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.45 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.85, %1656), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.86 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.45, %1654, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.86, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(17:3072, 4:768, 768:1) = aten::add(%input.83, %h.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/modeling_funnel.py:588:0
  %1661 : Tensor = prim::GetAttr[name="bias"](%1637)
  %1662 : Tensor = prim::GetAttr[name="weight"](%1637)
  %1663 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm
  %query.9 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.87, %1663, %1662, %1661, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1665 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1666 : Bool(17:56, 4:14, 7:1) = aten::slice(%1665, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1667 : Bool(17:56, 4:14, 1:1) = aten::slice(%1666, %28, %59, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1668 : Tensor[] = prim::ListConstruct(%1667, %token_type_mat.13), scope: __module.funnel/__module.funnel.encoder
  %tensor.16 : Bool(17:32, 4:8, 8:1) = aten::cat(%1668, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1670 : Bool(17:32, 4:8, 8:1) = aten::slice(%tensor.16, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1671 : Bool(17:32, 4:8, 8:1) = aten::slice(%1670, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.15 : Bool(17:32, 4:8, 4:2) = aten::slice(%1671, %28, %59, %50, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1673 : Float(4:14, 7:1) = aten::slice(%cls_mask.4, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1674 : Float(4:14, 1:1) = aten::slice(%1673, %58, %59, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1675 : Tensor[] = prim::ListConstruct(%1674, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder
  %tensor.17 : Float(4:8, 8:1) = aten::cat(%1675, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1677 : Float(4:8, 8:1) = aten::slice(%tensor.17, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask.5 : Float(4:8, 4:2) = aten::slice(%1677, %58, %59, %50, %28), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1679 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix : Float(17:7, 6:1) = aten::slice(%1679, %58, %59, %50, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1681 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1682 : Float(17:7, 1:1) = aten::slice(%1681, %58, %59, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1683 : Tensor[] = prim::ListConstruct(%1682, %suffix), scope: __module.funnel/__module.funnel.encoder
  %tensor.18 : Float(17:7, 7:1) = aten::cat(%1683, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1685 : Float(17:7, 7:1) = aten::slice(%tensor.18, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1686 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1685, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1687 : Float(17:7, 1:7, 7:1) = aten::slice(%1686, %28, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %tensor.19 : Float(17:7, 1:7, 7:1, 1:1) = aten::unsqueeze(%1687, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %input.88 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%tensor.19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1690 : int[] = prim::ListConstruct(%28, %58), scope: __module.funnel/__module.funnel.encoder
  %1691 : int[] = prim::ListConstruct(%28, %58), scope: __module.funnel/__module.funnel.encoder
  %1692 : int[] = prim::ListConstruct(%59, %59), scope: __module.funnel/__module.funnel.encoder
  %1693 : int[] = prim::ListConstruct(%58, %58), scope: __module.funnel/__module.funnel.encoder
  %1694 : Float(17:4, 1:4, 4:1, 1:1) = aten::max_pool2d(%input.88, %1690, %1691, %1692, %1693, %51), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
  %tensor : Float(17:4, 1:4, 4:1, 1:1) = aten::neg(%1694), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1696 : Float(17:4, 1:4, 4:1, 1:1) = aten::slice(%tensor, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1697 : Float(17:4, 4:1, 1:1) = aten::select(%1696, %58, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1698 : Float(17:4, 4:1, 1:1) = aten::slice(%1697, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask : Float(17:4, 4:1) = aten::select(%1698, %28, %59), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1700 : __torch__.transformers.modeling_funnel.___torch_mangle_3411.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%84)
  %1701 : __torch__.transformers.modeling_funnel.___torch_mangle_3405.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%84)
  %1702 : __torch__.torch.nn.modules.normalization.___torch_mangle_3404.LayerNorm = prim::GetAttr[name="layer_norm"](%1701)
  %1703 : __torch__.torch.nn.modules.linear.___torch_mangle_3403.Linear = prim::GetAttr[name="post_proj"](%1701)
  %1704 : Tensor = prim::GetAttr[name="seg_embed"](%1701)
  %1705 : Tensor = prim::GetAttr[name="r_s_bias"](%1701)
  %1706 : Tensor = prim::GetAttr[name="r_kernel"](%1701)
  %1707 : Tensor = prim::GetAttr[name="r_r_bias"](%1701)
  %1708 : Tensor = prim::GetAttr[name="r_w_bias"](%1701)
  %1709 : __torch__.torch.nn.modules.linear.___torch_mangle_3402.Linear = prim::GetAttr[name="v_head"](%1701)
  %1710 : __torch__.torch.nn.modules.linear.___torch_mangle_3401.Linear = prim::GetAttr[name="k_head"](%1701)
  %1711 : __torch__.torch.nn.modules.linear.___torch_mangle_3400.Linear = prim::GetAttr[name="q_head"](%1701)
  %1712 : int = aten::size(%query.9, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1713 : int = aten::size(%query.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1714 : int = aten::size(%query.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:531:0
  %1715 : Tensor = prim::GetAttr[name="weight"](%1711)
  %1716 : Float(768:1, 768:768) = aten::t(%1715), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1717 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1716), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1718 : int[] = prim::ListConstruct(%1712, %1713, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %q_head.19 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1717, %1718), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:535:0
  %1720 : Tensor = prim::GetAttr[name="bias"](%1710)
  %1721 : Tensor = prim::GetAttr[name="weight"](%1710)
  %1722 : Float(768:1, 768:768) = aten::t(%1721), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.46 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1722), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %1724 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.46, %1720, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1678:0
  %1725 : int[] = prim::ListConstruct(%1712, %1714, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1726 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1724, %1725), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:537:0
  %1727 : Tensor = prim::GetAttr[name="bias"](%1709)
  %1728 : Tensor = prim::GetAttr[name="weight"](%1709)
  %1729 : Float(768:1, 768:768) = aten::t(%1728), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.47 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1729), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %1731 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.47, %1727, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1678:0
  %1732 : int[] = prim::ListConstruct(%1712, %1714, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1733 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1731, %1732), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.20 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.19, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.10 : Float(12:64, 64:1) = aten::mul(%1708, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:542:0
  %1736 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_w_bias.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:544:0
  %1737 : Tensor[] = prim::ListConstruct(%1736, %1726), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %content_score.10 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%43, %1737), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %v.10 : Float(12:64, 64:1) = aten::mul(%1707, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:486:0
  %1740 : Tensor[] = prim::ListConstruct(%241, %1706), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1741 : Float(8:768, 12:64, 64:1) = aten::einsum(%44, %1740), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1742 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %v.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:493:0
  %1743 : Tensor[] = prim::ListConstruct(%1742, %1741), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.55 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%45, %1743), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1745 : int = aten::size(%positional_attn.55, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1746 : int = aten::size(%positional_attn.55, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1747 : int = aten::size(%positional_attn.55, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1748 : int = aten::size(%positional_attn.55, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.10 : Long() = prim::NumToTensor(%1748), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1750 : int[] = prim::ListConstruct(%1745, %1746, %1748, %1747), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.56 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.55, %1750), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:428:0
  %1752 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.56, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1753 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%1752, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1754 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1753, %28, %58, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.57 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1754, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1756 : Long() = aten::sub(%max_rel_len.10, %23, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %1757 : int = aten::Int(%1756), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1758 : int[] = prim::ListConstruct(%1745, %1746, %1747, %1757), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.58 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.57, %1758), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.59 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.58, %46, %59, %1714, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.60 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.59, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:498:0
  %1762 : int = aten::size(%token_type_mat.15, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1763 : int = aten::size(%token_type_mat.15, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1764 : int = aten::size(%token_type_mat.15, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.10 : Float(12:64, 64:1) = aten::mul(%1705, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:508:0
  %1766 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_s_bias.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:511:0
  %1767 : Tensor[] = prim::ListConstruct(%1766, %1704), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1768 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%47, %1767), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1769 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1770 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1769, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1771 : int = aten::size(%q_head.20, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1772 : int[] = prim::ListConstruct(%1762, %1771, %1763, %1764), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %token_type_mat.16 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%1770, %1772, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1774 : Tensor[] = aten::split(%1768, %58, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:371:0
  %diff_token_type.10 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.10 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1774), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1777 : int = aten::size(%token_type_mat.16, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1778 : int = aten::size(%token_type_mat.16, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1779 : int = aten::size(%token_type_mat.16, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1780 : int = aten::size(%token_type_mat.16, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1781 : int[] = prim::ListConstruct(%1777, %1778, %1779, %1780), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1782 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.10, %1781, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1783 : int = aten::size(%token_type_mat.16, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1784 : int = aten::size(%token_type_mat.16, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1785 : int = aten::size(%token_type_mat.16, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1786 : int = aten::size(%token_type_mat.16, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1787 : int[] = prim::ListConstruct(%1783, %1784, %1785, %1786), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1788 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.10, %1787, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.19 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.16, %1782, %1788), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.20 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.19, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:522:0
  %1791 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.10, %positional_attn.60, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.28 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%1791, %token_type_attn.20, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.29 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.28, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:553:0
  %1794 : Float(17:4, 4:1) = aten::slice(%attention_mask, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1795 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%1794, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1796 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1795, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1797 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%1796, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1798 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%1797, %58, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:396:0
  %1799 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%1798, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.30 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.29, %1799, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %input.89 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.30, %50, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:558:0
  %1802 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.89, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1803 : Tensor[] = prim::ListConstruct(%1802, %1733), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %attn_vec.10 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%49, %1803), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1805 : int[] = prim::ListConstruct(%1712, %1713, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %input.90 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.10, %1805), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:565:0
  %1807 : Tensor = prim::GetAttr[name="bias"](%1703)
  %1808 : Tensor = prim::GetAttr[name="weight"](%1703)
  %1809 : Float(768:1, 768:768) = aten::t(%1808), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.48 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.90, %1809), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.91 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.48, %1807, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.91, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.92 : Float(17:3072, 4:768, 768:1) = aten::add(%query.9, %attn_out.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:568:0
  %1814 : Tensor = prim::GetAttr[name="bias"](%1702)
  %1815 : Tensor = prim::GetAttr[name="weight"](%1702)
  %1816 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm
  %input.93 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.92, %1816, %1815, %1814, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1818 : __torch__.torch.nn.modules.normalization.___torch_mangle_3410.LayerNorm = prim::GetAttr[name="layer_norm"](%1700)
  %1819 : __torch__.torch.nn.modules.linear.___torch_mangle_3408.Linear = prim::GetAttr[name="linear_2"](%1700)
  %1820 : __torch__.torch.nn.modules.linear.___torch_mangle_3406.Linear = prim::GetAttr[name="linear_1"](%1700)
  %1821 : Tensor = prim::GetAttr[name="bias"](%1820)
  %1822 : Tensor = prim::GetAttr[name="weight"](%1820)
  %1823 : Float(768:1, 3072:768) = aten::t(%1822), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.93, %1823), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.10 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.49, %1821, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1826 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.10, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1827 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.10, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1828 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1827, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1829 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.10, %1828, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1830 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1829, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1831 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1830), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1832 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1831, %38, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.94 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1826, %1832), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.95 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.94, %39, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1835 : Tensor = prim::GetAttr[name="bias"](%1819)
  %1836 : Tensor = prim::GetAttr[name="weight"](%1819)
  %1837 : Float(3072:1, 768:3072) = aten::t(%1836), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.95, %1837), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.96 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.50, %1835, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.96, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.97 : Float(17:3072, 4:768, 768:1) = aten::add(%input.93, %h.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/modeling_funnel.py:588:0
  %1842 : Tensor = prim::GetAttr[name="bias"](%1818)
  %1843 : Tensor = prim::GetAttr[name="weight"](%1818)
  %1844 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm
  %query.10 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.97, %1844, %1843, %1842, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1846 : __torch__.transformers.modeling_funnel.___torch_mangle_3426.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%81)
  %1847 : __torch__.transformers.modeling_funnel.___torch_mangle_3420.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%81)
  %1848 : __torch__.torch.nn.modules.normalization.___torch_mangle_3419.LayerNorm = prim::GetAttr[name="layer_norm"](%1847)
  %1849 : __torch__.torch.nn.modules.linear.___torch_mangle_3418.Linear = prim::GetAttr[name="post_proj"](%1847)
  %1850 : Tensor = prim::GetAttr[name="seg_embed"](%1847)
  %1851 : Tensor = prim::GetAttr[name="r_s_bias"](%1847)
  %1852 : Tensor = prim::GetAttr[name="r_kernel"](%1847)
  %1853 : Tensor = prim::GetAttr[name="r_r_bias"](%1847)
  %1854 : Tensor = prim::GetAttr[name="r_w_bias"](%1847)
  %1855 : __torch__.torch.nn.modules.linear.___torch_mangle_3417.Linear = prim::GetAttr[name="v_head"](%1847)
  %1856 : __torch__.torch.nn.modules.linear.___torch_mangle_3416.Linear = prim::GetAttr[name="k_head"](%1847)
  %1857 : __torch__.torch.nn.modules.linear.___torch_mangle_3415.Linear = prim::GetAttr[name="q_head"](%1847)
  %1858 : int = aten::size(%query.10, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1859 : int = aten::size(%query.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1860 : int = aten::size(%query.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:531:0
  %1861 : Tensor = prim::GetAttr[name="weight"](%1857)
  %1862 : Float(768:1, 768:768) = aten::t(%1861), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1863 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1862), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1864 : int[] = prim::ListConstruct(%1858, %1859, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %q_head.21 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1863, %1864), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:535:0
  %1866 : Tensor = prim::GetAttr[name="bias"](%1856)
  %1867 : Tensor = prim::GetAttr[name="weight"](%1856)
  %1868 : Float(768:1, 768:768) = aten::t(%1867), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.51 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1868), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %1870 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.51, %1866, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1678:0
  %1871 : int[] = prim::ListConstruct(%1858, %1860, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1872 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1870, %1871), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:537:0
  %1873 : Tensor = prim::GetAttr[name="bias"](%1855)
  %1874 : Tensor = prim::GetAttr[name="weight"](%1855)
  %1875 : Float(768:1, 768:768) = aten::t(%1874), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.52 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1875), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %1877 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.52, %1873, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1678:0
  %1878 : int[] = prim::ListConstruct(%1858, %1860, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1879 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1877, %1878), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.22 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.21, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.11 : Float(12:64, 64:1) = aten::mul(%1854, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:542:0
  %1882 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_w_bias.11, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:544:0
  %1883 : Tensor[] = prim::ListConstruct(%1882, %1872), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %content_score.11 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%43, %1883), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %v.11 : Float(12:64, 64:1) = aten::mul(%1853, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:486:0
  %1886 : Tensor[] = prim::ListConstruct(%241, %1852), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1887 : Float(8:768, 12:64, 64:1) = aten::einsum(%44, %1886), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1888 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %v.11, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:493:0
  %1889 : Tensor[] = prim::ListConstruct(%1888, %1887), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.61 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%45, %1889), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1891 : int = aten::size(%positional_attn.61, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1892 : int = aten::size(%positional_attn.61, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1893 : int = aten::size(%positional_attn.61, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1894 : int = aten::size(%positional_attn.61, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.11 : Long() = prim::NumToTensor(%1894), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1896 : int[] = prim::ListConstruct(%1891, %1892, %1894, %1893), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.62 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.61, %1896), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:428:0
  %1898 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.62, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1899 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%1898, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1900 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1899, %28, %58, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.63 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1900, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1902 : Long() = aten::sub(%max_rel_len.11, %23, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %1903 : int = aten::Int(%1902), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1904 : int[] = prim::ListConstruct(%1891, %1892, %1893, %1903), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.64 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.63, %1904), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.65 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.64, %46, %59, %1860, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.66 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.65, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:498:0
  %1908 : int = aten::size(%token_type_mat.15, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1909 : int = aten::size(%token_type_mat.15, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1910 : int = aten::size(%token_type_mat.15, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.11 : Float(12:64, 64:1) = aten::mul(%1851, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:508:0
  %1912 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_s_bias.11, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:511:0
  %1913 : Tensor[] = prim::ListConstruct(%1912, %1850), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1914 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%47, %1913), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1915 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1916 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1915, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1917 : int = aten::size(%q_head.22, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1918 : int[] = prim::ListConstruct(%1908, %1917, %1909, %1910), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %token_type_mat.17 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%1916, %1918, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1920 : Tensor[] = aten::split(%1914, %58, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:371:0
  %diff_token_type.11 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.11 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1920), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1923 : int = aten::size(%token_type_mat.17, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1924 : int = aten::size(%token_type_mat.17, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1925 : int = aten::size(%token_type_mat.17, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1926 : int = aten::size(%token_type_mat.17, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1927 : int[] = prim::ListConstruct(%1923, %1924, %1925, %1926), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1928 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.11, %1927, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1929 : int = aten::size(%token_type_mat.17, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1930 : int = aten::size(%token_type_mat.17, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1931 : int = aten::size(%token_type_mat.17, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1932 : int = aten::size(%token_type_mat.17, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1933 : int[] = prim::ListConstruct(%1929, %1930, %1931, %1932), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1934 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.11, %1933, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.21 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.17, %1928, %1934), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.22 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.21, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:522:0
  %1937 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.11, %positional_attn.66, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.31 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%1937, %token_type_attn.22, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.32 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.31, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:553:0
  %1940 : Float(17:4, 4:1) = aten::slice(%attention_mask, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1941 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%1940, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1942 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1941, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1943 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%1942, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1944 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%1943, %58, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:396:0
  %1945 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%1944, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.33 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.32, %1945, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %input.98 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.33, %50, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:558:0
  %1948 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.98, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1949 : Tensor[] = prim::ListConstruct(%1948, %1879), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %attn_vec.11 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%49, %1949), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1951 : int[] = prim::ListConstruct(%1858, %1859, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %input.99 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.11, %1951), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:565:0
  %1953 : Tensor = prim::GetAttr[name="bias"](%1849)
  %1954 : Tensor = prim::GetAttr[name="weight"](%1849)
  %1955 : Float(768:1, 768:768) = aten::t(%1954), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.53 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.99, %1955), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.100 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.53, %1953, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.100, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.101 : Float(17:3072, 4:768, 768:1) = aten::add(%query.10, %attn_out.11, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:568:0
  %1960 : Tensor = prim::GetAttr[name="bias"](%1848)
  %1961 : Tensor = prim::GetAttr[name="weight"](%1848)
  %1962 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm
  %input.102 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.101, %1962, %1961, %1960, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1964 : __torch__.torch.nn.modules.normalization.___torch_mangle_3425.LayerNorm = prim::GetAttr[name="layer_norm"](%1846)
  %1965 : __torch__.torch.nn.modules.linear.___torch_mangle_3423.Linear = prim::GetAttr[name="linear_2"](%1846)
  %1966 : __torch__.torch.nn.modules.linear.___torch_mangle_3421.Linear = prim::GetAttr[name="linear_1"](%1846)
  %1967 : Tensor = prim::GetAttr[name="bias"](%1966)
  %1968 : Tensor = prim::GetAttr[name="weight"](%1966)
  %1969 : Float(768:1, 3072:768) = aten::t(%1968), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.102, %1969), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.11 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.54, %1967, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1972 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.11, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1973 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.11, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1974 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1973, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1975 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.11, %1974, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1976 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1975, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1977 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1976), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1978 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1977, %38, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.103 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1972, %1978), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.104 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.103, %39, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1981 : Tensor = prim::GetAttr[name="bias"](%1965)
  %1982 : Tensor = prim::GetAttr[name="weight"](%1965)
  %1983 : Float(3072:1, 768:3072) = aten::t(%1982), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.55 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.104, %1983), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.105 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.55, %1981, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.105, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.106 : Float(17:3072, 4:768, 768:1) = aten::add(%input.102, %h.11, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/modeling_funnel.py:588:0
  %1988 : Tensor = prim::GetAttr[name="bias"](%1964)
  %1989 : Tensor = prim::GetAttr[name="weight"](%1964)
  %1990 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm
  %query.11 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.106, %1990, %1989, %1988, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1992 : __torch__.transformers.modeling_funnel.___torch_mangle_3441.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%78)
  %1993 : __torch__.transformers.modeling_funnel.___torch_mangle_3435.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%78)
  %1994 : __torch__.torch.nn.modules.normalization.___torch_mangle_3434.LayerNorm = prim::GetAttr[name="layer_norm"](%1993)
  %1995 : __torch__.torch.nn.modules.linear.___torch_mangle_3433.Linear = prim::GetAttr[name="post_proj"](%1993)
  %1996 : Tensor = prim::GetAttr[name="seg_embed"](%1993)
  %1997 : Tensor = prim::GetAttr[name="r_s_bias"](%1993)
  %1998 : Tensor = prim::GetAttr[name="r_kernel"](%1993)
  %1999 : Tensor = prim::GetAttr[name="r_r_bias"](%1993)
  %2000 : Tensor = prim::GetAttr[name="r_w_bias"](%1993)
  %2001 : __torch__.torch.nn.modules.linear.___torch_mangle_3432.Linear = prim::GetAttr[name="v_head"](%1993)
  %2002 : __torch__.torch.nn.modules.linear.___torch_mangle_3431.Linear = prim::GetAttr[name="k_head"](%1993)
  %2003 : __torch__.torch.nn.modules.linear.___torch_mangle_3430.Linear = prim::GetAttr[name="q_head"](%1993)
  %2004 : int = aten::size(%query.11, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %2005 : int = aten::size(%query.11, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %2006 : int = aten::size(%query.11, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:531:0
  %2007 : Tensor = prim::GetAttr[name="weight"](%2003)
  %2008 : Float(768:1, 768:768) = aten::t(%2007), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2009 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.11, %2008), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2010 : int[] = prim::ListConstruct(%2004, %2005, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %q_head.23 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2009, %2010), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:535:0
  %2012 : Tensor = prim::GetAttr[name="bias"](%2002)
  %2013 : Tensor = prim::GetAttr[name="weight"](%2002)
  %2014 : Float(768:1, 768:768) = aten::t(%2013), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.56 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.11, %2014), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %2016 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.56, %2012, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1678:0
  %2017 : int[] = prim::ListConstruct(%2004, %2006, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2018 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2016, %2017), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:537:0
  %2019 : Tensor = prim::GetAttr[name="bias"](%2001)
  %2020 : Tensor = prim::GetAttr[name="weight"](%2001)
  %2021 : Float(768:1, 768:768) = aten::t(%2020), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.57 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.11, %2021), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %2023 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.57, %2019, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1678:0
  %2024 : int[] = prim::ListConstruct(%2004, %2006, %40, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2025 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2023, %2024), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.24 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.23, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.12 : Float(12:64, 64:1) = aten::mul(%2000, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:542:0
  %2028 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %r_w_bias.12, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:544:0
  %2029 : Tensor[] = prim::ListConstruct(%2028, %2018), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %content_score.12 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%43, %2029), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %v.12 : Float(12:64, 64:1) = aten::mul(%1999, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:486:0
  %2032 : Tensor[] = prim::ListConstruct(%241, %1998), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2033 : Float(8:768, 12:64, 64:1) = aten::einsum(%44, %2032), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2034 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %v.12, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:493:0
  %2035 : Tensor[] = prim::ListConstruct(%2034, %2033), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.67 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%45, %2035), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2037 : int = aten::size(%positional_attn.67, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2038 : int = aten::size(%positional_attn.67, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2039 : int = aten::size(%positional_attn.67, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2040 : int = aten::size(%positional_attn.67, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.12 : Long() = prim::NumToTensor(%2040), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2042 : int[] = prim::ListConstruct(%2037, %2038, %2040, %2039), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.68 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.67, %2042), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:428:0
  %2044 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.68, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2045 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%2044, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2046 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%2045, %28, %58, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.69 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%2046, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2048 : Long() = aten::sub(%max_rel_len.12, %23, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %2049 : int = aten::Int(%2048), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2050 : int[] = prim::ListConstruct(%2037, %2038, %2039, %2049), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.70 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.69, %2050), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.71 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.70, %46, %59, %2006, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.72 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.71, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:498:0
  %2054 : int = aten::size(%token_type_mat.15, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2055 : int = aten::size(%token_type_mat.15, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2056 : int = aten::size(%token_type_mat.15, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.12 : Float(12:64, 64:1) = aten::mul(%1997, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:508:0
  %2058 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %r_s_bias.12, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:511:0
  %2059 : Tensor[] = prim::ListConstruct(%2058, %1996), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2060 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%47, %2059), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2061 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2062 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%2061, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2063 : int = aten::size(%q_head.24, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2064 : int[] = prim::ListConstruct(%2054, %2063, %2055, %2056), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %token_type_mat.18 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%2062, %2064, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2066 : Tensor[] = aten::split(%2060, %58, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:371:0
  %diff_token_type.12 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.12 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%2066), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2069 : int = aten::size(%token_type_mat.18, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2070 : int = aten::size(%token_type_mat.18, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2071 : int = aten::size(%token_type_mat.18, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2072 : int = aten::size(%token_type_mat.18, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2073 : int[] = prim::ListConstruct(%2069, %2070, %2071, %2072), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2074 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.12, %2073, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2075 : int = aten::size(%token_type_mat.18, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2076 : int = aten::size(%token_type_mat.18, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2077 : int = aten::size(%token_type_mat.18, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2078 : int = aten::size(%token_type_mat.18, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2079 : int[] = prim::ListConstruct(%2075, %2076, %2077, %2078), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2080 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.12, %2079, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.23 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.18, %2074, %2080), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.24 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.23, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:522:0
  %2083 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.12, %positional_attn.72, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.34 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%2083, %token_type_attn.24, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.35 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.34, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:553:0
  %2086 : Float(17:4, 4:1) = aten::slice(%attention_mask, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2087 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%2086, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2088 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%2087, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2089 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%2088, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2090 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%2089, %58, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:396:0
  %2091 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%2090, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.36 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.35, %2091, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %input.107 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.36, %50, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:558:0
  %2094 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.107, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %2095 : Tensor[] = prim::ListConstruct(%2094, %2025), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %attn_vec.12 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%49, %2095), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2097 : int[] = prim::ListConstruct(%2004, %2005, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %input.108 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.12, %2097), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:565:0
  %2099 : Tensor = prim::GetAttr[name="bias"](%1995)
  %2100 : Tensor = prim::GetAttr[name="weight"](%1995)
  %2101 : Float(768:1, 768:768) = aten::t(%2100), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.58 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.108, %2101), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.109 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.58, %2099, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.12 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.109, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:3072, 4:768, 768:1) = aten::add(%query.11, %attn_out.12, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:568:0
  %2106 : Tensor = prim::GetAttr[name="bias"](%1994)
  %2107 : Tensor = prim::GetAttr[name="weight"](%1994)
  %2108 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm
  %input.111 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.110, %2108, %2107, %2106, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %2110 : __torch__.torch.nn.modules.normalization.___torch_mangle_3440.LayerNorm = prim::GetAttr[name="layer_norm"](%1992)
  %2111 : __torch__.torch.nn.modules.linear.___torch_mangle_3438.Linear = prim::GetAttr[name="linear_2"](%1992)
  %2112 : __torch__.torch.nn.modules.linear.___torch_mangle_3436.Linear = prim::GetAttr[name="linear_1"](%1992)
  %2113 : Tensor = prim::GetAttr[name="bias"](%2112)
  %2114 : Tensor = prim::GetAttr[name="weight"](%2112)
  %2115 : Float(768:1, 3072:768) = aten::t(%2114), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.111, %2115), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.12 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.59, %2113, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2118 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.12, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2119 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.12, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2120 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2119, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2121 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.12, %2120, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2122 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2121, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2123 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%2122), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2124 : Float(17:12288, 4:3072, 3072:1) = aten::add(%2123, %38, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.112 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2118, %2124), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.113 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.112, %39, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2127 : Tensor = prim::GetAttr[name="bias"](%2111)
  %2128 : Tensor = prim::GetAttr[name="weight"](%2111)
  %2129 : Float(3072:1, 768:3072) = aten::t(%2128), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.60 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.113, %2129), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.114 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.60, %2127, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.12 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.114, %54, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.115 : Float(17:3072, 4:768, 768:1) = aten::add(%input.111, %h.12, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/modeling_funnel.py:588:0
  %2134 : Tensor = prim::GetAttr[name="bias"](%2110)
  %2135 : Tensor = prim::GetAttr[name="weight"](%2110)
  %2136 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm
  %x.13 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.115, %2136, %2135, %2134, %52, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2138 : (Float(17:9984, 13:768, 768:1), Float(17:9984, 13:768, 768:1), Float(17:9984, 13:768, 768:1), Float(17:3072, 4:768, 768:1), Float(17:9984, 13:768, 768:1)) = prim::TupleConstruct(%hidden.1, %hidden.1, %hidden.1, %x.13, %hidden.1)
  %2139 : Float(17:9984, 13:768, 768:1), %2140 : Float(17:9984, 13:768, 768:1), %2141 : Float(17:9984, 13:768, 768:1), %2142 : Float(17:3072, 4:768, 768:1), %2143 : Float(17:9984, 13:768, 768:1) = prim::TupleUnpack(%2138)
  %2144 : __torch__.torch.nn.modules.container.___torch_mangle_3479.ModuleList = prim::GetAttr[name="layers"](%60)
  %2145 : __torch__.transformers.modeling_funnel.___torch_mangle_3478.FunnelLayer = prim::GetAttr[name="1"](%2144)
  %2146 : __torch__.torch.nn.modules.container.___torch_mangle_3479.ModuleList = prim::GetAttr[name="layers"](%60)
  %2147 : __torch__.transformers.modeling_funnel.___torch_mangle_3463.FunnelLayer = prim::GetAttr[name="0"](%2146)
  %2148 : int = aten::size(%2140, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:705:0
  %target_len : Long() = prim::NumToTensor(%2148), scope: __module.funnel/__module.funnel.decoder
  %2150 : Float(17:3072, 4:768, 768:1) = aten::slice(%2142, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
  %cls : Float(17:3072, 1:768, 768:1) = aten::slice(%2150, %58, %59, %58, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
  %2152 : Float(17:3072, 4:768, 768:1) = aten::slice(%2142, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
  %x.14 : Float(17:3072, 3:768, 768:1) = aten::slice(%2152, %58, %58, %25, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
  %input.116 : Float(17:9216, 12:768, 768:1) = aten::repeat_interleave(%x.14, %57, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:674:0
  %2155 : int[] = prim::ListConstruct(%59, %59, %59, %46, %59, %59), scope: __module.funnel/__module.funnel.decoder
  %output.61 : Float(17:11520, 15:768, 768:1) = aten::constant_pad_nd(%input.116, %2155, %59), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
  %2157 : Long() = aten::sub(%target_len, %23, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
  %2158 : int = aten::Int(%2157), scope: __module.funnel/__module.funnel.decoder
  %2159 : Float(17:11520, 15:768, 768:1) = aten::slice(%output.61, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
  %output.62 : Float(17:11520, 12:768, 768:1) = aten::slice(%2159, %58, %59, %2158, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
  %2161 : Tensor[] = prim::ListConstruct(%cls, %output.62), scope: __module.funnel/__module.funnel.decoder
  %upsampled_hidden : Float(17:9984, 13:768, 768:1) = aten::cat(%2161, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:679:0
  %input_embeds : Float(17:9984, 13:768, 768:1) = aten::add(%upsampled_hidden, %2143, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:710:0
  %2164 : int = aten::size(%input_embeds, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:195:0
  %seq_len.38 : Long() = prim::NumToTensor(%2164), scope: __module.funnel/__module.funnel.decoder
  %freq_seq : Float(384:1) = aten::arange(%59, %17, %18, %19, %59, %56, %55), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:247:0
  %2167 : Float(384:1) = aten::div(%freq_seq, %20), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:248:0
  %2168 : Float() = aten::to(%21, %56, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
  %2169 : Float() = aten::detach(%2168), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
  %2170 : Float(384:1) = aten::pow(%2169, %2167), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
  %2171 : Float(384:1) = aten::reciprocal(%2170), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:400:0
  %inv_freq : Float(384:1) = aten::mul(%2171, %23), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:400:0
  %2173 : Long() = aten::neg(%seq_len.38), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
  %2174 : Long() = aten::mul(%2173, %24), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
  %2175 : Scalar = aten::ScalarImplicit(%2174), scope: __module.funnel/__module.funnel.decoder
  %2176 : Long() = aten::mul(%seq_len.38, %24), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
  %2177 : Scalar = aten::ScalarImplicit(%2176), scope: __module.funnel/__module.funnel.decoder
  %rel_pos_id : Float(52:1) = aten::arange(%2175, %2177, %18, %19, %59, %56, %55), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
  %zero_offset : Long() = aten::mul(%seq_len.38, %24), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:251:0
  %2180 : Float(52:1) = aten::slice(%rel_pos_id, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
  %2181 : Float(52:1, 1:1) = aten::unsqueeze(%2180, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
  %2182 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq, %59), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
  %sinusoid : Float(52:384, 384:1) = aten::mul(%2181, %2182), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
  %input.117 : Float(52:384, 384:1) = aten::sin(%sinusoid), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:253:0
  %sin_embed : Float(52:384, 384:1) = aten::dropout(%input.117, %54, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %input.118 : Float(52:384, 384:1) = aten::cos(%sinusoid), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:254:0
  %cos_embed : Float(52:384, 384:1) = aten::dropout(%input.118, %54, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
  %2188 : Tensor[] = prim::ListConstruct(%sin_embed, %cos_embed), scope: __module.funnel/__module.funnel.decoder
  %pos_embed : Float(52:768, 768:1) = aten::cat(%2188, %50), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:255:0
  %pos : Float(13:1) = aten::arange(%59, %2164, %58, %19, %59, %56, %55), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:257:0
  %2191 : Float() = aten::select(%pos, %59, %59), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
  %2192 : Float() = aten::select(%pos, %59, %59), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
  %ref_point.6 : Float() = aten::sub(%2191, %2192, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
  %max_dist.6 : Float() = aten::add(%ref_point.6, %26, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:315:0
  %2195 : Scalar = aten::ScalarImplicit(%max_dist.6), scope: __module.funnel/__module.funnel.decoder
  %2196 : Float() = aten::select(%pos, %59, %59), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
  %2197 : Float() = aten::select(%pos, %59, %50), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
  %min_dist.6 : Float() = aten::sub(%2196, %2197, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
  %2199 : Float() = aten::sub(%min_dist.6, %23, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
  %2200 : Scalar = aten::ScalarImplicit(%2199), scope: __module.funnel/__module.funnel.decoder
  %rel_pos.16 : Long(26:1) = aten::arange(%2195, %2200, %50, %57, %59, %56, %55), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
  %2202 : Long(26:1) = aten::slice(%rel_pos.16, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
  %2203 : Long(26:1, 1:1) = aten::unsqueeze(%2202, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
  %rel_pos.17 : Long(26:1, 1:1) = aten::add(%2203, %zero_offset, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
  %2205 : int = aten::size(%rel_pos.17, %59), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:285:0
  %2206 : int[] = prim::ListConstruct(%2205, %53), scope: __module.funnel/__module.funnel.decoder
  %rel_pos.18 : Long(26:1, 768:0) = aten::expand(%rel_pos.17, %2206, %55), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:285:0
  %2208 : Float(26:768, 768:1) = aten::gather(%pos_embed, %59, %rel_pos.18, %55), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:286:0
  %2209 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %2210 : Long(17:13, 13:1) = aten::slice(%2209, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %2211 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%2210, %28), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %2212 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %2213 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%2212, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %token_type_mat.19 : Bool(17:169, 13:13, 13:1) = aten::eq(%2211, %2213), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:22:0
  %cls_ids : Bool(17:13, 13:1) = aten::eq(%token_type_ids, %28), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:22:0
  %2216 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %2217 : Bool(17:13, 13:1) = aten::slice(%2216, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %2218 : Bool(17:13, 13:1, 1:1) = aten::unsqueeze(%2217, %28), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %2219 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %2220 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%2219, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %cls_mat : Bool(17:169, 13:13, 13:1) = aten::__or__(%2218, %2220), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %token_type_mat.20 : Bool(17:169, 13:13, 13:1) = aten::__or__(%cls_mat, %token_type_mat.19), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:211:0
  %2223 : Long() = aten::sub(%seq_len.38, %23, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
  %2224 : int = aten::Int(%2223), scope: __module.funnel/__module.funnel.decoder
  %2225 : Long() = aten::sub(%seq_len.38, %23, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
  %2226 : int = aten::Int(%2225), scope: __module.funnel/__module.funnel.decoder
  %2227 : int[] = prim::ListConstruct(%2224, %2226), scope: __module.funnel/__module.funnel.decoder
  %input.119 : Float(12:12, 12:1) = aten::ones(%2227, %19, %59, %56, %55), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
  %2229 : int[] = prim::ListConstruct(%58, %59, %58, %59), scope: __module.funnel/__module.funnel.decoder
  %cls_mask : Float(13:13, 13:1) = aten::constant_pad_nd(%input.119, %2229, %59), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
  %2231 : __torch__.transformers.modeling_funnel.___torch_mangle_3462.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%2147)
  %2232 : __torch__.transformers.modeling_funnel.___torch_mangle_3456.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%2147)
  %2233 : __torch__.torch.nn.modules.normalization.___torch_mangle_3455.LayerNorm = prim::GetAttr[name="layer_norm"](%2232)
  %2234 : __torch__.torch.nn.modules.linear.___torch_mangle_3454.Linear = prim::GetAttr[name="post_proj"](%2232)
  %2235 : Tensor = prim::GetAttr[name="seg_embed"](%2232)
  %2236 : Tensor = prim::GetAttr[name="r_s_bias"](%2232)
  %2237 : Tensor = prim::GetAttr[name="r_kernel"](%2232)
  %2238 : Tensor = prim::GetAttr[name="r_r_bias"](%2232)
  %2239 : Tensor = prim::GetAttr[name="r_w_bias"](%2232)
  %2240 : __torch__.torch.nn.modules.linear.___torch_mangle_3453.Linear = prim::GetAttr[name="v_head"](%2232)
  %2241 : __torch__.torch.nn.modules.linear.___torch_mangle_3452.Linear = prim::GetAttr[name="k_head"](%2232)
  %2242 : __torch__.torch.nn.modules.linear.___torch_mangle_3451.Linear = prim::GetAttr[name="q_head"](%2232)
  %2243 : int = aten::size(%input_embeds, %59), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:530:0
  %2244 : int = aten::size(%input_embeds, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:530:0
  %2245 : int = aten::size(%input_embeds, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:531:0
  %2246 : Tensor = prim::GetAttr[name="weight"](%2242)
  %2247 : Float(768:1, 768:768) = aten::t(%2246), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.q_head # torch/nn/functional.py:1676:0
  %2248 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %2247), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.q_head # torch/nn/functional.py:1676:0
  %2249 : int[] = prim::ListConstruct(%2243, %2244, %40, %41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %q_head.25 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2248, %2249), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:535:0
  %2251 : Tensor = prim::GetAttr[name="bias"](%2241)
  %2252 : Tensor = prim::GetAttr[name="weight"](%2241)
  %2253 : Float(768:1, 768:768) = aten::t(%2252), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.63 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %2253), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1676:0
  %2255 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.63, %2251, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1678:0
  %2256 : int[] = prim::ListConstruct(%2243, %2245, %40, %41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2257 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2255, %2256), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:537:0
  %2258 : Tensor = prim::GetAttr[name="bias"](%2240)
  %2259 : Tensor = prim::GetAttr[name="weight"](%2240)
  %2260 : Float(768:1, 768:768) = aten::t(%2259), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.64 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %2260), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1676:0
  %2262 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.64, %2258, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1678:0
  %2263 : int[] = prim::ListConstruct(%2243, %2245, %40, %41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2264 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2262, %2263), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.25, %42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.13 : Float(12:64, 64:1) = aten::mul(%2239, %42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:542:0
  %2267 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %r_w_bias.13, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:544:0
  %2268 : Tensor[] = prim::ListConstruct(%2267, %2257), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %content_score.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%43, %2268), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %v.13 : Float(12:64, 64:1) = aten::mul(%2238, %42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:486:0
  %2271 : Tensor[] = prim::ListConstruct(%2208, %2237), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2272 : Float(26:768, 12:64, 64:1) = aten::einsum(%44, %2271), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %2273 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %v.13, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:493:0
  %2274 : Tensor[] = prim::ListConstruct(%2273, %2272), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %positional_attn.73 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%45, %2274), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %2276 : int = aten::size(%positional_attn.73, %59), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %2277 : int = aten::size(%positional_attn.73, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %2278 : int = aten::size(%positional_attn.73, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %2279 : int = aten::size(%positional_attn.73, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.13 : Long() = prim::NumToTensor(%2279), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2281 : int[] = prim::ListConstruct(%2276, %2277, %2279, %2278), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %positional_attn.74 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.73, %2281), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:428:0
  %2283 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.74, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %2284 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%2283, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %2285 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2284, %28, %58, %25, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.75 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2285, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %2287 : Long() = aten::sub(%max_rel_len.13, %23, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:430:0
  %2288 : int = aten::Int(%2287), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2289 : int[] = prim::ListConstruct(%2276, %2277, %2278, %2288), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %positional_attn.76 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.75, %2289), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.77 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.76, %46, %59, %2245, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.78 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.77, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:498:0
  %2293 : int = aten::size(%token_type_mat.20, %59), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
  %2294 : int = aten::size(%token_type_mat.20, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
  %2295 : int = aten::size(%token_type_mat.20, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.13 : Float(12:64, 64:1) = aten::mul(%2236, %42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:508:0
  %2297 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %r_s_bias.13, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:511:0
  %2298 : Tensor[] = prim::ListConstruct(%2297, %2235), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2299 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%47, %2298), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %2300 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.20, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2301 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%2300, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2302 : int = aten::size(%q_head.26, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2303 : int[] = prim::ListConstruct(%2293, %2302, %2294, %2295), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %token_type_mat.21 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%2301, %2303, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2305 : Tensor[] = aten::split(%2299, %58, %50), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/tensor.py:371:0
  %diff_token_type.13 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.13 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%2305), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2308 : int = aten::size(%token_type_mat.21, %59), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2309 : int = aten::size(%token_type_mat.21, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2310 : int = aten::size(%token_type_mat.21, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2311 : int = aten::size(%token_type_mat.21, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2312 : int[] = prim::ListConstruct(%2308, %2309, %2310, %2311), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2313 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.13, %2312, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2314 : int = aten::size(%token_type_mat.21, %59), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2315 : int = aten::size(%token_type_mat.21, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2316 : int = aten::size(%token_type_mat.21, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2317 : int = aten::size(%token_type_mat.21, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2318 : int[] = prim::ListConstruct(%2314, %2315, %2316, %2317), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2319 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.13, %2318, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.25 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.21, %2313, %2319), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.25, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:522:0
  %2322 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.13, %positional_attn.78, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%2322, %token_type_attn.26, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.38 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.37, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:553:0
  %2325 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2326 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%2325, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2327 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2326, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2328 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%2327, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2329 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%2328, %58, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/tensor.py:396:0
  %2330 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%2329, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.39 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.38, %2330, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %input.120 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.39, %50, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:558:0
  %2333 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.120, %54, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %2334 : Tensor[] = prim::ListConstruct(%2333, %2264), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %attn_vec.13 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%49, %2334), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %2336 : int[] = prim::ListConstruct(%2243, %2244, %53), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %input.121 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.13, %2336), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:565:0
  %2338 : Tensor = prim::GetAttr[name="bias"](%2234)
  %2339 : Tensor = prim::GetAttr[name="weight"](%2234)
  %2340 : Float(768:1, 768:768) = aten::t(%2339), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.65 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.121, %2340), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.122 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.65, %2338, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.122, %54, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.123 : Float(17:9984, 13:768, 768:1) = aten::add(%input_embeds, %attn_out.13, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:568:0
  %2345 : Tensor = prim::GetAttr[name="bias"](%2233)
  %2346 : Tensor = prim::GetAttr[name="weight"](%2233)
  %2347 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.layer_norm
  %input.124 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.123, %2347, %2346, %2345, %52, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %2349 : __torch__.torch.nn.modules.normalization.___torch_mangle_3461.LayerNorm = prim::GetAttr[name="layer_norm"](%2231)
  %2350 : __torch__.torch.nn.modules.linear.___torch_mangle_3459.Linear = prim::GetAttr[name="linear_2"](%2231)
  %2351 : __torch__.torch.nn.modules.linear.___torch_mangle_3457.Linear = prim::GetAttr[name="linear_1"](%2231)
  %2352 : Tensor = prim::GetAttr[name="bias"](%2351)
  %2353 : Tensor = prim::GetAttr[name="weight"](%2351)
  %2354 : Float(768:1, 3072:768) = aten::t(%2353), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.66 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.124, %2354), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.15 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.66, %2352, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2357 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.15, %34), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2358 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.15, %35), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2359 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2358, %36), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2360 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.15, %2359, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2361 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2360, %37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2362 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%2361), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2363 : Float(17:39936, 13:3072, 3072:1) = aten::add(%2362, %38, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %input.125 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2357, %2363), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %input.126 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.125, %39, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2366 : Tensor = prim::GetAttr[name="bias"](%2350)
  %2367 : Tensor = prim::GetAttr[name="weight"](%2350)
  %2368 : Float(3072:1, 768:3072) = aten::t(%2367), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.67 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.126, %2368), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.127 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.67, %2366, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.127, %54, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.128 : Float(17:9984, 13:768, 768:1) = aten::add(%input.124, %h.13, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/modeling_funnel.py:588:0
  %2373 : Tensor = prim::GetAttr[name="bias"](%2349)
  %2374 : Tensor = prim::GetAttr[name="weight"](%2349)
  %2375 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.layer_norm
  %query : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.128, %2375, %2374, %2373, %52, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2377 : __torch__.transformers.modeling_funnel.___torch_mangle_3477.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%2145)
  %2378 : __torch__.transformers.modeling_funnel.___torch_mangle_3471.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%2145)
  %2379 : __torch__.torch.nn.modules.normalization.___torch_mangle_3470.LayerNorm = prim::GetAttr[name="layer_norm"](%2378)
  %2380 : __torch__.torch.nn.modules.linear.___torch_mangle_3469.Linear = prim::GetAttr[name="post_proj"](%2378)
  %2381 : Tensor = prim::GetAttr[name="seg_embed"](%2378)
  %2382 : Tensor = prim::GetAttr[name="r_s_bias"](%2378)
  %2383 : Tensor = prim::GetAttr[name="r_kernel"](%2378)
  %2384 : Tensor = prim::GetAttr[name="r_r_bias"](%2378)
  %2385 : Tensor = prim::GetAttr[name="r_w_bias"](%2378)
  %2386 : __torch__.torch.nn.modules.linear.___torch_mangle_3468.Linear = prim::GetAttr[name="v_head"](%2378)
  %2387 : __torch__.torch.nn.modules.linear.___torch_mangle_3467.Linear = prim::GetAttr[name="k_head"](%2378)
  %2388 : __torch__.torch.nn.modules.linear.___torch_mangle_3466.Linear = prim::GetAttr[name="q_head"](%2378)
  %2389 : int = aten::size(%query, %59), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:530:0
  %2390 : int = aten::size(%query, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:530:0
  %2391 : int = aten::size(%query, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:531:0
  %2392 : Tensor = prim::GetAttr[name="weight"](%2388)
  %2393 : Float(768:1, 768:768) = aten::t(%2392), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.q_head # torch/nn/functional.py:1676:0
  %2394 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query, %2393), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.q_head # torch/nn/functional.py:1676:0
  %2395 : int[] = prim::ListConstruct(%2389, %2390, %40, %41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %q_head.27 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2394, %2395), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:535:0
  %2397 : Tensor = prim::GetAttr[name="bias"](%2387)
  %2398 : Tensor = prim::GetAttr[name="weight"](%2387)
  %2399 : Float(768:1, 768:768) = aten::t(%2398), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.68 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query, %2399), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1676:0
  %2401 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.68, %2397, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1678:0
  %2402 : int[] = prim::ListConstruct(%2389, %2391, %40, %41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2403 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2401, %2402), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:537:0
  %2404 : Tensor = prim::GetAttr[name="bias"](%2386)
  %2405 : Tensor = prim::GetAttr[name="weight"](%2386)
  %2406 : Float(768:1, 768:768) = aten::t(%2405), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.69 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query, %2406), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1676:0
  %2408 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.69, %2404, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1678:0
  %2409 : int[] = prim::ListConstruct(%2389, %2391, %40, %41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2410 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2408, %2409), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:538:0
  %q_head : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.27, %42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias : Float(12:64, 64:1) = aten::mul(%2385, %42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:542:0
  %2413 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %r_w_bias, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:544:0
  %2414 : Tensor[] = prim::ListConstruct(%2413, %2403), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %content_score : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%43, %2414), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %v : Float(12:64, 64:1) = aten::mul(%2384, %42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:486:0
  %2417 : Tensor[] = prim::ListConstruct(%2208, %2383), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2418 : Float(26:768, 12:64, 64:1) = aten::einsum(%44, %2417), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %2419 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %v, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:493:0
  %2420 : Tensor[] = prim::ListConstruct(%2419, %2418), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %positional_attn.79 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%45, %2420), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %2422 : int = aten::size(%positional_attn.79, %59), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %2423 : int = aten::size(%positional_attn.79, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %2424 : int = aten::size(%positional_attn.79, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %2425 : int = aten::size(%positional_attn.79, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len : Long() = prim::NumToTensor(%2425), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2427 : int[] = prim::ListConstruct(%2422, %2423, %2425, %2424), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %positional_attn.80 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.79, %2427), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:428:0
  %2429 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.80, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %2430 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%2429, %58, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %2431 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2430, %28, %58, %25, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.81 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2431, %46, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %2433 : Long() = aten::sub(%max_rel_len, %23, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:430:0
  %2434 : int = aten::Int(%2433), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2435 : int[] = prim::ListConstruct(%2422, %2423, %2424, %2434), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %positional_attn.82 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.81, %2435), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.83 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.82, %46, %59, %2391, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.83, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:498:0
  %2439 : int = aten::size(%token_type_mat.20, %59), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
  %2440 : int = aten::size(%token_type_mat.20, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
  %2441 : int = aten::size(%token_type_mat.20, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias : Float(12:64, 64:1) = aten::mul(%2382, %42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:508:0
  %2443 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %r_s_bias, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:511:0
  %2444 : Tensor[] = prim::ListConstruct(%2443, %2381), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2445 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%47, %2444), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %2446 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.20, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2447 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%2446, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2448 : int = aten::size(%q_head, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2449 : int[] = prim::ListConstruct(%2439, %2448, %2440, %2441), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %token_type_mat : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%2447, %2449, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2451 : Tensor[] = aten::split(%2445, %58, %50), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/tensor.py:371:0
  %diff_token_type : Float(17:26, 12:442, 13:2, 1:1), %same_token_type : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%2451), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2454 : int = aten::size(%token_type_mat, %59), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2455 : int = aten::size(%token_type_mat, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2456 : int = aten::size(%token_type_mat, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2457 : int = aten::size(%token_type_mat, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2458 : int[] = prim::ListConstruct(%2454, %2455, %2456, %2457), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2459 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type, %2458, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2460 : int = aten::size(%token_type_mat, %59), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2461 : int = aten::size(%token_type_mat, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2462 : int = aten::size(%token_type_mat, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2463 : int = aten::size(%token_type_mat, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2464 : int[] = prim::ListConstruct(%2460, %2461, %2462, %2463), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2465 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type, %2464, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat, %2459, %2465), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.27, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:522:0
  %2468 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score, %positional_attn, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.40 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%2468, %token_type_attn, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.41 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.40, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:553:0
  %2471 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %59, %59, %25, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2472 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%2471, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2473 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2472, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2474 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%2473, %19, %55, %55, %22), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2475 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%2474, %58, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/tensor.py:396:0
  %2476 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%2475, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.41, %2476, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %input.129 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score, %50, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:558:0
  %2479 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.129, %54, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %2480 : Tensor[] = prim::ListConstruct(%2479, %2410), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %attn_vec : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%49, %2480), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %2482 : int[] = prim::ListConstruct(%2389, %2390, %53), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %input.130 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec, %2482), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:565:0
  %2484 : Tensor = prim::GetAttr[name="bias"](%2380)
  %2485 : Tensor = prim::GetAttr[name="weight"](%2380)
  %2486 : Float(768:1, 768:768) = aten::t(%2485), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.70 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.130, %2486), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.131 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.70, %2484, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.131, %54, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.132 : Float(17:9984, 13:768, 768:1) = aten::add(%query, %attn_out, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:568:0
  %2491 : Tensor = prim::GetAttr[name="bias"](%2379)
  %2492 : Tensor = prim::GetAttr[name="weight"](%2379)
  %2493 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.layer_norm
  %input.133 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.132, %2493, %2492, %2491, %52, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %2495 : __torch__.torch.nn.modules.normalization.___torch_mangle_3476.LayerNorm = prim::GetAttr[name="layer_norm"](%2377)
  %2496 : __torch__.torch.nn.modules.linear.___torch_mangle_3474.Linear = prim::GetAttr[name="linear_2"](%2377)
  %2497 : __torch__.torch.nn.modules.linear.___torch_mangle_3472.Linear = prim::GetAttr[name="linear_1"](%2377)
  %2498 : Tensor = prim::GetAttr[name="bias"](%2497)
  %2499 : Tensor = prim::GetAttr[name="weight"](%2497)
  %2500 : Float(768:1, 3072:768) = aten::t(%2499), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.71 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.133, %2500), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.71, %2498, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2503 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x, %34), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2504 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x, %35), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2505 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2504, %36), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2506 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x, %2505, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2507 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2506, %37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2508 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%2507), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2509 : Float(17:39936, 13:3072, 3072:1) = aten::add(%2508, %38, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %input.134 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2503, %2509), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %input.135 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.134, %39, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2512 : Tensor = prim::GetAttr[name="bias"](%2496)
  %2513 : Tensor = prim::GetAttr[name="weight"](%2496)
  %2514 : Float(3072:1, 768:3072) = aten::t(%2513), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.72 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.135, %2514), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.136 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.72, %2512, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.136, %54, %55), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.137 : Float(17:9984, 13:768, 768:1) = aten::add(%input.133, %h, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/modeling_funnel.py:588:0
  %2519 : Tensor = prim::GetAttr[name="bias"](%2495)
  %2520 : Tensor = prim::GetAttr[name="weight"](%2495)
  %2521 : int[] = prim::ListConstruct(%53), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.layer_norm
  %input : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.137, %2521, %2520, %2519, %52, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2523 : int = prim::Constant[value=1](), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %2524 : Tensor = prim::GetAttr[name="bias"](%3)
  %2525 : Tensor = prim::GetAttr[name="weight"](%3)
  %2526 : Float(768:1, 2:768) = aten::t(%2525), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %2526), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %2528 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %2524, %2523), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %7 : int = prim::Constant[value=1]() # torch/tensor.py:371:0
  %8 : int = prim::Constant[value=-1]() # torch/tensor.py:371:0
  %9 : Tensor[] = aten::split(%2528, %7, %8) # torch/tensor.py:371:0
  %start_logits : Float(17:26, 13:2, 1:1), %end_logits : Float(17:26, 13:2, 1:1) = prim::ListUnpack(%9)
  %12 : int = prim::Constant[value=-1]() # transformers/modeling_funnel.py:1522:0
  %13 : Float(17:26, 13:2) = aten::squeeze(%start_logits, %12) # transformers/modeling_funnel.py:1522:0
  %14 : int = prim::Constant[value=-1]() # transformers/modeling_funnel.py:1523:0
  %15 : Float(17:26, 13:2) = aten::squeeze(%end_logits, %14) # transformers/modeling_funnel.py:1523:0
  %16 : (Float(17:26, 13:2), Float(17:26, 13:2)) = prim::TupleConstruct(%13, %15)
  return (%16)
