graph(%self.1 : __torch__.transformers.modeling_albert.AlbertForMultipleChoice,
      %input_ids.1 : Long(17:91, 7:13, 13:1),
      %attention_mask.1 : Long(17:91, 7:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_74.Linear = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.torch.nn.modules.dropout.___torch_mangle_73.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %5 : __torch__.transformers.modeling_albert.___torch_mangle_72.AlbertModel = prim::GetAttr[name="albert"](%self.1)
  %6 : int = prim::Constant[value=1]() # transformers/modeling_albert.py:1267:0
  %7 : int = aten::size(%input_ids.1, %6) # transformers/modeling_albert.py:1267:0
  %num_choices : Long() = prim::NumToTensor(%7)
  %9 : int = aten::Int(%num_choices)
  %10 : int = prim::Constant[value=-1]() # transformers/modeling_albert.py:1269:0
  %11 : int = aten::size(%input_ids.1, %10) # transformers/modeling_albert.py:1269:0
  %12 : Long() = prim::NumToTensor(%11)
  %13 : int = aten::Int(%12)
  %14 : int = prim::Constant[value=-1]() # transformers/modeling_albert.py:1269:0
  %15 : int[] = prim::ListConstruct(%14, %13)
  %input_ids : Long(119:13, 13:1) = aten::view(%input_ids.1, %15) # transformers/modeling_albert.py:1269:0
  %17 : int = prim::Constant[value=-1]() # transformers/modeling_albert.py:1270:0
  %18 : int = aten::size(%attention_mask.1, %17) # transformers/modeling_albert.py:1270:0
  %19 : Long() = prim::NumToTensor(%18)
  %20 : int = aten::Int(%19)
  %21 : int = prim::Constant[value=-1]() # transformers/modeling_albert.py:1270:0
  %22 : int[] = prim::ListConstruct(%21, %20)
  %attention_mask.2 : Long(119:13, 13:1) = aten::view(%attention_mask.1, %22) # transformers/modeling_albert.py:1270:0
  %31 : str = prim::Constant[value="bfnd,ndh->bfh"](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %32 : int = prim::Constant[value=4096](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %33 : Double() = prim::Constant[value={8}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %34 : int = prim::Constant[value=-2](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %35 : int = prim::Constant[value=3](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %36 : int = prim::Constant[value=64](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %37 : Double() = prim::Constant[value={0.5}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %38 : float = prim::Constant[value=3.](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %39 : Double() = prim::Constant[value={0.044715}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %40 : Double() = prim::Constant[value={0.797885}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %41 : Double() = prim::Constant[value={1}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %42 : int = prim::Constant[value=9223372036854775807](), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %43 : int = prim::Constant[value=-1](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %44 : bool = prim::Constant[value=1](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %45 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %46 : int = prim::Constant[value=128](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %47 : float = prim::Constant[value=0.](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.dropout # torch/nn/functional.py:973:0
  %48 : Double() = prim::Constant[value={-10000}](), scope: __module.albert # transformers/modeling_albert.py:678:0
  %49 : float = prim::Constant[value=1.](), scope: __module.albert # torch/tensor.py:396:0
  %50 : None = prim::Constant(), scope: __module.albert
  %51 : int = prim::Constant[value=6](), scope: __module.albert # transformers/modeling_albert.py:677:0
  %52 : int = prim::Constant[value=2](), scope: __module.albert # transformers/modeling_albert.py:676:0
  %53 : bool = prim::Constant[value=0](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %54 : Device = prim::Constant[value="cpu"](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %55 : int = prim::Constant[value=4](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %56 : int = prim::Constant[value=1](), scope: __module.albert # transformers/modeling_albert.py:663:0
  %57 : int = prim::Constant[value=0](), scope: __module.albert # transformers/modeling_albert.py:663:0
  %58 : __torch__.torch.nn.modules.linear.___torch_mangle_71.Linear = prim::GetAttr[name="pooler"](%5)
  %59 : __torch__.transformers.modeling_albert.___torch_mangle_70.AlbertTransformer = prim::GetAttr[name="encoder"](%5)
  %60 : __torch__.transformers.modeling_albert.___torch_mangle_52.AlbertEmbeddings = prim::GetAttr[name="embeddings"](%5)
  %61 : int = aten::size(%input_ids, %57), scope: __module.albert # transformers/modeling_albert.py:663:0
  %62 : int = aten::size(%input_ids, %56), scope: __module.albert # transformers/modeling_albert.py:663:0
  %63 : int[] = prim::ListConstruct(%61, %62), scope: __module.albert
  %input.2 : Long(119:13, 13:1) = aten::zeros(%63, %55, %57, %54, %53), scope: __module.albert # transformers/modeling_albert.py:674:0
  %65 : Long(119:13, 1:13, 13:1) = aten::unsqueeze(%attention_mask.2, %56), scope: __module.albert # transformers/modeling_albert.py:676:0
  %extended_attention_mask : Long(119:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%65, %52), scope: __module.albert # transformers/modeling_albert.py:676:0
  %67 : Float(119:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %51, %53, %53, %50), scope: __module.albert # transformers/modeling_albert.py:677:0
  %68 : Float(119:13, 1:13, 1:13, 13:1) = aten::rsub(%67, %49, %56), scope: __module.albert # torch/tensor.py:396:0
  %attention_mask : Float(119:13, 1:13, 1:13, 13:1) = aten::mul(%68, %48), scope: __module.albert # transformers/modeling_albert.py:678:0
  %70 : __torch__.torch.nn.modules.normalization.___torch_mangle_50.LayerNorm = prim::GetAttr[name="LayerNorm"](%60)
  %71 : __torch__.torch.nn.modules.sparse.___torch_mangle_49.Embedding = prim::GetAttr[name="token_type_embeddings"](%60)
  %72 : __torch__.torch.nn.modules.sparse.___torch_mangle_48.Embedding = prim::GetAttr[name="position_embeddings"](%60)
  %73 : __torch__.torch.nn.modules.sparse.___torch_mangle_47.Embedding = prim::GetAttr[name="word_embeddings"](%60)
  %74 : Tensor = prim::GetAttr[name="position_ids"](%60)
  %75 : int = aten::size(%input_ids, %56), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:222:0
  %76 : Long(1:512, 512:1) = aten::slice(%74, %57, %57, %42, %56), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%76, %56, %57, %75, %56), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %78 : Tensor = prim::GetAttr[name="weight"](%73)
  %inputs_embeds : Float(119:1664, 13:128, 128:1) = aten::embedding(%78, %input_ids, %57, %53, %53), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %80 : Tensor = prim::GetAttr[name="weight"](%72)
  %position_embeddings : Float(1:1664, 13:128, 128:1) = aten::embedding(%80, %input.1, %43, %53, %53), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %82 : Tensor = prim::GetAttr[name="weight"](%71)
  %token_type_embeddings : Float(119:1664, 13:128, 128:1) = aten::embedding(%82, %input.2, %43, %53, %53), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %84 : Float(119:1664, 13:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %56), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:239:0
  %input.3 : Float(119:1664, 13:128, 128:1) = aten::add(%84, %token_type_embeddings, %56), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:239:0
  %86 : Tensor = prim::GetAttr[name="bias"](%70)
  %87 : Tensor = prim::GetAttr[name="weight"](%70)
  %88 : int[] = prim::ListConstruct(%46), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm
  %input.4 : Float(119:1664, 13:128, 128:1) = aten::layer_norm(%input.3, %88, %87, %86, %45, %44), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(119:1664, 13:128, 128:1) = aten::dropout(%input.4, %47, %53), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.dropout # torch/nn/functional.py:973:0
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_69.ModuleList = prim::GetAttr[name="albert_layer_groups"](%59)
  %92 : __torch__.transformers.modeling_albert.___torch_mangle_68.AlbertLayerGroup = prim::GetAttr[name="0"](%91)
  %93 : __torch__.torch.nn.modules.linear.___torch_mangle_53.Linear = prim::GetAttr[name="embedding_hidden_mapping_in"](%59)
  %94 : Tensor = prim::GetAttr[name="bias"](%93)
  %95 : Tensor = prim::GetAttr[name="weight"](%93)
  %96 : Float(128:1, 4096:128) = aten::t(%95), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1676:0
  %output.1 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.5, %96), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1676:0
  %input.6 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.1, %94, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1678:0
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_67.ModuleList = prim::GetAttr[name="albert_layers"](%92)
  %100 : __torch__.transformers.modeling_albert.___torch_mangle_66.AlbertLayer = prim::GetAttr[name="0"](%99)
  %101 : __torch__.torch.nn.modules.normalization.___torch_mangle_54.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%100)
  %102 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="ffn_output"](%100)
  %103 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name="ffn"](%100)
  %104 : __torch__.transformers.modeling_albert.___torch_mangle_62.AlbertAttention = prim::GetAttr[name="attention"](%100)
  %105 : __torch__.torch.nn.modules.normalization.___torch_mangle_61.LayerNorm = prim::GetAttr[name="LayerNorm"](%104)
  %106 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%104)
  %107 : Tensor = prim::GetAttr[name="bias"](%106)
  %108 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%104)
  %109 : Tensor = prim::GetAttr[name="weight"](%108)
  %110 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name="value"](%104)
  %111 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name="key"](%104)
  %112 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name="query"](%104)
  %113 : Tensor = prim::GetAttr[name="bias"](%112)
  %114 : Tensor = prim::GetAttr[name="weight"](%112)
  %115 : Float(4096:1, 4096:4096) = aten::t(%114), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.2 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %115), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.1 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.2, %113, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %118 : Tensor = prim::GetAttr[name="bias"](%111)
  %119 : Tensor = prim::GetAttr[name="weight"](%111)
  %120 : Float(4096:1, 4096:4096) = aten::t(%119), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.3 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %120), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.3 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.3, %118, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %123 : Tensor = prim::GetAttr[name="bias"](%110)
  %124 : Tensor = prim::GetAttr[name="weight"](%110)
  %125 : Float(4096:1, 4096:4096) = aten::t(%124), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.4 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %125), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.5 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.4, %123, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %128 : int = aten::size(%x.1, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %129 : int = aten::size(%x.1, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %130 : int[] = prim::ListConstruct(%128, %129, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.2 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.1, %130), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %132 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.1 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.2, %132), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %134 : int = aten::size(%x.3, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %135 : int = aten::size(%x.3, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %136 : int[] = prim::ListConstruct(%134, %135, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.4 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.3, %136), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %138 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.1 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.4, %138), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %140 : int = aten::size(%x.5, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %141 : int = aten::size(%x.5, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %142 : int[] = prim::ListConstruct(%140, %141, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.6 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.5, %142), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %144 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.1 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.6, %144), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %146 : Float(119:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.1, %43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.1 : Float(119:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %146), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.2 : Float(119:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.1, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.7 : Float(119:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.8 : Float(119:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.7, %43, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(119:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.8, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(119:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %153 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %154 : Float(119:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.1, %153), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %155 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%154, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %156 : Float(4096:1, 4096:4096) = aten::t(%109), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %157 : int[] = prim::ListConstruct(%36, %36, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %158 : Float(64:64, 64:1, 4096:4096) = aten::view(%156, %157), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %159 : Float(64:64, 64:1, 4096:4096) = aten::to(%158, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.1 : Float(4096:1) = aten::to(%107, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %161 : Tensor[] = prim::ListConstruct(%155, %159), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %162 : Float(119:53248, 13:4096, 4096:1) = aten::einsum(%31, %161), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.9 : Float(119:53248, 13:4096, 4096:1) = aten::add(%162, %b.1, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.1 : Float(119:53248, 13:4096, 4096:1) = aten::dropout(%input.9, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.10 : Float(119:53248, 13:4096, 4096:1) = aten::add(%input.6, %projected_context_layer.1, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %166 : Tensor = prim::GetAttr[name="bias"](%105)
  %167 : Tensor = prim::GetAttr[name="weight"](%105)
  %168 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.1 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.10, %168, %167, %166, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %170 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.1, %b.1)
  %171 : Float(119:53248, 13:4096, 4096:1), %172 : Float(4096:1) = prim::TupleUnpack(%170)
  %173 : Tensor = prim::GetAttr[name="bias"](%103)
  %174 : Tensor = prim::GetAttr[name="weight"](%103)
  %175 : Float(4096:1, 16384:4096) = aten::t(%174), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.5 : Float(119:212992, 13:16384, 16384:1) = aten::matmul(%171, %175), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.7 : Float(119:212992, 13:16384, 16384:1) = aten::add_(%output.5, %173, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %178 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%x.7, %37), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %179 : Float(119:212992, 13:16384, 16384:1) = aten::pow(%x.7, %38), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %180 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%179, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %181 : Float(119:212992, 13:16384, 16384:1) = aten::add(%x.7, %180, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %182 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%181, %40), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %183 : Float(119:212992, 13:16384, 16384:1) = aten::tanh(%182), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %184 : Float(119:212992, 13:16384, 16384:1) = aten::add(%183, %41, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.11 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%178, %184), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %186 : Tensor = prim::GetAttr[name="bias"](%102)
  %187 : Tensor = prim::GetAttr[name="weight"](%102)
  %188 : Float(16384:1, 4096:16384) = aten::t(%187), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.6 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.11, %188), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.1 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.6, %186, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.12 : Float(119:53248, 13:4096, 4096:1) = aten::add(%ffn_output.1, %171, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %192 : Tensor = prim::GetAttr[name="bias"](%101)
  %193 : Tensor = prim::GetAttr[name="weight"](%101)
  %194 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.13 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.12, %194, %193, %192, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %196 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.13, %172)
  %197 : Float(119:53248, 13:4096, 4096:1), %198 : Float(4096:1) = prim::TupleUnpack(%196)
  %199 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%197, %198)
  %200 : Float(119:53248, 13:4096, 4096:1), %201 : Float(4096:1) = prim::TupleUnpack(%199)
  %202 : __torch__.torch.nn.modules.container.___torch_mangle_67.ModuleList = prim::GetAttr[name="albert_layers"](%92)
  %203 : __torch__.transformers.modeling_albert.___torch_mangle_66.AlbertLayer = prim::GetAttr[name="0"](%202)
  %204 : __torch__.torch.nn.modules.normalization.___torch_mangle_54.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%203)
  %205 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="ffn_output"](%203)
  %206 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name="ffn"](%203)
  %207 : __torch__.transformers.modeling_albert.___torch_mangle_62.AlbertAttention = prim::GetAttr[name="attention"](%203)
  %208 : __torch__.torch.nn.modules.normalization.___torch_mangle_61.LayerNorm = prim::GetAttr[name="LayerNorm"](%207)
  %209 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%207)
  %210 : Tensor = prim::GetAttr[name="weight"](%209)
  %211 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name="value"](%207)
  %212 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name="key"](%207)
  %213 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name="query"](%207)
  %214 : Tensor = prim::GetAttr[name="bias"](%213)
  %215 : Tensor = prim::GetAttr[name="weight"](%213)
  %216 : Float(4096:1, 4096:4096) = aten::t(%215), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.7 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%200, %216), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.8 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.7, %214, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %219 : Tensor = prim::GetAttr[name="bias"](%212)
  %220 : Tensor = prim::GetAttr[name="weight"](%212)
  %221 : Float(4096:1, 4096:4096) = aten::t(%220), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.8 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%200, %221), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.10 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.8, %219, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %224 : Tensor = prim::GetAttr[name="bias"](%211)
  %225 : Tensor = prim::GetAttr[name="weight"](%211)
  %226 : Float(4096:1, 4096:4096) = aten::t(%225), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.9 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%200, %226), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.12 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.9, %224, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %229 : int = aten::size(%x.8, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %230 : int = aten::size(%x.8, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %231 : int[] = prim::ListConstruct(%229, %230, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.9 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.8, %231), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %233 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.2 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.9, %233), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %235 : int = aten::size(%x.10, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %236 : int = aten::size(%x.10, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %237 : int[] = prim::ListConstruct(%235, %236, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.11 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.10, %237), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %239 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.2 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.11, %239), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %241 : int = aten::size(%x.12, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %242 : int = aten::size(%x.12, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %243 : int[] = prim::ListConstruct(%241, %242, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.13 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.12, %243), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %245 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.2 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.13, %245), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %247 : Float(119:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.2, %43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.3 : Float(119:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %247), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.4 : Float(119:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.3, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.14 : Float(119:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.15 : Float(119:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.14, %43, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(119:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.15, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.2 : Float(119:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %254 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %255 : Float(119:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.2, %254), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %256 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%255, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %257 : Float(4096:1, 4096:4096) = aten::t(%210), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %258 : int[] = prim::ListConstruct(%36, %36, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %259 : Float(64:64, 64:1, 4096:4096) = aten::view(%257, %258), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %260 : Float(64:64, 64:1, 4096:4096) = aten::to(%259, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.2 : Float(4096:1) = aten::to(%201, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %262 : Tensor[] = prim::ListConstruct(%256, %260), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %263 : Float(119:53248, 13:4096, 4096:1) = aten::einsum(%31, %262), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.16 : Float(119:53248, 13:4096, 4096:1) = aten::add(%263, %b.2, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.2 : Float(119:53248, 13:4096, 4096:1) = aten::dropout(%input.16, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.17 : Float(119:53248, 13:4096, 4096:1) = aten::add(%200, %projected_context_layer.2, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %267 : Tensor = prim::GetAttr[name="bias"](%208)
  %268 : Tensor = prim::GetAttr[name="weight"](%208)
  %269 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.2 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.17, %269, %268, %267, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %271 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.2, %b.2)
  %272 : Float(119:53248, 13:4096, 4096:1), %273 : Float(4096:1) = prim::TupleUnpack(%271)
  %274 : Tensor = prim::GetAttr[name="bias"](%206)
  %275 : Tensor = prim::GetAttr[name="weight"](%206)
  %276 : Float(4096:1, 16384:4096) = aten::t(%275), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.10 : Float(119:212992, 13:16384, 16384:1) = aten::matmul(%272, %276), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.14 : Float(119:212992, 13:16384, 16384:1) = aten::add_(%output.10, %274, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %279 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%x.14, %37), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %280 : Float(119:212992, 13:16384, 16384:1) = aten::pow(%x.14, %38), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %281 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%280, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %282 : Float(119:212992, 13:16384, 16384:1) = aten::add(%x.14, %281, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %283 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%282, %40), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %284 : Float(119:212992, 13:16384, 16384:1) = aten::tanh(%283), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %285 : Float(119:212992, 13:16384, 16384:1) = aten::add(%284, %41, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.18 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%279, %285), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %287 : Tensor = prim::GetAttr[name="bias"](%205)
  %288 : Tensor = prim::GetAttr[name="weight"](%205)
  %289 : Float(16384:1, 4096:16384) = aten::t(%288), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.11 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.18, %289), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.2 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.11, %287, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.19 : Float(119:53248, 13:4096, 4096:1) = aten::add(%ffn_output.2, %272, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %293 : Tensor = prim::GetAttr[name="bias"](%204)
  %294 : Tensor = prim::GetAttr[name="weight"](%204)
  %295 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.20 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.19, %295, %294, %293, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %297 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.20, %273)
  %298 : Float(119:53248, 13:4096, 4096:1), %299 : Float(4096:1) = prim::TupleUnpack(%297)
  %300 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%298, %299)
  %301 : Float(119:53248, 13:4096, 4096:1), %302 : Float(4096:1) = prim::TupleUnpack(%300)
  %303 : __torch__.torch.nn.modules.container.___torch_mangle_67.ModuleList = prim::GetAttr[name="albert_layers"](%92)
  %304 : __torch__.transformers.modeling_albert.___torch_mangle_66.AlbertLayer = prim::GetAttr[name="0"](%303)
  %305 : __torch__.torch.nn.modules.normalization.___torch_mangle_54.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%304)
  %306 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="ffn_output"](%304)
  %307 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name="ffn"](%304)
  %308 : __torch__.transformers.modeling_albert.___torch_mangle_62.AlbertAttention = prim::GetAttr[name="attention"](%304)
  %309 : __torch__.torch.nn.modules.normalization.___torch_mangle_61.LayerNorm = prim::GetAttr[name="LayerNorm"](%308)
  %310 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%308)
  %311 : Tensor = prim::GetAttr[name="weight"](%310)
  %312 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name="value"](%308)
  %313 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name="key"](%308)
  %314 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name="query"](%308)
  %315 : Tensor = prim::GetAttr[name="bias"](%314)
  %316 : Tensor = prim::GetAttr[name="weight"](%314)
  %317 : Float(4096:1, 4096:4096) = aten::t(%316), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.12 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%301, %317), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.15 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.12, %315, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %320 : Tensor = prim::GetAttr[name="bias"](%313)
  %321 : Tensor = prim::GetAttr[name="weight"](%313)
  %322 : Float(4096:1, 4096:4096) = aten::t(%321), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.13 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%301, %322), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.17 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.13, %320, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %325 : Tensor = prim::GetAttr[name="bias"](%312)
  %326 : Tensor = prim::GetAttr[name="weight"](%312)
  %327 : Float(4096:1, 4096:4096) = aten::t(%326), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.14 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%301, %327), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.19 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.14, %325, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %330 : int = aten::size(%x.15, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %331 : int = aten::size(%x.15, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %332 : int[] = prim::ListConstruct(%330, %331, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.16 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.15, %332), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %334 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.3 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.16, %334), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %336 : int = aten::size(%x.17, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %337 : int = aten::size(%x.17, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %338 : int[] = prim::ListConstruct(%336, %337, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.18 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.17, %338), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %340 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.3 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.18, %340), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %342 : int = aten::size(%x.19, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %343 : int = aten::size(%x.19, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %344 : int[] = prim::ListConstruct(%342, %343, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.20 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.19, %344), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %346 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.3 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.20, %346), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %348 : Float(119:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.3, %43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.5 : Float(119:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %348), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.6 : Float(119:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.5, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.21 : Float(119:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.22 : Float(119:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.21, %43, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(119:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.22, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(119:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %355 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %356 : Float(119:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.3, %355), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %357 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%356, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %358 : Float(4096:1, 4096:4096) = aten::t(%311), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %359 : int[] = prim::ListConstruct(%36, %36, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %360 : Float(64:64, 64:1, 4096:4096) = aten::view(%358, %359), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %361 : Float(64:64, 64:1, 4096:4096) = aten::to(%360, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.3 : Float(4096:1) = aten::to(%302, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %363 : Tensor[] = prim::ListConstruct(%357, %361), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %364 : Float(119:53248, 13:4096, 4096:1) = aten::einsum(%31, %363), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.23 : Float(119:53248, 13:4096, 4096:1) = aten::add(%364, %b.3, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.3 : Float(119:53248, 13:4096, 4096:1) = aten::dropout(%input.23, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.24 : Float(119:53248, 13:4096, 4096:1) = aten::add(%301, %projected_context_layer.3, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %368 : Tensor = prim::GetAttr[name="bias"](%309)
  %369 : Tensor = prim::GetAttr[name="weight"](%309)
  %370 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.3 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.24, %370, %369, %368, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %372 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.3, %b.3)
  %373 : Float(119:53248, 13:4096, 4096:1), %374 : Float(4096:1) = prim::TupleUnpack(%372)
  %375 : Tensor = prim::GetAttr[name="bias"](%307)
  %376 : Tensor = prim::GetAttr[name="weight"](%307)
  %377 : Float(4096:1, 16384:4096) = aten::t(%376), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.15 : Float(119:212992, 13:16384, 16384:1) = aten::matmul(%373, %377), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.21 : Float(119:212992, 13:16384, 16384:1) = aten::add_(%output.15, %375, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %380 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%x.21, %37), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %381 : Float(119:212992, 13:16384, 16384:1) = aten::pow(%x.21, %38), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %382 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%381, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %383 : Float(119:212992, 13:16384, 16384:1) = aten::add(%x.21, %382, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %384 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%383, %40), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %385 : Float(119:212992, 13:16384, 16384:1) = aten::tanh(%384), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %386 : Float(119:212992, 13:16384, 16384:1) = aten::add(%385, %41, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.25 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%380, %386), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %388 : Tensor = prim::GetAttr[name="bias"](%306)
  %389 : Tensor = prim::GetAttr[name="weight"](%306)
  %390 : Float(16384:1, 4096:16384) = aten::t(%389), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.16 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.25, %390), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.3 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.16, %388, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.26 : Float(119:53248, 13:4096, 4096:1) = aten::add(%ffn_output.3, %373, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %394 : Tensor = prim::GetAttr[name="bias"](%305)
  %395 : Tensor = prim::GetAttr[name="weight"](%305)
  %396 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.27 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.26, %396, %395, %394, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %398 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.27, %374)
  %399 : Float(119:53248, 13:4096, 4096:1), %400 : Float(4096:1) = prim::TupleUnpack(%398)
  %401 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%399, %400)
  %402 : Float(119:53248, 13:4096, 4096:1), %403 : Float(4096:1) = prim::TupleUnpack(%401)
  %404 : __torch__.torch.nn.modules.container.___torch_mangle_67.ModuleList = prim::GetAttr[name="albert_layers"](%92)
  %405 : __torch__.transformers.modeling_albert.___torch_mangle_66.AlbertLayer = prim::GetAttr[name="0"](%404)
  %406 : __torch__.torch.nn.modules.normalization.___torch_mangle_54.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%405)
  %407 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="ffn_output"](%405)
  %408 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name="ffn"](%405)
  %409 : __torch__.transformers.modeling_albert.___torch_mangle_62.AlbertAttention = prim::GetAttr[name="attention"](%405)
  %410 : __torch__.torch.nn.modules.normalization.___torch_mangle_61.LayerNorm = prim::GetAttr[name="LayerNorm"](%409)
  %411 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%409)
  %412 : Tensor = prim::GetAttr[name="weight"](%411)
  %413 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name="value"](%409)
  %414 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name="key"](%409)
  %415 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name="query"](%409)
  %416 : Tensor = prim::GetAttr[name="bias"](%415)
  %417 : Tensor = prim::GetAttr[name="weight"](%415)
  %418 : Float(4096:1, 4096:4096) = aten::t(%417), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.17 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%402, %418), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.22 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.17, %416, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %421 : Tensor = prim::GetAttr[name="bias"](%414)
  %422 : Tensor = prim::GetAttr[name="weight"](%414)
  %423 : Float(4096:1, 4096:4096) = aten::t(%422), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.18 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%402, %423), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.24 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.18, %421, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %426 : Tensor = prim::GetAttr[name="bias"](%413)
  %427 : Tensor = prim::GetAttr[name="weight"](%413)
  %428 : Float(4096:1, 4096:4096) = aten::t(%427), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.19 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%402, %428), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.26 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.19, %426, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %431 : int = aten::size(%x.22, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %432 : int = aten::size(%x.22, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %433 : int[] = prim::ListConstruct(%431, %432, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.23 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.22, %433), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %435 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.4 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.23, %435), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %437 : int = aten::size(%x.24, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %438 : int = aten::size(%x.24, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %439 : int[] = prim::ListConstruct(%437, %438, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.25 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.24, %439), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %441 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.4 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.25, %441), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %443 : int = aten::size(%x.26, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %444 : int = aten::size(%x.26, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %445 : int[] = prim::ListConstruct(%443, %444, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.27 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.26, %445), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %447 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.4 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.27, %447), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %449 : Float(119:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.4, %43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.7 : Float(119:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %449), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.8 : Float(119:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.7, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.28 : Float(119:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.29 : Float(119:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.28, %43, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(119:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.29, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.4 : Float(119:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %456 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %457 : Float(119:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.4, %456), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %458 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%457, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %459 : Float(4096:1, 4096:4096) = aten::t(%412), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %460 : int[] = prim::ListConstruct(%36, %36, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %461 : Float(64:64, 64:1, 4096:4096) = aten::view(%459, %460), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %462 : Float(64:64, 64:1, 4096:4096) = aten::to(%461, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.4 : Float(4096:1) = aten::to(%403, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %464 : Tensor[] = prim::ListConstruct(%458, %462), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %465 : Float(119:53248, 13:4096, 4096:1) = aten::einsum(%31, %464), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.30 : Float(119:53248, 13:4096, 4096:1) = aten::add(%465, %b.4, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.4 : Float(119:53248, 13:4096, 4096:1) = aten::dropout(%input.30, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.31 : Float(119:53248, 13:4096, 4096:1) = aten::add(%402, %projected_context_layer.4, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %469 : Tensor = prim::GetAttr[name="bias"](%410)
  %470 : Tensor = prim::GetAttr[name="weight"](%410)
  %471 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.4 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.31, %471, %470, %469, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %473 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.4, %b.4)
  %474 : Float(119:53248, 13:4096, 4096:1), %475 : Float(4096:1) = prim::TupleUnpack(%473)
  %476 : Tensor = prim::GetAttr[name="bias"](%408)
  %477 : Tensor = prim::GetAttr[name="weight"](%408)
  %478 : Float(4096:1, 16384:4096) = aten::t(%477), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.20 : Float(119:212992, 13:16384, 16384:1) = aten::matmul(%474, %478), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.28 : Float(119:212992, 13:16384, 16384:1) = aten::add_(%output.20, %476, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %481 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%x.28, %37), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %482 : Float(119:212992, 13:16384, 16384:1) = aten::pow(%x.28, %38), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %483 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%482, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %484 : Float(119:212992, 13:16384, 16384:1) = aten::add(%x.28, %483, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %485 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%484, %40), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %486 : Float(119:212992, 13:16384, 16384:1) = aten::tanh(%485), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %487 : Float(119:212992, 13:16384, 16384:1) = aten::add(%486, %41, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.32 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%481, %487), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %489 : Tensor = prim::GetAttr[name="bias"](%407)
  %490 : Tensor = prim::GetAttr[name="weight"](%407)
  %491 : Float(16384:1, 4096:16384) = aten::t(%490), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.21 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.32, %491), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.4 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.21, %489, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.33 : Float(119:53248, 13:4096, 4096:1) = aten::add(%ffn_output.4, %474, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %495 : Tensor = prim::GetAttr[name="bias"](%406)
  %496 : Tensor = prim::GetAttr[name="weight"](%406)
  %497 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.34 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.33, %497, %496, %495, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %499 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.34, %475)
  %500 : Float(119:53248, 13:4096, 4096:1), %501 : Float(4096:1) = prim::TupleUnpack(%499)
  %502 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%500, %501)
  %503 : Float(119:53248, 13:4096, 4096:1), %504 : Float(4096:1) = prim::TupleUnpack(%502)
  %505 : __torch__.torch.nn.modules.container.___torch_mangle_67.ModuleList = prim::GetAttr[name="albert_layers"](%92)
  %506 : __torch__.transformers.modeling_albert.___torch_mangle_66.AlbertLayer = prim::GetAttr[name="0"](%505)
  %507 : __torch__.torch.nn.modules.normalization.___torch_mangle_54.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%506)
  %508 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="ffn_output"](%506)
  %509 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name="ffn"](%506)
  %510 : __torch__.transformers.modeling_albert.___torch_mangle_62.AlbertAttention = prim::GetAttr[name="attention"](%506)
  %511 : __torch__.torch.nn.modules.normalization.___torch_mangle_61.LayerNorm = prim::GetAttr[name="LayerNorm"](%510)
  %512 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%510)
  %513 : Tensor = prim::GetAttr[name="weight"](%512)
  %514 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name="value"](%510)
  %515 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name="key"](%510)
  %516 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name="query"](%510)
  %517 : Tensor = prim::GetAttr[name="bias"](%516)
  %518 : Tensor = prim::GetAttr[name="weight"](%516)
  %519 : Float(4096:1, 4096:4096) = aten::t(%518), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.22 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%503, %519), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.29 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.22, %517, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %522 : Tensor = prim::GetAttr[name="bias"](%515)
  %523 : Tensor = prim::GetAttr[name="weight"](%515)
  %524 : Float(4096:1, 4096:4096) = aten::t(%523), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.23 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%503, %524), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.31 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.23, %522, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %527 : Tensor = prim::GetAttr[name="bias"](%514)
  %528 : Tensor = prim::GetAttr[name="weight"](%514)
  %529 : Float(4096:1, 4096:4096) = aten::t(%528), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.24 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%503, %529), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.33 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.24, %527, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %532 : int = aten::size(%x.29, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %533 : int = aten::size(%x.29, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %534 : int[] = prim::ListConstruct(%532, %533, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.30 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.29, %534), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %536 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.5 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.30, %536), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %538 : int = aten::size(%x.31, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %539 : int = aten::size(%x.31, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %540 : int[] = prim::ListConstruct(%538, %539, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.32 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.31, %540), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %542 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.5 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.32, %542), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %544 : int = aten::size(%x.33, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %545 : int = aten::size(%x.33, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %546 : int[] = prim::ListConstruct(%544, %545, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.34 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.33, %546), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %548 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.5 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.34, %548), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %550 : Float(119:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.5, %43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.9 : Float(119:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %550), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.10 : Float(119:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.9, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.35 : Float(119:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.36 : Float(119:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.35, %43, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(119:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.36, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(119:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %557 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %558 : Float(119:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.5, %557), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %559 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%558, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %560 : Float(4096:1, 4096:4096) = aten::t(%513), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %561 : int[] = prim::ListConstruct(%36, %36, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %562 : Float(64:64, 64:1, 4096:4096) = aten::view(%560, %561), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %563 : Float(64:64, 64:1, 4096:4096) = aten::to(%562, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.5 : Float(4096:1) = aten::to(%504, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %565 : Tensor[] = prim::ListConstruct(%559, %563), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %566 : Float(119:53248, 13:4096, 4096:1) = aten::einsum(%31, %565), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.37 : Float(119:53248, 13:4096, 4096:1) = aten::add(%566, %b.5, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.5 : Float(119:53248, 13:4096, 4096:1) = aten::dropout(%input.37, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.38 : Float(119:53248, 13:4096, 4096:1) = aten::add(%503, %projected_context_layer.5, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %570 : Tensor = prim::GetAttr[name="bias"](%511)
  %571 : Tensor = prim::GetAttr[name="weight"](%511)
  %572 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.5 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.38, %572, %571, %570, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %574 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.5, %b.5)
  %575 : Float(119:53248, 13:4096, 4096:1), %576 : Float(4096:1) = prim::TupleUnpack(%574)
  %577 : Tensor = prim::GetAttr[name="bias"](%509)
  %578 : Tensor = prim::GetAttr[name="weight"](%509)
  %579 : Float(4096:1, 16384:4096) = aten::t(%578), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.25 : Float(119:212992, 13:16384, 16384:1) = aten::matmul(%575, %579), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.35 : Float(119:212992, 13:16384, 16384:1) = aten::add_(%output.25, %577, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %582 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%x.35, %37), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %583 : Float(119:212992, 13:16384, 16384:1) = aten::pow(%x.35, %38), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %584 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%583, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %585 : Float(119:212992, 13:16384, 16384:1) = aten::add(%x.35, %584, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %586 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%585, %40), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %587 : Float(119:212992, 13:16384, 16384:1) = aten::tanh(%586), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %588 : Float(119:212992, 13:16384, 16384:1) = aten::add(%587, %41, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.39 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%582, %588), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %590 : Tensor = prim::GetAttr[name="bias"](%508)
  %591 : Tensor = prim::GetAttr[name="weight"](%508)
  %592 : Float(16384:1, 4096:16384) = aten::t(%591), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.26 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.39, %592), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.5 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.26, %590, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.40 : Float(119:53248, 13:4096, 4096:1) = aten::add(%ffn_output.5, %575, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %596 : Tensor = prim::GetAttr[name="bias"](%507)
  %597 : Tensor = prim::GetAttr[name="weight"](%507)
  %598 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.41 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.40, %598, %597, %596, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %600 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.41, %576)
  %601 : Float(119:53248, 13:4096, 4096:1), %602 : Float(4096:1) = prim::TupleUnpack(%600)
  %603 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%601, %602)
  %604 : Float(119:53248, 13:4096, 4096:1), %605 : Float(4096:1) = prim::TupleUnpack(%603)
  %606 : __torch__.torch.nn.modules.container.___torch_mangle_67.ModuleList = prim::GetAttr[name="albert_layers"](%92)
  %607 : __torch__.transformers.modeling_albert.___torch_mangle_66.AlbertLayer = prim::GetAttr[name="0"](%606)
  %608 : __torch__.torch.nn.modules.normalization.___torch_mangle_54.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%607)
  %609 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="ffn_output"](%607)
  %610 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name="ffn"](%607)
  %611 : __torch__.transformers.modeling_albert.___torch_mangle_62.AlbertAttention = prim::GetAttr[name="attention"](%607)
  %612 : __torch__.torch.nn.modules.normalization.___torch_mangle_61.LayerNorm = prim::GetAttr[name="LayerNorm"](%611)
  %613 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%611)
  %614 : Tensor = prim::GetAttr[name="weight"](%613)
  %615 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name="value"](%611)
  %616 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name="key"](%611)
  %617 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name="query"](%611)
  %618 : Tensor = prim::GetAttr[name="bias"](%617)
  %619 : Tensor = prim::GetAttr[name="weight"](%617)
  %620 : Float(4096:1, 4096:4096) = aten::t(%619), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.27 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%604, %620), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.36 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.27, %618, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %623 : Tensor = prim::GetAttr[name="bias"](%616)
  %624 : Tensor = prim::GetAttr[name="weight"](%616)
  %625 : Float(4096:1, 4096:4096) = aten::t(%624), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.28 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%604, %625), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.38 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.28, %623, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %628 : Tensor = prim::GetAttr[name="bias"](%615)
  %629 : Tensor = prim::GetAttr[name="weight"](%615)
  %630 : Float(4096:1, 4096:4096) = aten::t(%629), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.29 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%604, %630), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.40 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.29, %628, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %633 : int = aten::size(%x.36, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %634 : int = aten::size(%x.36, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %635 : int[] = prim::ListConstruct(%633, %634, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.37 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.36, %635), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %637 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.6 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.37, %637), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %639 : int = aten::size(%x.38, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %640 : int = aten::size(%x.38, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %641 : int[] = prim::ListConstruct(%639, %640, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.39 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.38, %641), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %643 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.6 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.39, %643), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %645 : int = aten::size(%x.40, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %646 : int = aten::size(%x.40, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %647 : int[] = prim::ListConstruct(%645, %646, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.41 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.40, %647), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %649 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.6 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.41, %649), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %651 : Float(119:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.6, %43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.11 : Float(119:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %651), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.12 : Float(119:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.11, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.42 : Float(119:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.43 : Float(119:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.42, %43, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(119:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.43, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.6 : Float(119:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %658 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %659 : Float(119:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.6, %658), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %660 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%659, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %661 : Float(4096:1, 4096:4096) = aten::t(%614), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %662 : int[] = prim::ListConstruct(%36, %36, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %663 : Float(64:64, 64:1, 4096:4096) = aten::view(%661, %662), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %664 : Float(64:64, 64:1, 4096:4096) = aten::to(%663, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.6 : Float(4096:1) = aten::to(%605, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %666 : Tensor[] = prim::ListConstruct(%660, %664), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %667 : Float(119:53248, 13:4096, 4096:1) = aten::einsum(%31, %666), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.44 : Float(119:53248, 13:4096, 4096:1) = aten::add(%667, %b.6, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.6 : Float(119:53248, 13:4096, 4096:1) = aten::dropout(%input.44, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(119:53248, 13:4096, 4096:1) = aten::add(%604, %projected_context_layer.6, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %671 : Tensor = prim::GetAttr[name="bias"](%612)
  %672 : Tensor = prim::GetAttr[name="weight"](%612)
  %673 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.6 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.45, %673, %672, %671, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %675 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.6, %b.6)
  %676 : Float(119:53248, 13:4096, 4096:1), %677 : Float(4096:1) = prim::TupleUnpack(%675)
  %678 : Tensor = prim::GetAttr[name="bias"](%610)
  %679 : Tensor = prim::GetAttr[name="weight"](%610)
  %680 : Float(4096:1, 16384:4096) = aten::t(%679), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.30 : Float(119:212992, 13:16384, 16384:1) = aten::matmul(%676, %680), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.42 : Float(119:212992, 13:16384, 16384:1) = aten::add_(%output.30, %678, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %683 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%x.42, %37), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %684 : Float(119:212992, 13:16384, 16384:1) = aten::pow(%x.42, %38), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %685 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%684, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %686 : Float(119:212992, 13:16384, 16384:1) = aten::add(%x.42, %685, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %687 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%686, %40), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %688 : Float(119:212992, 13:16384, 16384:1) = aten::tanh(%687), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %689 : Float(119:212992, 13:16384, 16384:1) = aten::add(%688, %41, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.46 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%683, %689), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %691 : Tensor = prim::GetAttr[name="bias"](%609)
  %692 : Tensor = prim::GetAttr[name="weight"](%609)
  %693 : Float(16384:1, 4096:16384) = aten::t(%692), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.31 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.46, %693), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.6 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.31, %691, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.47 : Float(119:53248, 13:4096, 4096:1) = aten::add(%ffn_output.6, %676, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %697 : Tensor = prim::GetAttr[name="bias"](%608)
  %698 : Tensor = prim::GetAttr[name="weight"](%608)
  %699 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.48 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.47, %699, %698, %697, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %701 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.48, %677)
  %702 : Float(119:53248, 13:4096, 4096:1), %703 : Float(4096:1) = prim::TupleUnpack(%701)
  %704 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%702, %703)
  %705 : Float(119:53248, 13:4096, 4096:1), %706 : Float(4096:1) = prim::TupleUnpack(%704)
  %707 : __torch__.torch.nn.modules.container.___torch_mangle_67.ModuleList = prim::GetAttr[name="albert_layers"](%92)
  %708 : __torch__.transformers.modeling_albert.___torch_mangle_66.AlbertLayer = prim::GetAttr[name="0"](%707)
  %709 : __torch__.torch.nn.modules.normalization.___torch_mangle_54.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%708)
  %710 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="ffn_output"](%708)
  %711 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name="ffn"](%708)
  %712 : __torch__.transformers.modeling_albert.___torch_mangle_62.AlbertAttention = prim::GetAttr[name="attention"](%708)
  %713 : __torch__.torch.nn.modules.normalization.___torch_mangle_61.LayerNorm = prim::GetAttr[name="LayerNorm"](%712)
  %714 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%712)
  %715 : Tensor = prim::GetAttr[name="weight"](%714)
  %716 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name="value"](%712)
  %717 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name="key"](%712)
  %718 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name="query"](%712)
  %719 : Tensor = prim::GetAttr[name="bias"](%718)
  %720 : Tensor = prim::GetAttr[name="weight"](%718)
  %721 : Float(4096:1, 4096:4096) = aten::t(%720), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.32 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%705, %721), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.43 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.32, %719, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %724 : Tensor = prim::GetAttr[name="bias"](%717)
  %725 : Tensor = prim::GetAttr[name="weight"](%717)
  %726 : Float(4096:1, 4096:4096) = aten::t(%725), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.33 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%705, %726), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.45 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.33, %724, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %729 : Tensor = prim::GetAttr[name="bias"](%716)
  %730 : Tensor = prim::GetAttr[name="weight"](%716)
  %731 : Float(4096:1, 4096:4096) = aten::t(%730), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.34 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%705, %731), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.47 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.34, %729, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %734 : int = aten::size(%x.43, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %735 : int = aten::size(%x.43, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %736 : int[] = prim::ListConstruct(%734, %735, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.44 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.43, %736), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %738 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.7 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.44, %738), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %740 : int = aten::size(%x.45, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %741 : int = aten::size(%x.45, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %742 : int[] = prim::ListConstruct(%740, %741, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.46 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.45, %742), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %744 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.7 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.46, %744), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %746 : int = aten::size(%x.47, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %747 : int = aten::size(%x.47, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %748 : int[] = prim::ListConstruct(%746, %747, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.48 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.47, %748), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %750 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.7 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.48, %750), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %752 : Float(119:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.7, %43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.13 : Float(119:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %752), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.14 : Float(119:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.13, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.49 : Float(119:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.50 : Float(119:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.49, %43, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(119:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.50, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(119:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %759 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %760 : Float(119:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.7, %759), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %761 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%760, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %762 : Float(4096:1, 4096:4096) = aten::t(%715), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %763 : int[] = prim::ListConstruct(%36, %36, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %764 : Float(64:64, 64:1, 4096:4096) = aten::view(%762, %763), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %765 : Float(64:64, 64:1, 4096:4096) = aten::to(%764, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.7 : Float(4096:1) = aten::to(%706, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %767 : Tensor[] = prim::ListConstruct(%761, %765), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %768 : Float(119:53248, 13:4096, 4096:1) = aten::einsum(%31, %767), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.51 : Float(119:53248, 13:4096, 4096:1) = aten::add(%768, %b.7, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.7 : Float(119:53248, 13:4096, 4096:1) = aten::dropout(%input.51, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.52 : Float(119:53248, 13:4096, 4096:1) = aten::add(%705, %projected_context_layer.7, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %772 : Tensor = prim::GetAttr[name="bias"](%713)
  %773 : Tensor = prim::GetAttr[name="weight"](%713)
  %774 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.7 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.52, %774, %773, %772, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %776 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.7, %b.7)
  %777 : Float(119:53248, 13:4096, 4096:1), %778 : Float(4096:1) = prim::TupleUnpack(%776)
  %779 : Tensor = prim::GetAttr[name="bias"](%711)
  %780 : Tensor = prim::GetAttr[name="weight"](%711)
  %781 : Float(4096:1, 16384:4096) = aten::t(%780), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.35 : Float(119:212992, 13:16384, 16384:1) = aten::matmul(%777, %781), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.49 : Float(119:212992, 13:16384, 16384:1) = aten::add_(%output.35, %779, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %784 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%x.49, %37), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %785 : Float(119:212992, 13:16384, 16384:1) = aten::pow(%x.49, %38), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %786 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%785, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %787 : Float(119:212992, 13:16384, 16384:1) = aten::add(%x.49, %786, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %788 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%787, %40), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %789 : Float(119:212992, 13:16384, 16384:1) = aten::tanh(%788), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %790 : Float(119:212992, 13:16384, 16384:1) = aten::add(%789, %41, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.53 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%784, %790), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %792 : Tensor = prim::GetAttr[name="bias"](%710)
  %793 : Tensor = prim::GetAttr[name="weight"](%710)
  %794 : Float(16384:1, 4096:16384) = aten::t(%793), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.36 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.53, %794), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.7 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.36, %792, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.54 : Float(119:53248, 13:4096, 4096:1) = aten::add(%ffn_output.7, %777, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %798 : Tensor = prim::GetAttr[name="bias"](%709)
  %799 : Tensor = prim::GetAttr[name="weight"](%709)
  %800 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.55 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.54, %800, %799, %798, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %802 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.55, %778)
  %803 : Float(119:53248, 13:4096, 4096:1), %804 : Float(4096:1) = prim::TupleUnpack(%802)
  %805 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%803, %804)
  %806 : Float(119:53248, 13:4096, 4096:1), %807 : Float(4096:1) = prim::TupleUnpack(%805)
  %808 : __torch__.torch.nn.modules.container.___torch_mangle_67.ModuleList = prim::GetAttr[name="albert_layers"](%92)
  %809 : __torch__.transformers.modeling_albert.___torch_mangle_66.AlbertLayer = prim::GetAttr[name="0"](%808)
  %810 : __torch__.torch.nn.modules.normalization.___torch_mangle_54.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%809)
  %811 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="ffn_output"](%809)
  %812 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name="ffn"](%809)
  %813 : __torch__.transformers.modeling_albert.___torch_mangle_62.AlbertAttention = prim::GetAttr[name="attention"](%809)
  %814 : __torch__.torch.nn.modules.normalization.___torch_mangle_61.LayerNorm = prim::GetAttr[name="LayerNorm"](%813)
  %815 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%813)
  %816 : Tensor = prim::GetAttr[name="weight"](%815)
  %817 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name="value"](%813)
  %818 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name="key"](%813)
  %819 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name="query"](%813)
  %820 : Tensor = prim::GetAttr[name="bias"](%819)
  %821 : Tensor = prim::GetAttr[name="weight"](%819)
  %822 : Float(4096:1, 4096:4096) = aten::t(%821), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.37 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%806, %822), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.50 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.37, %820, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %825 : Tensor = prim::GetAttr[name="bias"](%818)
  %826 : Tensor = prim::GetAttr[name="weight"](%818)
  %827 : Float(4096:1, 4096:4096) = aten::t(%826), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.38 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%806, %827), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.52 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.38, %825, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %830 : Tensor = prim::GetAttr[name="bias"](%817)
  %831 : Tensor = prim::GetAttr[name="weight"](%817)
  %832 : Float(4096:1, 4096:4096) = aten::t(%831), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.39 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%806, %832), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.54 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.39, %830, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %835 : int = aten::size(%x.50, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %836 : int = aten::size(%x.50, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %837 : int[] = prim::ListConstruct(%835, %836, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.51 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.50, %837), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %839 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.8 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.51, %839), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %841 : int = aten::size(%x.52, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %842 : int = aten::size(%x.52, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %843 : int[] = prim::ListConstruct(%841, %842, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.53 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.52, %843), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %845 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.8 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.53, %845), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %847 : int = aten::size(%x.54, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %848 : int = aten::size(%x.54, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %849 : int[] = prim::ListConstruct(%847, %848, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.55 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.54, %849), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %851 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.8 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.55, %851), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %853 : Float(119:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.8, %43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.15 : Float(119:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %853), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.16 : Float(119:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.15, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.56 : Float(119:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.57 : Float(119:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.56, %43, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(119:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.57, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.8 : Float(119:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %860 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %861 : Float(119:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.8, %860), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %862 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%861, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %863 : Float(4096:1, 4096:4096) = aten::t(%816), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %864 : int[] = prim::ListConstruct(%36, %36, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %865 : Float(64:64, 64:1, 4096:4096) = aten::view(%863, %864), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %866 : Float(64:64, 64:1, 4096:4096) = aten::to(%865, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.8 : Float(4096:1) = aten::to(%807, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %868 : Tensor[] = prim::ListConstruct(%862, %866), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %869 : Float(119:53248, 13:4096, 4096:1) = aten::einsum(%31, %868), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.58 : Float(119:53248, 13:4096, 4096:1) = aten::add(%869, %b.8, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.8 : Float(119:53248, 13:4096, 4096:1) = aten::dropout(%input.58, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.59 : Float(119:53248, 13:4096, 4096:1) = aten::add(%806, %projected_context_layer.8, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %873 : Tensor = prim::GetAttr[name="bias"](%814)
  %874 : Tensor = prim::GetAttr[name="weight"](%814)
  %875 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.8 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.59, %875, %874, %873, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %877 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.8, %b.8)
  %878 : Float(119:53248, 13:4096, 4096:1), %879 : Float(4096:1) = prim::TupleUnpack(%877)
  %880 : Tensor = prim::GetAttr[name="bias"](%812)
  %881 : Tensor = prim::GetAttr[name="weight"](%812)
  %882 : Float(4096:1, 16384:4096) = aten::t(%881), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.40 : Float(119:212992, 13:16384, 16384:1) = aten::matmul(%878, %882), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.56 : Float(119:212992, 13:16384, 16384:1) = aten::add_(%output.40, %880, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %885 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%x.56, %37), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %886 : Float(119:212992, 13:16384, 16384:1) = aten::pow(%x.56, %38), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %887 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%886, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %888 : Float(119:212992, 13:16384, 16384:1) = aten::add(%x.56, %887, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %889 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%888, %40), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %890 : Float(119:212992, 13:16384, 16384:1) = aten::tanh(%889), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %891 : Float(119:212992, 13:16384, 16384:1) = aten::add(%890, %41, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.60 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%885, %891), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %893 : Tensor = prim::GetAttr[name="bias"](%811)
  %894 : Tensor = prim::GetAttr[name="weight"](%811)
  %895 : Float(16384:1, 4096:16384) = aten::t(%894), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.41 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.60, %895), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.8 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.41, %893, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.61 : Float(119:53248, 13:4096, 4096:1) = aten::add(%ffn_output.8, %878, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %899 : Tensor = prim::GetAttr[name="bias"](%810)
  %900 : Tensor = prim::GetAttr[name="weight"](%810)
  %901 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.62 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.61, %901, %900, %899, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %903 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.62, %879)
  %904 : Float(119:53248, 13:4096, 4096:1), %905 : Float(4096:1) = prim::TupleUnpack(%903)
  %906 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%904, %905)
  %907 : Float(119:53248, 13:4096, 4096:1), %908 : Float(4096:1) = prim::TupleUnpack(%906)
  %909 : __torch__.torch.nn.modules.container.___torch_mangle_67.ModuleList = prim::GetAttr[name="albert_layers"](%92)
  %910 : __torch__.transformers.modeling_albert.___torch_mangle_66.AlbertLayer = prim::GetAttr[name="0"](%909)
  %911 : __torch__.torch.nn.modules.normalization.___torch_mangle_54.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%910)
  %912 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="ffn_output"](%910)
  %913 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name="ffn"](%910)
  %914 : __torch__.transformers.modeling_albert.___torch_mangle_62.AlbertAttention = prim::GetAttr[name="attention"](%910)
  %915 : __torch__.torch.nn.modules.normalization.___torch_mangle_61.LayerNorm = prim::GetAttr[name="LayerNorm"](%914)
  %916 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%914)
  %917 : Tensor = prim::GetAttr[name="weight"](%916)
  %918 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name="value"](%914)
  %919 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name="key"](%914)
  %920 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name="query"](%914)
  %921 : Tensor = prim::GetAttr[name="bias"](%920)
  %922 : Tensor = prim::GetAttr[name="weight"](%920)
  %923 : Float(4096:1, 4096:4096) = aten::t(%922), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.42 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%907, %923), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.57 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.42, %921, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %926 : Tensor = prim::GetAttr[name="bias"](%919)
  %927 : Tensor = prim::GetAttr[name="weight"](%919)
  %928 : Float(4096:1, 4096:4096) = aten::t(%927), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.43 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%907, %928), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.59 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.43, %926, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %931 : Tensor = prim::GetAttr[name="bias"](%918)
  %932 : Tensor = prim::GetAttr[name="weight"](%918)
  %933 : Float(4096:1, 4096:4096) = aten::t(%932), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.44 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%907, %933), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.61 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.44, %931, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %936 : int = aten::size(%x.57, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %937 : int = aten::size(%x.57, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %938 : int[] = prim::ListConstruct(%936, %937, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.58 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.57, %938), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %940 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.9 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.58, %940), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %942 : int = aten::size(%x.59, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %943 : int = aten::size(%x.59, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %944 : int[] = prim::ListConstruct(%942, %943, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.60 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.59, %944), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %946 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.9 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.60, %946), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %948 : int = aten::size(%x.61, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %949 : int = aten::size(%x.61, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %950 : int[] = prim::ListConstruct(%948, %949, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.62 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.61, %950), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %952 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.9 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.62, %952), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %954 : Float(119:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.9, %43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.17 : Float(119:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %954), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.18 : Float(119:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.17, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.63 : Float(119:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.64 : Float(119:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.63, %43, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(119:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.64, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(119:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %961 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %962 : Float(119:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.9, %961), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %963 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%962, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %964 : Float(4096:1, 4096:4096) = aten::t(%917), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %965 : int[] = prim::ListConstruct(%36, %36, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %966 : Float(64:64, 64:1, 4096:4096) = aten::view(%964, %965), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %967 : Float(64:64, 64:1, 4096:4096) = aten::to(%966, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.9 : Float(4096:1) = aten::to(%908, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %969 : Tensor[] = prim::ListConstruct(%963, %967), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %970 : Float(119:53248, 13:4096, 4096:1) = aten::einsum(%31, %969), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.65 : Float(119:53248, 13:4096, 4096:1) = aten::add(%970, %b.9, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.9 : Float(119:53248, 13:4096, 4096:1) = aten::dropout(%input.65, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.66 : Float(119:53248, 13:4096, 4096:1) = aten::add(%907, %projected_context_layer.9, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %974 : Tensor = prim::GetAttr[name="bias"](%915)
  %975 : Tensor = prim::GetAttr[name="weight"](%915)
  %976 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.9 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.66, %976, %975, %974, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %978 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.9, %b.9)
  %979 : Float(119:53248, 13:4096, 4096:1), %980 : Float(4096:1) = prim::TupleUnpack(%978)
  %981 : Tensor = prim::GetAttr[name="bias"](%913)
  %982 : Tensor = prim::GetAttr[name="weight"](%913)
  %983 : Float(4096:1, 16384:4096) = aten::t(%982), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.45 : Float(119:212992, 13:16384, 16384:1) = aten::matmul(%979, %983), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.63 : Float(119:212992, 13:16384, 16384:1) = aten::add_(%output.45, %981, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %986 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%x.63, %37), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %987 : Float(119:212992, 13:16384, 16384:1) = aten::pow(%x.63, %38), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %988 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%987, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %989 : Float(119:212992, 13:16384, 16384:1) = aten::add(%x.63, %988, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %990 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%989, %40), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %991 : Float(119:212992, 13:16384, 16384:1) = aten::tanh(%990), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %992 : Float(119:212992, 13:16384, 16384:1) = aten::add(%991, %41, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.67 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%986, %992), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %994 : Tensor = prim::GetAttr[name="bias"](%912)
  %995 : Tensor = prim::GetAttr[name="weight"](%912)
  %996 : Float(16384:1, 4096:16384) = aten::t(%995), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.46 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.67, %996), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.9 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.46, %994, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.68 : Float(119:53248, 13:4096, 4096:1) = aten::add(%ffn_output.9, %979, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1000 : Tensor = prim::GetAttr[name="bias"](%911)
  %1001 : Tensor = prim::GetAttr[name="weight"](%911)
  %1002 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.69 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.68, %1002, %1001, %1000, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1004 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.69, %980)
  %1005 : Float(119:53248, 13:4096, 4096:1), %1006 : Float(4096:1) = prim::TupleUnpack(%1004)
  %1007 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1005, %1006)
  %1008 : Float(119:53248, 13:4096, 4096:1), %1009 : Float(4096:1) = prim::TupleUnpack(%1007)
  %1010 : __torch__.torch.nn.modules.container.___torch_mangle_67.ModuleList = prim::GetAttr[name="albert_layers"](%92)
  %1011 : __torch__.transformers.modeling_albert.___torch_mangle_66.AlbertLayer = prim::GetAttr[name="0"](%1010)
  %1012 : __torch__.torch.nn.modules.normalization.___torch_mangle_54.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1011)
  %1013 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="ffn_output"](%1011)
  %1014 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name="ffn"](%1011)
  %1015 : __torch__.transformers.modeling_albert.___torch_mangle_62.AlbertAttention = prim::GetAttr[name="attention"](%1011)
  %1016 : __torch__.torch.nn.modules.normalization.___torch_mangle_61.LayerNorm = prim::GetAttr[name="LayerNorm"](%1015)
  %1017 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%1015)
  %1018 : Tensor = prim::GetAttr[name="weight"](%1017)
  %1019 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name="value"](%1015)
  %1020 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name="key"](%1015)
  %1021 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name="query"](%1015)
  %1022 : Tensor = prim::GetAttr[name="bias"](%1021)
  %1023 : Tensor = prim::GetAttr[name="weight"](%1021)
  %1024 : Float(4096:1, 4096:4096) = aten::t(%1023), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.47 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%1008, %1024), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.64 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.47, %1022, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1027 : Tensor = prim::GetAttr[name="bias"](%1020)
  %1028 : Tensor = prim::GetAttr[name="weight"](%1020)
  %1029 : Float(4096:1, 4096:4096) = aten::t(%1028), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.48 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%1008, %1029), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.66 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.48, %1027, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1032 : Tensor = prim::GetAttr[name="bias"](%1019)
  %1033 : Tensor = prim::GetAttr[name="weight"](%1019)
  %1034 : Float(4096:1, 4096:4096) = aten::t(%1033), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.49 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%1008, %1034), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.68 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.49, %1032, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1037 : int = aten::size(%x.64, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1038 : int = aten::size(%x.64, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1039 : int[] = prim::ListConstruct(%1037, %1038, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.65 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.64, %1039), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1041 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.10 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.65, %1041), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1043 : int = aten::size(%x.66, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1044 : int = aten::size(%x.66, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1045 : int[] = prim::ListConstruct(%1043, %1044, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.67 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.66, %1045), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1047 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.10 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.67, %1047), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1049 : int = aten::size(%x.68, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1050 : int = aten::size(%x.68, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1051 : int[] = prim::ListConstruct(%1049, %1050, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.69 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.68, %1051), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1053 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.10 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.69, %1053), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1055 : Float(119:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.10, %43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.19 : Float(119:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %1055), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.20 : Float(119:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.19, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.70 : Float(119:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.71 : Float(119:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.70, %43, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(119:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.71, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.10 : Float(119:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1062 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1063 : Float(119:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.10, %1062), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1064 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1063, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1065 : Float(4096:1, 4096:4096) = aten::t(%1018), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1066 : int[] = prim::ListConstruct(%36, %36, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1067 : Float(64:64, 64:1, 4096:4096) = aten::view(%1065, %1066), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1068 : Float(64:64, 64:1, 4096:4096) = aten::to(%1067, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.10 : Float(4096:1) = aten::to(%1009, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1070 : Tensor[] = prim::ListConstruct(%1064, %1068), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1071 : Float(119:53248, 13:4096, 4096:1) = aten::einsum(%31, %1070), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.72 : Float(119:53248, 13:4096, 4096:1) = aten::add(%1071, %b.10, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.10 : Float(119:53248, 13:4096, 4096:1) = aten::dropout(%input.72, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(119:53248, 13:4096, 4096:1) = aten::add(%1008, %projected_context_layer.10, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1075 : Tensor = prim::GetAttr[name="bias"](%1016)
  %1076 : Tensor = prim::GetAttr[name="weight"](%1016)
  %1077 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.10 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.73, %1077, %1076, %1075, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1079 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.10, %b.10)
  %1080 : Float(119:53248, 13:4096, 4096:1), %1081 : Float(4096:1) = prim::TupleUnpack(%1079)
  %1082 : Tensor = prim::GetAttr[name="bias"](%1014)
  %1083 : Tensor = prim::GetAttr[name="weight"](%1014)
  %1084 : Float(4096:1, 16384:4096) = aten::t(%1083), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.50 : Float(119:212992, 13:16384, 16384:1) = aten::matmul(%1080, %1084), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.70 : Float(119:212992, 13:16384, 16384:1) = aten::add_(%output.50, %1082, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1087 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%x.70, %37), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1088 : Float(119:212992, 13:16384, 16384:1) = aten::pow(%x.70, %38), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1089 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%1088, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1090 : Float(119:212992, 13:16384, 16384:1) = aten::add(%x.70, %1089, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1091 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%1090, %40), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1092 : Float(119:212992, 13:16384, 16384:1) = aten::tanh(%1091), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1093 : Float(119:212992, 13:16384, 16384:1) = aten::add(%1092, %41, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.74 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%1087, %1093), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1095 : Tensor = prim::GetAttr[name="bias"](%1013)
  %1096 : Tensor = prim::GetAttr[name="weight"](%1013)
  %1097 : Float(16384:1, 4096:16384) = aten::t(%1096), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.51 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.74, %1097), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.10 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.51, %1095, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.75 : Float(119:53248, 13:4096, 4096:1) = aten::add(%ffn_output.10, %1080, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1101 : Tensor = prim::GetAttr[name="bias"](%1012)
  %1102 : Tensor = prim::GetAttr[name="weight"](%1012)
  %1103 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.76 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.75, %1103, %1102, %1101, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1105 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.76, %1081)
  %1106 : Float(119:53248, 13:4096, 4096:1), %1107 : Float(4096:1) = prim::TupleUnpack(%1105)
  %1108 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1106, %1107)
  %1109 : Float(119:53248, 13:4096, 4096:1), %1110 : Float(4096:1) = prim::TupleUnpack(%1108)
  %1111 : __torch__.torch.nn.modules.container.___torch_mangle_67.ModuleList = prim::GetAttr[name="albert_layers"](%92)
  %1112 : __torch__.transformers.modeling_albert.___torch_mangle_66.AlbertLayer = prim::GetAttr[name="0"](%1111)
  %1113 : __torch__.torch.nn.modules.normalization.___torch_mangle_54.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1112)
  %1114 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="ffn_output"](%1112)
  %1115 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name="ffn"](%1112)
  %1116 : __torch__.transformers.modeling_albert.___torch_mangle_62.AlbertAttention = prim::GetAttr[name="attention"](%1112)
  %1117 : __torch__.torch.nn.modules.normalization.___torch_mangle_61.LayerNorm = prim::GetAttr[name="LayerNorm"](%1116)
  %1118 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%1116)
  %1119 : Tensor = prim::GetAttr[name="weight"](%1118)
  %1120 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name="value"](%1116)
  %1121 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name="key"](%1116)
  %1122 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name="query"](%1116)
  %1123 : Tensor = prim::GetAttr[name="bias"](%1122)
  %1124 : Tensor = prim::GetAttr[name="weight"](%1122)
  %1125 : Float(4096:1, 4096:4096) = aten::t(%1124), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.52 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%1109, %1125), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.71 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.52, %1123, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1128 : Tensor = prim::GetAttr[name="bias"](%1121)
  %1129 : Tensor = prim::GetAttr[name="weight"](%1121)
  %1130 : Float(4096:1, 4096:4096) = aten::t(%1129), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.53 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%1109, %1130), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.73 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.53, %1128, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1133 : Tensor = prim::GetAttr[name="bias"](%1120)
  %1134 : Tensor = prim::GetAttr[name="weight"](%1120)
  %1135 : Float(4096:1, 4096:4096) = aten::t(%1134), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.54 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%1109, %1135), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.75 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.54, %1133, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1138 : int = aten::size(%x.71, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1139 : int = aten::size(%x.71, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1140 : int[] = prim::ListConstruct(%1138, %1139, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.72 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.71, %1140), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1142 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.11 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.72, %1142), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1144 : int = aten::size(%x.73, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1145 : int = aten::size(%x.73, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1146 : int[] = prim::ListConstruct(%1144, %1145, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.74 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.73, %1146), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1148 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.11 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.74, %1148), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1150 : int = aten::size(%x.75, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1151 : int = aten::size(%x.75, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1152 : int[] = prim::ListConstruct(%1150, %1151, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.76 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.75, %1152), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1154 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.11 : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.76, %1154), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1156 : Float(119:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.11, %43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.21 : Float(119:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1156), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.22 : Float(119:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.21, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.77 : Float(119:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.78 : Float(119:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.77, %43, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(119:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.78, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(119:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1163 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1164 : Float(119:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.11, %1163), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1165 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1164, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1166 : Float(4096:1, 4096:4096) = aten::t(%1119), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1167 : int[] = prim::ListConstruct(%36, %36, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1168 : Float(64:64, 64:1, 4096:4096) = aten::view(%1166, %1167), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1169 : Float(64:64, 64:1, 4096:4096) = aten::to(%1168, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.11 : Float(4096:1) = aten::to(%1110, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1171 : Tensor[] = prim::ListConstruct(%1165, %1169), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1172 : Float(119:53248, 13:4096, 4096:1) = aten::einsum(%31, %1171), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.79 : Float(119:53248, 13:4096, 4096:1) = aten::add(%1172, %b.11, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.11 : Float(119:53248, 13:4096, 4096:1) = aten::dropout(%input.79, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.80 : Float(119:53248, 13:4096, 4096:1) = aten::add(%1109, %projected_context_layer.11, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1176 : Tensor = prim::GetAttr[name="bias"](%1117)
  %1177 : Tensor = prim::GetAttr[name="weight"](%1117)
  %1178 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.11 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.80, %1178, %1177, %1176, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1180 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.11, %b.11)
  %1181 : Float(119:53248, 13:4096, 4096:1), %1182 : Float(4096:1) = prim::TupleUnpack(%1180)
  %1183 : Tensor = prim::GetAttr[name="bias"](%1115)
  %1184 : Tensor = prim::GetAttr[name="weight"](%1115)
  %1185 : Float(4096:1, 16384:4096) = aten::t(%1184), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.55 : Float(119:212992, 13:16384, 16384:1) = aten::matmul(%1181, %1185), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.77 : Float(119:212992, 13:16384, 16384:1) = aten::add_(%output.55, %1183, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1188 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%x.77, %37), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1189 : Float(119:212992, 13:16384, 16384:1) = aten::pow(%x.77, %38), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1190 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%1189, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1191 : Float(119:212992, 13:16384, 16384:1) = aten::add(%x.77, %1190, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1192 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%1191, %40), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1193 : Float(119:212992, 13:16384, 16384:1) = aten::tanh(%1192), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1194 : Float(119:212992, 13:16384, 16384:1) = aten::add(%1193, %41, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.81 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%1188, %1194), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1196 : Tensor = prim::GetAttr[name="bias"](%1114)
  %1197 : Tensor = prim::GetAttr[name="weight"](%1114)
  %1198 : Float(16384:1, 4096:16384) = aten::t(%1197), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.56 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.81, %1198), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.11 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.56, %1196, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.82 : Float(119:53248, 13:4096, 4096:1) = aten::add(%ffn_output.11, %1181, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1202 : Tensor = prim::GetAttr[name="bias"](%1113)
  %1203 : Tensor = prim::GetAttr[name="weight"](%1113)
  %1204 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.83 : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.82, %1204, %1203, %1202, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1206 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.83, %1182)
  %1207 : Float(119:53248, 13:4096, 4096:1), %1208 : Float(4096:1) = prim::TupleUnpack(%1206)
  %1209 : (Float(119:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1207, %1208)
  %1210 : Float(119:53248, 13:4096, 4096:1), %1211 : Float(4096:1) = prim::TupleUnpack(%1209)
  %1212 : __torch__.torch.nn.modules.container.___torch_mangle_67.ModuleList = prim::GetAttr[name="albert_layers"](%92)
  %1213 : __torch__.transformers.modeling_albert.___torch_mangle_66.AlbertLayer = prim::GetAttr[name="0"](%1212)
  %1214 : __torch__.torch.nn.modules.normalization.___torch_mangle_54.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1213)
  %1215 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="ffn_output"](%1213)
  %1216 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name="ffn"](%1213)
  %1217 : __torch__.transformers.modeling_albert.___torch_mangle_62.AlbertAttention = prim::GetAttr[name="attention"](%1213)
  %1218 : __torch__.torch.nn.modules.normalization.___torch_mangle_61.LayerNorm = prim::GetAttr[name="LayerNorm"](%1217)
  %1219 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="dense"](%1217)
  %1220 : Tensor = prim::GetAttr[name="weight"](%1219)
  %1221 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name="value"](%1217)
  %1222 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name="key"](%1217)
  %1223 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name="query"](%1217)
  %1224 : Tensor = prim::GetAttr[name="bias"](%1223)
  %1225 : Tensor = prim::GetAttr[name="weight"](%1223)
  %1226 : Float(4096:1, 4096:4096) = aten::t(%1225), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.57 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%1210, %1226), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.78 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.57, %1224, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1229 : Tensor = prim::GetAttr[name="bias"](%1222)
  %1230 : Tensor = prim::GetAttr[name="weight"](%1222)
  %1231 : Float(4096:1, 4096:4096) = aten::t(%1230), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.58 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%1210, %1231), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.80 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.58, %1229, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1234 : Tensor = prim::GetAttr[name="bias"](%1221)
  %1235 : Tensor = prim::GetAttr[name="weight"](%1221)
  %1236 : Float(4096:1, 4096:4096) = aten::t(%1235), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.59 : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%1210, %1236), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.82 : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output.59, %1234, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1239 : int = aten::size(%x.78, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1240 : int = aten::size(%x.78, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1241 : int[] = prim::ListConstruct(%1239, %1240, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.79 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.78, %1241), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1243 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.79, %1243), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1245 : int = aten::size(%x.80, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1246 : int = aten::size(%x.80, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1247 : int[] = prim::ListConstruct(%1245, %1246, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.81 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.80, %1247), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1249 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.81, %1249), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1251 : int = aten::size(%x.82, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1252 : int = aten::size(%x.82, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1253 : int[] = prim::ListConstruct(%1251, %1252, %36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.83 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::view(%x.82, %1253), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1255 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer : Float(119:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.83, %1255), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1257 : Float(119:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer, %43, %34), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.23 : Float(119:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer, %1257), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores : Float(119:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.23, %33), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.84 : Float(119:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.85 : Float(119:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.84, %43, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs : Float(119:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.85, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer : Float(119:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1264 : int[] = prim::ListConstruct(%57, %52, %56, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1265 : Float(119:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer, %1264), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1266 : Float(119:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1265, %57), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1267 : Float(4096:1, 4096:4096) = aten::t(%1220), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1268 : int[] = prim::ListConstruct(%36, %36, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1269 : Float(64:64, 64:1, 4096:4096) = aten::view(%1267, %1268), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1270 : Float(64:64, 64:1, 4096:4096) = aten::to(%1269, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b : Float(4096:1) = aten::to(%1211, %51, %53, %53, %50), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1272 : Tensor[] = prim::ListConstruct(%1266, %1270), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1273 : Float(119:53248, 13:4096, 4096:1) = aten::einsum(%31, %1272), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.86 : Float(119:53248, 13:4096, 4096:1) = aten::add(%1273, %b, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer : Float(119:53248, 13:4096, 4096:1) = aten::dropout(%input.86, %47, %53), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.87 : Float(119:53248, 13:4096, 4096:1) = aten::add(%1210, %projected_context_layer, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1277 : Tensor = prim::GetAttr[name="bias"](%1218)
  %1278 : Tensor = prim::GetAttr[name="weight"](%1218)
  %1279 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.87, %1279, %1278, %1277, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1281 : Tensor = prim::GetAttr[name="bias"](%1216)
  %1282 : Tensor = prim::GetAttr[name="weight"](%1216)
  %1283 : Float(4096:1, 16384:4096) = aten::t(%1282), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.60 : Float(119:212992, 13:16384, 16384:1) = aten::matmul(%input_tensor, %1283), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x : Float(119:212992, 13:16384, 16384:1) = aten::add_(%output.60, %1281, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1286 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%x, %37), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1287 : Float(119:212992, 13:16384, 16384:1) = aten::pow(%x, %38), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1288 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%1287, %39), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1289 : Float(119:212992, 13:16384, 16384:1) = aten::add(%x, %1288, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1290 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%1289, %40), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1291 : Float(119:212992, 13:16384, 16384:1) = aten::tanh(%1290), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1292 : Float(119:212992, 13:16384, 16384:1) = aten::add(%1291, %41, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.88 : Float(119:212992, 13:16384, 16384:1) = aten::mul(%1286, %1292), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1294 : Tensor = prim::GetAttr[name="bias"](%1215)
  %1295 : Tensor = prim::GetAttr[name="weight"](%1215)
  %1296 : Float(16384:1, 4096:16384) = aten::t(%1295), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output : Float(119:53248, 13:4096, 4096:1) = aten::matmul(%input.88, %1296), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output : Float(119:53248, 13:4096, 4096:1) = aten::add_(%output, %1294, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.89 : Float(119:53248, 13:4096, 4096:1) = aten::add(%ffn_output, %input_tensor, %56), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1300 : Tensor = prim::GetAttr[name="bias"](%1214)
  %1301 : Tensor = prim::GetAttr[name="weight"](%1214)
  %1302 : int[] = prim::ListConstruct(%32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %sequence_output : Float(119:53248, 13:4096, 4096:1) = aten::layer_norm(%input.89, %1302, %1301, %1300, %45, %44), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1304 : Float(119:53248, 13:4096, 4096:1) = aten::slice(%sequence_output, %57, %57, %42, %56), scope: __module.albert # transformers/modeling_albert.py:695:0
  %input.90 : Float(119:53248, 4096:1) = aten::select(%1304, %56, %57), scope: __module.albert # transformers/modeling_albert.py:695:0
  %1306 : Tensor = prim::GetAttr[name="bias"](%58)
  %1307 : Tensor = prim::GetAttr[name="weight"](%58)
  %1308 : Float(4096:1, 4096:4096) = aten::t(%1307), scope: __module.albert/__module.albert.pooler # torch/nn/functional.py:1674:0
  %input.91 : Float(119:4096, 4096:1) = aten::addmm(%1306, %input.90, %1308, %56, %56), scope: __module.albert/__module.albert.pooler # torch/nn/functional.py:1674:0
  %input.92 : Float(119:4096, 4096:1) = aten::tanh(%input.91), scope: __module.albert/__module.albert.pooler_activation # torch/nn/modules/activation.py:350:0
  %1311 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %1312 : float = prim::Constant[value=0.](), scope: __module.dropout # torch/nn/functional.py:973:0
  %input : Float(119:4096, 4096:1) = aten::dropout(%input.92, %1312, %1311), scope: __module.dropout # torch/nn/functional.py:973:0
  %1314 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1674:0
  %1315 : Tensor = prim::GetAttr[name="bias"](%3)
  %1316 : Tensor = prim::GetAttr[name="weight"](%3)
  %1317 : Float(4096:1, 1:4096) = aten::t(%1316), scope: __module.classifier # torch/nn/functional.py:1674:0
  %logits : Float(119:1, 1:1) = aten::addmm(%1315, %input, %1317, %1314, %1314), scope: __module.classifier # torch/nn/functional.py:1674:0
  %27 : int = prim::Constant[value=-1]() # transformers/modeling_albert.py:1294:0
  %28 : int[] = prim::ListConstruct(%27, %9)
  %29 : Float(17:7, 7:1) = aten::view(%logits, %28) # transformers/modeling_albert.py:1294:0
  %30 : (Float(17:7, 7:1)) = prim::TupleConstruct(%29)
  return (%30)
