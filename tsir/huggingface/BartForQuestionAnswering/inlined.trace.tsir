graph(%self.1 : __torch__.transformers.modeling_bart.BartForQuestionAnswering,
      %input_ids : Long(17:13, 13:1),
      %attention_mask : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_1360.Linear = prim::GetAttr[name="qa_outputs"](%self.1)
  %4 : __torch__.transformers.modeling_bart.___torch_mangle_1359.BartModel = prim::GetAttr[name="model"](%self.1)
  %19 : Double() = prim::Constant[value={1}](), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:334:0
  %20 : Long() = prim::Constant[value={2}](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:822:0
  %21 : int = prim::Constant[value=4](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:821:0
  %22 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %23 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %24 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %25 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder # torch/nn/functional.py:973:0
  %26 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %27 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
  %28 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %29 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %30 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:718:0
  %31 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # torch/nn/functional.py:973:0
  %32 : float = prim::Constant[value=-inf](), scope: __module.model # transformers/modeling_bart.py:838:0
  %33 : Device = prim::Constant[value="cpu"](), scope: __module.model # transformers/modeling_bart.py:157:0
  %34 : int = prim::Constant[value=6](), scope: __module.model # transformers/modeling_bart.py:157:0
  %35 : int = prim::Constant[value=12](), scope: __module.model # transformers/modeling_bart.py:210:0
  %36 : int = prim::Constant[value=17](), scope: __module.model # transformers/modeling_bart.py:209:0
  %37 : int = prim::Constant[value=9223372036854775807](), scope: __module.model # transformers/modeling_bart.py:209:0
  %38 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:209:0
  %39 : int = prim::Constant[value=-1](), scope: __module.model # transformers/modeling_bart.py:208:0
  %40 : Long() = prim::Constant[value={1}](), scope: __module.model # transformers/modeling_bart.py:208:0
  %41 : bool = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:208:0
  %42 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:208:0
  %43 : None = prim::Constant(), scope: __module.model
  %44 : __torch__.transformers.modeling_bart.___torch_mangle_1358.BartDecoder = prim::GetAttr[name="decoder"](%4)
  %45 : __torch__.torch.nn.modules.sparse.___torch_mangle_1036.Embedding = prim::GetAttr[name="shared"](%4)
  %46 : Tensor = prim::GetAttr[name="weight"](%45)
  %47 : __torch__.transformers.modeling_bart.___torch_mangle_1161.BartEncoder = prim::GetAttr[name="encoder"](%4)
  %prev_output_tokens : Long(17:13, 13:1) = aten::clone(%input_ids, %43), scope: __module.model # transformers/modeling_bart.py:207:0
  %49 : Bool(17:13, 13:1) = aten::ne(%input_ids, %42), scope: __module.model # transformers/modeling_bart.py:208:0
  %50 : int[] = prim::ListConstruct(%42), scope: __module.model
  %51 : Long(17:1) = aten::sum(%49, %50, %41, %43), scope: __module.model # transformers/modeling_bart.py:208:0
  %52 : Long(17:1) = aten::sub(%51, %40, %42), scope: __module.model # transformers/modeling_bart.py:208:0
  %index_of_eos : Long(17:1, 1:1) = aten::unsqueeze(%52, %39), scope: __module.model # transformers/modeling_bart.py:208:0
  %54 : Long(17:1, 1:1) = aten::gather(%input_ids, %42, %index_of_eos, %41), scope: __module.model # transformers/modeling_bart.py:209:0
  %55 : Long(17:1) = aten::squeeze(%54), scope: __module.model # transformers/modeling_bart.py:209:0
  %56 : Long(17:13, 13:1) = aten::slice(%prev_output_tokens, %38, %38, %37, %42), scope: __module.model # transformers/modeling_bart.py:209:0
  %57 : Long(17:13) = aten::select(%56, %42, %38), scope: __module.model # transformers/modeling_bart.py:209:0
  %58 : int[] = prim::ListConstruct(%36), scope: __module.model
  %59 : Long(17:1) = aten::view(%55, %58), scope: __module.model # transformers/modeling_bart.py:209:0
  %60 : Long(17:13) = aten::copy_(%57, %59, %41), scope: __module.model # transformers/modeling_bart.py:209:0
  %61 : Long(17:13, 13:1) = aten::slice(%input_ids, %38, %38, %37, %42), scope: __module.model # transformers/modeling_bart.py:210:0
  %62 : Long(17:13, 12:1) = aten::slice(%61, %42, %38, %39, %42), scope: __module.model # transformers/modeling_bart.py:210:0
  %63 : Long(17:13, 13:1) = aten::slice(%prev_output_tokens, %38, %38, %37, %42), scope: __module.model # transformers/modeling_bart.py:210:0
  %64 : Long(17:13, 12:1) = aten::slice(%63, %42, %42, %37, %42), scope: __module.model # transformers/modeling_bart.py:210:0
  %65 : int[] = prim::ListConstruct(%36, %35), scope: __module.model
  %66 : Long(17:13, 12:1) = aten::view(%62, %65), scope: __module.model # transformers/modeling_bart.py:210:0
  %67 : Long(17:13, 12:1) = aten::copy_(%64, %66, %41), scope: __module.model # transformers/modeling_bart.py:210:0
  %68 : int = aten::size(%prev_output_tokens, %42), scope: __module.model # transformers/modeling_bart.py:149:0
  %69 : int[] = prim::ListConstruct(%68, %68), scope: __module.model
  %t.1 : Float(13:13, 13:1) = aten::zeros(%69, %34, %38, %33, %41), scope: __module.model # transformers/modeling_bart.py:157:0
  %t.2 : Float(13:13, 13:1) = aten::to(%t.1, %34, %41, %41, %43), scope: __module.model # transformers/modeling_bart.py:838:0
  %t : Float(13:13, 13:1) = aten::fill_(%t.2, %32), scope: __module.model # transformers/modeling_bart.py:838:0
  %73 : int = aten::size(%t, %39), scope: __module.model # transformers/modeling_bart.py:158:0
  %mask : Long(13:1) = aten::arange(%73, %43, %38, %33, %41), scope: __module.model # transformers/modeling_bart.py:158:0
  %75 : Long(13:1) = aten::add(%mask, %40, %42), scope: __module.model # transformers/modeling_bart.py:159:0
  %76 : int = aten::size(%t, %39), scope: __module.model # transformers/modeling_bart.py:159:0
  %77 : int[] = prim::ListConstruct(%76, %42), scope: __module.model
  %78 : Long(13:1, 1:1) = aten::view(%75, %77), scope: __module.model # transformers/modeling_bart.py:159:0
  %79 : Bool(13:13, 13:1) = aten::lt(%mask, %78), scope: __module.model # torch/tensor.py:22:0
  %tmp : Float(13:13, 13:1) = aten::masked_fill_(%t, %79, %38), scope: __module.model # transformers/modeling_bart.py:159:0
  %attn_mask : Float(13:13, 13:1) = aten::to(%tmp, %33, %34, %41, %41, %43), scope: __module.model # transformers/modeling_bart.py:160:0
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="layers"](%47)
  %83 : __torch__.transformers.modeling_bart.___torch_mangle_1158.EncoderLayer = prim::GetAttr[name="11"](%82)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="layers"](%47)
  %85 : __torch__.transformers.modeling_bart.___torch_mangle_1148.EncoderLayer = prim::GetAttr[name="10"](%84)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="layers"](%47)
  %87 : __torch__.transformers.modeling_bart.___torch_mangle_1138.EncoderLayer = prim::GetAttr[name="9"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="layers"](%47)
  %89 : __torch__.transformers.modeling_bart.___torch_mangle_1128.EncoderLayer = prim::GetAttr[name="8"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="layers"](%47)
  %91 : __torch__.transformers.modeling_bart.___torch_mangle_1118.EncoderLayer = prim::GetAttr[name="7"](%90)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="layers"](%47)
  %93 : __torch__.transformers.modeling_bart.___torch_mangle_1108.EncoderLayer = prim::GetAttr[name="6"](%92)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="layers"](%47)
  %95 : __torch__.transformers.modeling_bart.___torch_mangle_1098.EncoderLayer = prim::GetAttr[name="5"](%94)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="layers"](%47)
  %97 : __torch__.transformers.modeling_bart.___torch_mangle_1088.EncoderLayer = prim::GetAttr[name="4"](%96)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="layers"](%47)
  %99 : __torch__.transformers.modeling_bart.___torch_mangle_1078.EncoderLayer = prim::GetAttr[name="3"](%98)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="layers"](%47)
  %101 : __torch__.transformers.modeling_bart.___torch_mangle_1068.EncoderLayer = prim::GetAttr[name="2"](%100)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="layers"](%47)
  %103 : __torch__.transformers.modeling_bart.___torch_mangle_1058.EncoderLayer = prim::GetAttr[name="1"](%102)
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="layers"](%47)
  %105 : __torch__.transformers.modeling_bart.___torch_mangle_1048.EncoderLayer = prim::GetAttr[name="0"](%104)
  %106 : __torch__.torch.nn.modules.normalization.___torch_mangle_1160.LayerNorm = prim::GetAttr[name="layernorm_embedding"](%47)
  %107 : __torch__.transformers.modeling_bart.___torch_mangle_1038.LearnedPositionalEmbedding = prim::GetAttr[name="embed_positions"](%47)
  %key_padding_mask.1 : Bool(17:13, 13:1) = aten::eq(%attention_mask, %38), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:136:0
  %109 : Float(17:13312, 13:1024, 1024:1) = aten::embedding(%46, %input_ids, %42, %41, %41), scope: __module.model/__module.model.encoder/__module.model.decoder.embed_tokens # torch/nn/functional.py:1814:0
  %inputs_embeds : Float(17:13312, 13:1024, 1024:1) = aten::mul(%109, %19), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:334:0
  %111 : Tensor = prim::GetAttr[name="weight"](%107)
  %112 : int = aten::size(%input_ids, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:816:0
  %positions.1 : Long(13:1) = aten::arange(%112, %21, %38, %33, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:821:0
  %input.1 : Long(13:1) = aten::add(%positions.1, %20, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:822:0
  %embed_pos : Float(13:1024, 1024:1) = aten::embedding(%111, %input.1, %42, %41, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # torch/nn/functional.py:1814:0
  %input.2 : Float(17:13312, 13:1024, 1024:1) = aten::add(%inputs_embeds, %embed_pos, %42), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:336:0
  %117 : Tensor = prim::GetAttr[name="bias"](%106)
  %118 : Tensor = prim::GetAttr[name="weight"](%106)
  %119 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding
  %input.3 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.2, %119, %118, %117, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %x.1 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.3, %25, %41), scope: __module.model/__module.model.encoder # torch/nn/functional.py:973:0
  %query.1 : Float(13:1024, 17:13312, 1024:1) = aten::transpose(%x.1, %38, %42), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:341:0
  %123 : __torch__.torch.nn.modules.normalization.___torch_mangle_1047.LayerNorm = prim::GetAttr[name="final_layer_norm"](%105)
  %124 : __torch__.torch.nn.modules.linear.___torch_mangle_1046.Linear = prim::GetAttr[name="fc2"](%105)
  %125 : __torch__.torch.nn.modules.linear.___torch_mangle_1045.Linear = prim::GetAttr[name="fc1"](%105)
  %126 : __torch__.torch.nn.modules.normalization.___torch_mangle_1044.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%105)
  %127 : __torch__.transformers.modeling_bart.___torch_mangle_1043.Attention = prim::GetAttr[name="self_attn"](%105)
  %128 : __torch__.torch.nn.modules.linear.___torch_mangle_1042.Linear = prim::GetAttr[name="out_proj"](%127)
  %129 : __torch__.torch.nn.modules.linear.___torch_mangle_1040.Linear = prim::GetAttr[name="v_proj"](%127)
  %130 : __torch__.torch.nn.modules.linear.___torch_mangle_1039.Linear = prim::GetAttr[name="k_proj"](%127)
  %131 : __torch__.torch.nn.modules.linear.___torch_mangle_1041.Linear = prim::GetAttr[name="q_proj"](%127)
  %132 : int = aten::size(%query.1, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %133 : int = aten::size(%query.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %bsz.1 : Long() = prim::NumToTensor(%133), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %135 : int = aten::size(%query.1, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %136 : Tensor = prim::GetAttr[name="bias"](%131)
  %137 : Tensor = prim::GetAttr[name="weight"](%131)
  %138 : Float(1024:1, 1024:1024) = aten::t(%137), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.1 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.1, %138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
  %140 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.1, %136, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.1 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%140, %27), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
  %142 : Tensor = prim::GetAttr[name="bias"](%130)
  %143 : Tensor = prim::GetAttr[name="weight"](%130)
  %144 : Float(1024:1, 1024:1024) = aten::t(%143), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.2 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.1, %144), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.3 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.2, %142, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1678:0
  %147 : Tensor = prim::GetAttr[name="bias"](%129)
  %148 : Tensor = prim::GetAttr[name="weight"](%129)
  %149 : Float(1024:1, 1024:1024) = aten::t(%148), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.3 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.1, %149), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.5 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.3, %147, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.2 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.1, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %153 : Long() = aten::mul(%bsz.1, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %154 : int = aten::Int(%153), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %155 : int[] = prim::ListConstruct(%132, %154, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %156 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.2, %155), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %q.1 : Float(272:64, 13:17408, 64:1) = aten::transpose(%156, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %tensor.4 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.3, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %159 : Long() = aten::mul(%bsz.1, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %160 : int = aten::Int(%159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %161 : int[] = prim::ListConstruct(%39, %160, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %162 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.4, %161), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %k.1 : Float(272:64, 13:17408, 64:1) = aten::transpose(%162, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %tensor.6 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.5, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %165 : Long() = aten::mul(%bsz.1, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %166 : int = aten::Int(%165), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %167 : int[] = prim::ListConstruct(%39, %166, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %168 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.6, %167), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %v.1 : Float(272:64, 13:17408, 64:1) = aten::transpose(%168, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %170 : int = aten::size(%k.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:701:0
  %171 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.1, %42, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.1 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.1, %171), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
  %173 : int[] = prim::ListConstruct(%133, %30, %132, %170), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %attn_weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.1, %173), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:718:0
  %175 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.1 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%175, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.2, %reshaped.1, %32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:720:0
  %178 : Long() = aten::mul(%bsz.1, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:721:0
  %179 : int = aten::Int(%178), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %180 : int[] = prim::ListConstruct(%179, %132, %170), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %input.4 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.3, %180), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:721:0
  %input.5 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.4, %39, %43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.4 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.5, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # torch/nn/functional.py:973:0
  %attn_output.1 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.4, %v.1), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:730:0
  %185 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.1, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %186 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%185, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %187 : int[] = prim::ListConstruct(%132, %133, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %input.6 : Float(13:17408, 17:1024, 1024:1) = aten::view(%186, %187), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %189 : Tensor = prim::GetAttr[name="bias"](%128)
  %190 : Tensor = prim::GetAttr[name="weight"](%128)
  %191 : Float(1024:1, 1024:1024) = aten::t(%190), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.4 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.6, %191), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.4, %189, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.2 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.7, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
  %input.8 : Float(13:1024, 17:13312, 1024:1) = aten::add(%query.1, %x.2, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # transformers/modeling_bart.py:258:0
  %196 : Tensor = prim::GetAttr[name="bias"](%126)
  %197 : Tensor = prim::GetAttr[name="weight"](%126)
  %198 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn_layer_norm
  %input.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.8, %198, %197, %196, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %200 : Tensor = prim::GetAttr[name="bias"](%125)
  %201 : Tensor = prim::GetAttr[name="weight"](%125)
  %202 : Float(1024:1, 4096:1024) = aten::t(%201), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc1 # torch/nn/functional.py:1676:0
  %output.5 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.9, %202), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc1 # torch/nn/functional.py:1676:0
  %input.10 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.5, %200, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc1 # torch/nn/functional.py:1678:0
  %input.11 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:1369:0
  %input.12 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.11, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
  %207 : Tensor = prim::GetAttr[name="bias"](%124)
  %208 : Tensor = prim::GetAttr[name="weight"](%124)
  %209 : Float(4096:1, 1024:4096) = aten::t(%208), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc2 # torch/nn/functional.py:1676:0
  %output.6 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.12, %209), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc2 # torch/nn/functional.py:1676:0
  %input.13 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.6, %207, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc2 # torch/nn/functional.py:1678:0
  %x.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.13, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
  %input.14 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.9, %x.3, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # transformers/modeling_bart.py:269:0
  %214 : Tensor = prim::GetAttr[name="bias"](%123)
  %215 : Tensor = prim::GetAttr[name="weight"](%123)
  %216 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.final_layer_norm
  %query.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.14, %216, %215, %214, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
  %218 : __torch__.torch.nn.modules.normalization.___torch_mangle_1057.LayerNorm = prim::GetAttr[name="final_layer_norm"](%103)
  %219 : __torch__.torch.nn.modules.linear.___torch_mangle_1056.Linear = prim::GetAttr[name="fc2"](%103)
  %220 : __torch__.torch.nn.modules.linear.___torch_mangle_1055.Linear = prim::GetAttr[name="fc1"](%103)
  %221 : __torch__.torch.nn.modules.normalization.___torch_mangle_1054.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%103)
  %222 : __torch__.transformers.modeling_bart.___torch_mangle_1053.Attention = prim::GetAttr[name="self_attn"](%103)
  %223 : __torch__.torch.nn.modules.linear.___torch_mangle_1052.Linear = prim::GetAttr[name="out_proj"](%222)
  %224 : __torch__.torch.nn.modules.linear.___torch_mangle_1050.Linear = prim::GetAttr[name="v_proj"](%222)
  %225 : __torch__.torch.nn.modules.linear.___torch_mangle_1049.Linear = prim::GetAttr[name="k_proj"](%222)
  %226 : __torch__.torch.nn.modules.linear.___torch_mangle_1051.Linear = prim::GetAttr[name="q_proj"](%222)
  %227 : int = aten::size(%query.2, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %228 : int = aten::size(%query.2, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %bsz.2 : Long() = prim::NumToTensor(%228), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %230 : int = aten::size(%query.2, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %231 : Tensor = prim::GetAttr[name="bias"](%226)
  %232 : Tensor = prim::GetAttr[name="weight"](%226)
  %233 : Float(1024:1, 1024:1024) = aten::t(%232), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.7 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.2, %233), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
  %235 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.7, %231, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.7 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%235, %27), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:673:0
  %237 : Tensor = prim::GetAttr[name="bias"](%225)
  %238 : Tensor = prim::GetAttr[name="weight"](%225)
  %239 : Float(1024:1, 1024:1024) = aten::t(%238), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.8 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.2, %239), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.9 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.8, %237, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1678:0
  %242 : Tensor = prim::GetAttr[name="bias"](%224)
  %243 : Tensor = prim::GetAttr[name="weight"](%224)
  %244 : Float(1024:1, 1024:1024) = aten::t(%243), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.9 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.2, %244), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.11 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.9, %242, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.8 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.7, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %248 : Long() = aten::mul(%bsz.2, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %249 : int = aten::Int(%248), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %250 : int[] = prim::ListConstruct(%227, %249, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %251 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.8, %250), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %q.2 : Float(272:64, 13:17408, 64:1) = aten::transpose(%251, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %tensor.10 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.9, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %254 : Long() = aten::mul(%bsz.2, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %255 : int = aten::Int(%254), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %256 : int[] = prim::ListConstruct(%39, %255, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %257 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.10, %256), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %k.2 : Float(272:64, 13:17408, 64:1) = aten::transpose(%257, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %tensor.12 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.11, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %260 : Long() = aten::mul(%bsz.2, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %261 : int = aten::Int(%260), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %262 : int[] = prim::ListConstruct(%39, %261, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %263 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.12, %262), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %v.2 : Float(272:64, 13:17408, 64:1) = aten::transpose(%263, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %265 : int = aten::size(%k.2, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:701:0
  %266 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.2, %42, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.5 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.2, %266), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
  %268 : int[] = prim::ListConstruct(%228, %30, %227, %265), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %attn_weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.5, %268), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:718:0
  %270 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.2 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%270, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.6, %reshaped.2, %32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:720:0
  %273 : Long() = aten::mul(%bsz.2, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:721:0
  %274 : int = aten::Int(%273), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %275 : int[] = prim::ListConstruct(%274, %227, %265), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %input.15 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.7, %275), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:721:0
  %input.16 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.15, %39, %43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.8 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.16, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # torch/nn/functional.py:973:0
  %attn_output.2 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.8, %v.2), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:730:0
  %280 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.2, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %281 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%280, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %282 : int[] = prim::ListConstruct(%227, %228, %230), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %input.17 : Float(13:17408, 17:1024, 1024:1) = aten::view(%281, %282), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %284 : Tensor = prim::GetAttr[name="bias"](%223)
  %285 : Tensor = prim::GetAttr[name="weight"](%223)
  %286 : Float(1024:1, 1024:1024) = aten::t(%285), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.10 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.17, %286), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.18 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.10, %284, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.4 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.18, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
  %input.19 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.2, %x.4, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # transformers/modeling_bart.py:258:0
  %291 : Tensor = prim::GetAttr[name="bias"](%221)
  %292 : Tensor = prim::GetAttr[name="weight"](%221)
  %293 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn_layer_norm
  %input.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.19, %293, %292, %291, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %295 : Tensor = prim::GetAttr[name="bias"](%220)
  %296 : Tensor = prim::GetAttr[name="weight"](%220)
  %297 : Float(1024:1, 4096:1024) = aten::t(%296), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc1 # torch/nn/functional.py:1676:0
  %output.11 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.20, %297), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc1 # torch/nn/functional.py:1676:0
  %input.21 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.11, %295, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc1 # torch/nn/functional.py:1678:0
  %input.22 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:1369:0
  %input.23 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.22, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
  %302 : Tensor = prim::GetAttr[name="bias"](%219)
  %303 : Tensor = prim::GetAttr[name="weight"](%219)
  %304 : Float(4096:1, 1024:4096) = aten::t(%303), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc2 # torch/nn/functional.py:1676:0
  %output.12 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.23, %304), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc2 # torch/nn/functional.py:1676:0
  %input.24 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.12, %302, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc2 # torch/nn/functional.py:1678:0
  %x.5 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.24, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
  %input.25 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.20, %x.5, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # transformers/modeling_bart.py:269:0
  %309 : Tensor = prim::GetAttr[name="bias"](%218)
  %310 : Tensor = prim::GetAttr[name="weight"](%218)
  %311 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.final_layer_norm
  %query.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.25, %311, %310, %309, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
  %313 : __torch__.torch.nn.modules.normalization.___torch_mangle_1067.LayerNorm = prim::GetAttr[name="final_layer_norm"](%101)
  %314 : __torch__.torch.nn.modules.linear.___torch_mangle_1066.Linear = prim::GetAttr[name="fc2"](%101)
  %315 : __torch__.torch.nn.modules.linear.___torch_mangle_1065.Linear = prim::GetAttr[name="fc1"](%101)
  %316 : __torch__.torch.nn.modules.normalization.___torch_mangle_1064.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%101)
  %317 : __torch__.transformers.modeling_bart.___torch_mangle_1063.Attention = prim::GetAttr[name="self_attn"](%101)
  %318 : __torch__.torch.nn.modules.linear.___torch_mangle_1062.Linear = prim::GetAttr[name="out_proj"](%317)
  %319 : __torch__.torch.nn.modules.linear.___torch_mangle_1060.Linear = prim::GetAttr[name="v_proj"](%317)
  %320 : __torch__.torch.nn.modules.linear.___torch_mangle_1059.Linear = prim::GetAttr[name="k_proj"](%317)
  %321 : __torch__.torch.nn.modules.linear.___torch_mangle_1061.Linear = prim::GetAttr[name="q_proj"](%317)
  %322 : int = aten::size(%query.3, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %323 : int = aten::size(%query.3, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %bsz.3 : Long() = prim::NumToTensor(%323), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %325 : int = aten::size(%query.3, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %326 : Tensor = prim::GetAttr[name="bias"](%321)
  %327 : Tensor = prim::GetAttr[name="weight"](%321)
  %328 : Float(1024:1, 1024:1024) = aten::t(%327), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.13 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.3, %328), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
  %330 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.13, %326, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.13 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%330, %27), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:673:0
  %332 : Tensor = prim::GetAttr[name="bias"](%320)
  %333 : Tensor = prim::GetAttr[name="weight"](%320)
  %334 : Float(1024:1, 1024:1024) = aten::t(%333), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.14 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.3, %334), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.15 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.14, %332, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1678:0
  %337 : Tensor = prim::GetAttr[name="bias"](%319)
  %338 : Tensor = prim::GetAttr[name="weight"](%319)
  %339 : Float(1024:1, 1024:1024) = aten::t(%338), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.15 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.3, %339), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.17 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.15, %337, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.14 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.13, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %343 : Long() = aten::mul(%bsz.3, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %344 : int = aten::Int(%343), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %345 : int[] = prim::ListConstruct(%322, %344, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %346 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.14, %345), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %q.3 : Float(272:64, 13:17408, 64:1) = aten::transpose(%346, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %tensor.16 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.15, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %349 : Long() = aten::mul(%bsz.3, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %350 : int = aten::Int(%349), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %351 : int[] = prim::ListConstruct(%39, %350, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %352 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.16, %351), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %k.3 : Float(272:64, 13:17408, 64:1) = aten::transpose(%352, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %tensor.18 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.17, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %355 : Long() = aten::mul(%bsz.3, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %356 : int = aten::Int(%355), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %357 : int[] = prim::ListConstruct(%39, %356, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %358 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.18, %357), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %v.3 : Float(272:64, 13:17408, 64:1) = aten::transpose(%358, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %360 : int = aten::size(%k.3, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:701:0
  %361 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.3, %42, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.9 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.3, %361), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
  %363 : int[] = prim::ListConstruct(%323, %30, %322, %360), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %attn_weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.9, %363), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:718:0
  %365 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.3 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%365, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.10, %reshaped.3, %32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:720:0
  %368 : Long() = aten::mul(%bsz.3, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:721:0
  %369 : int = aten::Int(%368), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %370 : int[] = prim::ListConstruct(%369, %322, %360), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %input.26 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.11, %370), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:721:0
  %input.27 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.26, %39, %43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.12 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.27, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # torch/nn/functional.py:973:0
  %attn_output.3 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.12, %v.3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:730:0
  %375 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.3, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %376 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%375, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %377 : int[] = prim::ListConstruct(%322, %323, %325), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %input.28 : Float(13:17408, 17:1024, 1024:1) = aten::view(%376, %377), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %379 : Tensor = prim::GetAttr[name="bias"](%318)
  %380 : Tensor = prim::GetAttr[name="weight"](%318)
  %381 : Float(1024:1, 1024:1024) = aten::t(%380), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.16 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.28, %381), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.29 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.16, %379, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.29, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
  %input.30 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.3, %x.6, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # transformers/modeling_bart.py:258:0
  %386 : Tensor = prim::GetAttr[name="bias"](%316)
  %387 : Tensor = prim::GetAttr[name="weight"](%316)
  %388 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn_layer_norm
  %input.31 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.30, %388, %387, %386, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %390 : Tensor = prim::GetAttr[name="bias"](%315)
  %391 : Tensor = prim::GetAttr[name="weight"](%315)
  %392 : Float(1024:1, 4096:1024) = aten::t(%391), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc1 # torch/nn/functional.py:1676:0
  %output.17 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.31, %392), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc1 # torch/nn/functional.py:1676:0
  %input.32 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.17, %390, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc1 # torch/nn/functional.py:1678:0
  %input.33 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:1369:0
  %input.34 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.33, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
  %397 : Tensor = prim::GetAttr[name="bias"](%314)
  %398 : Tensor = prim::GetAttr[name="weight"](%314)
  %399 : Float(4096:1, 1024:4096) = aten::t(%398), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc2 # torch/nn/functional.py:1676:0
  %output.18 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.34, %399), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc2 # torch/nn/functional.py:1676:0
  %input.35 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.18, %397, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc2 # torch/nn/functional.py:1678:0
  %x.7 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.35, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
  %input.36 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.31, %x.7, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # transformers/modeling_bart.py:269:0
  %404 : Tensor = prim::GetAttr[name="bias"](%313)
  %405 : Tensor = prim::GetAttr[name="weight"](%313)
  %406 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.final_layer_norm
  %query.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.36, %406, %405, %404, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
  %408 : __torch__.torch.nn.modules.normalization.___torch_mangle_1077.LayerNorm = prim::GetAttr[name="final_layer_norm"](%99)
  %409 : __torch__.torch.nn.modules.linear.___torch_mangle_1076.Linear = prim::GetAttr[name="fc2"](%99)
  %410 : __torch__.torch.nn.modules.linear.___torch_mangle_1075.Linear = prim::GetAttr[name="fc1"](%99)
  %411 : __torch__.torch.nn.modules.normalization.___torch_mangle_1074.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%99)
  %412 : __torch__.transformers.modeling_bart.___torch_mangle_1073.Attention = prim::GetAttr[name="self_attn"](%99)
  %413 : __torch__.torch.nn.modules.linear.___torch_mangle_1072.Linear = prim::GetAttr[name="out_proj"](%412)
  %414 : __torch__.torch.nn.modules.linear.___torch_mangle_1070.Linear = prim::GetAttr[name="v_proj"](%412)
  %415 : __torch__.torch.nn.modules.linear.___torch_mangle_1069.Linear = prim::GetAttr[name="k_proj"](%412)
  %416 : __torch__.torch.nn.modules.linear.___torch_mangle_1071.Linear = prim::GetAttr[name="q_proj"](%412)
  %417 : int = aten::size(%query.4, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %418 : int = aten::size(%query.4, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %bsz.4 : Long() = prim::NumToTensor(%418), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %420 : int = aten::size(%query.4, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %421 : Tensor = prim::GetAttr[name="bias"](%416)
  %422 : Tensor = prim::GetAttr[name="weight"](%416)
  %423 : Float(1024:1, 1024:1024) = aten::t(%422), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.19 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.4, %423), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
  %425 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.19, %421, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.19 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%425, %27), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:673:0
  %427 : Tensor = prim::GetAttr[name="bias"](%415)
  %428 : Tensor = prim::GetAttr[name="weight"](%415)
  %429 : Float(1024:1, 1024:1024) = aten::t(%428), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.20 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.4, %429), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.21 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.20, %427, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1678:0
  %432 : Tensor = prim::GetAttr[name="bias"](%414)
  %433 : Tensor = prim::GetAttr[name="weight"](%414)
  %434 : Float(1024:1, 1024:1024) = aten::t(%433), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.21 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.4, %434), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.23 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.21, %432, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.20 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.19, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %438 : Long() = aten::mul(%bsz.4, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %439 : int = aten::Int(%438), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %440 : int[] = prim::ListConstruct(%417, %439, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %441 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.20, %440), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %q.4 : Float(272:64, 13:17408, 64:1) = aten::transpose(%441, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %tensor.22 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.21, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %444 : Long() = aten::mul(%bsz.4, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %445 : int = aten::Int(%444), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %446 : int[] = prim::ListConstruct(%39, %445, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %447 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.22, %446), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %k.4 : Float(272:64, 13:17408, 64:1) = aten::transpose(%447, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %tensor.24 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.23, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %450 : Long() = aten::mul(%bsz.4, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %451 : int = aten::Int(%450), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %452 : int[] = prim::ListConstruct(%39, %451, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %453 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.24, %452), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %v.4 : Float(272:64, 13:17408, 64:1) = aten::transpose(%453, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %455 : int = aten::size(%k.4, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:701:0
  %456 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.4, %42, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.13 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.4, %456), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
  %458 : int[] = prim::ListConstruct(%418, %30, %417, %455), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %attn_weights.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.13, %458), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:718:0
  %460 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.4 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%460, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.14, %reshaped.4, %32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:720:0
  %463 : Long() = aten::mul(%bsz.4, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:721:0
  %464 : int = aten::Int(%463), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %465 : int[] = prim::ListConstruct(%464, %417, %455), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %input.37 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.15, %465), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:721:0
  %input.38 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.37, %39, %43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.16 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.38, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # torch/nn/functional.py:973:0
  %attn_output.4 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.16, %v.4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:730:0
  %470 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.4, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %471 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%470, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %472 : int[] = prim::ListConstruct(%417, %418, %420), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %input.39 : Float(13:17408, 17:1024, 1024:1) = aten::view(%471, %472), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %474 : Tensor = prim::GetAttr[name="bias"](%413)
  %475 : Tensor = prim::GetAttr[name="weight"](%413)
  %476 : Float(1024:1, 1024:1024) = aten::t(%475), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.22 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.39, %476), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.40 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.22, %474, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.8 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.40, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
  %input.41 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.4, %x.8, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # transformers/modeling_bart.py:258:0
  %481 : Tensor = prim::GetAttr[name="bias"](%411)
  %482 : Tensor = prim::GetAttr[name="weight"](%411)
  %483 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn_layer_norm
  %input.42 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.41, %483, %482, %481, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %485 : Tensor = prim::GetAttr[name="bias"](%410)
  %486 : Tensor = prim::GetAttr[name="weight"](%410)
  %487 : Float(1024:1, 4096:1024) = aten::t(%486), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc1 # torch/nn/functional.py:1676:0
  %output.23 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.42, %487), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc1 # torch/nn/functional.py:1676:0
  %input.43 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.23, %485, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc1 # torch/nn/functional.py:1678:0
  %input.44 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:1369:0
  %input.45 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.44, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
  %492 : Tensor = prim::GetAttr[name="bias"](%409)
  %493 : Tensor = prim::GetAttr[name="weight"](%409)
  %494 : Float(4096:1, 1024:4096) = aten::t(%493), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc2 # torch/nn/functional.py:1676:0
  %output.24 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.45, %494), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc2 # torch/nn/functional.py:1676:0
  %input.46 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.24, %492, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc2 # torch/nn/functional.py:1678:0
  %x.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.46, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
  %input.47 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.42, %x.9, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # transformers/modeling_bart.py:269:0
  %499 : Tensor = prim::GetAttr[name="bias"](%408)
  %500 : Tensor = prim::GetAttr[name="weight"](%408)
  %501 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.final_layer_norm
  %query.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.47, %501, %500, %499, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
  %503 : __torch__.torch.nn.modules.normalization.___torch_mangle_1087.LayerNorm = prim::GetAttr[name="final_layer_norm"](%97)
  %504 : __torch__.torch.nn.modules.linear.___torch_mangle_1086.Linear = prim::GetAttr[name="fc2"](%97)
  %505 : __torch__.torch.nn.modules.linear.___torch_mangle_1085.Linear = prim::GetAttr[name="fc1"](%97)
  %506 : __torch__.torch.nn.modules.normalization.___torch_mangle_1084.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%97)
  %507 : __torch__.transformers.modeling_bart.___torch_mangle_1083.Attention = prim::GetAttr[name="self_attn"](%97)
  %508 : __torch__.torch.nn.modules.linear.___torch_mangle_1082.Linear = prim::GetAttr[name="out_proj"](%507)
  %509 : __torch__.torch.nn.modules.linear.___torch_mangle_1080.Linear = prim::GetAttr[name="v_proj"](%507)
  %510 : __torch__.torch.nn.modules.linear.___torch_mangle_1079.Linear = prim::GetAttr[name="k_proj"](%507)
  %511 : __torch__.torch.nn.modules.linear.___torch_mangle_1081.Linear = prim::GetAttr[name="q_proj"](%507)
  %512 : int = aten::size(%query.5, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %513 : int = aten::size(%query.5, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %bsz.5 : Long() = prim::NumToTensor(%513), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %515 : int = aten::size(%query.5, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %516 : Tensor = prim::GetAttr[name="bias"](%511)
  %517 : Tensor = prim::GetAttr[name="weight"](%511)
  %518 : Float(1024:1, 1024:1024) = aten::t(%517), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.25 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.5, %518), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
  %520 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.25, %516, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.25 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%520, %27), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:673:0
  %522 : Tensor = prim::GetAttr[name="bias"](%510)
  %523 : Tensor = prim::GetAttr[name="weight"](%510)
  %524 : Float(1024:1, 1024:1024) = aten::t(%523), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.26 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.5, %524), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.27 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.26, %522, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1678:0
  %527 : Tensor = prim::GetAttr[name="bias"](%509)
  %528 : Tensor = prim::GetAttr[name="weight"](%509)
  %529 : Float(1024:1, 1024:1024) = aten::t(%528), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.27 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.5, %529), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.29 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.27, %527, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.26 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.25, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %533 : Long() = aten::mul(%bsz.5, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %534 : int = aten::Int(%533), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %535 : int[] = prim::ListConstruct(%512, %534, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %536 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.26, %535), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %q.5 : Float(272:64, 13:17408, 64:1) = aten::transpose(%536, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %tensor.28 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.27, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %539 : Long() = aten::mul(%bsz.5, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %540 : int = aten::Int(%539), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %541 : int[] = prim::ListConstruct(%39, %540, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %542 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.28, %541), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %k.5 : Float(272:64, 13:17408, 64:1) = aten::transpose(%542, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %tensor.30 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.29, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %545 : Long() = aten::mul(%bsz.5, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %546 : int = aten::Int(%545), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %547 : int[] = prim::ListConstruct(%39, %546, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %548 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.30, %547), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %v.5 : Float(272:64, 13:17408, 64:1) = aten::transpose(%548, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %550 : int = aten::size(%k.5, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:701:0
  %551 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.5, %42, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.17 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.5, %551), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
  %553 : int[] = prim::ListConstruct(%513, %30, %512, %550), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %attn_weights.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.17, %553), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:718:0
  %555 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.5 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%555, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.18, %reshaped.5, %32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:720:0
  %558 : Long() = aten::mul(%bsz.5, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:721:0
  %559 : int = aten::Int(%558), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %560 : int[] = prim::ListConstruct(%559, %512, %550), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %input.48 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.19, %560), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:721:0
  %input.49 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.48, %39, %43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.20 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.49, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # torch/nn/functional.py:973:0
  %attn_output.5 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.20, %v.5), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:730:0
  %565 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.5, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %566 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%565, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %567 : int[] = prim::ListConstruct(%512, %513, %515), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %input.50 : Float(13:17408, 17:1024, 1024:1) = aten::view(%566, %567), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %569 : Tensor = prim::GetAttr[name="bias"](%508)
  %570 : Tensor = prim::GetAttr[name="weight"](%508)
  %571 : Float(1024:1, 1024:1024) = aten::t(%570), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.28 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.50, %571), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.51 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.28, %569, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.10 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.51, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
  %input.52 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.5, %x.10, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # transformers/modeling_bart.py:258:0
  %576 : Tensor = prim::GetAttr[name="bias"](%506)
  %577 : Tensor = prim::GetAttr[name="weight"](%506)
  %578 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn_layer_norm
  %input.53 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.52, %578, %577, %576, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %580 : Tensor = prim::GetAttr[name="bias"](%505)
  %581 : Tensor = prim::GetAttr[name="weight"](%505)
  %582 : Float(1024:1, 4096:1024) = aten::t(%581), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc1 # torch/nn/functional.py:1676:0
  %output.29 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.53, %582), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc1 # torch/nn/functional.py:1676:0
  %input.54 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.29, %580, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc1 # torch/nn/functional.py:1678:0
  %input.55 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.54), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:1369:0
  %input.56 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.55, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
  %587 : Tensor = prim::GetAttr[name="bias"](%504)
  %588 : Tensor = prim::GetAttr[name="weight"](%504)
  %589 : Float(4096:1, 1024:4096) = aten::t(%588), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc2 # torch/nn/functional.py:1676:0
  %output.30 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.56, %589), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc2 # torch/nn/functional.py:1676:0
  %input.57 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.30, %587, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc2 # torch/nn/functional.py:1678:0
  %x.11 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.57, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
  %input.58 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.53, %x.11, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # transformers/modeling_bart.py:269:0
  %594 : Tensor = prim::GetAttr[name="bias"](%503)
  %595 : Tensor = prim::GetAttr[name="weight"](%503)
  %596 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.final_layer_norm
  %query.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.58, %596, %595, %594, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
  %598 : __torch__.torch.nn.modules.normalization.___torch_mangle_1097.LayerNorm = prim::GetAttr[name="final_layer_norm"](%95)
  %599 : __torch__.torch.nn.modules.linear.___torch_mangle_1096.Linear = prim::GetAttr[name="fc2"](%95)
  %600 : __torch__.torch.nn.modules.linear.___torch_mangle_1095.Linear = prim::GetAttr[name="fc1"](%95)
  %601 : __torch__.torch.nn.modules.normalization.___torch_mangle_1094.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%95)
  %602 : __torch__.transformers.modeling_bart.___torch_mangle_1093.Attention = prim::GetAttr[name="self_attn"](%95)
  %603 : __torch__.torch.nn.modules.linear.___torch_mangle_1092.Linear = prim::GetAttr[name="out_proj"](%602)
  %604 : __torch__.torch.nn.modules.linear.___torch_mangle_1090.Linear = prim::GetAttr[name="v_proj"](%602)
  %605 : __torch__.torch.nn.modules.linear.___torch_mangle_1089.Linear = prim::GetAttr[name="k_proj"](%602)
  %606 : __torch__.torch.nn.modules.linear.___torch_mangle_1091.Linear = prim::GetAttr[name="q_proj"](%602)
  %607 : int = aten::size(%query.6, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %608 : int = aten::size(%query.6, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %bsz.6 : Long() = prim::NumToTensor(%608), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %610 : int = aten::size(%query.6, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %611 : Tensor = prim::GetAttr[name="bias"](%606)
  %612 : Tensor = prim::GetAttr[name="weight"](%606)
  %613 : Float(1024:1, 1024:1024) = aten::t(%612), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.31 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.6, %613), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
  %615 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.31, %611, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.31 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%615, %27), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:673:0
  %617 : Tensor = prim::GetAttr[name="bias"](%605)
  %618 : Tensor = prim::GetAttr[name="weight"](%605)
  %619 : Float(1024:1, 1024:1024) = aten::t(%618), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.32 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.6, %619), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.33 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.32, %617, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1678:0
  %622 : Tensor = prim::GetAttr[name="bias"](%604)
  %623 : Tensor = prim::GetAttr[name="weight"](%604)
  %624 : Float(1024:1, 1024:1024) = aten::t(%623), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.33 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.6, %624), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.35 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.33, %622, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.32 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.31, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %628 : Long() = aten::mul(%bsz.6, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %629 : int = aten::Int(%628), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %630 : int[] = prim::ListConstruct(%607, %629, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %631 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.32, %630), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %q.6 : Float(272:64, 13:17408, 64:1) = aten::transpose(%631, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %tensor.34 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.33, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %634 : Long() = aten::mul(%bsz.6, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %635 : int = aten::Int(%634), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %636 : int[] = prim::ListConstruct(%39, %635, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %637 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.34, %636), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %k.6 : Float(272:64, 13:17408, 64:1) = aten::transpose(%637, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %tensor.36 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.35, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %640 : Long() = aten::mul(%bsz.6, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %641 : int = aten::Int(%640), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %642 : int[] = prim::ListConstruct(%39, %641, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %643 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.36, %642), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %v.6 : Float(272:64, 13:17408, 64:1) = aten::transpose(%643, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %645 : int = aten::size(%k.6, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:701:0
  %646 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.6, %42, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.21 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.6, %646), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
  %648 : int[] = prim::ListConstruct(%608, %30, %607, %645), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %attn_weights.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.21, %648), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:718:0
  %650 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.6 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%650, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.22, %reshaped.6, %32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:720:0
  %653 : Long() = aten::mul(%bsz.6, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:721:0
  %654 : int = aten::Int(%653), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %655 : int[] = prim::ListConstruct(%654, %607, %645), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %input.59 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.23, %655), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:721:0
  %input.60 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.59, %39, %43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.24 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.60, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # torch/nn/functional.py:973:0
  %attn_output.6 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.24, %v.6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:730:0
  %660 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.6, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %661 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%660, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %662 : int[] = prim::ListConstruct(%607, %608, %610), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %input.61 : Float(13:17408, 17:1024, 1024:1) = aten::view(%661, %662), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %664 : Tensor = prim::GetAttr[name="bias"](%603)
  %665 : Tensor = prim::GetAttr[name="weight"](%603)
  %666 : Float(1024:1, 1024:1024) = aten::t(%665), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.34 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.61, %666), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.62 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.34, %664, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.62, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
  %input.63 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.6, %x.12, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # transformers/modeling_bart.py:258:0
  %671 : Tensor = prim::GetAttr[name="bias"](%601)
  %672 : Tensor = prim::GetAttr[name="weight"](%601)
  %673 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn_layer_norm
  %input.64 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.63, %673, %672, %671, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %675 : Tensor = prim::GetAttr[name="bias"](%600)
  %676 : Tensor = prim::GetAttr[name="weight"](%600)
  %677 : Float(1024:1, 4096:1024) = aten::t(%676), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc1 # torch/nn/functional.py:1676:0
  %output.35 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.64, %677), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc1 # torch/nn/functional.py:1676:0
  %input.65 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.35, %675, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc1 # torch/nn/functional.py:1678:0
  %input.66 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:1369:0
  %input.67 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.66, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
  %682 : Tensor = prim::GetAttr[name="bias"](%599)
  %683 : Tensor = prim::GetAttr[name="weight"](%599)
  %684 : Float(4096:1, 1024:4096) = aten::t(%683), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc2 # torch/nn/functional.py:1676:0
  %output.36 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.67, %684), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc2 # torch/nn/functional.py:1676:0
  %input.68 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.36, %682, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc2 # torch/nn/functional.py:1678:0
  %x.13 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.68, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
  %input.69 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.64, %x.13, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # transformers/modeling_bart.py:269:0
  %689 : Tensor = prim::GetAttr[name="bias"](%598)
  %690 : Tensor = prim::GetAttr[name="weight"](%598)
  %691 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.final_layer_norm
  %query.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.69, %691, %690, %689, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
  %693 : __torch__.torch.nn.modules.normalization.___torch_mangle_1107.LayerNorm = prim::GetAttr[name="final_layer_norm"](%93)
  %694 : __torch__.torch.nn.modules.linear.___torch_mangle_1106.Linear = prim::GetAttr[name="fc2"](%93)
  %695 : __torch__.torch.nn.modules.linear.___torch_mangle_1105.Linear = prim::GetAttr[name="fc1"](%93)
  %696 : __torch__.torch.nn.modules.normalization.___torch_mangle_1104.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%93)
  %697 : __torch__.transformers.modeling_bart.___torch_mangle_1103.Attention = prim::GetAttr[name="self_attn"](%93)
  %698 : __torch__.torch.nn.modules.linear.___torch_mangle_1102.Linear = prim::GetAttr[name="out_proj"](%697)
  %699 : __torch__.torch.nn.modules.linear.___torch_mangle_1100.Linear = prim::GetAttr[name="v_proj"](%697)
  %700 : __torch__.torch.nn.modules.linear.___torch_mangle_1099.Linear = prim::GetAttr[name="k_proj"](%697)
  %701 : __torch__.torch.nn.modules.linear.___torch_mangle_1101.Linear = prim::GetAttr[name="q_proj"](%697)
  %702 : int = aten::size(%query.7, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %703 : int = aten::size(%query.7, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %bsz.7 : Long() = prim::NumToTensor(%703), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %705 : int = aten::size(%query.7, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %706 : Tensor = prim::GetAttr[name="bias"](%701)
  %707 : Tensor = prim::GetAttr[name="weight"](%701)
  %708 : Float(1024:1, 1024:1024) = aten::t(%707), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.37 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.7, %708), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
  %710 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.37, %706, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.37 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%710, %27), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:673:0
  %712 : Tensor = prim::GetAttr[name="bias"](%700)
  %713 : Tensor = prim::GetAttr[name="weight"](%700)
  %714 : Float(1024:1, 1024:1024) = aten::t(%713), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.38 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.7, %714), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.39 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.38, %712, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1678:0
  %717 : Tensor = prim::GetAttr[name="bias"](%699)
  %718 : Tensor = prim::GetAttr[name="weight"](%699)
  %719 : Float(1024:1, 1024:1024) = aten::t(%718), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.39 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.7, %719), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.41 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.39, %717, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.38 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.37, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %723 : Long() = aten::mul(%bsz.7, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %724 : int = aten::Int(%723), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %725 : int[] = prim::ListConstruct(%702, %724, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %726 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.38, %725), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %q.7 : Float(272:64, 13:17408, 64:1) = aten::transpose(%726, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %tensor.40 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.39, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %729 : Long() = aten::mul(%bsz.7, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %730 : int = aten::Int(%729), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %731 : int[] = prim::ListConstruct(%39, %730, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %732 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.40, %731), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %k.7 : Float(272:64, 13:17408, 64:1) = aten::transpose(%732, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %tensor.42 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.41, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %735 : Long() = aten::mul(%bsz.7, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %736 : int = aten::Int(%735), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %737 : int[] = prim::ListConstruct(%39, %736, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %738 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.42, %737), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %v.7 : Float(272:64, 13:17408, 64:1) = aten::transpose(%738, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %740 : int = aten::size(%k.7, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:701:0
  %741 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.7, %42, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.25 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.7, %741), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
  %743 : int[] = prim::ListConstruct(%703, %30, %702, %740), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %attn_weights.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.25, %743), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:718:0
  %745 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.7 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%745, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.27 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.26, %reshaped.7, %32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:720:0
  %748 : Long() = aten::mul(%bsz.7, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:721:0
  %749 : int = aten::Int(%748), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %750 : int[] = prim::ListConstruct(%749, %702, %740), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %input.70 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.27, %750), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:721:0
  %input.71 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.70, %39, %43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.28 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.71, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # torch/nn/functional.py:973:0
  %attn_output.7 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.28, %v.7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:730:0
  %755 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.7, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %756 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%755, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %757 : int[] = prim::ListConstruct(%702, %703, %705), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %input.72 : Float(13:17408, 17:1024, 1024:1) = aten::view(%756, %757), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %759 : Tensor = prim::GetAttr[name="bias"](%698)
  %760 : Tensor = prim::GetAttr[name="weight"](%698)
  %761 : Float(1024:1, 1024:1024) = aten::t(%760), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.40 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.72, %761), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.73 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.40, %759, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.14 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.73, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
  %input.74 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.7, %x.14, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # transformers/modeling_bart.py:258:0
  %766 : Tensor = prim::GetAttr[name="bias"](%696)
  %767 : Tensor = prim::GetAttr[name="weight"](%696)
  %768 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn_layer_norm
  %input.75 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.74, %768, %767, %766, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %770 : Tensor = prim::GetAttr[name="bias"](%695)
  %771 : Tensor = prim::GetAttr[name="weight"](%695)
  %772 : Float(1024:1, 4096:1024) = aten::t(%771), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc1 # torch/nn/functional.py:1676:0
  %output.41 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.75, %772), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc1 # torch/nn/functional.py:1676:0
  %input.76 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.41, %770, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc1 # torch/nn/functional.py:1678:0
  %input.77 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.76), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:1369:0
  %input.78 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.77, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
  %777 : Tensor = prim::GetAttr[name="bias"](%694)
  %778 : Tensor = prim::GetAttr[name="weight"](%694)
  %779 : Float(4096:1, 1024:4096) = aten::t(%778), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc2 # torch/nn/functional.py:1676:0
  %output.42 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.78, %779), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc2 # torch/nn/functional.py:1676:0
  %input.79 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.42, %777, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc2 # torch/nn/functional.py:1678:0
  %x.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.79, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
  %input.80 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.75, %x.15, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # transformers/modeling_bart.py:269:0
  %784 : Tensor = prim::GetAttr[name="bias"](%693)
  %785 : Tensor = prim::GetAttr[name="weight"](%693)
  %786 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.final_layer_norm
  %query.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.80, %786, %785, %784, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
  %788 : __torch__.torch.nn.modules.normalization.___torch_mangle_1117.LayerNorm = prim::GetAttr[name="final_layer_norm"](%91)
  %789 : __torch__.torch.nn.modules.linear.___torch_mangle_1116.Linear = prim::GetAttr[name="fc2"](%91)
  %790 : __torch__.torch.nn.modules.linear.___torch_mangle_1115.Linear = prim::GetAttr[name="fc1"](%91)
  %791 : __torch__.torch.nn.modules.normalization.___torch_mangle_1114.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%91)
  %792 : __torch__.transformers.modeling_bart.___torch_mangle_1113.Attention = prim::GetAttr[name="self_attn"](%91)
  %793 : __torch__.torch.nn.modules.linear.___torch_mangle_1112.Linear = prim::GetAttr[name="out_proj"](%792)
  %794 : __torch__.torch.nn.modules.linear.___torch_mangle_1110.Linear = prim::GetAttr[name="v_proj"](%792)
  %795 : __torch__.torch.nn.modules.linear.___torch_mangle_1109.Linear = prim::GetAttr[name="k_proj"](%792)
  %796 : __torch__.torch.nn.modules.linear.___torch_mangle_1111.Linear = prim::GetAttr[name="q_proj"](%792)
  %797 : int = aten::size(%query.8, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %798 : int = aten::size(%query.8, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %bsz.8 : Long() = prim::NumToTensor(%798), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %800 : int = aten::size(%query.8, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %801 : Tensor = prim::GetAttr[name="bias"](%796)
  %802 : Tensor = prim::GetAttr[name="weight"](%796)
  %803 : Float(1024:1, 1024:1024) = aten::t(%802), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.43 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.8, %803), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
  %805 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.43, %801, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.43 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%805, %27), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:673:0
  %807 : Tensor = prim::GetAttr[name="bias"](%795)
  %808 : Tensor = prim::GetAttr[name="weight"](%795)
  %809 : Float(1024:1, 1024:1024) = aten::t(%808), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.44 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.8, %809), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.45 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.44, %807, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1678:0
  %812 : Tensor = prim::GetAttr[name="bias"](%794)
  %813 : Tensor = prim::GetAttr[name="weight"](%794)
  %814 : Float(1024:1, 1024:1024) = aten::t(%813), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.45 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.8, %814), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.47 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.45, %812, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.44 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.43, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %818 : Long() = aten::mul(%bsz.8, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %819 : int = aten::Int(%818), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %820 : int[] = prim::ListConstruct(%797, %819, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %821 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.44, %820), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %q.8 : Float(272:64, 13:17408, 64:1) = aten::transpose(%821, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %tensor.46 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.45, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %824 : Long() = aten::mul(%bsz.8, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %825 : int = aten::Int(%824), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %826 : int[] = prim::ListConstruct(%39, %825, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %827 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.46, %826), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %k.8 : Float(272:64, 13:17408, 64:1) = aten::transpose(%827, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %tensor.48 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.47, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %830 : Long() = aten::mul(%bsz.8, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %831 : int = aten::Int(%830), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %832 : int[] = prim::ListConstruct(%39, %831, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %833 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.48, %832), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %v.8 : Float(272:64, 13:17408, 64:1) = aten::transpose(%833, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %835 : int = aten::size(%k.8, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:701:0
  %836 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.8, %42, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.29 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.8, %836), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
  %838 : int[] = prim::ListConstruct(%798, %30, %797, %835), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %attn_weights.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.29, %838), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:718:0
  %840 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.8 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%840, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.30, %reshaped.8, %32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:720:0
  %843 : Long() = aten::mul(%bsz.8, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:721:0
  %844 : int = aten::Int(%843), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %845 : int[] = prim::ListConstruct(%844, %797, %835), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %input.81 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.31, %845), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:721:0
  %input.82 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.81, %39, %43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.32 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.82, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # torch/nn/functional.py:973:0
  %attn_output.8 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.32, %v.8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:730:0
  %850 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.8, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %851 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%850, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %852 : int[] = prim::ListConstruct(%797, %798, %800), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %input.83 : Float(13:17408, 17:1024, 1024:1) = aten::view(%851, %852), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %854 : Tensor = prim::GetAttr[name="bias"](%793)
  %855 : Tensor = prim::GetAttr[name="weight"](%793)
  %856 : Float(1024:1, 1024:1024) = aten::t(%855), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.46 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.83, %856), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.84 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.46, %854, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.16 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.84, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
  %input.85 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.8, %x.16, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # transformers/modeling_bart.py:258:0
  %861 : Tensor = prim::GetAttr[name="bias"](%791)
  %862 : Tensor = prim::GetAttr[name="weight"](%791)
  %863 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn_layer_norm
  %input.86 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.85, %863, %862, %861, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %865 : Tensor = prim::GetAttr[name="bias"](%790)
  %866 : Tensor = prim::GetAttr[name="weight"](%790)
  %867 : Float(1024:1, 4096:1024) = aten::t(%866), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc1 # torch/nn/functional.py:1676:0
  %output.47 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.86, %867), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc1 # torch/nn/functional.py:1676:0
  %input.87 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.47, %865, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc1 # torch/nn/functional.py:1678:0
  %input.88 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.87), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:1369:0
  %input.89 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.88, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
  %872 : Tensor = prim::GetAttr[name="bias"](%789)
  %873 : Tensor = prim::GetAttr[name="weight"](%789)
  %874 : Float(4096:1, 1024:4096) = aten::t(%873), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc2 # torch/nn/functional.py:1676:0
  %output.48 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.89, %874), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc2 # torch/nn/functional.py:1676:0
  %input.90 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.48, %872, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc2 # torch/nn/functional.py:1678:0
  %x.17 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.90, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
  %input.91 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.86, %x.17, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # transformers/modeling_bart.py:269:0
  %879 : Tensor = prim::GetAttr[name="bias"](%788)
  %880 : Tensor = prim::GetAttr[name="weight"](%788)
  %881 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.final_layer_norm
  %query.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.91, %881, %880, %879, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
  %883 : __torch__.torch.nn.modules.normalization.___torch_mangle_1127.LayerNorm = prim::GetAttr[name="final_layer_norm"](%89)
  %884 : __torch__.torch.nn.modules.linear.___torch_mangle_1126.Linear = prim::GetAttr[name="fc2"](%89)
  %885 : __torch__.torch.nn.modules.linear.___torch_mangle_1125.Linear = prim::GetAttr[name="fc1"](%89)
  %886 : __torch__.torch.nn.modules.normalization.___torch_mangle_1124.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%89)
  %887 : __torch__.transformers.modeling_bart.___torch_mangle_1123.Attention = prim::GetAttr[name="self_attn"](%89)
  %888 : __torch__.torch.nn.modules.linear.___torch_mangle_1122.Linear = prim::GetAttr[name="out_proj"](%887)
  %889 : __torch__.torch.nn.modules.linear.___torch_mangle_1120.Linear = prim::GetAttr[name="v_proj"](%887)
  %890 : __torch__.torch.nn.modules.linear.___torch_mangle_1119.Linear = prim::GetAttr[name="k_proj"](%887)
  %891 : __torch__.torch.nn.modules.linear.___torch_mangle_1121.Linear = prim::GetAttr[name="q_proj"](%887)
  %892 : int = aten::size(%query.9, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %893 : int = aten::size(%query.9, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %bsz.9 : Long() = prim::NumToTensor(%893), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %895 : int = aten::size(%query.9, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %896 : Tensor = prim::GetAttr[name="bias"](%891)
  %897 : Tensor = prim::GetAttr[name="weight"](%891)
  %898 : Float(1024:1, 1024:1024) = aten::t(%897), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.49 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.9, %898), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
  %900 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.49, %896, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.49 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%900, %27), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:673:0
  %902 : Tensor = prim::GetAttr[name="bias"](%890)
  %903 : Tensor = prim::GetAttr[name="weight"](%890)
  %904 : Float(1024:1, 1024:1024) = aten::t(%903), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.50 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.9, %904), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.51 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.50, %902, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1678:0
  %907 : Tensor = prim::GetAttr[name="bias"](%889)
  %908 : Tensor = prim::GetAttr[name="weight"](%889)
  %909 : Float(1024:1, 1024:1024) = aten::t(%908), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.51 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.9, %909), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.53 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.51, %907, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.50 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.49, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %913 : Long() = aten::mul(%bsz.9, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %914 : int = aten::Int(%913), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %915 : int[] = prim::ListConstruct(%892, %914, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %916 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.50, %915), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %q.9 : Float(272:64, 13:17408, 64:1) = aten::transpose(%916, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %tensor.52 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.51, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %919 : Long() = aten::mul(%bsz.9, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %920 : int = aten::Int(%919), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %921 : int[] = prim::ListConstruct(%39, %920, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %922 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.52, %921), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %k.9 : Float(272:64, 13:17408, 64:1) = aten::transpose(%922, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %tensor.54 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.53, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %925 : Long() = aten::mul(%bsz.9, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %926 : int = aten::Int(%925), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %927 : int[] = prim::ListConstruct(%39, %926, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %928 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.54, %927), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %v.9 : Float(272:64, 13:17408, 64:1) = aten::transpose(%928, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %930 : int = aten::size(%k.9, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:701:0
  %931 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.9, %42, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.33 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.9, %931), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
  %933 : int[] = prim::ListConstruct(%893, %30, %892, %930), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %attn_weights.34 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.33, %933), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:718:0
  %935 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.9 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%935, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.34, %reshaped.9, %32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:720:0
  %938 : Long() = aten::mul(%bsz.9, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:721:0
  %939 : int = aten::Int(%938), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %940 : int[] = prim::ListConstruct(%939, %892, %930), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %input.92 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.35, %940), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:721:0
  %input.93 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.92, %39, %43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.36 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.93, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # torch/nn/functional.py:973:0
  %attn_output.9 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.36, %v.9), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:730:0
  %945 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.9, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %946 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%945, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %947 : int[] = prim::ListConstruct(%892, %893, %895), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %input.94 : Float(13:17408, 17:1024, 1024:1) = aten::view(%946, %947), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %949 : Tensor = prim::GetAttr[name="bias"](%888)
  %950 : Tensor = prim::GetAttr[name="weight"](%888)
  %951 : Float(1024:1, 1024:1024) = aten::t(%950), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.52 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.94, %951), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.95 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.52, %949, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.95, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
  %input.96 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.9, %x.18, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # transformers/modeling_bart.py:258:0
  %956 : Tensor = prim::GetAttr[name="bias"](%886)
  %957 : Tensor = prim::GetAttr[name="weight"](%886)
  %958 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn_layer_norm
  %input.97 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.96, %958, %957, %956, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %960 : Tensor = prim::GetAttr[name="bias"](%885)
  %961 : Tensor = prim::GetAttr[name="weight"](%885)
  %962 : Float(1024:1, 4096:1024) = aten::t(%961), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc1 # torch/nn/functional.py:1676:0
  %output.53 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.97, %962), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc1 # torch/nn/functional.py:1676:0
  %input.98 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.53, %960, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc1 # torch/nn/functional.py:1678:0
  %input.99 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.98), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:1369:0
  %input.100 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.99, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
  %967 : Tensor = prim::GetAttr[name="bias"](%884)
  %968 : Tensor = prim::GetAttr[name="weight"](%884)
  %969 : Float(4096:1, 1024:4096) = aten::t(%968), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc2 # torch/nn/functional.py:1676:0
  %output.54 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.100, %969), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc2 # torch/nn/functional.py:1676:0
  %input.101 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.54, %967, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc2 # torch/nn/functional.py:1678:0
  %x.19 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.101, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
  %input.102 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.97, %x.19, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # transformers/modeling_bart.py:269:0
  %974 : Tensor = prim::GetAttr[name="bias"](%883)
  %975 : Tensor = prim::GetAttr[name="weight"](%883)
  %976 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.final_layer_norm
  %query.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.102, %976, %975, %974, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
  %978 : __torch__.torch.nn.modules.normalization.___torch_mangle_1137.LayerNorm = prim::GetAttr[name="final_layer_norm"](%87)
  %979 : __torch__.torch.nn.modules.linear.___torch_mangle_1136.Linear = prim::GetAttr[name="fc2"](%87)
  %980 : __torch__.torch.nn.modules.linear.___torch_mangle_1135.Linear = prim::GetAttr[name="fc1"](%87)
  %981 : __torch__.torch.nn.modules.normalization.___torch_mangle_1134.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%87)
  %982 : __torch__.transformers.modeling_bart.___torch_mangle_1133.Attention = prim::GetAttr[name="self_attn"](%87)
  %983 : __torch__.torch.nn.modules.linear.___torch_mangle_1132.Linear = prim::GetAttr[name="out_proj"](%982)
  %984 : __torch__.torch.nn.modules.linear.___torch_mangle_1130.Linear = prim::GetAttr[name="v_proj"](%982)
  %985 : __torch__.torch.nn.modules.linear.___torch_mangle_1129.Linear = prim::GetAttr[name="k_proj"](%982)
  %986 : __torch__.torch.nn.modules.linear.___torch_mangle_1131.Linear = prim::GetAttr[name="q_proj"](%982)
  %987 : int = aten::size(%query.10, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %988 : int = aten::size(%query.10, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %bsz.10 : Long() = prim::NumToTensor(%988), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %990 : int = aten::size(%query.10, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %991 : Tensor = prim::GetAttr[name="bias"](%986)
  %992 : Tensor = prim::GetAttr[name="weight"](%986)
  %993 : Float(1024:1, 1024:1024) = aten::t(%992), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.55 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.10, %993), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
  %995 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.55, %991, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.55 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%995, %27), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:673:0
  %997 : Tensor = prim::GetAttr[name="bias"](%985)
  %998 : Tensor = prim::GetAttr[name="weight"](%985)
  %999 : Float(1024:1, 1024:1024) = aten::t(%998), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.56 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.10, %999), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.57 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.56, %997, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1002 : Tensor = prim::GetAttr[name="bias"](%984)
  %1003 : Tensor = prim::GetAttr[name="weight"](%984)
  %1004 : Float(1024:1, 1024:1024) = aten::t(%1003), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.57 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.10, %1004), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.59 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.57, %1002, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.56 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.55, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1008 : Long() = aten::mul(%bsz.10, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1009 : int = aten::Int(%1008), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1010 : int[] = prim::ListConstruct(%987, %1009, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1011 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.56, %1010), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %q.10 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1011, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %tensor.58 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.57, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1014 : Long() = aten::mul(%bsz.10, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1015 : int = aten::Int(%1014), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1016 : int[] = prim::ListConstruct(%39, %1015, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1017 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.58, %1016), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %k.10 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1017, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %tensor.60 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.59, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1020 : Long() = aten::mul(%bsz.10, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1021 : int = aten::Int(%1020), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1022 : int[] = prim::ListConstruct(%39, %1021, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1023 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.60, %1022), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %v.10 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1023, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1025 : int = aten::size(%k.10, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:701:0
  %1026 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.10, %42, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.37 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.10, %1026), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
  %1028 : int[] = prim::ListConstruct(%988, %30, %987, %1025), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %attn_weights.38 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.37, %1028), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:718:0
  %1030 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.10 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1030, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.38, %reshaped.10, %32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:720:0
  %1033 : Long() = aten::mul(%bsz.10, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:721:0
  %1034 : int = aten::Int(%1033), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1035 : int[] = prim::ListConstruct(%1034, %987, %1025), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %input.103 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.39, %1035), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:721:0
  %input.104 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.103, %39, %43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.40 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.104, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # torch/nn/functional.py:973:0
  %attn_output.10 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.40, %v.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:730:0
  %1040 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.10, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %1041 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1040, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %1042 : int[] = prim::ListConstruct(%987, %988, %990), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %input.105 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1041, %1042), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %1044 : Tensor = prim::GetAttr[name="bias"](%983)
  %1045 : Tensor = prim::GetAttr[name="weight"](%983)
  %1046 : Float(1024:1, 1024:1024) = aten::t(%1045), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.58 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.105, %1046), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.106 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.58, %1044, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.20 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.106, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
  %input.107 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.10, %x.20, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # transformers/modeling_bart.py:258:0
  %1051 : Tensor = prim::GetAttr[name="bias"](%981)
  %1052 : Tensor = prim::GetAttr[name="weight"](%981)
  %1053 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn_layer_norm
  %input.108 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.107, %1053, %1052, %1051, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1055 : Tensor = prim::GetAttr[name="bias"](%980)
  %1056 : Tensor = prim::GetAttr[name="weight"](%980)
  %1057 : Float(1024:1, 4096:1024) = aten::t(%1056), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc1 # torch/nn/functional.py:1676:0
  %output.59 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.108, %1057), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc1 # torch/nn/functional.py:1676:0
  %input.109 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.59, %1055, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc1 # torch/nn/functional.py:1678:0
  %input.110 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.109), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:1369:0
  %input.111 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.110, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
  %1062 : Tensor = prim::GetAttr[name="bias"](%979)
  %1063 : Tensor = prim::GetAttr[name="weight"](%979)
  %1064 : Float(4096:1, 1024:4096) = aten::t(%1063), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc2 # torch/nn/functional.py:1676:0
  %output.60 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.111, %1064), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc2 # torch/nn/functional.py:1676:0
  %input.112 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.60, %1062, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc2 # torch/nn/functional.py:1678:0
  %x.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.112, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
  %input.113 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.108, %x.21, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # transformers/modeling_bart.py:269:0
  %1069 : Tensor = prim::GetAttr[name="bias"](%978)
  %1070 : Tensor = prim::GetAttr[name="weight"](%978)
  %1071 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.final_layer_norm
  %query.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.113, %1071, %1070, %1069, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
  %1073 : __torch__.torch.nn.modules.normalization.___torch_mangle_1147.LayerNorm = prim::GetAttr[name="final_layer_norm"](%85)
  %1074 : __torch__.torch.nn.modules.linear.___torch_mangle_1146.Linear = prim::GetAttr[name="fc2"](%85)
  %1075 : __torch__.torch.nn.modules.linear.___torch_mangle_1145.Linear = prim::GetAttr[name="fc1"](%85)
  %1076 : __torch__.torch.nn.modules.normalization.___torch_mangle_1144.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%85)
  %1077 : __torch__.transformers.modeling_bart.___torch_mangle_1143.Attention = prim::GetAttr[name="self_attn"](%85)
  %1078 : __torch__.torch.nn.modules.linear.___torch_mangle_1142.Linear = prim::GetAttr[name="out_proj"](%1077)
  %1079 : __torch__.torch.nn.modules.linear.___torch_mangle_1140.Linear = prim::GetAttr[name="v_proj"](%1077)
  %1080 : __torch__.torch.nn.modules.linear.___torch_mangle_1139.Linear = prim::GetAttr[name="k_proj"](%1077)
  %1081 : __torch__.torch.nn.modules.linear.___torch_mangle_1141.Linear = prim::GetAttr[name="q_proj"](%1077)
  %1082 : int = aten::size(%query.11, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %1083 : int = aten::size(%query.11, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %bsz.11 : Long() = prim::NumToTensor(%1083), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1085 : int = aten::size(%query.11, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %1086 : Tensor = prim::GetAttr[name="bias"](%1081)
  %1087 : Tensor = prim::GetAttr[name="weight"](%1081)
  %1088 : Float(1024:1, 1024:1024) = aten::t(%1087), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.61 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.11, %1088), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1090 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.61, %1086, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.61 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1090, %27), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:673:0
  %1092 : Tensor = prim::GetAttr[name="bias"](%1080)
  %1093 : Tensor = prim::GetAttr[name="weight"](%1080)
  %1094 : Float(1024:1, 1024:1024) = aten::t(%1093), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.62 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.11, %1094), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.63 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.62, %1092, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1097 : Tensor = prim::GetAttr[name="bias"](%1079)
  %1098 : Tensor = prim::GetAttr[name="weight"](%1079)
  %1099 : Float(1024:1, 1024:1024) = aten::t(%1098), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.63 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.11, %1099), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.65 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.63, %1097, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.62 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.61, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1103 : Long() = aten::mul(%bsz.11, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1104 : int = aten::Int(%1103), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1105 : int[] = prim::ListConstruct(%1082, %1104, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1106 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.62, %1105), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %q.11 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1106, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %tensor.64 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.63, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1109 : Long() = aten::mul(%bsz.11, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1110 : int = aten::Int(%1109), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1111 : int[] = prim::ListConstruct(%39, %1110, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1112 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.64, %1111), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %k.11 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1112, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %tensor.66 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.65, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1115 : Long() = aten::mul(%bsz.11, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1116 : int = aten::Int(%1115), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1117 : int[] = prim::ListConstruct(%39, %1116, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1118 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.66, %1117), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %v.11 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1118, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1120 : int = aten::size(%k.11, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:701:0
  %1121 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.11, %42, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.41 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.11, %1121), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
  %1123 : int[] = prim::ListConstruct(%1083, %30, %1082, %1120), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %attn_weights.42 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.41, %1123), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:718:0
  %1125 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.11 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1125, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.43 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.42, %reshaped.11, %32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:720:0
  %1128 : Long() = aten::mul(%bsz.11, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:721:0
  %1129 : int = aten::Int(%1128), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1130 : int[] = prim::ListConstruct(%1129, %1082, %1120), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %input.114 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.43, %1130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:721:0
  %input.115 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.114, %39, %43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.44 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.115, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # torch/nn/functional.py:973:0
  %attn_output.11 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.44, %v.11), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:730:0
  %1135 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.11, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %1136 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1135, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %1137 : int[] = prim::ListConstruct(%1082, %1083, %1085), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %input.116 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1136, %1137), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %1139 : Tensor = prim::GetAttr[name="bias"](%1078)
  %1140 : Tensor = prim::GetAttr[name="weight"](%1078)
  %1141 : Float(1024:1, 1024:1024) = aten::t(%1140), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.64 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.116, %1141), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.117 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.64, %1139, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.22 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.117, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
  %input.118 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.11, %x.22, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # transformers/modeling_bart.py:258:0
  %1146 : Tensor = prim::GetAttr[name="bias"](%1076)
  %1147 : Tensor = prim::GetAttr[name="weight"](%1076)
  %1148 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn_layer_norm
  %input.119 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.118, %1148, %1147, %1146, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1150 : Tensor = prim::GetAttr[name="bias"](%1075)
  %1151 : Tensor = prim::GetAttr[name="weight"](%1075)
  %1152 : Float(1024:1, 4096:1024) = aten::t(%1151), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc1 # torch/nn/functional.py:1676:0
  %output.65 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.119, %1152), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc1 # torch/nn/functional.py:1676:0
  %input.120 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.65, %1150, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc1 # torch/nn/functional.py:1678:0
  %input.121 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.120), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:1369:0
  %input.122 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.121, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
  %1157 : Tensor = prim::GetAttr[name="bias"](%1074)
  %1158 : Tensor = prim::GetAttr[name="weight"](%1074)
  %1159 : Float(4096:1, 1024:4096) = aten::t(%1158), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc2 # torch/nn/functional.py:1676:0
  %output.66 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.122, %1159), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc2 # torch/nn/functional.py:1676:0
  %input.123 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.66, %1157, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc2 # torch/nn/functional.py:1678:0
  %x.23 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.123, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
  %input.124 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.119, %x.23, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # transformers/modeling_bart.py:269:0
  %1164 : Tensor = prim::GetAttr[name="bias"](%1073)
  %1165 : Tensor = prim::GetAttr[name="weight"](%1073)
  %1166 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.final_layer_norm
  %query.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.124, %1166, %1165, %1164, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
  %1168 : __torch__.torch.nn.modules.normalization.___torch_mangle_1157.LayerNorm = prim::GetAttr[name="final_layer_norm"](%83)
  %1169 : __torch__.torch.nn.modules.linear.___torch_mangle_1156.Linear = prim::GetAttr[name="fc2"](%83)
  %1170 : __torch__.torch.nn.modules.linear.___torch_mangle_1155.Linear = prim::GetAttr[name="fc1"](%83)
  %1171 : __torch__.torch.nn.modules.normalization.___torch_mangle_1154.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%83)
  %1172 : __torch__.transformers.modeling_bart.___torch_mangle_1153.Attention = prim::GetAttr[name="self_attn"](%83)
  %1173 : __torch__.torch.nn.modules.linear.___torch_mangle_1152.Linear = prim::GetAttr[name="out_proj"](%1172)
  %1174 : __torch__.torch.nn.modules.linear.___torch_mangle_1150.Linear = prim::GetAttr[name="v_proj"](%1172)
  %1175 : __torch__.torch.nn.modules.linear.___torch_mangle_1149.Linear = prim::GetAttr[name="k_proj"](%1172)
  %1176 : __torch__.torch.nn.modules.linear.___torch_mangle_1151.Linear = prim::GetAttr[name="q_proj"](%1172)
  %1177 : int = aten::size(%query.12, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %1178 : int = aten::size(%query.12, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %bsz.12 : Long() = prim::NumToTensor(%1178), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1180 : int = aten::size(%query.12, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %1181 : Tensor = prim::GetAttr[name="bias"](%1176)
  %1182 : Tensor = prim::GetAttr[name="weight"](%1176)
  %1183 : Float(1024:1, 1024:1024) = aten::t(%1182), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.67 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.12, %1183), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1185 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.67, %1181, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.67 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1185, %27), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:673:0
  %1187 : Tensor = prim::GetAttr[name="bias"](%1175)
  %1188 : Tensor = prim::GetAttr[name="weight"](%1175)
  %1189 : Float(1024:1, 1024:1024) = aten::t(%1188), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.68 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.12, %1189), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.69 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.68, %1187, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1192 : Tensor = prim::GetAttr[name="bias"](%1174)
  %1193 : Tensor = prim::GetAttr[name="weight"](%1174)
  %1194 : Float(1024:1, 1024:1024) = aten::t(%1193), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.69 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.12, %1194), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.71 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.69, %1192, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.68 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.67, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1198 : Long() = aten::mul(%bsz.12, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1199 : int = aten::Int(%1198), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1200 : int[] = prim::ListConstruct(%1177, %1199, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1201 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.68, %1200), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %q.12 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1201, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %tensor.70 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.69, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1204 : Long() = aten::mul(%bsz.12, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1205 : int = aten::Int(%1204), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1206 : int[] = prim::ListConstruct(%39, %1205, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1207 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.70, %1206), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %k.12 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1207, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %tensor.72 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.71, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1210 : Long() = aten::mul(%bsz.12, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1211 : int = aten::Int(%1210), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1212 : int[] = prim::ListConstruct(%39, %1211, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1213 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.72, %1212), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %v.12 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1213, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1215 : int = aten::size(%k.12, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:701:0
  %1216 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.12, %42, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.45 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.12, %1216), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
  %1218 : int[] = prim::ListConstruct(%1178, %30, %1177, %1215), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %attn_weights.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.45, %1218), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:718:0
  %1220 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.12 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1220, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.47 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.46, %reshaped.12, %32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:720:0
  %1223 : Long() = aten::mul(%bsz.12, %28), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:721:0
  %1224 : int = aten::Int(%1223), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1225 : int[] = prim::ListConstruct(%1224, %1177, %1215), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %input.125 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.47, %1225), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:721:0
  %input.126 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.125, %39, %43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.48 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.126, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # torch/nn/functional.py:973:0
  %attn_output.12 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.48, %v.12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:730:0
  %1230 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.12, %38, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %1231 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1230, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %1232 : int[] = prim::ListConstruct(%1177, %1178, %1180), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %input.127 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1231, %1232), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %1234 : Tensor = prim::GetAttr[name="bias"](%1173)
  %1235 : Tensor = prim::GetAttr[name="weight"](%1173)
  %1236 : Float(1024:1, 1024:1024) = aten::t(%1235), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.70 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.127, %1236), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.128 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.70, %1234, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.24 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.128, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
  %input.129 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.12, %x.24, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # transformers/modeling_bart.py:258:0
  %1241 : Tensor = prim::GetAttr[name="bias"](%1171)
  %1242 : Tensor = prim::GetAttr[name="weight"](%1171)
  %1243 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn_layer_norm
  %input.130 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.129, %1243, %1242, %1241, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1245 : Tensor = prim::GetAttr[name="bias"](%1170)
  %1246 : Tensor = prim::GetAttr[name="weight"](%1170)
  %1247 : Float(1024:1, 4096:1024) = aten::t(%1246), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc1 # torch/nn/functional.py:1676:0
  %output.71 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.130, %1247), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc1 # torch/nn/functional.py:1676:0
  %input.131 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.71, %1245, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc1 # torch/nn/functional.py:1678:0
  %input.132 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.131), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:1369:0
  %input.133 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.132, %31, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
  %1252 : Tensor = prim::GetAttr[name="bias"](%1169)
  %1253 : Tensor = prim::GetAttr[name="weight"](%1169)
  %1254 : Float(4096:1, 1024:4096) = aten::t(%1253), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc2 # torch/nn/functional.py:1676:0
  %output.72 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.133, %1254), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc2 # torch/nn/functional.py:1676:0
  %input.134 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.72, %1252, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc2 # torch/nn/functional.py:1678:0
  %x.25 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.134, %25, %41), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
  %input.135 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.130, %x.25, %42), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # transformers/modeling_bart.py:269:0
  %1259 : Tensor = prim::GetAttr[name="bias"](%1168)
  %1260 : Tensor = prim::GetAttr[name="weight"](%1168)
  %1261 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.final_layer_norm
  %x.26 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.135, %1261, %1260, %1259, %23, %22), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
  %encoder_hidden_states : Float(17:1024, 13:17408, 1024:1) = aten::transpose(%x.26, %38, %42), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:366:0
  %1264 : __torch__.torch.nn.modules.container.___torch_mangle_1356.ModuleList = prim::GetAttr[name="layers"](%44)
  %1265 : __torch__.transformers.modeling_bart.___torch_mangle_1355.DecoderLayer = prim::GetAttr[name="11"](%1264)
  %1266 : __torch__.torch.nn.modules.container.___torch_mangle_1356.ModuleList = prim::GetAttr[name="layers"](%44)
  %1267 : __torch__.transformers.modeling_bart.___torch_mangle_1339.DecoderLayer = prim::GetAttr[name="10"](%1266)
  %1268 : __torch__.torch.nn.modules.container.___torch_mangle_1356.ModuleList = prim::GetAttr[name="layers"](%44)
  %1269 : __torch__.transformers.modeling_bart.___torch_mangle_1323.DecoderLayer = prim::GetAttr[name="9"](%1268)
  %1270 : __torch__.torch.nn.modules.container.___torch_mangle_1356.ModuleList = prim::GetAttr[name="layers"](%44)
  %1271 : __torch__.transformers.modeling_bart.___torch_mangle_1307.DecoderLayer = prim::GetAttr[name="8"](%1270)
  %1272 : __torch__.torch.nn.modules.container.___torch_mangle_1356.ModuleList = prim::GetAttr[name="layers"](%44)
  %1273 : __torch__.transformers.modeling_bart.___torch_mangle_1291.DecoderLayer = prim::GetAttr[name="7"](%1272)
  %1274 : __torch__.torch.nn.modules.container.___torch_mangle_1356.ModuleList = prim::GetAttr[name="layers"](%44)
  %1275 : __torch__.transformers.modeling_bart.___torch_mangle_1275.DecoderLayer = prim::GetAttr[name="6"](%1274)
  %1276 : __torch__.torch.nn.modules.container.___torch_mangle_1356.ModuleList = prim::GetAttr[name="layers"](%44)
  %1277 : __torch__.transformers.modeling_bart.___torch_mangle_1259.DecoderLayer = prim::GetAttr[name="5"](%1276)
  %1278 : __torch__.torch.nn.modules.container.___torch_mangle_1356.ModuleList = prim::GetAttr[name="layers"](%44)
  %1279 : __torch__.transformers.modeling_bart.___torch_mangle_1243.DecoderLayer = prim::GetAttr[name="4"](%1278)
  %1280 : __torch__.torch.nn.modules.container.___torch_mangle_1356.ModuleList = prim::GetAttr[name="layers"](%44)
  %1281 : __torch__.transformers.modeling_bart.___torch_mangle_1227.DecoderLayer = prim::GetAttr[name="3"](%1280)
  %1282 : __torch__.torch.nn.modules.container.___torch_mangle_1356.ModuleList = prim::GetAttr[name="layers"](%44)
  %1283 : __torch__.transformers.modeling_bart.___torch_mangle_1211.DecoderLayer = prim::GetAttr[name="2"](%1282)
  %1284 : __torch__.torch.nn.modules.container.___torch_mangle_1356.ModuleList = prim::GetAttr[name="layers"](%44)
  %1285 : __torch__.transformers.modeling_bart.___torch_mangle_1195.DecoderLayer = prim::GetAttr[name="1"](%1284)
  %1286 : __torch__.torch.nn.modules.container.___torch_mangle_1356.ModuleList = prim::GetAttr[name="layers"](%44)
  %1287 : __torch__.transformers.modeling_bart.___torch_mangle_1179.DecoderLayer = prim::GetAttr[name="0"](%1286)
  %1288 : __torch__.torch.nn.modules.normalization.___torch_mangle_1357.LayerNorm = prim::GetAttr[name="layernorm_embedding"](%44)
  %1289 : __torch__.transformers.modeling_bart.___torch_mangle_1163.LearnedPositionalEmbedding = prim::GetAttr[name="embed_positions"](%44)
  %key_padding_mask : Bool(17:13, 13:1) = aten::eq(%attention_mask, %38), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:136:0
  %1291 : Tensor = prim::GetAttr[name="weight"](%1289)
  %1292 : int = aten::size(%prev_output_tokens, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:816:0
  %positions.2 : Long(13:1) = aten::arange(%1292, %21, %38, %33, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:821:0
  %input.136 : Long(13:1) = aten::add(%positions.2, %20, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:822:0
  %positions : Float(13:1024, 1024:1) = aten::embedding(%1291, %input.136, %42, %41, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # torch/nn/functional.py:1814:0
  %1296 : Float(17:13312, 13:1024, 1024:1) = aten::embedding(%46, %prev_output_tokens, %42, %41, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_tokens # torch/nn/functional.py:1814:0
  %x.27 : Float(17:13312, 13:1024, 1024:1) = aten::mul(%1296, %19), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:556:0
  %input.137 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%x.27, %positions, %42), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:557:0
  %1299 : Tensor = prim::GetAttr[name="bias"](%1288)
  %1300 : Tensor = prim::GetAttr[name="weight"](%1288)
  %1301 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layernorm_embedding
  %input.138 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.137, %1301, %1300, %1299, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %x.28 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.138, %25, %41), scope: __module.model/__module.model.decoder # torch/nn/functional.py:973:0
  %query.13 : Float(13:1024, 17:13312, 1024:1) = aten::transpose(%x.28, %38, %42), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:562:0
  %input.144 : Float(13:17408, 17:1024, 1024:1) = aten::transpose(%encoder_hidden_states, %38, %42), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:563:0
  %1306 : __torch__.torch.nn.modules.normalization.___torch_mangle_1178.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1287)
  %1307 : __torch__.torch.nn.modules.linear.___torch_mangle_1177.Linear = prim::GetAttr[name="fc2"](%1287)
  %1308 : __torch__.torch.nn.modules.linear.___torch_mangle_1176.Linear = prim::GetAttr[name="fc1"](%1287)
  %1309 : __torch__.torch.nn.modules.normalization.___torch_mangle_1175.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1287)
  %1310 : __torch__.transformers.modeling_bart.___torch_mangle_1174.Attention = prim::GetAttr[name="encoder_attn"](%1287)
  %1311 : __torch__.torch.nn.modules.normalization.___torch_mangle_1169.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1287)
  %1312 : __torch__.transformers.modeling_bart.___torch_mangle_1168.Attention = prim::GetAttr[name="self_attn"](%1287)
  %1313 : __torch__.torch.nn.modules.linear.___torch_mangle_1167.Linear = prim::GetAttr[name="out_proj"](%1312)
  %1314 : __torch__.torch.nn.modules.linear.___torch_mangle_1165.Linear = prim::GetAttr[name="v_proj"](%1312)
  %1315 : __torch__.torch.nn.modules.linear.___torch_mangle_1164.Linear = prim::GetAttr[name="k_proj"](%1312)
  %1316 : __torch__.torch.nn.modules.linear.___torch_mangle_1166.Linear = prim::GetAttr[name="q_proj"](%1312)
  %1317 : int = aten::size(%query.13, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %1318 : int = aten::size(%query.13, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %bsz.13 : Long() = prim::NumToTensor(%1318), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1320 : int = aten::size(%query.13, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %1321 : Tensor = prim::GetAttr[name="bias"](%1316)
  %1322 : Tensor = prim::GetAttr[name="weight"](%1316)
  %1323 : Float(1024:1, 1024:1024) = aten::t(%1322), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.73 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.13, %1323), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1325 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.73, %1321, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.73 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1325, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
  %1327 : Tensor = prim::GetAttr[name="bias"](%1315)
  %1328 : Tensor = prim::GetAttr[name="weight"](%1315)
  %1329 : Float(1024:1, 1024:1024) = aten::t(%1328), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.74 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.13, %1329), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.75 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.74, %1327, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1332 : Tensor = prim::GetAttr[name="bias"](%1314)
  %1333 : Tensor = prim::GetAttr[name="weight"](%1314)
  %1334 : Float(1024:1, 1024:1024) = aten::t(%1333), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.75 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.13, %1334), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.77 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.75, %1332, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.74 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.73, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1338 : Long() = aten::mul(%bsz.13, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1339 : int = aten::Int(%1338), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1340 : int[] = prim::ListConstruct(%1317, %1339, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1341 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.74, %1340), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %q.13 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1341, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %tensor.76 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.75, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1344 : Long() = aten::mul(%bsz.13, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1345 : int = aten::Int(%1344), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1346 : int[] = prim::ListConstruct(%39, %1345, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1347 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.76, %1346), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %k.13 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1347, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %tensor.78 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.77, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1350 : Long() = aten::mul(%bsz.13, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1351 : int = aten::Int(%1350), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1352 : int[] = prim::ListConstruct(%39, %1351, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1353 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.78, %1352), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %v.13 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1353, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1355 : int = aten::size(%k.13, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:701:0
  %1356 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.13, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.49 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.13, %1356), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
  %1358 : int[] = prim::ListConstruct(%1318, %30, %1317, %1355), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1359 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.49, %1358), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.50 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1359, %attn_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:706:0
  %1361 : Long() = aten::mul(%bsz.13, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:707:0
  %1362 : int = aten::Int(%1361), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1363 : int[] = prim::ListConstruct(%1362, %1317, %1355), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %input.139 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.50, %1363), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:707:0
  %input.140 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.139, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.51 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.140, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # torch/nn/functional.py:973:0
  %attn_output.13 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.51, %v.13), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:730:0
  %1368 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.13, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %1369 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1368, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %1370 : int[] = prim::ListConstruct(%1317, %1318, %1320), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %input.141 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1369, %1370), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %1372 : Tensor = prim::GetAttr[name="bias"](%1313)
  %1373 : Tensor = prim::GetAttr[name="weight"](%1313)
  %1374 : Float(1024:1, 1024:1024) = aten::t(%1373), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.76 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.141, %1374), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.142 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.76, %1372, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.29 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.142, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
  %input.143 : Float(13:1024, 17:13312, 1024:1) = aten::add(%query.13, %x.29, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # transformers/modeling_bart.py:427:0
  %1379 : Tensor = prim::GetAttr[name="bias"](%1311)
  %1380 : Tensor = prim::GetAttr[name="weight"](%1311)
  %1381 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn_layer_norm
  %query.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.143, %1381, %1380, %1379, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1383 : __torch__.torch.nn.modules.linear.___torch_mangle_1173.Linear = prim::GetAttr[name="out_proj"](%1310)
  %1384 : __torch__.torch.nn.modules.linear.___torch_mangle_1171.Linear = prim::GetAttr[name="v_proj"](%1310)
  %1385 : __torch__.torch.nn.modules.linear.___torch_mangle_1170.Linear = prim::GetAttr[name="k_proj"](%1310)
  %1386 : __torch__.torch.nn.modules.linear.___torch_mangle_1172.Linear = prim::GetAttr[name="q_proj"](%1310)
  %1387 : int = aten::size(%query.14, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
  %1388 : int = aten::size(%query.14, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.14 : Long() = prim::NumToTensor(%1388), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1390 : int = aten::size(%query.14, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
  %1391 : Tensor = prim::GetAttr[name="bias"](%1386)
  %1392 : Tensor = prim::GetAttr[name="weight"](%1386)
  %1393 : Float(1024:1, 1024:1024) = aten::t(%1392), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.77 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.14, %1393), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %1395 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.77, %1391, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.79 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1395, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:673:0
  %1397 : Tensor = prim::GetAttr[name="bias"](%1385)
  %1398 : Tensor = prim::GetAttr[name="weight"](%1385)
  %1399 : Float(1024:1, 1024:1024) = aten::t(%1398), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.78 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1399), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.81 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.78, %1397, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %1402 : Tensor = prim::GetAttr[name="bias"](%1384)
  %1403 : Tensor = prim::GetAttr[name="weight"](%1384)
  %1404 : Float(1024:1, 1024:1024) = aten::t(%1403), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.79 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1404), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.83 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.79, %1402, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.80 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.79, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1408 : Long() = aten::mul(%bsz.14, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1409 : int = aten::Int(%1408), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1410 : int[] = prim::ListConstruct(%1387, %1409, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1411 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.80, %1410), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %q.14 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1411, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.82 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.81, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1414 : Long() = aten::mul(%bsz.14, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1415 : int = aten::Int(%1414), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1416 : int[] = prim::ListConstruct(%39, %1415, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1417 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.82, %1416), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %k.14 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1417, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.84 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.83, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1420 : Long() = aten::mul(%bsz.14, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1421 : int = aten::Int(%1420), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1422 : int[] = prim::ListConstruct(%39, %1421, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1423 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.84, %1422), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %v.14 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1423, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1425 : int = aten::size(%k.14, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:701:0
  %1426 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.14, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.52 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.14, %1426), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:702:0
  %1428 : int[] = prim::ListConstruct(%1388, %30, %1387, %1425), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %attn_weights.53 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.52, %1428), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:718:0
  %1430 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.13 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1430, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.54 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.53, %reshaped.13, %32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:720:0
  %1433 : Long() = aten::mul(%bsz.14, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:721:0
  %1434 : int = aten::Int(%1433), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1435 : int[] = prim::ListConstruct(%1434, %1387, %1425), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %input.145 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.54, %1435), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:721:0
  %input.146 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.145, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.55 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.146, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.14 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.55, %v.14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:730:0
  %1440 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.14, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
  %1441 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1440, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
  %1442 : int[] = prim::ListConstruct(%1387, %1388, %1390), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %input.147 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1441, %1442), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
  %1444 : Tensor = prim::GetAttr[name="bias"](%1383)
  %1445 : Tensor = prim::GetAttr[name="weight"](%1383)
  %1446 : Float(1024:1, 1024:1024) = aten::t(%1445), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.80 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.147, %1446), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.148 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.80, %1444, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.30 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.148, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
  %input.149 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.14, %x.30, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # transformers/modeling_bart.py:443:0
  %1451 : Tensor = prim::GetAttr[name="bias"](%1309)
  %1452 : Tensor = prim::GetAttr[name="weight"](%1309)
  %1453 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn_layer_norm
  %input.150 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.149, %1453, %1452, %1451, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %1455 : Tensor = prim::GetAttr[name="bias"](%1308)
  %1456 : Tensor = prim::GetAttr[name="weight"](%1308)
  %1457 : Float(1024:1, 4096:1024) = aten::t(%1456), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc1 # torch/nn/functional.py:1676:0
  %output.81 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.150, %1457), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc1 # torch/nn/functional.py:1676:0
  %input.151 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.81, %1455, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc1 # torch/nn/functional.py:1678:0
  %input.152 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.151), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:1369:0
  %input.153 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.152, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
  %1462 : Tensor = prim::GetAttr[name="bias"](%1307)
  %1463 : Tensor = prim::GetAttr[name="weight"](%1307)
  %1464 : Float(4096:1, 1024:4096) = aten::t(%1463), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc2 # torch/nn/functional.py:1676:0
  %output.82 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.153, %1464), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc2 # torch/nn/functional.py:1676:0
  %input.154 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.82, %1462, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc2 # torch/nn/functional.py:1678:0
  %x.31 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.154, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
  %input.155 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.150, %x.31, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # transformers/modeling_bart.py:455:0
  %1469 : Tensor = prim::GetAttr[name="bias"](%1306)
  %1470 : Tensor = prim::GetAttr[name="weight"](%1306)
  %1471 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.final_layer_norm
  %query.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.155, %1471, %1470, %1469, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
  %1473 : __torch__.torch.nn.modules.normalization.___torch_mangle_1194.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1285)
  %1474 : __torch__.torch.nn.modules.linear.___torch_mangle_1193.Linear = prim::GetAttr[name="fc2"](%1285)
  %1475 : __torch__.torch.nn.modules.linear.___torch_mangle_1192.Linear = prim::GetAttr[name="fc1"](%1285)
  %1476 : __torch__.torch.nn.modules.normalization.___torch_mangle_1191.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1285)
  %1477 : __torch__.transformers.modeling_bart.___torch_mangle_1190.Attention = prim::GetAttr[name="encoder_attn"](%1285)
  %1478 : __torch__.torch.nn.modules.normalization.___torch_mangle_1185.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1285)
  %1479 : __torch__.transformers.modeling_bart.___torch_mangle_1184.Attention = prim::GetAttr[name="self_attn"](%1285)
  %1480 : __torch__.torch.nn.modules.linear.___torch_mangle_1183.Linear = prim::GetAttr[name="out_proj"](%1479)
  %1481 : __torch__.torch.nn.modules.linear.___torch_mangle_1181.Linear = prim::GetAttr[name="v_proj"](%1479)
  %1482 : __torch__.torch.nn.modules.linear.___torch_mangle_1180.Linear = prim::GetAttr[name="k_proj"](%1479)
  %1483 : __torch__.torch.nn.modules.linear.___torch_mangle_1182.Linear = prim::GetAttr[name="q_proj"](%1479)
  %1484 : int = aten::size(%query.15, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %1485 : int = aten::size(%query.15, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %bsz.15 : Long() = prim::NumToTensor(%1485), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1487 : int = aten::size(%query.15, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %1488 : Tensor = prim::GetAttr[name="bias"](%1483)
  %1489 : Tensor = prim::GetAttr[name="weight"](%1483)
  %1490 : Float(1024:1, 1024:1024) = aten::t(%1489), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.83 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.15, %1490), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1492 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.83, %1488, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.85 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1492, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:673:0
  %1494 : Tensor = prim::GetAttr[name="bias"](%1482)
  %1495 : Tensor = prim::GetAttr[name="weight"](%1482)
  %1496 : Float(1024:1, 1024:1024) = aten::t(%1495), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.84 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.15, %1496), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.87 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.84, %1494, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1499 : Tensor = prim::GetAttr[name="bias"](%1481)
  %1500 : Tensor = prim::GetAttr[name="weight"](%1481)
  %1501 : Float(1024:1, 1024:1024) = aten::t(%1500), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.85 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.15, %1501), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.89 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.85, %1499, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.86 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.85, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1505 : Long() = aten::mul(%bsz.15, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1506 : int = aten::Int(%1505), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1507 : int[] = prim::ListConstruct(%1484, %1506, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1508 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.86, %1507), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %q.15 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1508, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %tensor.88 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.87, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1511 : Long() = aten::mul(%bsz.15, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1512 : int = aten::Int(%1511), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1513 : int[] = prim::ListConstruct(%39, %1512, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1514 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.88, %1513), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %k.15 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1514, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %tensor.90 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.89, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1517 : Long() = aten::mul(%bsz.15, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1518 : int = aten::Int(%1517), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1519 : int[] = prim::ListConstruct(%39, %1518, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1520 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.90, %1519), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %v.15 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1520, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1522 : int = aten::size(%k.15, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:701:0
  %1523 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.15, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.56 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.15, %1523), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
  %1525 : int[] = prim::ListConstruct(%1485, %30, %1484, %1522), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1526 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.56, %1525), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.57 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1526, %attn_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:706:0
  %1528 : Long() = aten::mul(%bsz.15, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:707:0
  %1529 : int = aten::Int(%1528), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1530 : int[] = prim::ListConstruct(%1529, %1484, %1522), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %input.156 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.57, %1530), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:707:0
  %input.157 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.156, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.58 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.157, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # torch/nn/functional.py:973:0
  %attn_output.15 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.58, %v.15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:730:0
  %1535 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.15, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %1536 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1535, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %1537 : int[] = prim::ListConstruct(%1484, %1485, %1487), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %input.158 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1536, %1537), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %1539 : Tensor = prim::GetAttr[name="bias"](%1480)
  %1540 : Tensor = prim::GetAttr[name="weight"](%1480)
  %1541 : Float(1024:1, 1024:1024) = aten::t(%1540), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.86 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.158, %1541), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.159 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.86, %1539, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.32 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.159, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
  %input.160 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.15, %x.32, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # transformers/modeling_bart.py:427:0
  %1546 : Tensor = prim::GetAttr[name="bias"](%1478)
  %1547 : Tensor = prim::GetAttr[name="weight"](%1478)
  %1548 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn_layer_norm
  %query.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.160, %1548, %1547, %1546, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1550 : __torch__.torch.nn.modules.linear.___torch_mangle_1189.Linear = prim::GetAttr[name="out_proj"](%1477)
  %1551 : __torch__.torch.nn.modules.linear.___torch_mangle_1187.Linear = prim::GetAttr[name="v_proj"](%1477)
  %1552 : __torch__.torch.nn.modules.linear.___torch_mangle_1186.Linear = prim::GetAttr[name="k_proj"](%1477)
  %1553 : __torch__.torch.nn.modules.linear.___torch_mangle_1188.Linear = prim::GetAttr[name="q_proj"](%1477)
  %1554 : int = aten::size(%query.16, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
  %1555 : int = aten::size(%query.16, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.16 : Long() = prim::NumToTensor(%1555), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1557 : int = aten::size(%query.16, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
  %1558 : Tensor = prim::GetAttr[name="bias"](%1553)
  %1559 : Tensor = prim::GetAttr[name="weight"](%1553)
  %1560 : Float(1024:1, 1024:1024) = aten::t(%1559), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.87 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.16, %1560), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %1562 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.87, %1558, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.91 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1562, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:673:0
  %1564 : Tensor = prim::GetAttr[name="bias"](%1552)
  %1565 : Tensor = prim::GetAttr[name="weight"](%1552)
  %1566 : Float(1024:1, 1024:1024) = aten::t(%1565), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.88 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1566), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.93 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.88, %1564, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %1569 : Tensor = prim::GetAttr[name="bias"](%1551)
  %1570 : Tensor = prim::GetAttr[name="weight"](%1551)
  %1571 : Float(1024:1, 1024:1024) = aten::t(%1570), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.89 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1571), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.95 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.89, %1569, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.92 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.91, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1575 : Long() = aten::mul(%bsz.16, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1576 : int = aten::Int(%1575), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1577 : int[] = prim::ListConstruct(%1554, %1576, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1578 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.92, %1577), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %q.16 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1578, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.94 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.93, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1581 : Long() = aten::mul(%bsz.16, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1582 : int = aten::Int(%1581), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1583 : int[] = prim::ListConstruct(%39, %1582, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1584 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.94, %1583), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %k.16 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1584, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.96 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.95, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1587 : Long() = aten::mul(%bsz.16, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1588 : int = aten::Int(%1587), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1589 : int[] = prim::ListConstruct(%39, %1588, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1590 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.96, %1589), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %v.16 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1590, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1592 : int = aten::size(%k.16, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:701:0
  %1593 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.16, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.59 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.16, %1593), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:702:0
  %1595 : int[] = prim::ListConstruct(%1555, %30, %1554, %1592), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %attn_weights.60 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.59, %1595), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:718:0
  %1597 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.14 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1597, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.61 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.60, %reshaped.14, %32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:720:0
  %1600 : Long() = aten::mul(%bsz.16, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:721:0
  %1601 : int = aten::Int(%1600), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1602 : int[] = prim::ListConstruct(%1601, %1554, %1592), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %input.161 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.61, %1602), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:721:0
  %input.162 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.161, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.62 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.162, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.16 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.62, %v.16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:730:0
  %1607 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.16, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
  %1608 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1607, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
  %1609 : int[] = prim::ListConstruct(%1554, %1555, %1557), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %input.163 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1608, %1609), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
  %1611 : Tensor = prim::GetAttr[name="bias"](%1550)
  %1612 : Tensor = prim::GetAttr[name="weight"](%1550)
  %1613 : Float(1024:1, 1024:1024) = aten::t(%1612), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.90 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.163, %1613), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.164 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.90, %1611, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.33 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.164, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
  %input.165 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.16, %x.33, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # transformers/modeling_bart.py:443:0
  %1618 : Tensor = prim::GetAttr[name="bias"](%1476)
  %1619 : Tensor = prim::GetAttr[name="weight"](%1476)
  %1620 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn_layer_norm
  %input.166 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.165, %1620, %1619, %1618, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %1622 : Tensor = prim::GetAttr[name="bias"](%1475)
  %1623 : Tensor = prim::GetAttr[name="weight"](%1475)
  %1624 : Float(1024:1, 4096:1024) = aten::t(%1623), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc1 # torch/nn/functional.py:1676:0
  %output.91 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.166, %1624), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc1 # torch/nn/functional.py:1676:0
  %input.167 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.91, %1622, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc1 # torch/nn/functional.py:1678:0
  %input.168 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.167), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:1369:0
  %input.169 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.168, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
  %1629 : Tensor = prim::GetAttr[name="bias"](%1474)
  %1630 : Tensor = prim::GetAttr[name="weight"](%1474)
  %1631 : Float(4096:1, 1024:4096) = aten::t(%1630), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc2 # torch/nn/functional.py:1676:0
  %output.92 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.169, %1631), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc2 # torch/nn/functional.py:1676:0
  %input.170 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.92, %1629, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc2 # torch/nn/functional.py:1678:0
  %x.34 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.170, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
  %input.171 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.166, %x.34, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # transformers/modeling_bart.py:455:0
  %1636 : Tensor = prim::GetAttr[name="bias"](%1473)
  %1637 : Tensor = prim::GetAttr[name="weight"](%1473)
  %1638 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.final_layer_norm
  %query.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.171, %1638, %1637, %1636, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
  %1640 : __torch__.torch.nn.modules.normalization.___torch_mangle_1210.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1283)
  %1641 : __torch__.torch.nn.modules.linear.___torch_mangle_1209.Linear = prim::GetAttr[name="fc2"](%1283)
  %1642 : __torch__.torch.nn.modules.linear.___torch_mangle_1208.Linear = prim::GetAttr[name="fc1"](%1283)
  %1643 : __torch__.torch.nn.modules.normalization.___torch_mangle_1207.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1283)
  %1644 : __torch__.transformers.modeling_bart.___torch_mangle_1206.Attention = prim::GetAttr[name="encoder_attn"](%1283)
  %1645 : __torch__.torch.nn.modules.normalization.___torch_mangle_1201.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1283)
  %1646 : __torch__.transformers.modeling_bart.___torch_mangle_1200.Attention = prim::GetAttr[name="self_attn"](%1283)
  %1647 : __torch__.torch.nn.modules.linear.___torch_mangle_1199.Linear = prim::GetAttr[name="out_proj"](%1646)
  %1648 : __torch__.torch.nn.modules.linear.___torch_mangle_1197.Linear = prim::GetAttr[name="v_proj"](%1646)
  %1649 : __torch__.torch.nn.modules.linear.___torch_mangle_1196.Linear = prim::GetAttr[name="k_proj"](%1646)
  %1650 : __torch__.torch.nn.modules.linear.___torch_mangle_1198.Linear = prim::GetAttr[name="q_proj"](%1646)
  %1651 : int = aten::size(%query.17, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %1652 : int = aten::size(%query.17, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %bsz.17 : Long() = prim::NumToTensor(%1652), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1654 : int = aten::size(%query.17, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %1655 : Tensor = prim::GetAttr[name="bias"](%1650)
  %1656 : Tensor = prim::GetAttr[name="weight"](%1650)
  %1657 : Float(1024:1, 1024:1024) = aten::t(%1656), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.93 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.17, %1657), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1659 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.93, %1655, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.97 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1659, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:673:0
  %1661 : Tensor = prim::GetAttr[name="bias"](%1649)
  %1662 : Tensor = prim::GetAttr[name="weight"](%1649)
  %1663 : Float(1024:1, 1024:1024) = aten::t(%1662), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.94 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.17, %1663), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.99 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.94, %1661, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1666 : Tensor = prim::GetAttr[name="bias"](%1648)
  %1667 : Tensor = prim::GetAttr[name="weight"](%1648)
  %1668 : Float(1024:1, 1024:1024) = aten::t(%1667), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.95 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.17, %1668), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.101 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.95, %1666, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.98 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.97, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1672 : Long() = aten::mul(%bsz.17, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1673 : int = aten::Int(%1672), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1674 : int[] = prim::ListConstruct(%1651, %1673, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1675 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.98, %1674), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %q.17 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1675, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %tensor.100 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.99, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1678 : Long() = aten::mul(%bsz.17, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1679 : int = aten::Int(%1678), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1680 : int[] = prim::ListConstruct(%39, %1679, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1681 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.100, %1680), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %k.17 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1681, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %tensor.102 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.101, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1684 : Long() = aten::mul(%bsz.17, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1685 : int = aten::Int(%1684), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1686 : int[] = prim::ListConstruct(%39, %1685, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1687 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.102, %1686), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %v.17 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1687, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1689 : int = aten::size(%k.17, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:701:0
  %1690 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.17, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.63 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.17, %1690), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
  %1692 : int[] = prim::ListConstruct(%1652, %30, %1651, %1689), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1693 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.63, %1692), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.64 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1693, %attn_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:706:0
  %1695 : Long() = aten::mul(%bsz.17, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:707:0
  %1696 : int = aten::Int(%1695), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1697 : int[] = prim::ListConstruct(%1696, %1651, %1689), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %input.172 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.64, %1697), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:707:0
  %input.173 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.172, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.65 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.173, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # torch/nn/functional.py:973:0
  %attn_output.17 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.65, %v.17), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:730:0
  %1702 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.17, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %1703 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1702, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %1704 : int[] = prim::ListConstruct(%1651, %1652, %1654), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %input.174 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1703, %1704), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %1706 : Tensor = prim::GetAttr[name="bias"](%1647)
  %1707 : Tensor = prim::GetAttr[name="weight"](%1647)
  %1708 : Float(1024:1, 1024:1024) = aten::t(%1707), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.96 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.174, %1708), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.175 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.96, %1706, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.35 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.175, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
  %input.176 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.17, %x.35, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # transformers/modeling_bart.py:427:0
  %1713 : Tensor = prim::GetAttr[name="bias"](%1645)
  %1714 : Tensor = prim::GetAttr[name="weight"](%1645)
  %1715 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn_layer_norm
  %query.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.176, %1715, %1714, %1713, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1717 : __torch__.torch.nn.modules.linear.___torch_mangle_1205.Linear = prim::GetAttr[name="out_proj"](%1644)
  %1718 : __torch__.torch.nn.modules.linear.___torch_mangle_1203.Linear = prim::GetAttr[name="v_proj"](%1644)
  %1719 : __torch__.torch.nn.modules.linear.___torch_mangle_1202.Linear = prim::GetAttr[name="k_proj"](%1644)
  %1720 : __torch__.torch.nn.modules.linear.___torch_mangle_1204.Linear = prim::GetAttr[name="q_proj"](%1644)
  %1721 : int = aten::size(%query.18, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
  %1722 : int = aten::size(%query.18, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.18 : Long() = prim::NumToTensor(%1722), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1724 : int = aten::size(%query.18, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
  %1725 : Tensor = prim::GetAttr[name="bias"](%1720)
  %1726 : Tensor = prim::GetAttr[name="weight"](%1720)
  %1727 : Float(1024:1, 1024:1024) = aten::t(%1726), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.97 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.18, %1727), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %1729 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.97, %1725, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.103 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1729, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:673:0
  %1731 : Tensor = prim::GetAttr[name="bias"](%1719)
  %1732 : Tensor = prim::GetAttr[name="weight"](%1719)
  %1733 : Float(1024:1, 1024:1024) = aten::t(%1732), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.98 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1733), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.105 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.98, %1731, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %1736 : Tensor = prim::GetAttr[name="bias"](%1718)
  %1737 : Tensor = prim::GetAttr[name="weight"](%1718)
  %1738 : Float(1024:1, 1024:1024) = aten::t(%1737), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.99 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1738), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.107 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.99, %1736, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.104 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.103, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1742 : Long() = aten::mul(%bsz.18, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1743 : int = aten::Int(%1742), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1744 : int[] = prim::ListConstruct(%1721, %1743, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1745 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.104, %1744), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %q.18 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1745, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.106 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.105, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1748 : Long() = aten::mul(%bsz.18, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1749 : int = aten::Int(%1748), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1750 : int[] = prim::ListConstruct(%39, %1749, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1751 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.106, %1750), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %k.18 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1751, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.108 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.107, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1754 : Long() = aten::mul(%bsz.18, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1755 : int = aten::Int(%1754), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1756 : int[] = prim::ListConstruct(%39, %1755, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1757 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.108, %1756), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %v.18 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1757, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1759 : int = aten::size(%k.18, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:701:0
  %1760 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.18, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.66 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.18, %1760), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:702:0
  %1762 : int[] = prim::ListConstruct(%1722, %30, %1721, %1759), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %attn_weights.67 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.66, %1762), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:718:0
  %1764 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.15 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1764, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.68 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.67, %reshaped.15, %32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:720:0
  %1767 : Long() = aten::mul(%bsz.18, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:721:0
  %1768 : int = aten::Int(%1767), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1769 : int[] = prim::ListConstruct(%1768, %1721, %1759), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %input.177 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.68, %1769), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:721:0
  %input.178 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.177, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.69 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.178, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.18 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.69, %v.18), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:730:0
  %1774 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.18, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
  %1775 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1774, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
  %1776 : int[] = prim::ListConstruct(%1721, %1722, %1724), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %input.179 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1775, %1776), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
  %1778 : Tensor = prim::GetAttr[name="bias"](%1717)
  %1779 : Tensor = prim::GetAttr[name="weight"](%1717)
  %1780 : Float(1024:1, 1024:1024) = aten::t(%1779), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.100 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.179, %1780), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.180 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.100, %1778, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.36 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.180, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
  %input.181 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.18, %x.36, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # transformers/modeling_bart.py:443:0
  %1785 : Tensor = prim::GetAttr[name="bias"](%1643)
  %1786 : Tensor = prim::GetAttr[name="weight"](%1643)
  %1787 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn_layer_norm
  %input.182 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.181, %1787, %1786, %1785, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %1789 : Tensor = prim::GetAttr[name="bias"](%1642)
  %1790 : Tensor = prim::GetAttr[name="weight"](%1642)
  %1791 : Float(1024:1, 4096:1024) = aten::t(%1790), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc1 # torch/nn/functional.py:1676:0
  %output.101 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.182, %1791), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc1 # torch/nn/functional.py:1676:0
  %input.183 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.101, %1789, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc1 # torch/nn/functional.py:1678:0
  %input.184 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.183), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:1369:0
  %input.185 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.184, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
  %1796 : Tensor = prim::GetAttr[name="bias"](%1641)
  %1797 : Tensor = prim::GetAttr[name="weight"](%1641)
  %1798 : Float(4096:1, 1024:4096) = aten::t(%1797), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc2 # torch/nn/functional.py:1676:0
  %output.102 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.185, %1798), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc2 # torch/nn/functional.py:1676:0
  %input.186 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.102, %1796, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc2 # torch/nn/functional.py:1678:0
  %x.37 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.186, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
  %input.187 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.182, %x.37, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # transformers/modeling_bart.py:455:0
  %1803 : Tensor = prim::GetAttr[name="bias"](%1640)
  %1804 : Tensor = prim::GetAttr[name="weight"](%1640)
  %1805 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.final_layer_norm
  %query.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.187, %1805, %1804, %1803, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
  %1807 : __torch__.torch.nn.modules.normalization.___torch_mangle_1226.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1281)
  %1808 : __torch__.torch.nn.modules.linear.___torch_mangle_1225.Linear = prim::GetAttr[name="fc2"](%1281)
  %1809 : __torch__.torch.nn.modules.linear.___torch_mangle_1224.Linear = prim::GetAttr[name="fc1"](%1281)
  %1810 : __torch__.torch.nn.modules.normalization.___torch_mangle_1223.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1281)
  %1811 : __torch__.transformers.modeling_bart.___torch_mangle_1222.Attention = prim::GetAttr[name="encoder_attn"](%1281)
  %1812 : __torch__.torch.nn.modules.normalization.___torch_mangle_1217.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1281)
  %1813 : __torch__.transformers.modeling_bart.___torch_mangle_1216.Attention = prim::GetAttr[name="self_attn"](%1281)
  %1814 : __torch__.torch.nn.modules.linear.___torch_mangle_1215.Linear = prim::GetAttr[name="out_proj"](%1813)
  %1815 : __torch__.torch.nn.modules.linear.___torch_mangle_1213.Linear = prim::GetAttr[name="v_proj"](%1813)
  %1816 : __torch__.torch.nn.modules.linear.___torch_mangle_1212.Linear = prim::GetAttr[name="k_proj"](%1813)
  %1817 : __torch__.torch.nn.modules.linear.___torch_mangle_1214.Linear = prim::GetAttr[name="q_proj"](%1813)
  %1818 : int = aten::size(%query.19, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %1819 : int = aten::size(%query.19, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %bsz.19 : Long() = prim::NumToTensor(%1819), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1821 : int = aten::size(%query.19, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %1822 : Tensor = prim::GetAttr[name="bias"](%1817)
  %1823 : Tensor = prim::GetAttr[name="weight"](%1817)
  %1824 : Float(1024:1, 1024:1024) = aten::t(%1823), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.103 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.19, %1824), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1826 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.103, %1822, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.109 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1826, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:673:0
  %1828 : Tensor = prim::GetAttr[name="bias"](%1816)
  %1829 : Tensor = prim::GetAttr[name="weight"](%1816)
  %1830 : Float(1024:1, 1024:1024) = aten::t(%1829), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.104 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.19, %1830), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.111 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.104, %1828, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1833 : Tensor = prim::GetAttr[name="bias"](%1815)
  %1834 : Tensor = prim::GetAttr[name="weight"](%1815)
  %1835 : Float(1024:1, 1024:1024) = aten::t(%1834), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.105 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.19, %1835), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.113 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.105, %1833, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.110 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.109, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1839 : Long() = aten::mul(%bsz.19, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1840 : int = aten::Int(%1839), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1841 : int[] = prim::ListConstruct(%1818, %1840, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1842 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.110, %1841), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %q.19 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1842, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %tensor.112 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.111, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1845 : Long() = aten::mul(%bsz.19, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1846 : int = aten::Int(%1845), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1847 : int[] = prim::ListConstruct(%39, %1846, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1848 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.112, %1847), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %k.19 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1848, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %tensor.114 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.113, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1851 : Long() = aten::mul(%bsz.19, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1852 : int = aten::Int(%1851), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1853 : int[] = prim::ListConstruct(%39, %1852, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1854 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.114, %1853), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %v.19 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1854, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1856 : int = aten::size(%k.19, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:701:0
  %1857 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.19, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.70 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.19, %1857), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
  %1859 : int[] = prim::ListConstruct(%1819, %30, %1818, %1856), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1860 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.70, %1859), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1860, %attn_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:706:0
  %1862 : Long() = aten::mul(%bsz.19, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:707:0
  %1863 : int = aten::Int(%1862), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1864 : int[] = prim::ListConstruct(%1863, %1818, %1856), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %input.188 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.71, %1864), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:707:0
  %input.189 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.188, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.72 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.189, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # torch/nn/functional.py:973:0
  %attn_output.19 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.72, %v.19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:730:0
  %1869 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.19, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %1870 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1869, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %1871 : int[] = prim::ListConstruct(%1818, %1819, %1821), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %input.190 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1870, %1871), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %1873 : Tensor = prim::GetAttr[name="bias"](%1814)
  %1874 : Tensor = prim::GetAttr[name="weight"](%1814)
  %1875 : Float(1024:1, 1024:1024) = aten::t(%1874), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.106 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.190, %1875), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.191 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.106, %1873, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.38 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.191, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
  %input.192 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.19, %x.38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # transformers/modeling_bart.py:427:0
  %1880 : Tensor = prim::GetAttr[name="bias"](%1812)
  %1881 : Tensor = prim::GetAttr[name="weight"](%1812)
  %1882 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn_layer_norm
  %query.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.192, %1882, %1881, %1880, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1884 : __torch__.torch.nn.modules.linear.___torch_mangle_1221.Linear = prim::GetAttr[name="out_proj"](%1811)
  %1885 : __torch__.torch.nn.modules.linear.___torch_mangle_1219.Linear = prim::GetAttr[name="v_proj"](%1811)
  %1886 : __torch__.torch.nn.modules.linear.___torch_mangle_1218.Linear = prim::GetAttr[name="k_proj"](%1811)
  %1887 : __torch__.torch.nn.modules.linear.___torch_mangle_1220.Linear = prim::GetAttr[name="q_proj"](%1811)
  %1888 : int = aten::size(%query.20, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
  %1889 : int = aten::size(%query.20, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.20 : Long() = prim::NumToTensor(%1889), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1891 : int = aten::size(%query.20, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
  %1892 : Tensor = prim::GetAttr[name="bias"](%1887)
  %1893 : Tensor = prim::GetAttr[name="weight"](%1887)
  %1894 : Float(1024:1, 1024:1024) = aten::t(%1893), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.107 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.20, %1894), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %1896 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.107, %1892, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.115 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1896, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:673:0
  %1898 : Tensor = prim::GetAttr[name="bias"](%1886)
  %1899 : Tensor = prim::GetAttr[name="weight"](%1886)
  %1900 : Float(1024:1, 1024:1024) = aten::t(%1899), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.108 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1900), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.117 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.108, %1898, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %1903 : Tensor = prim::GetAttr[name="bias"](%1885)
  %1904 : Tensor = prim::GetAttr[name="weight"](%1885)
  %1905 : Float(1024:1, 1024:1024) = aten::t(%1904), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.109 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1905), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.119 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.109, %1903, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.116 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.115, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1909 : Long() = aten::mul(%bsz.20, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1910 : int = aten::Int(%1909), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1911 : int[] = prim::ListConstruct(%1888, %1910, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1912 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.116, %1911), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %q.20 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1912, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.118 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.117, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1915 : Long() = aten::mul(%bsz.20, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1916 : int = aten::Int(%1915), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1917 : int[] = prim::ListConstruct(%39, %1916, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1918 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.118, %1917), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %k.20 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1918, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.120 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.119, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1921 : Long() = aten::mul(%bsz.20, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1922 : int = aten::Int(%1921), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1923 : int[] = prim::ListConstruct(%39, %1922, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1924 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.120, %1923), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %v.20 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1924, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1926 : int = aten::size(%k.20, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:701:0
  %1927 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.20, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.73 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.20, %1927), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:702:0
  %1929 : int[] = prim::ListConstruct(%1889, %30, %1888, %1926), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %attn_weights.74 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.73, %1929), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:718:0
  %1931 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.16 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1931, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.74, %reshaped.16, %32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:720:0
  %1934 : Long() = aten::mul(%bsz.20, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:721:0
  %1935 : int = aten::Int(%1934), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1936 : int[] = prim::ListConstruct(%1935, %1888, %1926), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %input.193 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.75, %1936), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:721:0
  %input.194 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.193, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.76 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.194, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.20 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.76, %v.20), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:730:0
  %1941 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.20, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
  %1942 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1941, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
  %1943 : int[] = prim::ListConstruct(%1888, %1889, %1891), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %input.195 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1942, %1943), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
  %1945 : Tensor = prim::GetAttr[name="bias"](%1884)
  %1946 : Tensor = prim::GetAttr[name="weight"](%1884)
  %1947 : Float(1024:1, 1024:1024) = aten::t(%1946), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.110 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.195, %1947), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.196 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.110, %1945, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.39 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.196, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
  %input.197 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.20, %x.39, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # transformers/modeling_bart.py:443:0
  %1952 : Tensor = prim::GetAttr[name="bias"](%1810)
  %1953 : Tensor = prim::GetAttr[name="weight"](%1810)
  %1954 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn_layer_norm
  %input.198 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.197, %1954, %1953, %1952, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %1956 : Tensor = prim::GetAttr[name="bias"](%1809)
  %1957 : Tensor = prim::GetAttr[name="weight"](%1809)
  %1958 : Float(1024:1, 4096:1024) = aten::t(%1957), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc1 # torch/nn/functional.py:1676:0
  %output.111 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.198, %1958), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc1 # torch/nn/functional.py:1676:0
  %input.199 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.111, %1956, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc1 # torch/nn/functional.py:1678:0
  %input.200 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.199), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:1369:0
  %input.201 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.200, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
  %1963 : Tensor = prim::GetAttr[name="bias"](%1808)
  %1964 : Tensor = prim::GetAttr[name="weight"](%1808)
  %1965 : Float(4096:1, 1024:4096) = aten::t(%1964), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc2 # torch/nn/functional.py:1676:0
  %output.112 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.201, %1965), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc2 # torch/nn/functional.py:1676:0
  %input.202 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.112, %1963, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc2 # torch/nn/functional.py:1678:0
  %x.40 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.202, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
  %input.203 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.198, %x.40, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # transformers/modeling_bart.py:455:0
  %1970 : Tensor = prim::GetAttr[name="bias"](%1807)
  %1971 : Tensor = prim::GetAttr[name="weight"](%1807)
  %1972 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.final_layer_norm
  %query.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.203, %1972, %1971, %1970, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
  %1974 : __torch__.torch.nn.modules.normalization.___torch_mangle_1242.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1279)
  %1975 : __torch__.torch.nn.modules.linear.___torch_mangle_1241.Linear = prim::GetAttr[name="fc2"](%1279)
  %1976 : __torch__.torch.nn.modules.linear.___torch_mangle_1240.Linear = prim::GetAttr[name="fc1"](%1279)
  %1977 : __torch__.torch.nn.modules.normalization.___torch_mangle_1239.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1279)
  %1978 : __torch__.transformers.modeling_bart.___torch_mangle_1238.Attention = prim::GetAttr[name="encoder_attn"](%1279)
  %1979 : __torch__.torch.nn.modules.normalization.___torch_mangle_1233.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1279)
  %1980 : __torch__.transformers.modeling_bart.___torch_mangle_1232.Attention = prim::GetAttr[name="self_attn"](%1279)
  %1981 : __torch__.torch.nn.modules.linear.___torch_mangle_1231.Linear = prim::GetAttr[name="out_proj"](%1980)
  %1982 : __torch__.torch.nn.modules.linear.___torch_mangle_1229.Linear = prim::GetAttr[name="v_proj"](%1980)
  %1983 : __torch__.torch.nn.modules.linear.___torch_mangle_1228.Linear = prim::GetAttr[name="k_proj"](%1980)
  %1984 : __torch__.torch.nn.modules.linear.___torch_mangle_1230.Linear = prim::GetAttr[name="q_proj"](%1980)
  %1985 : int = aten::size(%query.21, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %1986 : int = aten::size(%query.21, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %bsz.21 : Long() = prim::NumToTensor(%1986), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %1988 : int = aten::size(%query.21, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %1989 : Tensor = prim::GetAttr[name="bias"](%1984)
  %1990 : Tensor = prim::GetAttr[name="weight"](%1984)
  %1991 : Float(1024:1, 1024:1024) = aten::t(%1990), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.113 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.21, %1991), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1993 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.113, %1989, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.121 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1993, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:673:0
  %1995 : Tensor = prim::GetAttr[name="bias"](%1983)
  %1996 : Tensor = prim::GetAttr[name="weight"](%1983)
  %1997 : Float(1024:1, 1024:1024) = aten::t(%1996), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.114 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.21, %1997), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.123 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.114, %1995, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2000 : Tensor = prim::GetAttr[name="bias"](%1982)
  %2001 : Tensor = prim::GetAttr[name="weight"](%1982)
  %2002 : Float(1024:1, 1024:1024) = aten::t(%2001), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.115 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.21, %2002), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.125 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.115, %2000, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.122 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.121, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2006 : Long() = aten::mul(%bsz.21, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2007 : int = aten::Int(%2006), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2008 : int[] = prim::ListConstruct(%1985, %2007, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2009 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.122, %2008), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %q.21 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2009, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %tensor.124 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.123, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2012 : Long() = aten::mul(%bsz.21, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2013 : int = aten::Int(%2012), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2014 : int[] = prim::ListConstruct(%39, %2013, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2015 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.124, %2014), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %k.21 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2015, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %tensor.126 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.125, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2018 : Long() = aten::mul(%bsz.21, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2019 : int = aten::Int(%2018), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2020 : int[] = prim::ListConstruct(%39, %2019, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2021 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.126, %2020), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %v.21 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2021, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2023 : int = aten::size(%k.21, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:701:0
  %2024 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.21, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.77 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.21, %2024), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
  %2026 : int[] = prim::ListConstruct(%1986, %30, %1985, %2023), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2027 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.77, %2026), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.78 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2027, %attn_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:706:0
  %2029 : Long() = aten::mul(%bsz.21, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:707:0
  %2030 : int = aten::Int(%2029), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2031 : int[] = prim::ListConstruct(%2030, %1985, %2023), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %input.204 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.78, %2031), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:707:0
  %input.205 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.204, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.79 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.205, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # torch/nn/functional.py:973:0
  %attn_output.21 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.79, %v.21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:730:0
  %2036 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.21, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %2037 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2036, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %2038 : int[] = prim::ListConstruct(%1985, %1986, %1988), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %input.206 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2037, %2038), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %2040 : Tensor = prim::GetAttr[name="bias"](%1981)
  %2041 : Tensor = prim::GetAttr[name="weight"](%1981)
  %2042 : Float(1024:1, 1024:1024) = aten::t(%2041), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.116 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.206, %2042), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.207 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.116, %2040, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.41 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.207, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
  %input.208 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.21, %x.41, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # transformers/modeling_bart.py:427:0
  %2047 : Tensor = prim::GetAttr[name="bias"](%1979)
  %2048 : Tensor = prim::GetAttr[name="weight"](%1979)
  %2049 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn_layer_norm
  %query.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.208, %2049, %2048, %2047, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2051 : __torch__.torch.nn.modules.linear.___torch_mangle_1237.Linear = prim::GetAttr[name="out_proj"](%1978)
  %2052 : __torch__.torch.nn.modules.linear.___torch_mangle_1235.Linear = prim::GetAttr[name="v_proj"](%1978)
  %2053 : __torch__.torch.nn.modules.linear.___torch_mangle_1234.Linear = prim::GetAttr[name="k_proj"](%1978)
  %2054 : __torch__.torch.nn.modules.linear.___torch_mangle_1236.Linear = prim::GetAttr[name="q_proj"](%1978)
  %2055 : int = aten::size(%query.22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
  %2056 : int = aten::size(%query.22, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.22 : Long() = prim::NumToTensor(%2056), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2058 : int = aten::size(%query.22, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
  %2059 : Tensor = prim::GetAttr[name="bias"](%2054)
  %2060 : Tensor = prim::GetAttr[name="weight"](%2054)
  %2061 : Float(1024:1, 1024:1024) = aten::t(%2060), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.117 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.22, %2061), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2063 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.117, %2059, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.127 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2063, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:673:0
  %2065 : Tensor = prim::GetAttr[name="bias"](%2053)
  %2066 : Tensor = prim::GetAttr[name="weight"](%2053)
  %2067 : Float(1024:1, 1024:1024) = aten::t(%2066), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.118 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2067), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.129 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.118, %2065, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2070 : Tensor = prim::GetAttr[name="bias"](%2052)
  %2071 : Tensor = prim::GetAttr[name="weight"](%2052)
  %2072 : Float(1024:1, 1024:1024) = aten::t(%2071), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.119 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2072), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.131 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.119, %2070, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.128 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.127, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2076 : Long() = aten::mul(%bsz.22, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2077 : int = aten::Int(%2076), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2078 : int[] = prim::ListConstruct(%2055, %2077, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2079 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.128, %2078), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %q.22 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2079, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.130 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.129, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2082 : Long() = aten::mul(%bsz.22, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2083 : int = aten::Int(%2082), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2084 : int[] = prim::ListConstruct(%39, %2083, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2085 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.130, %2084), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %k.22 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2085, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.132 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.131, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2088 : Long() = aten::mul(%bsz.22, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2089 : int = aten::Int(%2088), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2090 : int[] = prim::ListConstruct(%39, %2089, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2091 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.132, %2090), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %v.22 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2091, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2093 : int = aten::size(%k.22, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:701:0
  %2094 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.22, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.80 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.22, %2094), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:702:0
  %2096 : int[] = prim::ListConstruct(%2056, %30, %2055, %2093), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %attn_weights.81 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.80, %2096), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:718:0
  %2098 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.17 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2098, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.82 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.81, %reshaped.17, %32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:720:0
  %2101 : Long() = aten::mul(%bsz.22, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:721:0
  %2102 : int = aten::Int(%2101), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2103 : int[] = prim::ListConstruct(%2102, %2055, %2093), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %input.209 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.82, %2103), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:721:0
  %input.210 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.209, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.83 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.210, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.22 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.83, %v.22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:730:0
  %2108 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.22, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
  %2109 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2108, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
  %2110 : int[] = prim::ListConstruct(%2055, %2056, %2058), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %input.211 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2109, %2110), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
  %2112 : Tensor = prim::GetAttr[name="bias"](%2051)
  %2113 : Tensor = prim::GetAttr[name="weight"](%2051)
  %2114 : Float(1024:1, 1024:1024) = aten::t(%2113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.120 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.211, %2114), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.212 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.120, %2112, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.42 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.212, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
  %input.213 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.22, %x.42, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # transformers/modeling_bart.py:443:0
  %2119 : Tensor = prim::GetAttr[name="bias"](%1977)
  %2120 : Tensor = prim::GetAttr[name="weight"](%1977)
  %2121 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn_layer_norm
  %input.214 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.213, %2121, %2120, %2119, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2123 : Tensor = prim::GetAttr[name="bias"](%1976)
  %2124 : Tensor = prim::GetAttr[name="weight"](%1976)
  %2125 : Float(1024:1, 4096:1024) = aten::t(%2124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc1 # torch/nn/functional.py:1676:0
  %output.121 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.214, %2125), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc1 # torch/nn/functional.py:1676:0
  %input.215 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.121, %2123, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc1 # torch/nn/functional.py:1678:0
  %input.216 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.215), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:1369:0
  %input.217 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.216, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
  %2130 : Tensor = prim::GetAttr[name="bias"](%1975)
  %2131 : Tensor = prim::GetAttr[name="weight"](%1975)
  %2132 : Float(4096:1, 1024:4096) = aten::t(%2131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc2 # torch/nn/functional.py:1676:0
  %output.122 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.217, %2132), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc2 # torch/nn/functional.py:1676:0
  %input.218 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.122, %2130, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc2 # torch/nn/functional.py:1678:0
  %x.43 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.218, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
  %input.219 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.214, %x.43, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # transformers/modeling_bart.py:455:0
  %2137 : Tensor = prim::GetAttr[name="bias"](%1974)
  %2138 : Tensor = prim::GetAttr[name="weight"](%1974)
  %2139 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.final_layer_norm
  %query.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.219, %2139, %2138, %2137, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
  %2141 : __torch__.torch.nn.modules.normalization.___torch_mangle_1258.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1277)
  %2142 : __torch__.torch.nn.modules.linear.___torch_mangle_1257.Linear = prim::GetAttr[name="fc2"](%1277)
  %2143 : __torch__.torch.nn.modules.linear.___torch_mangle_1256.Linear = prim::GetAttr[name="fc1"](%1277)
  %2144 : __torch__.torch.nn.modules.normalization.___torch_mangle_1255.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1277)
  %2145 : __torch__.transformers.modeling_bart.___torch_mangle_1254.Attention = prim::GetAttr[name="encoder_attn"](%1277)
  %2146 : __torch__.torch.nn.modules.normalization.___torch_mangle_1249.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1277)
  %2147 : __torch__.transformers.modeling_bart.___torch_mangle_1248.Attention = prim::GetAttr[name="self_attn"](%1277)
  %2148 : __torch__.torch.nn.modules.linear.___torch_mangle_1247.Linear = prim::GetAttr[name="out_proj"](%2147)
  %2149 : __torch__.torch.nn.modules.linear.___torch_mangle_1245.Linear = prim::GetAttr[name="v_proj"](%2147)
  %2150 : __torch__.torch.nn.modules.linear.___torch_mangle_1244.Linear = prim::GetAttr[name="k_proj"](%2147)
  %2151 : __torch__.torch.nn.modules.linear.___torch_mangle_1246.Linear = prim::GetAttr[name="q_proj"](%2147)
  %2152 : int = aten::size(%query.23, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %2153 : int = aten::size(%query.23, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %bsz.23 : Long() = prim::NumToTensor(%2153), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2155 : int = aten::size(%query.23, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %2156 : Tensor = prim::GetAttr[name="bias"](%2151)
  %2157 : Tensor = prim::GetAttr[name="weight"](%2151)
  %2158 : Float(1024:1, 1024:1024) = aten::t(%2157), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.123 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.23, %2158), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2160 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.123, %2156, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.133 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2160, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:673:0
  %2162 : Tensor = prim::GetAttr[name="bias"](%2150)
  %2163 : Tensor = prim::GetAttr[name="weight"](%2150)
  %2164 : Float(1024:1, 1024:1024) = aten::t(%2163), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.124 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.23, %2164), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.135 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.124, %2162, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2167 : Tensor = prim::GetAttr[name="bias"](%2149)
  %2168 : Tensor = prim::GetAttr[name="weight"](%2149)
  %2169 : Float(1024:1, 1024:1024) = aten::t(%2168), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.125 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.23, %2169), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.137 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.125, %2167, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.134 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.133, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2173 : Long() = aten::mul(%bsz.23, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2174 : int = aten::Int(%2173), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2175 : int[] = prim::ListConstruct(%2152, %2174, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2176 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.134, %2175), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %q.23 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2176, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %tensor.136 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.135, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2179 : Long() = aten::mul(%bsz.23, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2180 : int = aten::Int(%2179), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2181 : int[] = prim::ListConstruct(%39, %2180, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2182 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.136, %2181), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %k.23 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2182, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %tensor.138 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.137, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2185 : Long() = aten::mul(%bsz.23, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2186 : int = aten::Int(%2185), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2187 : int[] = prim::ListConstruct(%39, %2186, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2188 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.138, %2187), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %v.23 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2188, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2190 : int = aten::size(%k.23, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:701:0
  %2191 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.23, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.84 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.23, %2191), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
  %2193 : int[] = prim::ListConstruct(%2153, %30, %2152, %2190), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2194 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.84, %2193), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2194, %attn_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:706:0
  %2196 : Long() = aten::mul(%bsz.23, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:707:0
  %2197 : int = aten::Int(%2196), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2198 : int[] = prim::ListConstruct(%2197, %2152, %2190), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %input.220 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.85, %2198), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:707:0
  %input.221 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.220, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.86 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.221, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # torch/nn/functional.py:973:0
  %attn_output.23 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.86, %v.23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:730:0
  %2203 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.23, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %2204 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2203, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %2205 : int[] = prim::ListConstruct(%2152, %2153, %2155), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %input.222 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2204, %2205), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %2207 : Tensor = prim::GetAttr[name="bias"](%2148)
  %2208 : Tensor = prim::GetAttr[name="weight"](%2148)
  %2209 : Float(1024:1, 1024:1024) = aten::t(%2208), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.126 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.222, %2209), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.223 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.126, %2207, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.44 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.223, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
  %input.224 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.23, %x.44, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # transformers/modeling_bart.py:427:0
  %2214 : Tensor = prim::GetAttr[name="bias"](%2146)
  %2215 : Tensor = prim::GetAttr[name="weight"](%2146)
  %2216 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn_layer_norm
  %query.24 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.224, %2216, %2215, %2214, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2218 : __torch__.torch.nn.modules.linear.___torch_mangle_1253.Linear = prim::GetAttr[name="out_proj"](%2145)
  %2219 : __torch__.torch.nn.modules.linear.___torch_mangle_1251.Linear = prim::GetAttr[name="v_proj"](%2145)
  %2220 : __torch__.torch.nn.modules.linear.___torch_mangle_1250.Linear = prim::GetAttr[name="k_proj"](%2145)
  %2221 : __torch__.torch.nn.modules.linear.___torch_mangle_1252.Linear = prim::GetAttr[name="q_proj"](%2145)
  %2222 : int = aten::size(%query.24, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
  %2223 : int = aten::size(%query.24, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.24 : Long() = prim::NumToTensor(%2223), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2225 : int = aten::size(%query.24, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
  %2226 : Tensor = prim::GetAttr[name="bias"](%2221)
  %2227 : Tensor = prim::GetAttr[name="weight"](%2221)
  %2228 : Float(1024:1, 1024:1024) = aten::t(%2227), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.127 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.24, %2228), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2230 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.127, %2226, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.139 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2230, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:673:0
  %2232 : Tensor = prim::GetAttr[name="bias"](%2220)
  %2233 : Tensor = prim::GetAttr[name="weight"](%2220)
  %2234 : Float(1024:1, 1024:1024) = aten::t(%2233), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.128 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2234), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.141 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.128, %2232, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2237 : Tensor = prim::GetAttr[name="bias"](%2219)
  %2238 : Tensor = prim::GetAttr[name="weight"](%2219)
  %2239 : Float(1024:1, 1024:1024) = aten::t(%2238), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.129 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2239), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.143 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.129, %2237, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.140 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.139, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2243 : Long() = aten::mul(%bsz.24, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2244 : int = aten::Int(%2243), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2245 : int[] = prim::ListConstruct(%2222, %2244, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2246 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.140, %2245), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %q.24 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2246, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.142 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.141, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2249 : Long() = aten::mul(%bsz.24, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2250 : int = aten::Int(%2249), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2251 : int[] = prim::ListConstruct(%39, %2250, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2252 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.142, %2251), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %k.24 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2252, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.144 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.143, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2255 : Long() = aten::mul(%bsz.24, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2256 : int = aten::Int(%2255), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2257 : int[] = prim::ListConstruct(%39, %2256, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2258 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.144, %2257), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %v.24 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2258, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2260 : int = aten::size(%k.24, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:701:0
  %2261 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.24, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.87 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.24, %2261), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:702:0
  %2263 : int[] = prim::ListConstruct(%2223, %30, %2222, %2260), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %attn_weights.88 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.87, %2263), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:718:0
  %2265 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.18 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2265, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.89 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.88, %reshaped.18, %32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:720:0
  %2268 : Long() = aten::mul(%bsz.24, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:721:0
  %2269 : int = aten::Int(%2268), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2270 : int[] = prim::ListConstruct(%2269, %2222, %2260), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %input.225 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.89, %2270), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:721:0
  %input.226 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.225, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.90 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.226, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.24 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.90, %v.24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:730:0
  %2275 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.24, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
  %2276 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2275, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
  %2277 : int[] = prim::ListConstruct(%2222, %2223, %2225), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %input.227 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2276, %2277), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
  %2279 : Tensor = prim::GetAttr[name="bias"](%2218)
  %2280 : Tensor = prim::GetAttr[name="weight"](%2218)
  %2281 : Float(1024:1, 1024:1024) = aten::t(%2280), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.130 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.227, %2281), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.228 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.130, %2279, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.45 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.228, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
  %input.229 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.24, %x.45, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # transformers/modeling_bart.py:443:0
  %2286 : Tensor = prim::GetAttr[name="bias"](%2144)
  %2287 : Tensor = prim::GetAttr[name="weight"](%2144)
  %2288 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn_layer_norm
  %input.230 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.229, %2288, %2287, %2286, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2290 : Tensor = prim::GetAttr[name="bias"](%2143)
  %2291 : Tensor = prim::GetAttr[name="weight"](%2143)
  %2292 : Float(1024:1, 4096:1024) = aten::t(%2291), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc1 # torch/nn/functional.py:1676:0
  %output.131 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.230, %2292), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc1 # torch/nn/functional.py:1676:0
  %input.231 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.131, %2290, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc1 # torch/nn/functional.py:1678:0
  %input.232 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.231), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:1369:0
  %input.233 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.232, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
  %2297 : Tensor = prim::GetAttr[name="bias"](%2142)
  %2298 : Tensor = prim::GetAttr[name="weight"](%2142)
  %2299 : Float(4096:1, 1024:4096) = aten::t(%2298), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc2 # torch/nn/functional.py:1676:0
  %output.132 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.233, %2299), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc2 # torch/nn/functional.py:1676:0
  %input.234 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.132, %2297, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc2 # torch/nn/functional.py:1678:0
  %x.46 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.234, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
  %input.235 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.230, %x.46, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # transformers/modeling_bart.py:455:0
  %2304 : Tensor = prim::GetAttr[name="bias"](%2141)
  %2305 : Tensor = prim::GetAttr[name="weight"](%2141)
  %2306 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.final_layer_norm
  %query.25 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.235, %2306, %2305, %2304, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
  %2308 : __torch__.torch.nn.modules.normalization.___torch_mangle_1274.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1275)
  %2309 : __torch__.torch.nn.modules.linear.___torch_mangle_1273.Linear = prim::GetAttr[name="fc2"](%1275)
  %2310 : __torch__.torch.nn.modules.linear.___torch_mangle_1272.Linear = prim::GetAttr[name="fc1"](%1275)
  %2311 : __torch__.torch.nn.modules.normalization.___torch_mangle_1271.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1275)
  %2312 : __torch__.transformers.modeling_bart.___torch_mangle_1270.Attention = prim::GetAttr[name="encoder_attn"](%1275)
  %2313 : __torch__.torch.nn.modules.normalization.___torch_mangle_1265.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1275)
  %2314 : __torch__.transformers.modeling_bart.___torch_mangle_1264.Attention = prim::GetAttr[name="self_attn"](%1275)
  %2315 : __torch__.torch.nn.modules.linear.___torch_mangle_1263.Linear = prim::GetAttr[name="out_proj"](%2314)
  %2316 : __torch__.torch.nn.modules.linear.___torch_mangle_1261.Linear = prim::GetAttr[name="v_proj"](%2314)
  %2317 : __torch__.torch.nn.modules.linear.___torch_mangle_1260.Linear = prim::GetAttr[name="k_proj"](%2314)
  %2318 : __torch__.torch.nn.modules.linear.___torch_mangle_1262.Linear = prim::GetAttr[name="q_proj"](%2314)
  %2319 : int = aten::size(%query.25, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %2320 : int = aten::size(%query.25, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %bsz.25 : Long() = prim::NumToTensor(%2320), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2322 : int = aten::size(%query.25, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %2323 : Tensor = prim::GetAttr[name="bias"](%2318)
  %2324 : Tensor = prim::GetAttr[name="weight"](%2318)
  %2325 : Float(1024:1, 1024:1024) = aten::t(%2324), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.133 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.25, %2325), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2327 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.133, %2323, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.145 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2327, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:673:0
  %2329 : Tensor = prim::GetAttr[name="bias"](%2317)
  %2330 : Tensor = prim::GetAttr[name="weight"](%2317)
  %2331 : Float(1024:1, 1024:1024) = aten::t(%2330), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.134 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.25, %2331), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.147 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.134, %2329, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2334 : Tensor = prim::GetAttr[name="bias"](%2316)
  %2335 : Tensor = prim::GetAttr[name="weight"](%2316)
  %2336 : Float(1024:1, 1024:1024) = aten::t(%2335), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.135 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.25, %2336), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.149 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.135, %2334, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.146 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.145, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2340 : Long() = aten::mul(%bsz.25, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2341 : int = aten::Int(%2340), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2342 : int[] = prim::ListConstruct(%2319, %2341, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2343 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.146, %2342), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %q.25 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2343, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %tensor.148 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.147, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2346 : Long() = aten::mul(%bsz.25, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2347 : int = aten::Int(%2346), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2348 : int[] = prim::ListConstruct(%39, %2347, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2349 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.148, %2348), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %k.25 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2349, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %tensor.150 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.149, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2352 : Long() = aten::mul(%bsz.25, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2353 : int = aten::Int(%2352), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2354 : int[] = prim::ListConstruct(%39, %2353, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2355 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.150, %2354), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %v.25 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2355, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2357 : int = aten::size(%k.25, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:701:0
  %2358 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.25, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.91 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.25, %2358), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
  %2360 : int[] = prim::ListConstruct(%2320, %30, %2319, %2357), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2361 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.91, %2360), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.92 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2361, %attn_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:706:0
  %2363 : Long() = aten::mul(%bsz.25, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:707:0
  %2364 : int = aten::Int(%2363), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2365 : int[] = prim::ListConstruct(%2364, %2319, %2357), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %input.236 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.92, %2365), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:707:0
  %input.237 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.236, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.93 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.237, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # torch/nn/functional.py:973:0
  %attn_output.25 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.93, %v.25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:730:0
  %2370 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.25, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %2371 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2370, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %2372 : int[] = prim::ListConstruct(%2319, %2320, %2322), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %input.238 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2371, %2372), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %2374 : Tensor = prim::GetAttr[name="bias"](%2315)
  %2375 : Tensor = prim::GetAttr[name="weight"](%2315)
  %2376 : Float(1024:1, 1024:1024) = aten::t(%2375), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.136 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.238, %2376), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.239 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.136, %2374, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.47 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.239, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
  %input.240 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.25, %x.47, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # transformers/modeling_bart.py:427:0
  %2381 : Tensor = prim::GetAttr[name="bias"](%2313)
  %2382 : Tensor = prim::GetAttr[name="weight"](%2313)
  %2383 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn_layer_norm
  %query.26 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.240, %2383, %2382, %2381, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2385 : __torch__.torch.nn.modules.linear.___torch_mangle_1269.Linear = prim::GetAttr[name="out_proj"](%2312)
  %2386 : __torch__.torch.nn.modules.linear.___torch_mangle_1267.Linear = prim::GetAttr[name="v_proj"](%2312)
  %2387 : __torch__.torch.nn.modules.linear.___torch_mangle_1266.Linear = prim::GetAttr[name="k_proj"](%2312)
  %2388 : __torch__.torch.nn.modules.linear.___torch_mangle_1268.Linear = prim::GetAttr[name="q_proj"](%2312)
  %2389 : int = aten::size(%query.26, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
  %2390 : int = aten::size(%query.26, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.26 : Long() = prim::NumToTensor(%2390), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2392 : int = aten::size(%query.26, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
  %2393 : Tensor = prim::GetAttr[name="bias"](%2388)
  %2394 : Tensor = prim::GetAttr[name="weight"](%2388)
  %2395 : Float(1024:1, 1024:1024) = aten::t(%2394), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.137 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.26, %2395), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2397 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.137, %2393, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.151 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2397, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:673:0
  %2399 : Tensor = prim::GetAttr[name="bias"](%2387)
  %2400 : Tensor = prim::GetAttr[name="weight"](%2387)
  %2401 : Float(1024:1, 1024:1024) = aten::t(%2400), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.138 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2401), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.153 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.138, %2399, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2404 : Tensor = prim::GetAttr[name="bias"](%2386)
  %2405 : Tensor = prim::GetAttr[name="weight"](%2386)
  %2406 : Float(1024:1, 1024:1024) = aten::t(%2405), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.139 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2406), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.155 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.139, %2404, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.152 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.151, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2410 : Long() = aten::mul(%bsz.26, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2411 : int = aten::Int(%2410), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2412 : int[] = prim::ListConstruct(%2389, %2411, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2413 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.152, %2412), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %q.26 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2413, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.154 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.153, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2416 : Long() = aten::mul(%bsz.26, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2417 : int = aten::Int(%2416), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2418 : int[] = prim::ListConstruct(%39, %2417, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2419 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.154, %2418), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %k.26 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2419, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.156 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.155, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2422 : Long() = aten::mul(%bsz.26, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2423 : int = aten::Int(%2422), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2424 : int[] = prim::ListConstruct(%39, %2423, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2425 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.156, %2424), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %v.26 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2425, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2427 : int = aten::size(%k.26, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:701:0
  %2428 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.26, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.94 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.26, %2428), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:702:0
  %2430 : int[] = prim::ListConstruct(%2390, %30, %2389, %2427), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %attn_weights.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.94, %2430), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:718:0
  %2432 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.19 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2432, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.96 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.95, %reshaped.19, %32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:720:0
  %2435 : Long() = aten::mul(%bsz.26, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:721:0
  %2436 : int = aten::Int(%2435), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2437 : int[] = prim::ListConstruct(%2436, %2389, %2427), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %input.241 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.96, %2437), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:721:0
  %input.242 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.241, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.97 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.242, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.26 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.97, %v.26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:730:0
  %2442 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.26, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
  %2443 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2442, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
  %2444 : int[] = prim::ListConstruct(%2389, %2390, %2392), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %input.243 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2443, %2444), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
  %2446 : Tensor = prim::GetAttr[name="bias"](%2385)
  %2447 : Tensor = prim::GetAttr[name="weight"](%2385)
  %2448 : Float(1024:1, 1024:1024) = aten::t(%2447), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.140 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.243, %2448), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.244 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.140, %2446, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.48 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.244, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
  %input.245 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.26, %x.48, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # transformers/modeling_bart.py:443:0
  %2453 : Tensor = prim::GetAttr[name="bias"](%2311)
  %2454 : Tensor = prim::GetAttr[name="weight"](%2311)
  %2455 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn_layer_norm
  %input.246 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.245, %2455, %2454, %2453, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2457 : Tensor = prim::GetAttr[name="bias"](%2310)
  %2458 : Tensor = prim::GetAttr[name="weight"](%2310)
  %2459 : Float(1024:1, 4096:1024) = aten::t(%2458), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc1 # torch/nn/functional.py:1676:0
  %output.141 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.246, %2459), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc1 # torch/nn/functional.py:1676:0
  %input.247 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.141, %2457, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc1 # torch/nn/functional.py:1678:0
  %input.248 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.247), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:1369:0
  %input.249 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.248, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
  %2464 : Tensor = prim::GetAttr[name="bias"](%2309)
  %2465 : Tensor = prim::GetAttr[name="weight"](%2309)
  %2466 : Float(4096:1, 1024:4096) = aten::t(%2465), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc2 # torch/nn/functional.py:1676:0
  %output.142 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.249, %2466), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc2 # torch/nn/functional.py:1676:0
  %input.250 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.142, %2464, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc2 # torch/nn/functional.py:1678:0
  %x.49 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.250, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
  %input.251 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.246, %x.49, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # transformers/modeling_bart.py:455:0
  %2471 : Tensor = prim::GetAttr[name="bias"](%2308)
  %2472 : Tensor = prim::GetAttr[name="weight"](%2308)
  %2473 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.final_layer_norm
  %query.27 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.251, %2473, %2472, %2471, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
  %2475 : __torch__.torch.nn.modules.normalization.___torch_mangle_1290.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1273)
  %2476 : __torch__.torch.nn.modules.linear.___torch_mangle_1289.Linear = prim::GetAttr[name="fc2"](%1273)
  %2477 : __torch__.torch.nn.modules.linear.___torch_mangle_1288.Linear = prim::GetAttr[name="fc1"](%1273)
  %2478 : __torch__.torch.nn.modules.normalization.___torch_mangle_1287.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1273)
  %2479 : __torch__.transformers.modeling_bart.___torch_mangle_1286.Attention = prim::GetAttr[name="encoder_attn"](%1273)
  %2480 : __torch__.torch.nn.modules.normalization.___torch_mangle_1281.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1273)
  %2481 : __torch__.transformers.modeling_bart.___torch_mangle_1280.Attention = prim::GetAttr[name="self_attn"](%1273)
  %2482 : __torch__.torch.nn.modules.linear.___torch_mangle_1279.Linear = prim::GetAttr[name="out_proj"](%2481)
  %2483 : __torch__.torch.nn.modules.linear.___torch_mangle_1277.Linear = prim::GetAttr[name="v_proj"](%2481)
  %2484 : __torch__.torch.nn.modules.linear.___torch_mangle_1276.Linear = prim::GetAttr[name="k_proj"](%2481)
  %2485 : __torch__.torch.nn.modules.linear.___torch_mangle_1278.Linear = prim::GetAttr[name="q_proj"](%2481)
  %2486 : int = aten::size(%query.27, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %2487 : int = aten::size(%query.27, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %bsz.27 : Long() = prim::NumToTensor(%2487), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2489 : int = aten::size(%query.27, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %2490 : Tensor = prim::GetAttr[name="bias"](%2485)
  %2491 : Tensor = prim::GetAttr[name="weight"](%2485)
  %2492 : Float(1024:1, 1024:1024) = aten::t(%2491), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.143 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.27, %2492), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2494 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.143, %2490, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.157 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2494, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:673:0
  %2496 : Tensor = prim::GetAttr[name="bias"](%2484)
  %2497 : Tensor = prim::GetAttr[name="weight"](%2484)
  %2498 : Float(1024:1, 1024:1024) = aten::t(%2497), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.144 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.27, %2498), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.159 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.144, %2496, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2501 : Tensor = prim::GetAttr[name="bias"](%2483)
  %2502 : Tensor = prim::GetAttr[name="weight"](%2483)
  %2503 : Float(1024:1, 1024:1024) = aten::t(%2502), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.145 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.27, %2503), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.161 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.145, %2501, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.158 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.157, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2507 : Long() = aten::mul(%bsz.27, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2508 : int = aten::Int(%2507), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2509 : int[] = prim::ListConstruct(%2486, %2508, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2510 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.158, %2509), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %q.27 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2510, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %tensor.160 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.159, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2513 : Long() = aten::mul(%bsz.27, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2514 : int = aten::Int(%2513), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2515 : int[] = prim::ListConstruct(%39, %2514, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2516 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.160, %2515), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %k.27 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2516, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %tensor.162 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.161, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2519 : Long() = aten::mul(%bsz.27, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2520 : int = aten::Int(%2519), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2521 : int[] = prim::ListConstruct(%39, %2520, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2522 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.162, %2521), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %v.27 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2522, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2524 : int = aten::size(%k.27, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:701:0
  %2525 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.27, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.98 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.27, %2525), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
  %2527 : int[] = prim::ListConstruct(%2487, %30, %2486, %2524), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2528 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.98, %2527), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.99 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2528, %attn_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:706:0
  %2530 : Long() = aten::mul(%bsz.27, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:707:0
  %2531 : int = aten::Int(%2530), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2532 : int[] = prim::ListConstruct(%2531, %2486, %2524), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %input.252 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.99, %2532), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:707:0
  %input.253 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.252, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.100 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.253, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # torch/nn/functional.py:973:0
  %attn_output.27 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.100, %v.27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:730:0
  %2537 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.27, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %2538 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2537, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %2539 : int[] = prim::ListConstruct(%2486, %2487, %2489), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %input.254 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2538, %2539), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %2541 : Tensor = prim::GetAttr[name="bias"](%2482)
  %2542 : Tensor = prim::GetAttr[name="weight"](%2482)
  %2543 : Float(1024:1, 1024:1024) = aten::t(%2542), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.146 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.254, %2543), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.255 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.146, %2541, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.50 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.255, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
  %input.256 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.27, %x.50, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # transformers/modeling_bart.py:427:0
  %2548 : Tensor = prim::GetAttr[name="bias"](%2480)
  %2549 : Tensor = prim::GetAttr[name="weight"](%2480)
  %2550 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn_layer_norm
  %query.28 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.256, %2550, %2549, %2548, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2552 : __torch__.torch.nn.modules.linear.___torch_mangle_1285.Linear = prim::GetAttr[name="out_proj"](%2479)
  %2553 : __torch__.torch.nn.modules.linear.___torch_mangle_1283.Linear = prim::GetAttr[name="v_proj"](%2479)
  %2554 : __torch__.torch.nn.modules.linear.___torch_mangle_1282.Linear = prim::GetAttr[name="k_proj"](%2479)
  %2555 : __torch__.torch.nn.modules.linear.___torch_mangle_1284.Linear = prim::GetAttr[name="q_proj"](%2479)
  %2556 : int = aten::size(%query.28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
  %2557 : int = aten::size(%query.28, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.28 : Long() = prim::NumToTensor(%2557), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2559 : int = aten::size(%query.28, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
  %2560 : Tensor = prim::GetAttr[name="bias"](%2555)
  %2561 : Tensor = prim::GetAttr[name="weight"](%2555)
  %2562 : Float(1024:1, 1024:1024) = aten::t(%2561), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.147 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.28, %2562), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2564 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.147, %2560, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.163 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2564, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:673:0
  %2566 : Tensor = prim::GetAttr[name="bias"](%2554)
  %2567 : Tensor = prim::GetAttr[name="weight"](%2554)
  %2568 : Float(1024:1, 1024:1024) = aten::t(%2567), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.148 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2568), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.165 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.148, %2566, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2571 : Tensor = prim::GetAttr[name="bias"](%2553)
  %2572 : Tensor = prim::GetAttr[name="weight"](%2553)
  %2573 : Float(1024:1, 1024:1024) = aten::t(%2572), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.149 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2573), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.167 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.149, %2571, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.164 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.163, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2577 : Long() = aten::mul(%bsz.28, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2578 : int = aten::Int(%2577), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2579 : int[] = prim::ListConstruct(%2556, %2578, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2580 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.164, %2579), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %q.28 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2580, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.166 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.165, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2583 : Long() = aten::mul(%bsz.28, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2584 : int = aten::Int(%2583), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2585 : int[] = prim::ListConstruct(%39, %2584, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2586 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.166, %2585), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %k.28 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2586, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.168 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.167, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2589 : Long() = aten::mul(%bsz.28, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2590 : int = aten::Int(%2589), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2591 : int[] = prim::ListConstruct(%39, %2590, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2592 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.168, %2591), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %v.28 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2592, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2594 : int = aten::size(%k.28, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:701:0
  %2595 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.28, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.101 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.28, %2595), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:702:0
  %2597 : int[] = prim::ListConstruct(%2557, %30, %2556, %2594), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %attn_weights.102 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.101, %2597), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:718:0
  %2599 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.20 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2599, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.103 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.102, %reshaped.20, %32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:720:0
  %2602 : Long() = aten::mul(%bsz.28, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:721:0
  %2603 : int = aten::Int(%2602), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2604 : int[] = prim::ListConstruct(%2603, %2556, %2594), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %input.257 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.103, %2604), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:721:0
  %input.258 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.257, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.104 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.258, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.28 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.104, %v.28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:730:0
  %2609 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.28, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
  %2610 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2609, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
  %2611 : int[] = prim::ListConstruct(%2556, %2557, %2559), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %input.259 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2610, %2611), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
  %2613 : Tensor = prim::GetAttr[name="bias"](%2552)
  %2614 : Tensor = prim::GetAttr[name="weight"](%2552)
  %2615 : Float(1024:1, 1024:1024) = aten::t(%2614), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.150 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.259, %2615), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.260 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.150, %2613, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.51 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.260, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
  %input.261 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.28, %x.51, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # transformers/modeling_bart.py:443:0
  %2620 : Tensor = prim::GetAttr[name="bias"](%2478)
  %2621 : Tensor = prim::GetAttr[name="weight"](%2478)
  %2622 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn_layer_norm
  %input.262 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.261, %2622, %2621, %2620, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2624 : Tensor = prim::GetAttr[name="bias"](%2477)
  %2625 : Tensor = prim::GetAttr[name="weight"](%2477)
  %2626 : Float(1024:1, 4096:1024) = aten::t(%2625), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc1 # torch/nn/functional.py:1676:0
  %output.151 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.262, %2626), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc1 # torch/nn/functional.py:1676:0
  %input.263 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.151, %2624, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc1 # torch/nn/functional.py:1678:0
  %input.264 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.263), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:1369:0
  %input.265 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.264, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
  %2631 : Tensor = prim::GetAttr[name="bias"](%2476)
  %2632 : Tensor = prim::GetAttr[name="weight"](%2476)
  %2633 : Float(4096:1, 1024:4096) = aten::t(%2632), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc2 # torch/nn/functional.py:1676:0
  %output.152 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.265, %2633), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc2 # torch/nn/functional.py:1676:0
  %input.266 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.152, %2631, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc2 # torch/nn/functional.py:1678:0
  %x.52 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.266, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
  %input.267 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.262, %x.52, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # transformers/modeling_bart.py:455:0
  %2638 : Tensor = prim::GetAttr[name="bias"](%2475)
  %2639 : Tensor = prim::GetAttr[name="weight"](%2475)
  %2640 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.final_layer_norm
  %query.29 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.267, %2640, %2639, %2638, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
  %2642 : __torch__.torch.nn.modules.normalization.___torch_mangle_1306.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1271)
  %2643 : __torch__.torch.nn.modules.linear.___torch_mangle_1305.Linear = prim::GetAttr[name="fc2"](%1271)
  %2644 : __torch__.torch.nn.modules.linear.___torch_mangle_1304.Linear = prim::GetAttr[name="fc1"](%1271)
  %2645 : __torch__.torch.nn.modules.normalization.___torch_mangle_1303.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1271)
  %2646 : __torch__.transformers.modeling_bart.___torch_mangle_1302.Attention = prim::GetAttr[name="encoder_attn"](%1271)
  %2647 : __torch__.torch.nn.modules.normalization.___torch_mangle_1297.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1271)
  %2648 : __torch__.transformers.modeling_bart.___torch_mangle_1296.Attention = prim::GetAttr[name="self_attn"](%1271)
  %2649 : __torch__.torch.nn.modules.linear.___torch_mangle_1295.Linear = prim::GetAttr[name="out_proj"](%2648)
  %2650 : __torch__.torch.nn.modules.linear.___torch_mangle_1293.Linear = prim::GetAttr[name="v_proj"](%2648)
  %2651 : __torch__.torch.nn.modules.linear.___torch_mangle_1292.Linear = prim::GetAttr[name="k_proj"](%2648)
  %2652 : __torch__.torch.nn.modules.linear.___torch_mangle_1294.Linear = prim::GetAttr[name="q_proj"](%2648)
  %2653 : int = aten::size(%query.29, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %2654 : int = aten::size(%query.29, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %bsz.29 : Long() = prim::NumToTensor(%2654), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2656 : int = aten::size(%query.29, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %2657 : Tensor = prim::GetAttr[name="bias"](%2652)
  %2658 : Tensor = prim::GetAttr[name="weight"](%2652)
  %2659 : Float(1024:1, 1024:1024) = aten::t(%2658), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.153 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.29, %2659), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2661 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.153, %2657, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.169 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2661, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:673:0
  %2663 : Tensor = prim::GetAttr[name="bias"](%2651)
  %2664 : Tensor = prim::GetAttr[name="weight"](%2651)
  %2665 : Float(1024:1, 1024:1024) = aten::t(%2664), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.154 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.29, %2665), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.171 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.154, %2663, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2668 : Tensor = prim::GetAttr[name="bias"](%2650)
  %2669 : Tensor = prim::GetAttr[name="weight"](%2650)
  %2670 : Float(1024:1, 1024:1024) = aten::t(%2669), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.155 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.29, %2670), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.173 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.155, %2668, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.170 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.169, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2674 : Long() = aten::mul(%bsz.29, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2675 : int = aten::Int(%2674), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2676 : int[] = prim::ListConstruct(%2653, %2675, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2677 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.170, %2676), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %q.29 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2677, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %tensor.172 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.171, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2680 : Long() = aten::mul(%bsz.29, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2681 : int = aten::Int(%2680), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2682 : int[] = prim::ListConstruct(%39, %2681, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2683 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.172, %2682), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %k.29 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2683, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %tensor.174 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.173, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2686 : Long() = aten::mul(%bsz.29, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2687 : int = aten::Int(%2686), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2688 : int[] = prim::ListConstruct(%39, %2687, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2689 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.174, %2688), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %v.29 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2689, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2691 : int = aten::size(%k.29, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:701:0
  %2692 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.29, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.105 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.29, %2692), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
  %2694 : int[] = prim::ListConstruct(%2654, %30, %2653, %2691), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2695 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.105, %2694), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.106 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2695, %attn_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:706:0
  %2697 : Long() = aten::mul(%bsz.29, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:707:0
  %2698 : int = aten::Int(%2697), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2699 : int[] = prim::ListConstruct(%2698, %2653, %2691), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %input.268 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.106, %2699), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:707:0
  %input.269 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.268, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.107 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.269, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # torch/nn/functional.py:973:0
  %attn_output.29 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.107, %v.29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:730:0
  %2704 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.29, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %2705 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2704, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %2706 : int[] = prim::ListConstruct(%2653, %2654, %2656), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %input.270 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2705, %2706), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %2708 : Tensor = prim::GetAttr[name="bias"](%2649)
  %2709 : Tensor = prim::GetAttr[name="weight"](%2649)
  %2710 : Float(1024:1, 1024:1024) = aten::t(%2709), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.156 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.270, %2710), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.271 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.156, %2708, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.53 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.271, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
  %input.272 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.29, %x.53, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # transformers/modeling_bart.py:427:0
  %2715 : Tensor = prim::GetAttr[name="bias"](%2647)
  %2716 : Tensor = prim::GetAttr[name="weight"](%2647)
  %2717 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn_layer_norm
  %query.30 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.272, %2717, %2716, %2715, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2719 : __torch__.torch.nn.modules.linear.___torch_mangle_1301.Linear = prim::GetAttr[name="out_proj"](%2646)
  %2720 : __torch__.torch.nn.modules.linear.___torch_mangle_1299.Linear = prim::GetAttr[name="v_proj"](%2646)
  %2721 : __torch__.torch.nn.modules.linear.___torch_mangle_1298.Linear = prim::GetAttr[name="k_proj"](%2646)
  %2722 : __torch__.torch.nn.modules.linear.___torch_mangle_1300.Linear = prim::GetAttr[name="q_proj"](%2646)
  %2723 : int = aten::size(%query.30, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
  %2724 : int = aten::size(%query.30, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.30 : Long() = prim::NumToTensor(%2724), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2726 : int = aten::size(%query.30, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
  %2727 : Tensor = prim::GetAttr[name="bias"](%2722)
  %2728 : Tensor = prim::GetAttr[name="weight"](%2722)
  %2729 : Float(1024:1, 1024:1024) = aten::t(%2728), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.157 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.30, %2729), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2731 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.157, %2727, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.175 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2731, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:673:0
  %2733 : Tensor = prim::GetAttr[name="bias"](%2721)
  %2734 : Tensor = prim::GetAttr[name="weight"](%2721)
  %2735 : Float(1024:1, 1024:1024) = aten::t(%2734), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.158 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2735), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.177 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.158, %2733, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2738 : Tensor = prim::GetAttr[name="bias"](%2720)
  %2739 : Tensor = prim::GetAttr[name="weight"](%2720)
  %2740 : Float(1024:1, 1024:1024) = aten::t(%2739), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.159 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2740), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.179 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.159, %2738, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.176 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.175, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2744 : Long() = aten::mul(%bsz.30, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2745 : int = aten::Int(%2744), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2746 : int[] = prim::ListConstruct(%2723, %2745, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2747 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.176, %2746), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %q.30 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2747, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.178 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.177, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2750 : Long() = aten::mul(%bsz.30, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2751 : int = aten::Int(%2750), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2752 : int[] = prim::ListConstruct(%39, %2751, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2753 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.178, %2752), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %k.30 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2753, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.180 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.179, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2756 : Long() = aten::mul(%bsz.30, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2757 : int = aten::Int(%2756), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2758 : int[] = prim::ListConstruct(%39, %2757, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2759 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.180, %2758), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %v.30 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2759, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2761 : int = aten::size(%k.30, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:701:0
  %2762 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.30, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.108 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.30, %2762), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:702:0
  %2764 : int[] = prim::ListConstruct(%2724, %30, %2723, %2761), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %attn_weights.109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.108, %2764), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:718:0
  %2766 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.21 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2766, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.110 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.109, %reshaped.21, %32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:720:0
  %2769 : Long() = aten::mul(%bsz.30, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:721:0
  %2770 : int = aten::Int(%2769), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2771 : int[] = prim::ListConstruct(%2770, %2723, %2761), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %input.273 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.110, %2771), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:721:0
  %input.274 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.273, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.111 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.274, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.30 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.111, %v.30), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:730:0
  %2776 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.30, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
  %2777 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2776, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
  %2778 : int[] = prim::ListConstruct(%2723, %2724, %2726), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %input.275 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2777, %2778), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
  %2780 : Tensor = prim::GetAttr[name="bias"](%2719)
  %2781 : Tensor = prim::GetAttr[name="weight"](%2719)
  %2782 : Float(1024:1, 1024:1024) = aten::t(%2781), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.160 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.275, %2782), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.276 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.160, %2780, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.54 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.276, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
  %input.277 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.30, %x.54, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # transformers/modeling_bart.py:443:0
  %2787 : Tensor = prim::GetAttr[name="bias"](%2645)
  %2788 : Tensor = prim::GetAttr[name="weight"](%2645)
  %2789 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn_layer_norm
  %input.278 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.277, %2789, %2788, %2787, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2791 : Tensor = prim::GetAttr[name="bias"](%2644)
  %2792 : Tensor = prim::GetAttr[name="weight"](%2644)
  %2793 : Float(1024:1, 4096:1024) = aten::t(%2792), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc1 # torch/nn/functional.py:1676:0
  %output.161 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.278, %2793), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc1 # torch/nn/functional.py:1676:0
  %input.279 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.161, %2791, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc1 # torch/nn/functional.py:1678:0
  %input.280 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.279), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:1369:0
  %input.281 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.280, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
  %2798 : Tensor = prim::GetAttr[name="bias"](%2643)
  %2799 : Tensor = prim::GetAttr[name="weight"](%2643)
  %2800 : Float(4096:1, 1024:4096) = aten::t(%2799), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc2 # torch/nn/functional.py:1676:0
  %output.162 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.281, %2800), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc2 # torch/nn/functional.py:1676:0
  %input.282 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.162, %2798, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc2 # torch/nn/functional.py:1678:0
  %x.55 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.282, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
  %input.283 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.278, %x.55, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # transformers/modeling_bart.py:455:0
  %2805 : Tensor = prim::GetAttr[name="bias"](%2642)
  %2806 : Tensor = prim::GetAttr[name="weight"](%2642)
  %2807 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.final_layer_norm
  %query.31 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.283, %2807, %2806, %2805, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
  %2809 : __torch__.torch.nn.modules.normalization.___torch_mangle_1322.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1269)
  %2810 : __torch__.torch.nn.modules.linear.___torch_mangle_1321.Linear = prim::GetAttr[name="fc2"](%1269)
  %2811 : __torch__.torch.nn.modules.linear.___torch_mangle_1320.Linear = prim::GetAttr[name="fc1"](%1269)
  %2812 : __torch__.torch.nn.modules.normalization.___torch_mangle_1319.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1269)
  %2813 : __torch__.transformers.modeling_bart.___torch_mangle_1318.Attention = prim::GetAttr[name="encoder_attn"](%1269)
  %2814 : __torch__.torch.nn.modules.normalization.___torch_mangle_1313.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1269)
  %2815 : __torch__.transformers.modeling_bart.___torch_mangle_1312.Attention = prim::GetAttr[name="self_attn"](%1269)
  %2816 : __torch__.torch.nn.modules.linear.___torch_mangle_1311.Linear = prim::GetAttr[name="out_proj"](%2815)
  %2817 : __torch__.torch.nn.modules.linear.___torch_mangle_1309.Linear = prim::GetAttr[name="v_proj"](%2815)
  %2818 : __torch__.torch.nn.modules.linear.___torch_mangle_1308.Linear = prim::GetAttr[name="k_proj"](%2815)
  %2819 : __torch__.torch.nn.modules.linear.___torch_mangle_1310.Linear = prim::GetAttr[name="q_proj"](%2815)
  %2820 : int = aten::size(%query.31, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %2821 : int = aten::size(%query.31, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %bsz.31 : Long() = prim::NumToTensor(%2821), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2823 : int = aten::size(%query.31, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %2824 : Tensor = prim::GetAttr[name="bias"](%2819)
  %2825 : Tensor = prim::GetAttr[name="weight"](%2819)
  %2826 : Float(1024:1, 1024:1024) = aten::t(%2825), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.163 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.31, %2826), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2828 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.163, %2824, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.181 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2828, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:673:0
  %2830 : Tensor = prim::GetAttr[name="bias"](%2818)
  %2831 : Tensor = prim::GetAttr[name="weight"](%2818)
  %2832 : Float(1024:1, 1024:1024) = aten::t(%2831), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.164 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.31, %2832), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.183 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.164, %2830, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2835 : Tensor = prim::GetAttr[name="bias"](%2817)
  %2836 : Tensor = prim::GetAttr[name="weight"](%2817)
  %2837 : Float(1024:1, 1024:1024) = aten::t(%2836), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.165 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.31, %2837), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.185 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.165, %2835, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.182 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.181, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2841 : Long() = aten::mul(%bsz.31, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2842 : int = aten::Int(%2841), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2843 : int[] = prim::ListConstruct(%2820, %2842, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2844 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.182, %2843), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %q.31 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2844, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %tensor.184 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.183, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2847 : Long() = aten::mul(%bsz.31, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2848 : int = aten::Int(%2847), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2849 : int[] = prim::ListConstruct(%39, %2848, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2850 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.184, %2849), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %k.31 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2850, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %tensor.186 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.185, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2853 : Long() = aten::mul(%bsz.31, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2854 : int = aten::Int(%2853), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2855 : int[] = prim::ListConstruct(%39, %2854, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2856 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.186, %2855), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %v.31 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2856, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2858 : int = aten::size(%k.31, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:701:0
  %2859 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.31, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.112 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.31, %2859), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
  %2861 : int[] = prim::ListConstruct(%2821, %30, %2820, %2858), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2862 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.112, %2861), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.113 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2862, %attn_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:706:0
  %2864 : Long() = aten::mul(%bsz.31, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:707:0
  %2865 : int = aten::Int(%2864), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2866 : int[] = prim::ListConstruct(%2865, %2820, %2858), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %input.284 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.113, %2866), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:707:0
  %input.285 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.284, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.114 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.285, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # torch/nn/functional.py:973:0
  %attn_output.31 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.114, %v.31), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:730:0
  %2871 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.31, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %2872 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2871, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %2873 : int[] = prim::ListConstruct(%2820, %2821, %2823), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %input.286 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2872, %2873), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %2875 : Tensor = prim::GetAttr[name="bias"](%2816)
  %2876 : Tensor = prim::GetAttr[name="weight"](%2816)
  %2877 : Float(1024:1, 1024:1024) = aten::t(%2876), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.166 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.286, %2877), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.287 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.166, %2875, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.56 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.287, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
  %input.288 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.31, %x.56, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # transformers/modeling_bart.py:427:0
  %2882 : Tensor = prim::GetAttr[name="bias"](%2814)
  %2883 : Tensor = prim::GetAttr[name="weight"](%2814)
  %2884 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn_layer_norm
  %query.32 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.288, %2884, %2883, %2882, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2886 : __torch__.torch.nn.modules.linear.___torch_mangle_1317.Linear = prim::GetAttr[name="out_proj"](%2813)
  %2887 : __torch__.torch.nn.modules.linear.___torch_mangle_1315.Linear = prim::GetAttr[name="v_proj"](%2813)
  %2888 : __torch__.torch.nn.modules.linear.___torch_mangle_1314.Linear = prim::GetAttr[name="k_proj"](%2813)
  %2889 : __torch__.torch.nn.modules.linear.___torch_mangle_1316.Linear = prim::GetAttr[name="q_proj"](%2813)
  %2890 : int = aten::size(%query.32, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
  %2891 : int = aten::size(%query.32, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.32 : Long() = prim::NumToTensor(%2891), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2893 : int = aten::size(%query.32, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
  %2894 : Tensor = prim::GetAttr[name="bias"](%2889)
  %2895 : Tensor = prim::GetAttr[name="weight"](%2889)
  %2896 : Float(1024:1, 1024:1024) = aten::t(%2895), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.167 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.32, %2896), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2898 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.167, %2894, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.187 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2898, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:673:0
  %2900 : Tensor = prim::GetAttr[name="bias"](%2888)
  %2901 : Tensor = prim::GetAttr[name="weight"](%2888)
  %2902 : Float(1024:1, 1024:1024) = aten::t(%2901), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.168 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2902), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.189 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.168, %2900, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2905 : Tensor = prim::GetAttr[name="bias"](%2887)
  %2906 : Tensor = prim::GetAttr[name="weight"](%2887)
  %2907 : Float(1024:1, 1024:1024) = aten::t(%2906), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.169 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2907), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.191 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.169, %2905, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.188 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.187, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2911 : Long() = aten::mul(%bsz.32, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2912 : int = aten::Int(%2911), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2913 : int[] = prim::ListConstruct(%2890, %2912, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2914 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.188, %2913), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %q.32 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2914, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.190 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.189, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2917 : Long() = aten::mul(%bsz.32, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2918 : int = aten::Int(%2917), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2919 : int[] = prim::ListConstruct(%39, %2918, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2920 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.190, %2919), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %k.32 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2920, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.192 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.191, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2923 : Long() = aten::mul(%bsz.32, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2924 : int = aten::Int(%2923), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2925 : int[] = prim::ListConstruct(%39, %2924, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2926 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.192, %2925), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %v.32 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2926, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2928 : int = aten::size(%k.32, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:701:0
  %2929 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.32, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.115 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.32, %2929), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:702:0
  %2931 : int[] = prim::ListConstruct(%2891, %30, %2890, %2928), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %attn_weights.116 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.115, %2931), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:718:0
  %2933 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.22 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2933, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.117 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.116, %reshaped.22, %32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:720:0
  %2936 : Long() = aten::mul(%bsz.32, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:721:0
  %2937 : int = aten::Int(%2936), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2938 : int[] = prim::ListConstruct(%2937, %2890, %2928), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %input.289 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.117, %2938), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:721:0
  %input.290 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.289, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.118 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.290, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.32 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.118, %v.32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:730:0
  %2943 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.32, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
  %2944 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2943, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
  %2945 : int[] = prim::ListConstruct(%2890, %2891, %2893), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %input.291 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2944, %2945), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
  %2947 : Tensor = prim::GetAttr[name="bias"](%2886)
  %2948 : Tensor = prim::GetAttr[name="weight"](%2886)
  %2949 : Float(1024:1, 1024:1024) = aten::t(%2948), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.170 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.291, %2949), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.292 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.170, %2947, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.57 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.292, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
  %input.293 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.32, %x.57, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # transformers/modeling_bart.py:443:0
  %2954 : Tensor = prim::GetAttr[name="bias"](%2812)
  %2955 : Tensor = prim::GetAttr[name="weight"](%2812)
  %2956 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn_layer_norm
  %input.294 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.293, %2956, %2955, %2954, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2958 : Tensor = prim::GetAttr[name="bias"](%2811)
  %2959 : Tensor = prim::GetAttr[name="weight"](%2811)
  %2960 : Float(1024:1, 4096:1024) = aten::t(%2959), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc1 # torch/nn/functional.py:1676:0
  %output.171 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.294, %2960), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc1 # torch/nn/functional.py:1676:0
  %input.295 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.171, %2958, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc1 # torch/nn/functional.py:1678:0
  %input.296 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.295), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:1369:0
  %input.297 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.296, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
  %2965 : Tensor = prim::GetAttr[name="bias"](%2810)
  %2966 : Tensor = prim::GetAttr[name="weight"](%2810)
  %2967 : Float(4096:1, 1024:4096) = aten::t(%2966), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc2 # torch/nn/functional.py:1676:0
  %output.172 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.297, %2967), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc2 # torch/nn/functional.py:1676:0
  %input.298 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.172, %2965, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc2 # torch/nn/functional.py:1678:0
  %x.58 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.298, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
  %input.299 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.294, %x.58, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # transformers/modeling_bart.py:455:0
  %2972 : Tensor = prim::GetAttr[name="bias"](%2809)
  %2973 : Tensor = prim::GetAttr[name="weight"](%2809)
  %2974 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.final_layer_norm
  %query.33 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.299, %2974, %2973, %2972, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
  %2976 : __torch__.torch.nn.modules.normalization.___torch_mangle_1338.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1267)
  %2977 : __torch__.torch.nn.modules.linear.___torch_mangle_1337.Linear = prim::GetAttr[name="fc2"](%1267)
  %2978 : __torch__.torch.nn.modules.linear.___torch_mangle_1336.Linear = prim::GetAttr[name="fc1"](%1267)
  %2979 : __torch__.torch.nn.modules.normalization.___torch_mangle_1335.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1267)
  %2980 : __torch__.transformers.modeling_bart.___torch_mangle_1334.Attention = prim::GetAttr[name="encoder_attn"](%1267)
  %2981 : __torch__.torch.nn.modules.normalization.___torch_mangle_1329.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1267)
  %2982 : __torch__.transformers.modeling_bart.___torch_mangle_1328.Attention = prim::GetAttr[name="self_attn"](%1267)
  %2983 : __torch__.torch.nn.modules.linear.___torch_mangle_1327.Linear = prim::GetAttr[name="out_proj"](%2982)
  %2984 : __torch__.torch.nn.modules.linear.___torch_mangle_1325.Linear = prim::GetAttr[name="v_proj"](%2982)
  %2985 : __torch__.torch.nn.modules.linear.___torch_mangle_1324.Linear = prim::GetAttr[name="k_proj"](%2982)
  %2986 : __torch__.torch.nn.modules.linear.___torch_mangle_1326.Linear = prim::GetAttr[name="q_proj"](%2982)
  %2987 : int = aten::size(%query.33, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %2988 : int = aten::size(%query.33, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %bsz.33 : Long() = prim::NumToTensor(%2988), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %2990 : int = aten::size(%query.33, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %2991 : Tensor = prim::GetAttr[name="bias"](%2986)
  %2992 : Tensor = prim::GetAttr[name="weight"](%2986)
  %2993 : Float(1024:1, 1024:1024) = aten::t(%2992), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.173 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.33, %2993), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2995 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.173, %2991, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.193 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2995, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:673:0
  %2997 : Tensor = prim::GetAttr[name="bias"](%2985)
  %2998 : Tensor = prim::GetAttr[name="weight"](%2985)
  %2999 : Float(1024:1, 1024:1024) = aten::t(%2998), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.174 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.33, %2999), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.195 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.174, %2997, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1678:0
  %3002 : Tensor = prim::GetAttr[name="bias"](%2984)
  %3003 : Tensor = prim::GetAttr[name="weight"](%2984)
  %3004 : Float(1024:1, 1024:1024) = aten::t(%3003), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.175 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.33, %3004), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.197 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.175, %3002, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.194 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.193, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3008 : Long() = aten::mul(%bsz.33, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3009 : int = aten::Int(%3008), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3010 : int[] = prim::ListConstruct(%2987, %3009, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3011 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.194, %3010), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %q.33 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3011, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %tensor.196 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.195, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3014 : Long() = aten::mul(%bsz.33, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3015 : int = aten::Int(%3014), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3016 : int[] = prim::ListConstruct(%39, %3015, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3017 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.196, %3016), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %k.33 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3017, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %tensor.198 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.197, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3020 : Long() = aten::mul(%bsz.33, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3021 : int = aten::Int(%3020), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3022 : int[] = prim::ListConstruct(%39, %3021, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3023 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.198, %3022), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %v.33 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3023, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3025 : int = aten::size(%k.33, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:701:0
  %3026 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.33, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.119 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.33, %3026), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
  %3028 : int[] = prim::ListConstruct(%2988, %30, %2987, %3025), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3029 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.119, %3028), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%3029, %attn_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:706:0
  %3031 : Long() = aten::mul(%bsz.33, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:707:0
  %3032 : int = aten::Int(%3031), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3033 : int[] = prim::ListConstruct(%3032, %2987, %3025), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %input.300 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.120, %3033), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:707:0
  %input.301 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.300, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.121 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.301, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # torch/nn/functional.py:973:0
  %attn_output.33 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.121, %v.33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:730:0
  %3038 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.33, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %3039 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3038, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %3040 : int[] = prim::ListConstruct(%2987, %2988, %2990), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %input.302 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3039, %3040), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %3042 : Tensor = prim::GetAttr[name="bias"](%2983)
  %3043 : Tensor = prim::GetAttr[name="weight"](%2983)
  %3044 : Float(1024:1, 1024:1024) = aten::t(%3043), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.176 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.302, %3044), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.303 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.176, %3042, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.59 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.303, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
  %input.304 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.33, %x.59, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # transformers/modeling_bart.py:427:0
  %3049 : Tensor = prim::GetAttr[name="bias"](%2981)
  %3050 : Tensor = prim::GetAttr[name="weight"](%2981)
  %3051 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn_layer_norm
  %query.34 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.304, %3051, %3050, %3049, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %3053 : __torch__.torch.nn.modules.linear.___torch_mangle_1333.Linear = prim::GetAttr[name="out_proj"](%2980)
  %3054 : __torch__.torch.nn.modules.linear.___torch_mangle_1331.Linear = prim::GetAttr[name="v_proj"](%2980)
  %3055 : __torch__.torch.nn.modules.linear.___torch_mangle_1330.Linear = prim::GetAttr[name="k_proj"](%2980)
  %3056 : __torch__.torch.nn.modules.linear.___torch_mangle_1332.Linear = prim::GetAttr[name="q_proj"](%2980)
  %3057 : int = aten::size(%query.34, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
  %3058 : int = aten::size(%query.34, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.34 : Long() = prim::NumToTensor(%3058), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3060 : int = aten::size(%query.34, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
  %3061 : Tensor = prim::GetAttr[name="bias"](%3056)
  %3062 : Tensor = prim::GetAttr[name="weight"](%3056)
  %3063 : Float(1024:1, 1024:1024) = aten::t(%3062), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.177 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.34, %3063), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %3065 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.177, %3061, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.199 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%3065, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:673:0
  %3067 : Tensor = prim::GetAttr[name="bias"](%3055)
  %3068 : Tensor = prim::GetAttr[name="weight"](%3055)
  %3069 : Float(1024:1, 1024:1024) = aten::t(%3068), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.178 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %3069), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.201 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.178, %3067, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %3072 : Tensor = prim::GetAttr[name="bias"](%3054)
  %3073 : Tensor = prim::GetAttr[name="weight"](%3054)
  %3074 : Float(1024:1, 1024:1024) = aten::t(%3073), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.179 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %3074), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.203 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.179, %3072, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.200 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.199, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3078 : Long() = aten::mul(%bsz.34, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3079 : int = aten::Int(%3078), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3080 : int[] = prim::ListConstruct(%3057, %3079, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3081 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.200, %3080), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %q.34 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3081, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.202 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.201, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3084 : Long() = aten::mul(%bsz.34, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3085 : int = aten::Int(%3084), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3086 : int[] = prim::ListConstruct(%39, %3085, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3087 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.202, %3086), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %k.34 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3087, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.204 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.203, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3090 : Long() = aten::mul(%bsz.34, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3091 : int = aten::Int(%3090), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3092 : int[] = prim::ListConstruct(%39, %3091, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3093 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.204, %3092), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %v.34 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3093, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3095 : int = aten::size(%k.34, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:701:0
  %3096 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.34, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.122 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.34, %3096), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:702:0
  %3098 : int[] = prim::ListConstruct(%3058, %30, %3057, %3095), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %attn_weights.123 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.122, %3098), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:718:0
  %3100 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.23 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%3100, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.124 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.123, %reshaped.23, %32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:720:0
  %3103 : Long() = aten::mul(%bsz.34, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:721:0
  %3104 : int = aten::Int(%3103), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3105 : int[] = prim::ListConstruct(%3104, %3057, %3095), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %input.305 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.124, %3105), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:721:0
  %input.306 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.305, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.125 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.306, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.34 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.125, %v.34), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:730:0
  %3110 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.34, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
  %3111 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3110, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
  %3112 : int[] = prim::ListConstruct(%3057, %3058, %3060), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %input.307 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3111, %3112), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
  %3114 : Tensor = prim::GetAttr[name="bias"](%3053)
  %3115 : Tensor = prim::GetAttr[name="weight"](%3053)
  %3116 : Float(1024:1, 1024:1024) = aten::t(%3115), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.180 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.307, %3116), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.308 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.180, %3114, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.60 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.308, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
  %input.309 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.34, %x.60, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # transformers/modeling_bart.py:443:0
  %3121 : Tensor = prim::GetAttr[name="bias"](%2979)
  %3122 : Tensor = prim::GetAttr[name="weight"](%2979)
  %3123 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn_layer_norm
  %input.310 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.309, %3123, %3122, %3121, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %3125 : Tensor = prim::GetAttr[name="bias"](%2978)
  %3126 : Tensor = prim::GetAttr[name="weight"](%2978)
  %3127 : Float(1024:1, 4096:1024) = aten::t(%3126), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc1 # torch/nn/functional.py:1676:0
  %output.181 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.310, %3127), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc1 # torch/nn/functional.py:1676:0
  %input.311 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.181, %3125, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc1 # torch/nn/functional.py:1678:0
  %input.312 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.311), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:1369:0
  %input.313 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.312, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
  %3132 : Tensor = prim::GetAttr[name="bias"](%2977)
  %3133 : Tensor = prim::GetAttr[name="weight"](%2977)
  %3134 : Float(4096:1, 1024:4096) = aten::t(%3133), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc2 # torch/nn/functional.py:1676:0
  %output.182 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.313, %3134), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc2 # torch/nn/functional.py:1676:0
  %input.314 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.182, %3132, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc2 # torch/nn/functional.py:1678:0
  %x.61 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.314, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
  %input.315 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.310, %x.61, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # transformers/modeling_bart.py:455:0
  %3139 : Tensor = prim::GetAttr[name="bias"](%2976)
  %3140 : Tensor = prim::GetAttr[name="weight"](%2976)
  %3141 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.final_layer_norm
  %query.35 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.315, %3141, %3140, %3139, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
  %3143 : __torch__.torch.nn.modules.normalization.___torch_mangle_1354.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1265)
  %3144 : __torch__.torch.nn.modules.linear.___torch_mangle_1353.Linear = prim::GetAttr[name="fc2"](%1265)
  %3145 : __torch__.torch.nn.modules.linear.___torch_mangle_1352.Linear = prim::GetAttr[name="fc1"](%1265)
  %3146 : __torch__.torch.nn.modules.normalization.___torch_mangle_1351.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1265)
  %3147 : __torch__.transformers.modeling_bart.___torch_mangle_1350.Attention = prim::GetAttr[name="encoder_attn"](%1265)
  %3148 : __torch__.torch.nn.modules.normalization.___torch_mangle_1345.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1265)
  %3149 : __torch__.transformers.modeling_bart.___torch_mangle_1344.Attention = prim::GetAttr[name="self_attn"](%1265)
  %3150 : __torch__.torch.nn.modules.linear.___torch_mangle_1343.Linear = prim::GetAttr[name="out_proj"](%3149)
  %3151 : __torch__.torch.nn.modules.linear.___torch_mangle_1341.Linear = prim::GetAttr[name="v_proj"](%3149)
  %3152 : __torch__.torch.nn.modules.linear.___torch_mangle_1340.Linear = prim::GetAttr[name="k_proj"](%3149)
  %3153 : __torch__.torch.nn.modules.linear.___torch_mangle_1342.Linear = prim::GetAttr[name="q_proj"](%3149)
  %3154 : int = aten::size(%query.35, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %3155 : int = aten::size(%query.35, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %bsz.35 : Long() = prim::NumToTensor(%3155), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3157 : int = aten::size(%query.35, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %3158 : Tensor = prim::GetAttr[name="bias"](%3153)
  %3159 : Tensor = prim::GetAttr[name="weight"](%3153)
  %3160 : Float(1024:1, 1024:1024) = aten::t(%3159), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.183 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.35, %3160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
  %3162 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.183, %3158, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.205 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%3162, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:673:0
  %3164 : Tensor = prim::GetAttr[name="bias"](%3152)
  %3165 : Tensor = prim::GetAttr[name="weight"](%3152)
  %3166 : Float(1024:1, 1024:1024) = aten::t(%3165), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.184 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.35, %3166), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.207 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.184, %3164, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1678:0
  %3169 : Tensor = prim::GetAttr[name="bias"](%3151)
  %3170 : Tensor = prim::GetAttr[name="weight"](%3151)
  %3171 : Float(1024:1, 1024:1024) = aten::t(%3170), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.185 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.35, %3171), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.209 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.185, %3169, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.206 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.205, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3175 : Long() = aten::mul(%bsz.35, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3176 : int = aten::Int(%3175), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3177 : int[] = prim::ListConstruct(%3154, %3176, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3178 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.206, %3177), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %q.35 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3178, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %tensor.208 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.207, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3181 : Long() = aten::mul(%bsz.35, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3182 : int = aten::Int(%3181), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3183 : int[] = prim::ListConstruct(%39, %3182, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3184 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.208, %3183), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %k.35 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3184, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %tensor.210 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.209, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3187 : Long() = aten::mul(%bsz.35, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3188 : int = aten::Int(%3187), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3189 : int[] = prim::ListConstruct(%39, %3188, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3190 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.210, %3189), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %v.35 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3190, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3192 : int = aten::size(%k.35, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:701:0
  %3193 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.35, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.126 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.35, %3193), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
  %3195 : int[] = prim::ListConstruct(%3155, %30, %3154, %3192), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3196 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.126, %3195), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.127 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%3196, %attn_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:706:0
  %3198 : Long() = aten::mul(%bsz.35, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:707:0
  %3199 : int = aten::Int(%3198), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3200 : int[] = prim::ListConstruct(%3199, %3154, %3192), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %input.316 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.127, %3200), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:707:0
  %input.317 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.316, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.128 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.317, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # torch/nn/functional.py:973:0
  %attn_output.35 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.128, %v.35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:730:0
  %3205 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.35, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %3206 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3205, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %3207 : int[] = prim::ListConstruct(%3154, %3155, %3157), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %input.318 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3206, %3207), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %3209 : Tensor = prim::GetAttr[name="bias"](%3150)
  %3210 : Tensor = prim::GetAttr[name="weight"](%3150)
  %3211 : Float(1024:1, 1024:1024) = aten::t(%3210), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.186 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.318, %3211), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.319 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.186, %3209, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.62 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.319, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
  %input.320 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.35, %x.62, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # transformers/modeling_bart.py:427:0
  %3216 : Tensor = prim::GetAttr[name="bias"](%3148)
  %3217 : Tensor = prim::GetAttr[name="weight"](%3148)
  %3218 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn_layer_norm
  %query : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.320, %3218, %3217, %3216, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %3220 : __torch__.torch.nn.modules.linear.___torch_mangle_1349.Linear = prim::GetAttr[name="out_proj"](%3147)
  %3221 : __torch__.torch.nn.modules.linear.___torch_mangle_1347.Linear = prim::GetAttr[name="v_proj"](%3147)
  %3222 : __torch__.torch.nn.modules.linear.___torch_mangle_1346.Linear = prim::GetAttr[name="k_proj"](%3147)
  %3223 : __torch__.torch.nn.modules.linear.___torch_mangle_1348.Linear = prim::GetAttr[name="q_proj"](%3147)
  %3224 : int = aten::size(%query, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
  %3225 : int = aten::size(%query, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz : Long() = prim::NumToTensor(%3225), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3227 : int = aten::size(%query, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
  %3228 : Tensor = prim::GetAttr[name="bias"](%3223)
  %3229 : Tensor = prim::GetAttr[name="weight"](%3223)
  %3230 : Float(1024:1, 1024:1024) = aten::t(%3229), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.187 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query, %3230), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %3232 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.187, %3228, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.211 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%3232, %27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:673:0
  %3234 : Tensor = prim::GetAttr[name="bias"](%3222)
  %3235 : Tensor = prim::GetAttr[name="weight"](%3222)
  %3236 : Float(1024:1, 1024:1024) = aten::t(%3235), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.188 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %3236), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.213 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.188, %3234, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %3239 : Tensor = prim::GetAttr[name="bias"](%3221)
  %3240 : Tensor = prim::GetAttr[name="weight"](%3221)
  %3241 : Float(1024:1, 1024:1024) = aten::t(%3240), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.189 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %3241), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.215 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.189, %3239, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.212 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.211, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3245 : Long() = aten::mul(%bsz, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3246 : int = aten::Int(%3245), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3247 : int[] = prim::ListConstruct(%3224, %3246, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3248 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.212, %3247), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %q : Float(272:64, 13:17408, 64:1) = aten::transpose(%3248, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.214 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.213, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3251 : Long() = aten::mul(%bsz, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3252 : int = aten::Int(%3251), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3253 : int[] = prim::ListConstruct(%39, %3252, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3254 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.214, %3253), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %k : Float(272:64, 13:17408, 64:1) = aten::transpose(%3254, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.215, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3257 : Long() = aten::mul(%bsz, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3258 : int = aten::Int(%3257), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3259 : int[] = prim::ListConstruct(%39, %3258, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3260 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor, %3259), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %v : Float(272:64, 13:17408, 64:1) = aten::transpose(%3260, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3262 : int = aten::size(%k, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:701:0
  %3263 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k, %42, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.129 : Float(272:169, 13:13, 13:1) = aten::bmm(%q, %3263), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:702:0
  %3265 : int[] = prim::ListConstruct(%3225, %30, %3224, %3262), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %attn_weights.130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.129, %3265), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:718:0
  %3267 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%3267, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.131 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.130, %reshaped, %32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:720:0
  %3270 : Long() = aten::mul(%bsz, %28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:721:0
  %3271 : int = aten::Int(%3270), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3272 : int[] = prim::ListConstruct(%3271, %3224, %3262), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %input.321 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.131, %3272), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:721:0
  %input.322 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.321, %39, %43), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights : Float(272:169, 13:13, 13:1) = aten::dropout(%input.322, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # torch/nn/functional.py:973:0
  %attn_output : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights, %v), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:730:0
  %3277 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output, %38, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
  %3278 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3277, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
  %3279 : int[] = prim::ListConstruct(%3224, %3225, %3227), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %input.323 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3278, %3279), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
  %3281 : Tensor = prim::GetAttr[name="bias"](%3220)
  %3282 : Tensor = prim::GetAttr[name="weight"](%3220)
  %3283 : Float(1024:1, 1024:1024) = aten::t(%3282), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.190 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.323, %3283), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.324 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.190, %3281, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.63 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.324, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
  %input.325 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query, %x.63, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # transformers/modeling_bart.py:443:0
  %3288 : Tensor = prim::GetAttr[name="bias"](%3146)
  %3289 : Tensor = prim::GetAttr[name="weight"](%3146)
  %3290 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn_layer_norm
  %input.326 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.325, %3290, %3289, %3288, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %3292 : Tensor = prim::GetAttr[name="bias"](%3145)
  %3293 : Tensor = prim::GetAttr[name="weight"](%3145)
  %3294 : Float(1024:1, 4096:1024) = aten::t(%3293), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc1 # torch/nn/functional.py:1676:0
  %output.191 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.326, %3294), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc1 # torch/nn/functional.py:1676:0
  %input.327 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.191, %3292, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc1 # torch/nn/functional.py:1678:0
  %input.328 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.327), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:1369:0
  %input.329 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.328, %31, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
  %3299 : Tensor = prim::GetAttr[name="bias"](%3144)
  %3300 : Tensor = prim::GetAttr[name="weight"](%3144)
  %3301 : Float(4096:1, 1024:4096) = aten::t(%3300), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc2 # torch/nn/functional.py:1676:0
  %output.192 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.329, %3301), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc2 # torch/nn/functional.py:1676:0
  %input.330 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.192, %3299, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc2 # torch/nn/functional.py:1678:0
  %x.64 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.330, %25, %41), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
  %input.331 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.326, %x.64, %42), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # transformers/modeling_bart.py:455:0
  %3306 : Tensor = prim::GetAttr[name="bias"](%3143)
  %3307 : Tensor = prim::GetAttr[name="weight"](%3143)
  %3308 : int[] = prim::ListConstruct(%24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.final_layer_norm
  %x : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.331, %3308, %3307, %3306, %23, %22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
  %input : Float(17:1024, 13:17408, 1024:1) = aten::transpose(%x, %38, %42), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:601:0
  %3311 : (Float(17:1024, 13:17408, 1024:1), Float(17:1024, 13:17408, 1024:1)) = prim::TupleConstruct(%input, %encoder_hidden_states)
  %6 : Float(17:1024, 13:17408, 1024:1), %7 : Float(17:1024, 13:17408, 1024:1) = prim::TupleUnpack(%3311)
  %3312 : int = prim::Constant[value=1](), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %3313 : Tensor = prim::GetAttr[name="bias"](%3)
  %3314 : Tensor = prim::GetAttr[name="weight"](%3)
  %3315 : Float(1024:1, 2:1024) = aten::t(%3314), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%6, %3315), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %3317 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %3313, %3312), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %9 : int = prim::Constant[value=1]() # torch/tensor.py:371:0
  %10 : int = prim::Constant[value=-1]() # torch/tensor.py:371:0
  %11 : Tensor[] = aten::split(%3317, %9, %10) # torch/tensor.py:371:0
  %start_logits : Float(17:26, 13:2, 1:1), %end_logits : Float(17:26, 13:2, 1:1) = prim::ListUnpack(%11)
  %14 : int = prim::Constant[value=-1]() # transformers/modeling_bart.py:1297:0
  %15 : Float(17:26, 13:2) = aten::squeeze(%start_logits, %14) # transformers/modeling_bart.py:1297:0
  %16 : int = prim::Constant[value=-1]() # transformers/modeling_bart.py:1298:0
  %17 : Float(17:26, 13:2) = aten::squeeze(%end_logits, %16) # transformers/modeling_bart.py:1298:0
  %18 : (Float(17:26, 13:2), Float(17:26, 13:2), Float(17:1024, 13:17408, 1024:1)) = prim::TupleConstruct(%15, %17, %7)
  return (%18)
