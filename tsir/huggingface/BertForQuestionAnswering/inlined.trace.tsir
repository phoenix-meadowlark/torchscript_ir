graph(%self.1 : __torch__.transformers.modeling_bert.BertForQuestionAnswering,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_4620.Linear = prim::GetAttr[name="qa_outputs"](%self.1)
  %4 : __torch__.transformers.modeling_bert.___torch_mangle_4619.BertModel = prim::GetAttr[name="bert"](%self.1)
  %17 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
  %18 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %19 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %20 : int = prim::Constant[value=12](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %21 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %22 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %23 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %24 : int = prim::Constant[value=768](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %25 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
  %26 : Double() = prim::Constant[value={-10000}](), scope: __module.bert # transformers/modeling_utils.py:258:0
  %27 : float = prim::Constant[value=1.](), scope: __module.bert # torch/tensor.py:396:0
  %28 : None = prim::Constant(), scope: __module.bert
  %29 : int = prim::Constant[value=6](), scope: __module.bert # transformers/modeling_utils.py:257:0
  %30 : int = prim::Constant[value=3](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %31 : int = prim::Constant[value=2](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %32 : int = prim::Constant[value=9223372036854775807](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %33 : bool = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_bert.py:806:0
  %34 : Device = prim::Constant[value="cpu"](), scope: __module.bert # transformers/modeling_bert.py:806:0
  %35 : int = prim::Constant[value=4](), scope: __module.bert # transformers/modeling_bert.py:806:0
  %36 : int = prim::Constant[value=1](), scope: __module.bert # transformers/modeling_bert.py:795:0
  %37 : int = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_bert.py:795:0
  %38 : __torch__.transformers.modeling_bert.___torch_mangle_4618.BertEncoder = prim::GetAttr[name="encoder"](%4)
  %39 : __torch__.transformers.modeling_bert.___torch_mangle_4412.BertEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %40 : int = aten::size(%input_ids, %37), scope: __module.bert # transformers/modeling_bert.py:795:0
  %41 : int = aten::size(%input_ids, %36), scope: __module.bert # transformers/modeling_bert.py:795:0
  %42 : int[] = prim::ListConstruct(%40, %41), scope: __module.bert
  %input.2 : Long(17:13, 13:1) = aten::zeros(%42, %35, %37, %34, %33), scope: __module.bert # transformers/modeling_bert.py:806:0
  %44 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %37, %37, %32, %36), scope: __module.bert # transformers/modeling_utils.py:244:0
  %45 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%44, %36), scope: __module.bert # transformers/modeling_utils.py:244:0
  %46 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%45, %31), scope: __module.bert # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%46, %30, %37, %32, %36), scope: __module.bert # transformers/modeling_utils.py:244:0
  %48 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %29, %33, %33, %28), scope: __module.bert # transformers/modeling_utils.py:257:0
  %49 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%48, %27, %36), scope: __module.bert # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%49, %26), scope: __module.bert # transformers/modeling_utils.py:258:0
  %51 : __torch__.torch.nn.modules.normalization.___torch_mangle_4410.LayerNorm = prim::GetAttr[name="LayerNorm"](%39)
  %52 : __torch__.torch.nn.modules.sparse.___torch_mangle_4409.Embedding = prim::GetAttr[name="token_type_embeddings"](%39)
  %53 : __torch__.torch.nn.modules.sparse.___torch_mangle_4408.Embedding = prim::GetAttr[name="position_embeddings"](%39)
  %54 : __torch__.torch.nn.modules.sparse.___torch_mangle_4407.Embedding = prim::GetAttr[name="word_embeddings"](%39)
  %55 : Tensor = prim::GetAttr[name="position_ids"](%39)
  %56 : int = aten::size(%input_ids, %36), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:184:0
  %57 : Long(1:512, 512:1) = aten::slice(%55, %37, %37, %32, %36), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:191:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%57, %36, %37, %56, %36), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:191:0
  %59 : Tensor = prim::GetAttr[name="weight"](%54)
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::embedding(%59, %input_ids, %37, %33, %33), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %61 : Tensor = prim::GetAttr[name="weight"](%53)
  %position_embeddings : Float(1:9984, 13:768, 768:1) = aten::embedding(%61, %input.1, %21, %33, %33), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %63 : Tensor = prim::GetAttr[name="weight"](%52)
  %token_type_embeddings : Float(17:9984, 13:768, 768:1) = aten::embedding(%63, %input.2, %21, %33, %33), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %65 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %position_embeddings, %36), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:201:0
  %input.3 : Float(17:9984, 13:768, 768:1) = aten::add(%65, %token_type_embeddings, %36), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:201:0
  %67 : Tensor = prim::GetAttr[name="bias"](%51)
  %68 : Tensor = prim::GetAttr[name="weight"](%51)
  %69 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm
  %input.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.3, %69, %68, %67, %23, %22), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.4, %25, %33), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
  %72 : __torch__.torch.nn.modules.container.___torch_mangle_4617.ModuleList = prim::GetAttr[name="layer"](%38)
  %73 : __torch__.transformers.modeling_bert.___torch_mangle_4616.BertLayer = prim::GetAttr[name="11"](%72)
  %74 : __torch__.torch.nn.modules.container.___torch_mangle_4617.ModuleList = prim::GetAttr[name="layer"](%38)
  %75 : __torch__.transformers.modeling_bert.___torch_mangle_4599.BertLayer = prim::GetAttr[name="10"](%74)
  %76 : __torch__.torch.nn.modules.container.___torch_mangle_4617.ModuleList = prim::GetAttr[name="layer"](%38)
  %77 : __torch__.transformers.modeling_bert.___torch_mangle_4582.BertLayer = prim::GetAttr[name="9"](%76)
  %78 : __torch__.torch.nn.modules.container.___torch_mangle_4617.ModuleList = prim::GetAttr[name="layer"](%38)
  %79 : __torch__.transformers.modeling_bert.___torch_mangle_4565.BertLayer = prim::GetAttr[name="8"](%78)
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_4617.ModuleList = prim::GetAttr[name="layer"](%38)
  %81 : __torch__.transformers.modeling_bert.___torch_mangle_4548.BertLayer = prim::GetAttr[name="7"](%80)
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_4617.ModuleList = prim::GetAttr[name="layer"](%38)
  %83 : __torch__.transformers.modeling_bert.___torch_mangle_4531.BertLayer = prim::GetAttr[name="6"](%82)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_4617.ModuleList = prim::GetAttr[name="layer"](%38)
  %85 : __torch__.transformers.modeling_bert.___torch_mangle_4514.BertLayer = prim::GetAttr[name="5"](%84)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_4617.ModuleList = prim::GetAttr[name="layer"](%38)
  %87 : __torch__.transformers.modeling_bert.___torch_mangle_4497.BertLayer = prim::GetAttr[name="4"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_4617.ModuleList = prim::GetAttr[name="layer"](%38)
  %89 : __torch__.transformers.modeling_bert.___torch_mangle_4480.BertLayer = prim::GetAttr[name="3"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_4617.ModuleList = prim::GetAttr[name="layer"](%38)
  %91 : __torch__.transformers.modeling_bert.___torch_mangle_4463.BertLayer = prim::GetAttr[name="2"](%90)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_4617.ModuleList = prim::GetAttr[name="layer"](%38)
  %93 : __torch__.transformers.modeling_bert.___torch_mangle_4446.BertLayer = prim::GetAttr[name="1"](%92)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_4617.ModuleList = prim::GetAttr[name="layer"](%38)
  %95 : __torch__.transformers.modeling_bert.___torch_mangle_4429.BertLayer = prim::GetAttr[name="0"](%94)
  %96 : __torch__.transformers.modeling_bert.___torch_mangle_4428.BertOutput = prim::GetAttr[name="output"](%95)
  %97 : __torch__.transformers.modeling_bert.___torch_mangle_4424.BertIntermediate = prim::GetAttr[name="intermediate"](%95)
  %98 : __torch__.transformers.modeling_bert.___torch_mangle_4422.BertAttention = prim::GetAttr[name="attention"](%95)
  %99 : __torch__.transformers.modeling_bert.___torch_mangle_4421.BertSelfOutput = prim::GetAttr[name="output"](%98)
  %100 : __torch__.transformers.modeling_bert.___torch_mangle_4417.BertSelfAttention = prim::GetAttr[name="self"](%98)
  %101 : __torch__.torch.nn.modules.linear.___torch_mangle_4415.Linear = prim::GetAttr[name="value"](%100)
  %102 : __torch__.torch.nn.modules.linear.___torch_mangle_4414.Linear = prim::GetAttr[name="key"](%100)
  %103 : __torch__.torch.nn.modules.linear.___torch_mangle_4413.Linear = prim::GetAttr[name="query"](%100)
  %104 : Tensor = prim::GetAttr[name="bias"](%103)
  %105 : Tensor = prim::GetAttr[name="weight"](%103)
  %106 : Float(768:1, 768:768) = aten::t(%105), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %106), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %104, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %109 : Tensor = prim::GetAttr[name="bias"](%102)
  %110 : Tensor = prim::GetAttr[name="weight"](%102)
  %111 : Float(768:1, 768:768) = aten::t(%110), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %111), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %109, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %114 : Tensor = prim::GetAttr[name="bias"](%101)
  %115 : Tensor = prim::GetAttr[name="weight"](%101)
  %116 : Float(768:1, 768:768) = aten::t(%115), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %116), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %114, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %119 : int = aten::size(%x.1, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %120 : int = aten::size(%x.1, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %121 : int[] = prim::ListConstruct(%119, %120, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.1, %121), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %123 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.2, %123), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %125 : int = aten::size(%x.3, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %126 : int = aten::size(%x.3, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %127 : int[] = prim::ListConstruct(%125, %126, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.3, %127), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %129 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.4, %129), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %131 : int = aten::size(%x.5, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %132 : int = aten::size(%x.5, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %133 : int[] = prim::ListConstruct(%131, %132, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.5, %133), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %135 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.6, %135), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %137 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.1, %21, %18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %137), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.1, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:262:0
  %input.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.6, %21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.7, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:275:0
  %144 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %145 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.1, %144), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%145, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
  %147 : int = aten::size(%context_layer.2, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
  %148 : int = aten::size(%context_layer.2, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
  %149 : int[] = prim::ListConstruct(%147, %148, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.2, %149), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:279:0
  %151 : __torch__.torch.nn.modules.normalization.___torch_mangle_4419.LayerNorm = prim::GetAttr[name="LayerNorm"](%99)
  %152 : __torch__.torch.nn.modules.linear.___torch_mangle_4418.Linear = prim::GetAttr[name="dense"](%99)
  %153 : Tensor = prim::GetAttr[name="bias"](%152)
  %154 : Tensor = prim::GetAttr[name="weight"](%152)
  %155 : Float(768:1, 768:768) = aten::t(%154), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.8, %155), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.4, %153, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.9, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.1, %input.5, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output # transformers/modeling_bert.py:295:0
  %160 : Tensor = prim::GetAttr[name="bias"](%151)
  %161 : Tensor = prim::GetAttr[name="weight"](%151)
  %162 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.10, %162, %161, %160, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %164 : __torch__.torch.nn.modules.linear.___torch_mangle_4423.Linear = prim::GetAttr[name="dense"](%97)
  %165 : Tensor = prim::GetAttr[name="bias"](%164)
  %166 : Tensor = prim::GetAttr[name="weight"](%164)
  %167 : Float(768:1, 3072:768) = aten::t(%166), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.1, %167), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.5, %165, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %171 : __torch__.torch.nn.modules.normalization.___torch_mangle_4426.LayerNorm = prim::GetAttr[name="LayerNorm"](%96)
  %172 : __torch__.torch.nn.modules.linear.___torch_mangle_4425.Linear = prim::GetAttr[name="dense"](%96)
  %173 : Tensor = prim::GetAttr[name="bias"](%172)
  %174 : Tensor = prim::GetAttr[name="weight"](%172)
  %175 : Float(3072:1, 768:3072) = aten::t(%174), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %175), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %173, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.2, %input_tensor.1, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output # transformers/modeling_bert.py:371:0
  %180 : Tensor = prim::GetAttr[name="bias"](%171)
  %181 : Tensor = prim::GetAttr[name="weight"](%171)
  %182 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm
  %input.15 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %182, %181, %180, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %184 : __torch__.transformers.modeling_bert.___torch_mangle_4445.BertOutput = prim::GetAttr[name="output"](%93)
  %185 : __torch__.transformers.modeling_bert.___torch_mangle_4441.BertIntermediate = prim::GetAttr[name="intermediate"](%93)
  %186 : __torch__.transformers.modeling_bert.___torch_mangle_4439.BertAttention = prim::GetAttr[name="attention"](%93)
  %187 : __torch__.transformers.modeling_bert.___torch_mangle_4438.BertSelfOutput = prim::GetAttr[name="output"](%186)
  %188 : __torch__.transformers.modeling_bert.___torch_mangle_4434.BertSelfAttention = prim::GetAttr[name="self"](%186)
  %189 : __torch__.torch.nn.modules.linear.___torch_mangle_4432.Linear = prim::GetAttr[name="value"](%188)
  %190 : __torch__.torch.nn.modules.linear.___torch_mangle_4431.Linear = prim::GetAttr[name="key"](%188)
  %191 : __torch__.torch.nn.modules.linear.___torch_mangle_4430.Linear = prim::GetAttr[name="query"](%188)
  %192 : Tensor = prim::GetAttr[name="bias"](%191)
  %193 : Tensor = prim::GetAttr[name="weight"](%191)
  %194 : Float(768:1, 768:768) = aten::t(%193), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %194), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %192, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %197 : Tensor = prim::GetAttr[name="bias"](%190)
  %198 : Tensor = prim::GetAttr[name="weight"](%190)
  %199 : Float(768:1, 768:768) = aten::t(%198), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %199), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %197, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %202 : Tensor = prim::GetAttr[name="bias"](%189)
  %203 : Tensor = prim::GetAttr[name="weight"](%189)
  %204 : Float(768:1, 768:768) = aten::t(%203), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %204), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.9, %202, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %207 : int = aten::size(%x.7, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %208 : int = aten::size(%x.7, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %209 : int[] = prim::ListConstruct(%207, %208, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.7, %209), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %211 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.8, %211), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %213 : int = aten::size(%x.9, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %214 : int = aten::size(%x.9, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %215 : int[] = prim::ListConstruct(%213, %214, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.9, %215), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %217 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.10, %217), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %219 : int = aten::size(%x.11, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %220 : int = aten::size(%x.11, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %221 : int[] = prim::ListConstruct(%219, %220, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.11, %221), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %223 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.12, %223), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %225 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.2, %21, %18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %225), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.3, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:259:0
  %input.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:262:0
  %input.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.16, %21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.17, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:275:0
  %232 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %233 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.3, %232), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%233, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
  %235 : int = aten::size(%context_layer.4, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
  %236 : int = aten::size(%context_layer.4, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
  %237 : int[] = prim::ListConstruct(%235, %236, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.4, %237), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:279:0
  %239 : __torch__.torch.nn.modules.normalization.___torch_mangle_4436.LayerNorm = prim::GetAttr[name="LayerNorm"](%187)
  %240 : __torch__.torch.nn.modules.linear.___torch_mangle_4435.Linear = prim::GetAttr[name="dense"](%187)
  %241 : Tensor = prim::GetAttr[name="bias"](%240)
  %242 : Tensor = prim::GetAttr[name="weight"](%240)
  %243 : Float(768:1, 768:768) = aten::t(%242), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.18, %243), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %241, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.19, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.3, %input.15, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output # transformers/modeling_bert.py:295:0
  %248 : Tensor = prim::GetAttr[name="bias"](%239)
  %249 : Tensor = prim::GetAttr[name="weight"](%239)
  %250 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.20, %250, %249, %248, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %252 : __torch__.torch.nn.modules.linear.___torch_mangle_4440.Linear = prim::GetAttr[name="dense"](%185)
  %253 : Tensor = prim::GetAttr[name="bias"](%252)
  %254 : Tensor = prim::GetAttr[name="weight"](%252)
  %255 : Float(768:1, 3072:768) = aten::t(%254), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.2, %255), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.11, %253, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.22 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %259 : __torch__.torch.nn.modules.normalization.___torch_mangle_4443.LayerNorm = prim::GetAttr[name="LayerNorm"](%184)
  %260 : __torch__.torch.nn.modules.linear.___torch_mangle_4442.Linear = prim::GetAttr[name="dense"](%184)
  %261 : Tensor = prim::GetAttr[name="bias"](%260)
  %262 : Tensor = prim::GetAttr[name="weight"](%260)
  %263 : Float(3072:1, 768:3072) = aten::t(%262), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.22, %263), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %261, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.23, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.4, %input_tensor.2, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output # transformers/modeling_bert.py:371:0
  %268 : Tensor = prim::GetAttr[name="bias"](%259)
  %269 : Tensor = prim::GetAttr[name="weight"](%259)
  %270 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.24, %270, %269, %268, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %272 : __torch__.transformers.modeling_bert.___torch_mangle_4462.BertOutput = prim::GetAttr[name="output"](%91)
  %273 : __torch__.transformers.modeling_bert.___torch_mangle_4458.BertIntermediate = prim::GetAttr[name="intermediate"](%91)
  %274 : __torch__.transformers.modeling_bert.___torch_mangle_4456.BertAttention = prim::GetAttr[name="attention"](%91)
  %275 : __torch__.transformers.modeling_bert.___torch_mangle_4455.BertSelfOutput = prim::GetAttr[name="output"](%274)
  %276 : __torch__.transformers.modeling_bert.___torch_mangle_4451.BertSelfAttention = prim::GetAttr[name="self"](%274)
  %277 : __torch__.torch.nn.modules.linear.___torch_mangle_4449.Linear = prim::GetAttr[name="value"](%276)
  %278 : __torch__.torch.nn.modules.linear.___torch_mangle_4448.Linear = prim::GetAttr[name="key"](%276)
  %279 : __torch__.torch.nn.modules.linear.___torch_mangle_4447.Linear = prim::GetAttr[name="query"](%276)
  %280 : Tensor = prim::GetAttr[name="bias"](%279)
  %281 : Tensor = prim::GetAttr[name="weight"](%279)
  %282 : Float(768:1, 768:768) = aten::t(%281), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %282), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %280, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %285 : Tensor = prim::GetAttr[name="bias"](%278)
  %286 : Tensor = prim::GetAttr[name="weight"](%278)
  %287 : Float(768:1, 768:768) = aten::t(%286), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %287), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.14, %285, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %290 : Tensor = prim::GetAttr[name="bias"](%277)
  %291 : Tensor = prim::GetAttr[name="weight"](%277)
  %292 : Float(768:1, 768:768) = aten::t(%291), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %292), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %290, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %295 : int = aten::size(%x.13, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %296 : int = aten::size(%x.13, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %297 : int[] = prim::ListConstruct(%295, %296, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.13, %297), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %299 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.14, %299), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %301 : int = aten::size(%x.15, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %302 : int = aten::size(%x.15, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %303 : int[] = prim::ListConstruct(%301, %302, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.15, %303), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %305 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.16, %305), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %307 : int = aten::size(%x.17, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %308 : int = aten::size(%x.17, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %309 : int[] = prim::ListConstruct(%307, %308, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.17, %309), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %311 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.18, %311), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %313 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.3, %21, %18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %313), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.5, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:259:0
  %input.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:262:0
  %input.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.26, %21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.27, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:275:0
  %320 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %321 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.5, %320), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%321, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
  %323 : int = aten::size(%context_layer.6, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
  %324 : int = aten::size(%context_layer.6, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
  %325 : int[] = prim::ListConstruct(%323, %324, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.6, %325), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:279:0
  %327 : __torch__.torch.nn.modules.normalization.___torch_mangle_4453.LayerNorm = prim::GetAttr[name="LayerNorm"](%275)
  %328 : __torch__.torch.nn.modules.linear.___torch_mangle_4452.Linear = prim::GetAttr[name="dense"](%275)
  %329 : Tensor = prim::GetAttr[name="bias"](%328)
  %330 : Tensor = prim::GetAttr[name="weight"](%328)
  %331 : Float(768:1, 768:768) = aten::t(%330), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.28, %331), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %329, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.29, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.30 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.5, %input.25, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output # transformers/modeling_bert.py:295:0
  %336 : Tensor = prim::GetAttr[name="bias"](%327)
  %337 : Tensor = prim::GetAttr[name="weight"](%327)
  %338 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.30, %338, %337, %336, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %340 : __torch__.torch.nn.modules.linear.___torch_mangle_4457.Linear = prim::GetAttr[name="dense"](%273)
  %341 : Tensor = prim::GetAttr[name="bias"](%340)
  %342 : Tensor = prim::GetAttr[name="weight"](%340)
  %343 : Float(768:1, 3072:768) = aten::t(%342), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.3, %343), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.31 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.17, %341, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.32 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %347 : __torch__.torch.nn.modules.normalization.___torch_mangle_4460.LayerNorm = prim::GetAttr[name="LayerNorm"](%272)
  %348 : __torch__.torch.nn.modules.linear.___torch_mangle_4459.Linear = prim::GetAttr[name="dense"](%272)
  %349 : Tensor = prim::GetAttr[name="bias"](%348)
  %350 : Tensor = prim::GetAttr[name="weight"](%348)
  %351 : Float(3072:1, 768:3072) = aten::t(%350), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.32, %351), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %349, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.33, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.6, %input_tensor.3, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output # transformers/modeling_bert.py:371:0
  %356 : Tensor = prim::GetAttr[name="bias"](%347)
  %357 : Tensor = prim::GetAttr[name="weight"](%347)
  %358 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.34, %358, %357, %356, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %360 : __torch__.transformers.modeling_bert.___torch_mangle_4479.BertOutput = prim::GetAttr[name="output"](%89)
  %361 : __torch__.transformers.modeling_bert.___torch_mangle_4475.BertIntermediate = prim::GetAttr[name="intermediate"](%89)
  %362 : __torch__.transformers.modeling_bert.___torch_mangle_4473.BertAttention = prim::GetAttr[name="attention"](%89)
  %363 : __torch__.transformers.modeling_bert.___torch_mangle_4472.BertSelfOutput = prim::GetAttr[name="output"](%362)
  %364 : __torch__.transformers.modeling_bert.___torch_mangle_4468.BertSelfAttention = prim::GetAttr[name="self"](%362)
  %365 : __torch__.torch.nn.modules.linear.___torch_mangle_4466.Linear = prim::GetAttr[name="value"](%364)
  %366 : __torch__.torch.nn.modules.linear.___torch_mangle_4465.Linear = prim::GetAttr[name="key"](%364)
  %367 : __torch__.torch.nn.modules.linear.___torch_mangle_4464.Linear = prim::GetAttr[name="query"](%364)
  %368 : Tensor = prim::GetAttr[name="bias"](%367)
  %369 : Tensor = prim::GetAttr[name="weight"](%367)
  %370 : Float(768:1, 768:768) = aten::t(%369), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %370), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.19, %368, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %373 : Tensor = prim::GetAttr[name="bias"](%366)
  %374 : Tensor = prim::GetAttr[name="weight"](%366)
  %375 : Float(768:1, 768:768) = aten::t(%374), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %375), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %373, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %378 : Tensor = prim::GetAttr[name="bias"](%365)
  %379 : Tensor = prim::GetAttr[name="weight"](%365)
  %380 : Float(768:1, 768:768) = aten::t(%379), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %380), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %378, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %383 : int = aten::size(%x.19, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %384 : int = aten::size(%x.19, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %385 : int[] = prim::ListConstruct(%383, %384, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.19, %385), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %387 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.20, %387), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %389 : int = aten::size(%x.21, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %390 : int = aten::size(%x.21, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %391 : int[] = prim::ListConstruct(%389, %390, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.21, %391), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %393 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.22, %393), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %395 : int = aten::size(%x.23, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %396 : int = aten::size(%x.23, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %397 : int[] = prim::ListConstruct(%395, %396, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.24 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.23, %397), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %399 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.24, %399), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %401 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.4, %21, %18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %401), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.7, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:259:0
  %input.36 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:262:0
  %input.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.36, %21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.37, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:275:0
  %408 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %409 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.7, %408), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%409, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
  %411 : int = aten::size(%context_layer.8, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
  %412 : int = aten::size(%context_layer.8, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
  %413 : int[] = prim::ListConstruct(%411, %412, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %input.38 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.8, %413), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:279:0
  %415 : __torch__.torch.nn.modules.normalization.___torch_mangle_4470.LayerNorm = prim::GetAttr[name="LayerNorm"](%363)
  %416 : __torch__.torch.nn.modules.linear.___torch_mangle_4469.Linear = prim::GetAttr[name="dense"](%363)
  %417 : Tensor = prim::GetAttr[name="bias"](%416)
  %418 : Tensor = prim::GetAttr[name="weight"](%416)
  %419 : Float(768:1, 768:768) = aten::t(%418), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.38, %419), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %417, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.39, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.7, %input.35, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output # transformers/modeling_bert.py:295:0
  %424 : Tensor = prim::GetAttr[name="bias"](%415)
  %425 : Tensor = prim::GetAttr[name="weight"](%415)
  %426 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.40, %426, %425, %424, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %428 : __torch__.torch.nn.modules.linear.___torch_mangle_4474.Linear = prim::GetAttr[name="dense"](%361)
  %429 : Tensor = prim::GetAttr[name="bias"](%428)
  %430 : Tensor = prim::GetAttr[name="weight"](%428)
  %431 : Float(768:1, 3072:768) = aten::t(%430), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.4, %431), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.41 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.23, %429, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.42 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.41), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %435 : __torch__.torch.nn.modules.normalization.___torch_mangle_4477.LayerNorm = prim::GetAttr[name="LayerNorm"](%360)
  %436 : __torch__.torch.nn.modules.linear.___torch_mangle_4476.Linear = prim::GetAttr[name="dense"](%360)
  %437 : Tensor = prim::GetAttr[name="bias"](%436)
  %438 : Tensor = prim::GetAttr[name="weight"](%436)
  %439 : Float(3072:1, 768:3072) = aten::t(%438), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.42, %439), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.24, %437, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.43, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.44 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.8, %input_tensor.4, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output # transformers/modeling_bert.py:371:0
  %444 : Tensor = prim::GetAttr[name="bias"](%435)
  %445 : Tensor = prim::GetAttr[name="weight"](%435)
  %446 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm
  %input.45 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.44, %446, %445, %444, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %448 : __torch__.transformers.modeling_bert.___torch_mangle_4496.BertOutput = prim::GetAttr[name="output"](%87)
  %449 : __torch__.transformers.modeling_bert.___torch_mangle_4492.BertIntermediate = prim::GetAttr[name="intermediate"](%87)
  %450 : __torch__.transformers.modeling_bert.___torch_mangle_4490.BertAttention = prim::GetAttr[name="attention"](%87)
  %451 : __torch__.transformers.modeling_bert.___torch_mangle_4489.BertSelfOutput = prim::GetAttr[name="output"](%450)
  %452 : __torch__.transformers.modeling_bert.___torch_mangle_4485.BertSelfAttention = prim::GetAttr[name="self"](%450)
  %453 : __torch__.torch.nn.modules.linear.___torch_mangle_4483.Linear = prim::GetAttr[name="value"](%452)
  %454 : __torch__.torch.nn.modules.linear.___torch_mangle_4482.Linear = prim::GetAttr[name="key"](%452)
  %455 : __torch__.torch.nn.modules.linear.___torch_mangle_4481.Linear = prim::GetAttr[name="query"](%452)
  %456 : Tensor = prim::GetAttr[name="bias"](%455)
  %457 : Tensor = prim::GetAttr[name="weight"](%455)
  %458 : Float(768:1, 768:768) = aten::t(%457), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %458), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.25, %456, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %461 : Tensor = prim::GetAttr[name="bias"](%454)
  %462 : Tensor = prim::GetAttr[name="weight"](%454)
  %463 : Float(768:1, 768:768) = aten::t(%462), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %463), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.26, %461, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %466 : Tensor = prim::GetAttr[name="bias"](%453)
  %467 : Tensor = prim::GetAttr[name="weight"](%453)
  %468 : Float(768:1, 768:768) = aten::t(%467), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %468), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.27, %466, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %471 : int = aten::size(%x.25, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %472 : int = aten::size(%x.25, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %473 : int[] = prim::ListConstruct(%471, %472, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.25, %473), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %475 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.26, %475), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %477 : int = aten::size(%x.27, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %478 : int = aten::size(%x.27, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %479 : int[] = prim::ListConstruct(%477, %478, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.28 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.27, %479), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %481 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.28, %481), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %483 : int = aten::size(%x.29, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %484 : int = aten::size(%x.29, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %485 : int[] = prim::ListConstruct(%483, %484, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.30 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.29, %485), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %487 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.30, %487), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %489 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.5, %21, %18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %489), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.9, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:259:0
  %input.46 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:262:0
  %input.47 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.46, %21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.47, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:275:0
  %496 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %497 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.9, %496), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%497, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
  %499 : int = aten::size(%context_layer.10, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
  %500 : int = aten::size(%context_layer.10, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
  %501 : int[] = prim::ListConstruct(%499, %500, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %input.48 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.10, %501), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:279:0
  %503 : __torch__.torch.nn.modules.normalization.___torch_mangle_4487.LayerNorm = prim::GetAttr[name="LayerNorm"](%451)
  %504 : __torch__.torch.nn.modules.linear.___torch_mangle_4486.Linear = prim::GetAttr[name="dense"](%451)
  %505 : Tensor = prim::GetAttr[name="bias"](%504)
  %506 : Tensor = prim::GetAttr[name="weight"](%504)
  %507 : Float(768:1, 768:768) = aten::t(%506), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.48, %507), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.28, %505, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.49, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.9, %input.45, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output # transformers/modeling_bert.py:295:0
  %512 : Tensor = prim::GetAttr[name="bias"](%503)
  %513 : Tensor = prim::GetAttr[name="weight"](%503)
  %514 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.50, %514, %513, %512, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %516 : __torch__.torch.nn.modules.linear.___torch_mangle_4491.Linear = prim::GetAttr[name="dense"](%449)
  %517 : Tensor = prim::GetAttr[name="bias"](%516)
  %518 : Tensor = prim::GetAttr[name="weight"](%516)
  %519 : Float(768:1, 3072:768) = aten::t(%518), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.5, %519), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.51 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.29, %517, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.52 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %523 : __torch__.torch.nn.modules.normalization.___torch_mangle_4494.LayerNorm = prim::GetAttr[name="LayerNorm"](%448)
  %524 : __torch__.torch.nn.modules.linear.___torch_mangle_4493.Linear = prim::GetAttr[name="dense"](%448)
  %525 : Tensor = prim::GetAttr[name="bias"](%524)
  %526 : Tensor = prim::GetAttr[name="weight"](%524)
  %527 : Float(3072:1, 768:3072) = aten::t(%526), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.52, %527), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.30, %525, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.53, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.54 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.10, %input_tensor.5, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output # transformers/modeling_bert.py:371:0
  %532 : Tensor = prim::GetAttr[name="bias"](%523)
  %533 : Tensor = prim::GetAttr[name="weight"](%523)
  %534 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm
  %input.55 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.54, %534, %533, %532, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %536 : __torch__.transformers.modeling_bert.___torch_mangle_4513.BertOutput = prim::GetAttr[name="output"](%85)
  %537 : __torch__.transformers.modeling_bert.___torch_mangle_4509.BertIntermediate = prim::GetAttr[name="intermediate"](%85)
  %538 : __torch__.transformers.modeling_bert.___torch_mangle_4507.BertAttention = prim::GetAttr[name="attention"](%85)
  %539 : __torch__.transformers.modeling_bert.___torch_mangle_4506.BertSelfOutput = prim::GetAttr[name="output"](%538)
  %540 : __torch__.transformers.modeling_bert.___torch_mangle_4502.BertSelfAttention = prim::GetAttr[name="self"](%538)
  %541 : __torch__.torch.nn.modules.linear.___torch_mangle_4500.Linear = prim::GetAttr[name="value"](%540)
  %542 : __torch__.torch.nn.modules.linear.___torch_mangle_4499.Linear = prim::GetAttr[name="key"](%540)
  %543 : __torch__.torch.nn.modules.linear.___torch_mangle_4498.Linear = prim::GetAttr[name="query"](%540)
  %544 : Tensor = prim::GetAttr[name="bias"](%543)
  %545 : Tensor = prim::GetAttr[name="weight"](%543)
  %546 : Float(768:1, 768:768) = aten::t(%545), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %546), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.31, %544, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %549 : Tensor = prim::GetAttr[name="bias"](%542)
  %550 : Tensor = prim::GetAttr[name="weight"](%542)
  %551 : Float(768:1, 768:768) = aten::t(%550), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %551), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.32, %549, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %554 : Tensor = prim::GetAttr[name="bias"](%541)
  %555 : Tensor = prim::GetAttr[name="weight"](%541)
  %556 : Float(768:1, 768:768) = aten::t(%555), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %556), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.33, %554, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %559 : int = aten::size(%x.31, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %560 : int = aten::size(%x.31, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %561 : int[] = prim::ListConstruct(%559, %560, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.32 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.31, %561), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %563 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.32, %563), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %565 : int = aten::size(%x.33, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %566 : int = aten::size(%x.33, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %567 : int[] = prim::ListConstruct(%565, %566, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.34 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.33, %567), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %569 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.34, %569), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %571 : int = aten::size(%x.35, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %572 : int = aten::size(%x.35, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %573 : int[] = prim::ListConstruct(%571, %572, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.36 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.35, %573), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %575 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.36, %575), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %577 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.6, %21, %18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %577), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.11, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:259:0
  %input.56 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:262:0
  %input.57 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.56, %21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.57, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:275:0
  %584 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %585 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.11, %584), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%585, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
  %587 : int = aten::size(%context_layer.12, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
  %588 : int = aten::size(%context_layer.12, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
  %589 : int[] = prim::ListConstruct(%587, %588, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %input.58 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.12, %589), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:279:0
  %591 : __torch__.torch.nn.modules.normalization.___torch_mangle_4504.LayerNorm = prim::GetAttr[name="LayerNorm"](%539)
  %592 : __torch__.torch.nn.modules.linear.___torch_mangle_4503.Linear = prim::GetAttr[name="dense"](%539)
  %593 : Tensor = prim::GetAttr[name="bias"](%592)
  %594 : Tensor = prim::GetAttr[name="weight"](%592)
  %595 : Float(768:1, 768:768) = aten::t(%594), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.58, %595), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.34, %593, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.59, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.11, %input.55, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output # transformers/modeling_bert.py:295:0
  %600 : Tensor = prim::GetAttr[name="bias"](%591)
  %601 : Tensor = prim::GetAttr[name="weight"](%591)
  %602 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.60, %602, %601, %600, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %604 : __torch__.torch.nn.modules.linear.___torch_mangle_4508.Linear = prim::GetAttr[name="dense"](%537)
  %605 : Tensor = prim::GetAttr[name="bias"](%604)
  %606 : Tensor = prim::GetAttr[name="weight"](%604)
  %607 : Float(768:1, 3072:768) = aten::t(%606), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.6, %607), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.61 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.35, %605, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.62 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.61), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %611 : __torch__.torch.nn.modules.normalization.___torch_mangle_4511.LayerNorm = prim::GetAttr[name="LayerNorm"](%536)
  %612 : __torch__.torch.nn.modules.linear.___torch_mangle_4510.Linear = prim::GetAttr[name="dense"](%536)
  %613 : Tensor = prim::GetAttr[name="bias"](%612)
  %614 : Tensor = prim::GetAttr[name="weight"](%612)
  %615 : Float(3072:1, 768:3072) = aten::t(%614), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.62, %615), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.36, %613, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.63, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.12, %input_tensor.6, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output # transformers/modeling_bert.py:371:0
  %620 : Tensor = prim::GetAttr[name="bias"](%611)
  %621 : Tensor = prim::GetAttr[name="weight"](%611)
  %622 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm
  %input.65 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.64, %622, %621, %620, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %624 : __torch__.transformers.modeling_bert.___torch_mangle_4530.BertOutput = prim::GetAttr[name="output"](%83)
  %625 : __torch__.transformers.modeling_bert.___torch_mangle_4526.BertIntermediate = prim::GetAttr[name="intermediate"](%83)
  %626 : __torch__.transformers.modeling_bert.___torch_mangle_4524.BertAttention = prim::GetAttr[name="attention"](%83)
  %627 : __torch__.transformers.modeling_bert.___torch_mangle_4523.BertSelfOutput = prim::GetAttr[name="output"](%626)
  %628 : __torch__.transformers.modeling_bert.___torch_mangle_4519.BertSelfAttention = prim::GetAttr[name="self"](%626)
  %629 : __torch__.torch.nn.modules.linear.___torch_mangle_4517.Linear = prim::GetAttr[name="value"](%628)
  %630 : __torch__.torch.nn.modules.linear.___torch_mangle_4516.Linear = prim::GetAttr[name="key"](%628)
  %631 : __torch__.torch.nn.modules.linear.___torch_mangle_4515.Linear = prim::GetAttr[name="query"](%628)
  %632 : Tensor = prim::GetAttr[name="bias"](%631)
  %633 : Tensor = prim::GetAttr[name="weight"](%631)
  %634 : Float(768:1, 768:768) = aten::t(%633), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %634), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.37, %632, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %637 : Tensor = prim::GetAttr[name="bias"](%630)
  %638 : Tensor = prim::GetAttr[name="weight"](%630)
  %639 : Float(768:1, 768:768) = aten::t(%638), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %639), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.38, %637, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %642 : Tensor = prim::GetAttr[name="bias"](%629)
  %643 : Tensor = prim::GetAttr[name="weight"](%629)
  %644 : Float(768:1, 768:768) = aten::t(%643), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %644), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.39, %642, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %647 : int = aten::size(%x.37, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %648 : int = aten::size(%x.37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %649 : int[] = prim::ListConstruct(%647, %648, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.38 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.37, %649), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %651 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.38, %651), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %653 : int = aten::size(%x.39, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %654 : int = aten::size(%x.39, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %655 : int[] = prim::ListConstruct(%653, %654, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.40 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.39, %655), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %657 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.40, %657), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %659 : int = aten::size(%x.41, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %660 : int = aten::size(%x.41, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %661 : int[] = prim::ListConstruct(%659, %660, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.42 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.41, %661), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %663 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.42, %663), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %665 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.7, %21, %18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %665), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.14 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.13, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:259:0
  %input.66 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:262:0
  %input.67 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.66, %21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.67, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:275:0
  %672 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %673 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.13, %672), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%673, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
  %675 : int = aten::size(%context_layer.14, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
  %676 : int = aten::size(%context_layer.14, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
  %677 : int[] = prim::ListConstruct(%675, %676, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %input.68 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.14, %677), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:279:0
  %679 : __torch__.torch.nn.modules.normalization.___torch_mangle_4521.LayerNorm = prim::GetAttr[name="LayerNorm"](%627)
  %680 : __torch__.torch.nn.modules.linear.___torch_mangle_4520.Linear = prim::GetAttr[name="dense"](%627)
  %681 : Tensor = prim::GetAttr[name="bias"](%680)
  %682 : Tensor = prim::GetAttr[name="weight"](%680)
  %683 : Float(768:1, 768:768) = aten::t(%682), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.68, %683), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.40, %681, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.69, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.70 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.13, %input.65, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output # transformers/modeling_bert.py:295:0
  %688 : Tensor = prim::GetAttr[name="bias"](%679)
  %689 : Tensor = prim::GetAttr[name="weight"](%679)
  %690 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.70, %690, %689, %688, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %692 : __torch__.torch.nn.modules.linear.___torch_mangle_4525.Linear = prim::GetAttr[name="dense"](%625)
  %693 : Tensor = prim::GetAttr[name="bias"](%692)
  %694 : Tensor = prim::GetAttr[name="weight"](%692)
  %695 : Float(768:1, 3072:768) = aten::t(%694), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.7, %695), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.71 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.41, %693, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.72 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %699 : __torch__.torch.nn.modules.normalization.___torch_mangle_4528.LayerNorm = prim::GetAttr[name="LayerNorm"](%624)
  %700 : __torch__.torch.nn.modules.linear.___torch_mangle_4527.Linear = prim::GetAttr[name="dense"](%624)
  %701 : Tensor = prim::GetAttr[name="bias"](%700)
  %702 : Tensor = prim::GetAttr[name="weight"](%700)
  %703 : Float(3072:1, 768:3072) = aten::t(%702), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.72, %703), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.73 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.42, %701, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.73, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.14, %input_tensor.7, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output # transformers/modeling_bert.py:371:0
  %708 : Tensor = prim::GetAttr[name="bias"](%699)
  %709 : Tensor = prim::GetAttr[name="weight"](%699)
  %710 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm
  %input.75 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.74, %710, %709, %708, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %712 : __torch__.transformers.modeling_bert.___torch_mangle_4547.BertOutput = prim::GetAttr[name="output"](%81)
  %713 : __torch__.transformers.modeling_bert.___torch_mangle_4543.BertIntermediate = prim::GetAttr[name="intermediate"](%81)
  %714 : __torch__.transformers.modeling_bert.___torch_mangle_4541.BertAttention = prim::GetAttr[name="attention"](%81)
  %715 : __torch__.transformers.modeling_bert.___torch_mangle_4540.BertSelfOutput = prim::GetAttr[name="output"](%714)
  %716 : __torch__.transformers.modeling_bert.___torch_mangle_4536.BertSelfAttention = prim::GetAttr[name="self"](%714)
  %717 : __torch__.torch.nn.modules.linear.___torch_mangle_4534.Linear = prim::GetAttr[name="value"](%716)
  %718 : __torch__.torch.nn.modules.linear.___torch_mangle_4533.Linear = prim::GetAttr[name="key"](%716)
  %719 : __torch__.torch.nn.modules.linear.___torch_mangle_4532.Linear = prim::GetAttr[name="query"](%716)
  %720 : Tensor = prim::GetAttr[name="bias"](%719)
  %721 : Tensor = prim::GetAttr[name="weight"](%719)
  %722 : Float(768:1, 768:768) = aten::t(%721), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %722), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.43, %720, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %725 : Tensor = prim::GetAttr[name="bias"](%718)
  %726 : Tensor = prim::GetAttr[name="weight"](%718)
  %727 : Float(768:1, 768:768) = aten::t(%726), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %727), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.44, %725, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %730 : Tensor = prim::GetAttr[name="bias"](%717)
  %731 : Tensor = prim::GetAttr[name="weight"](%717)
  %732 : Float(768:1, 768:768) = aten::t(%731), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %732), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.45, %730, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %735 : int = aten::size(%x.43, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %736 : int = aten::size(%x.43, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %737 : int[] = prim::ListConstruct(%735, %736, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.44 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.43, %737), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %739 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.44, %739), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %741 : int = aten::size(%x.45, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %742 : int = aten::size(%x.45, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %743 : int[] = prim::ListConstruct(%741, %742, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.46 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.45, %743), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %745 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.46, %745), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %747 : int = aten::size(%x.47, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %748 : int = aten::size(%x.47, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %749 : int[] = prim::ListConstruct(%747, %748, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.48 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.47, %749), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %751 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.48, %751), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %753 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.8, %21, %18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %753), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.15, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:259:0
  %input.76 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:262:0
  %input.77 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.76, %21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.77, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:275:0
  %760 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %761 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.15, %760), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%761, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
  %763 : int = aten::size(%context_layer.16, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
  %764 : int = aten::size(%context_layer.16, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
  %765 : int[] = prim::ListConstruct(%763, %764, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %input.78 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.16, %765), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:279:0
  %767 : __torch__.torch.nn.modules.normalization.___torch_mangle_4538.LayerNorm = prim::GetAttr[name="LayerNorm"](%715)
  %768 : __torch__.torch.nn.modules.linear.___torch_mangle_4537.Linear = prim::GetAttr[name="dense"](%715)
  %769 : Tensor = prim::GetAttr[name="bias"](%768)
  %770 : Tensor = prim::GetAttr[name="weight"](%768)
  %771 : Float(768:1, 768:768) = aten::t(%770), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.78, %771), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.79 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.46, %769, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.79, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.80 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.15, %input.75, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output # transformers/modeling_bert.py:295:0
  %776 : Tensor = prim::GetAttr[name="bias"](%767)
  %777 : Tensor = prim::GetAttr[name="weight"](%767)
  %778 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.80, %778, %777, %776, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %780 : __torch__.torch.nn.modules.linear.___torch_mangle_4542.Linear = prim::GetAttr[name="dense"](%713)
  %781 : Tensor = prim::GetAttr[name="bias"](%780)
  %782 : Tensor = prim::GetAttr[name="weight"](%780)
  %783 : Float(768:1, 3072:768) = aten::t(%782), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.8, %783), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.81 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.47, %781, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.82 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.81), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %787 : __torch__.torch.nn.modules.normalization.___torch_mangle_4545.LayerNorm = prim::GetAttr[name="LayerNorm"](%712)
  %788 : __torch__.torch.nn.modules.linear.___torch_mangle_4544.Linear = prim::GetAttr[name="dense"](%712)
  %789 : Tensor = prim::GetAttr[name="bias"](%788)
  %790 : Tensor = prim::GetAttr[name="weight"](%788)
  %791 : Float(3072:1, 768:3072) = aten::t(%790), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.82, %791), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.83 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.48, %789, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.83, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.84 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.16, %input_tensor.8, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output # transformers/modeling_bert.py:371:0
  %796 : Tensor = prim::GetAttr[name="bias"](%787)
  %797 : Tensor = prim::GetAttr[name="weight"](%787)
  %798 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm
  %input.85 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.84, %798, %797, %796, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %800 : __torch__.transformers.modeling_bert.___torch_mangle_4564.BertOutput = prim::GetAttr[name="output"](%79)
  %801 : __torch__.transformers.modeling_bert.___torch_mangle_4560.BertIntermediate = prim::GetAttr[name="intermediate"](%79)
  %802 : __torch__.transformers.modeling_bert.___torch_mangle_4558.BertAttention = prim::GetAttr[name="attention"](%79)
  %803 : __torch__.transformers.modeling_bert.___torch_mangle_4557.BertSelfOutput = prim::GetAttr[name="output"](%802)
  %804 : __torch__.transformers.modeling_bert.___torch_mangle_4553.BertSelfAttention = prim::GetAttr[name="self"](%802)
  %805 : __torch__.torch.nn.modules.linear.___torch_mangle_4551.Linear = prim::GetAttr[name="value"](%804)
  %806 : __torch__.torch.nn.modules.linear.___torch_mangle_4550.Linear = prim::GetAttr[name="key"](%804)
  %807 : __torch__.torch.nn.modules.linear.___torch_mangle_4549.Linear = prim::GetAttr[name="query"](%804)
  %808 : Tensor = prim::GetAttr[name="bias"](%807)
  %809 : Tensor = prim::GetAttr[name="weight"](%807)
  %810 : Float(768:1, 768:768) = aten::t(%809), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %810), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.49, %808, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %813 : Tensor = prim::GetAttr[name="bias"](%806)
  %814 : Tensor = prim::GetAttr[name="weight"](%806)
  %815 : Float(768:1, 768:768) = aten::t(%814), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %815), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.50, %813, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %818 : Tensor = prim::GetAttr[name="bias"](%805)
  %819 : Tensor = prim::GetAttr[name="weight"](%805)
  %820 : Float(768:1, 768:768) = aten::t(%819), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %820), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.51, %818, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %823 : int = aten::size(%x.49, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %824 : int = aten::size(%x.49, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %825 : int[] = prim::ListConstruct(%823, %824, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.50 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.49, %825), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %827 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.50, %827), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %829 : int = aten::size(%x.51, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %830 : int = aten::size(%x.51, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %831 : int[] = prim::ListConstruct(%829, %830, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.52 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.51, %831), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %833 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.52, %833), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %835 : int = aten::size(%x.53, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %836 : int = aten::size(%x.53, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %837 : int[] = prim::ListConstruct(%835, %836, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.53, %837), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %839 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.54, %839), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %841 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.9, %21, %18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %841), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.18 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.17, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:259:0
  %input.86 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:262:0
  %input.87 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.86, %21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.87, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:275:0
  %848 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %849 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.17, %848), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%849, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
  %851 : int = aten::size(%context_layer.18, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
  %852 : int = aten::size(%context_layer.18, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
  %853 : int[] = prim::ListConstruct(%851, %852, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %input.88 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.18, %853), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:279:0
  %855 : __torch__.torch.nn.modules.normalization.___torch_mangle_4555.LayerNorm = prim::GetAttr[name="LayerNorm"](%803)
  %856 : __torch__.torch.nn.modules.linear.___torch_mangle_4554.Linear = prim::GetAttr[name="dense"](%803)
  %857 : Tensor = prim::GetAttr[name="bias"](%856)
  %858 : Tensor = prim::GetAttr[name="weight"](%856)
  %859 : Float(768:1, 768:768) = aten::t(%858), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.88, %859), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.89 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.52, %857, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.89, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.90 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.17, %input.85, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output # transformers/modeling_bert.py:295:0
  %864 : Tensor = prim::GetAttr[name="bias"](%855)
  %865 : Tensor = prim::GetAttr[name="weight"](%855)
  %866 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.90, %866, %865, %864, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %868 : __torch__.torch.nn.modules.linear.___torch_mangle_4559.Linear = prim::GetAttr[name="dense"](%801)
  %869 : Tensor = prim::GetAttr[name="bias"](%868)
  %870 : Tensor = prim::GetAttr[name="weight"](%868)
  %871 : Float(768:1, 3072:768) = aten::t(%870), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.9, %871), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.91 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.53, %869, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.92 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.91), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %875 : __torch__.torch.nn.modules.normalization.___torch_mangle_4562.LayerNorm = prim::GetAttr[name="LayerNorm"](%800)
  %876 : __torch__.torch.nn.modules.linear.___torch_mangle_4561.Linear = prim::GetAttr[name="dense"](%800)
  %877 : Tensor = prim::GetAttr[name="bias"](%876)
  %878 : Tensor = prim::GetAttr[name="weight"](%876)
  %879 : Float(3072:1, 768:3072) = aten::t(%878), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.92, %879), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.93 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.54, %877, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.93, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.94 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.18, %input_tensor.9, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output # transformers/modeling_bert.py:371:0
  %884 : Tensor = prim::GetAttr[name="bias"](%875)
  %885 : Tensor = prim::GetAttr[name="weight"](%875)
  %886 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm
  %input.95 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.94, %886, %885, %884, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %888 : __torch__.transformers.modeling_bert.___torch_mangle_4581.BertOutput = prim::GetAttr[name="output"](%77)
  %889 : __torch__.transformers.modeling_bert.___torch_mangle_4577.BertIntermediate = prim::GetAttr[name="intermediate"](%77)
  %890 : __torch__.transformers.modeling_bert.___torch_mangle_4575.BertAttention = prim::GetAttr[name="attention"](%77)
  %891 : __torch__.transformers.modeling_bert.___torch_mangle_4574.BertSelfOutput = prim::GetAttr[name="output"](%890)
  %892 : __torch__.transformers.modeling_bert.___torch_mangle_4570.BertSelfAttention = prim::GetAttr[name="self"](%890)
  %893 : __torch__.torch.nn.modules.linear.___torch_mangle_4568.Linear = prim::GetAttr[name="value"](%892)
  %894 : __torch__.torch.nn.modules.linear.___torch_mangle_4567.Linear = prim::GetAttr[name="key"](%892)
  %895 : __torch__.torch.nn.modules.linear.___torch_mangle_4566.Linear = prim::GetAttr[name="query"](%892)
  %896 : Tensor = prim::GetAttr[name="bias"](%895)
  %897 : Tensor = prim::GetAttr[name="weight"](%895)
  %898 : Float(768:1, 768:768) = aten::t(%897), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %898), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.55, %896, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %901 : Tensor = prim::GetAttr[name="bias"](%894)
  %902 : Tensor = prim::GetAttr[name="weight"](%894)
  %903 : Float(768:1, 768:768) = aten::t(%902), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %903), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.56, %901, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %906 : Tensor = prim::GetAttr[name="bias"](%893)
  %907 : Tensor = prim::GetAttr[name="weight"](%893)
  %908 : Float(768:1, 768:768) = aten::t(%907), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %908), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.57, %906, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %911 : int = aten::size(%x.55, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %912 : int = aten::size(%x.55, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %913 : int[] = prim::ListConstruct(%911, %912, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.56 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.55, %913), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %915 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.56, %915), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %917 : int = aten::size(%x.57, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %918 : int = aten::size(%x.57, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %919 : int[] = prim::ListConstruct(%917, %918, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.58 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.57, %919), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %921 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.58, %921), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %923 : int = aten::size(%x.59, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %924 : int = aten::size(%x.59, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %925 : int[] = prim::ListConstruct(%923, %924, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.60 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.59, %925), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %927 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.60, %927), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %929 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.10, %21, %18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.19 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %929), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.20 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.19, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:259:0
  %input.96 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:262:0
  %input.97 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.96, %21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.97, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:275:0
  %936 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %937 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.19, %936), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%937, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
  %939 : int = aten::size(%context_layer.20, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
  %940 : int = aten::size(%context_layer.20, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
  %941 : int[] = prim::ListConstruct(%939, %940, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %input.98 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.20, %941), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:279:0
  %943 : __torch__.torch.nn.modules.normalization.___torch_mangle_4572.LayerNorm = prim::GetAttr[name="LayerNorm"](%891)
  %944 : __torch__.torch.nn.modules.linear.___torch_mangle_4571.Linear = prim::GetAttr[name="dense"](%891)
  %945 : Tensor = prim::GetAttr[name="bias"](%944)
  %946 : Tensor = prim::GetAttr[name="weight"](%944)
  %947 : Float(768:1, 768:768) = aten::t(%946), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.98, %947), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.99 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.58, %945, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.99, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.100 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.19, %input.95, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output # transformers/modeling_bert.py:295:0
  %952 : Tensor = prim::GetAttr[name="bias"](%943)
  %953 : Tensor = prim::GetAttr[name="weight"](%943)
  %954 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.100, %954, %953, %952, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %956 : __torch__.torch.nn.modules.linear.___torch_mangle_4576.Linear = prim::GetAttr[name="dense"](%889)
  %957 : Tensor = prim::GetAttr[name="bias"](%956)
  %958 : Tensor = prim::GetAttr[name="weight"](%956)
  %959 : Float(768:1, 3072:768) = aten::t(%958), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.10, %959), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.101 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.59, %957, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.102 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.101), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %963 : __torch__.torch.nn.modules.normalization.___torch_mangle_4579.LayerNorm = prim::GetAttr[name="LayerNorm"](%888)
  %964 : __torch__.torch.nn.modules.linear.___torch_mangle_4578.Linear = prim::GetAttr[name="dense"](%888)
  %965 : Tensor = prim::GetAttr[name="bias"](%964)
  %966 : Tensor = prim::GetAttr[name="weight"](%964)
  %967 : Float(3072:1, 768:3072) = aten::t(%966), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.102, %967), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.103 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.60, %965, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.103, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.104 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.20, %input_tensor.10, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output # transformers/modeling_bert.py:371:0
  %972 : Tensor = prim::GetAttr[name="bias"](%963)
  %973 : Tensor = prim::GetAttr[name="weight"](%963)
  %974 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm
  %input.105 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.104, %974, %973, %972, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %976 : __torch__.transformers.modeling_bert.___torch_mangle_4598.BertOutput = prim::GetAttr[name="output"](%75)
  %977 : __torch__.transformers.modeling_bert.___torch_mangle_4594.BertIntermediate = prim::GetAttr[name="intermediate"](%75)
  %978 : __torch__.transformers.modeling_bert.___torch_mangle_4592.BertAttention = prim::GetAttr[name="attention"](%75)
  %979 : __torch__.transformers.modeling_bert.___torch_mangle_4591.BertSelfOutput = prim::GetAttr[name="output"](%978)
  %980 : __torch__.transformers.modeling_bert.___torch_mangle_4587.BertSelfAttention = prim::GetAttr[name="self"](%978)
  %981 : __torch__.torch.nn.modules.linear.___torch_mangle_4585.Linear = prim::GetAttr[name="value"](%980)
  %982 : __torch__.torch.nn.modules.linear.___torch_mangle_4584.Linear = prim::GetAttr[name="key"](%980)
  %983 : __torch__.torch.nn.modules.linear.___torch_mangle_4583.Linear = prim::GetAttr[name="query"](%980)
  %984 : Tensor = prim::GetAttr[name="bias"](%983)
  %985 : Tensor = prim::GetAttr[name="weight"](%983)
  %986 : Float(768:1, 768:768) = aten::t(%985), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %986), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.61, %984, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %989 : Tensor = prim::GetAttr[name="bias"](%982)
  %990 : Tensor = prim::GetAttr[name="weight"](%982)
  %991 : Float(768:1, 768:768) = aten::t(%990), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %991), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.62, %989, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %994 : Tensor = prim::GetAttr[name="bias"](%981)
  %995 : Tensor = prim::GetAttr[name="weight"](%981)
  %996 : Float(768:1, 768:768) = aten::t(%995), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %996), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.63, %994, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %999 : int = aten::size(%x.61, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1000 : int = aten::size(%x.61, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1001 : int[] = prim::ListConstruct(%999, %1000, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.62 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.61, %1001), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1003 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.62, %1003), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1005 : int = aten::size(%x.63, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1006 : int = aten::size(%x.63, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1007 : int[] = prim::ListConstruct(%1005, %1006, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.64 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.63, %1007), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1009 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.64, %1009), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1011 : int = aten::size(%x.65, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1012 : int = aten::size(%x.65, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1013 : int[] = prim::ListConstruct(%1011, %1012, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.66 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.65, %1013), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1015 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.66, %1015), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1017 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.11, %21, %18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.21 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1017), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.22 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.21, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:259:0
  %input.106 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:262:0
  %input.107 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.106, %21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.107, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:275:0
  %1024 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %1025 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.21, %1024), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1025, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
  %1027 : int = aten::size(%context_layer.22, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
  %1028 : int = aten::size(%context_layer.22, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
  %1029 : int[] = prim::ListConstruct(%1027, %1028, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %input.108 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.22, %1029), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:279:0
  %1031 : __torch__.torch.nn.modules.normalization.___torch_mangle_4589.LayerNorm = prim::GetAttr[name="LayerNorm"](%979)
  %1032 : __torch__.torch.nn.modules.linear.___torch_mangle_4588.Linear = prim::GetAttr[name="dense"](%979)
  %1033 : Tensor = prim::GetAttr[name="bias"](%1032)
  %1034 : Tensor = prim::GetAttr[name="weight"](%1032)
  %1035 : Float(768:1, 768:768) = aten::t(%1034), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.108, %1035), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.109 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.64, %1033, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.109, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.21, %input.105, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output # transformers/modeling_bert.py:295:0
  %1040 : Tensor = prim::GetAttr[name="bias"](%1031)
  %1041 : Tensor = prim::GetAttr[name="weight"](%1031)
  %1042 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.110, %1042, %1041, %1040, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1044 : __torch__.torch.nn.modules.linear.___torch_mangle_4593.Linear = prim::GetAttr[name="dense"](%977)
  %1045 : Tensor = prim::GetAttr[name="bias"](%1044)
  %1046 : Tensor = prim::GetAttr[name="weight"](%1044)
  %1047 : Float(768:1, 3072:768) = aten::t(%1046), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.11, %1047), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.111 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.65, %1045, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.112 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.111), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1051 : __torch__.torch.nn.modules.normalization.___torch_mangle_4596.LayerNorm = prim::GetAttr[name="LayerNorm"](%976)
  %1052 : __torch__.torch.nn.modules.linear.___torch_mangle_4595.Linear = prim::GetAttr[name="dense"](%976)
  %1053 : Tensor = prim::GetAttr[name="bias"](%1052)
  %1054 : Tensor = prim::GetAttr[name="weight"](%1052)
  %1055 : Float(3072:1, 768:3072) = aten::t(%1054), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.112, %1055), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.113 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.66, %1053, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.113, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.22, %input_tensor.11, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output # transformers/modeling_bert.py:371:0
  %1060 : Tensor = prim::GetAttr[name="bias"](%1051)
  %1061 : Tensor = prim::GetAttr[name="weight"](%1051)
  %1062 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm
  %input.115 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.114, %1062, %1061, %1060, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1064 : __torch__.transformers.modeling_bert.___torch_mangle_4615.BertOutput = prim::GetAttr[name="output"](%73)
  %1065 : __torch__.transformers.modeling_bert.___torch_mangle_4611.BertIntermediate = prim::GetAttr[name="intermediate"](%73)
  %1066 : __torch__.transformers.modeling_bert.___torch_mangle_4609.BertAttention = prim::GetAttr[name="attention"](%73)
  %1067 : __torch__.transformers.modeling_bert.___torch_mangle_4608.BertSelfOutput = prim::GetAttr[name="output"](%1066)
  %1068 : __torch__.transformers.modeling_bert.___torch_mangle_4604.BertSelfAttention = prim::GetAttr[name="self"](%1066)
  %1069 : __torch__.torch.nn.modules.linear.___torch_mangle_4602.Linear = prim::GetAttr[name="value"](%1068)
  %1070 : __torch__.torch.nn.modules.linear.___torch_mangle_4601.Linear = prim::GetAttr[name="key"](%1068)
  %1071 : __torch__.torch.nn.modules.linear.___torch_mangle_4600.Linear = prim::GetAttr[name="query"](%1068)
  %1072 : Tensor = prim::GetAttr[name="bias"](%1071)
  %1073 : Tensor = prim::GetAttr[name="weight"](%1071)
  %1074 : Float(768:1, 768:768) = aten::t(%1073), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1074), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.67, %1072, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1077 : Tensor = prim::GetAttr[name="bias"](%1070)
  %1078 : Tensor = prim::GetAttr[name="weight"](%1070)
  %1079 : Float(768:1, 768:768) = aten::t(%1078), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1079), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.68, %1077, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1082 : Tensor = prim::GetAttr[name="bias"](%1069)
  %1083 : Tensor = prim::GetAttr[name="weight"](%1069)
  %1084 : Float(768:1, 768:768) = aten::t(%1083), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1084), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.69, %1082, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1087 : int = aten::size(%x.67, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1088 : int = aten::size(%x.67, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1089 : int[] = prim::ListConstruct(%1087, %1088, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x.68 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.67, %1089), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1091 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %query_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.68, %1091), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1093 : int = aten::size(%x.69, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1094 : int = aten::size(%x.69, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1095 : int[] = prim::ListConstruct(%1093, %1094, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x.70 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.69, %1095), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1097 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %key_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.70, %1097), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1099 : int = aten::size(%x.71, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1100 : int = aten::size(%x.71, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1101 : int[] = prim::ListConstruct(%1099, %1100, %20, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.71, %1101), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1103 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %value_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x, %1103), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1105 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer, %21, %18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.23 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer, %1105), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.23, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:259:0
  %input.116 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:262:0
  %input.117 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.116, %21, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.117, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:275:0
  %1112 : int[] = prim::ListConstruct(%37, %31, %36, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %1113 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.23, %1112), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
  %context_layer : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1113, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
  %1115 : int = aten::size(%context_layer, %37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
  %1116 : int = aten::size(%context_layer, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
  %1117 : int[] = prim::ListConstruct(%1115, %1116, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %input.118 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer, %1117), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:279:0
  %1119 : __torch__.torch.nn.modules.normalization.___torch_mangle_4606.LayerNorm = prim::GetAttr[name="LayerNorm"](%1067)
  %1120 : __torch__.torch.nn.modules.linear.___torch_mangle_4605.Linear = prim::GetAttr[name="dense"](%1067)
  %1121 : Tensor = prim::GetAttr[name="bias"](%1120)
  %1122 : Tensor = prim::GetAttr[name="weight"](%1120)
  %1123 : Float(768:1, 768:768) = aten::t(%1122), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.118, %1123), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.119 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.70, %1121, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.119, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.120 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.23, %input.115, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output # transformers/modeling_bert.py:295:0
  %1128 : Tensor = prim::GetAttr[name="bias"](%1119)
  %1129 : Tensor = prim::GetAttr[name="weight"](%1119)
  %1130 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.120, %1130, %1129, %1128, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1132 : __torch__.torch.nn.modules.linear.___torch_mangle_4610.Linear = prim::GetAttr[name="dense"](%1065)
  %1133 : Tensor = prim::GetAttr[name="bias"](%1132)
  %1134 : Tensor = prim::GetAttr[name="weight"](%1132)
  %1135 : Float(768:1, 3072:768) = aten::t(%1134), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor, %1135), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.121 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.71, %1133, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.122 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.121), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1139 : __torch__.torch.nn.modules.normalization.___torch_mangle_4613.LayerNorm = prim::GetAttr[name="LayerNorm"](%1064)
  %1140 : __torch__.torch.nn.modules.linear.___torch_mangle_4612.Linear = prim::GetAttr[name="dense"](%1064)
  %1141 : Tensor = prim::GetAttr[name="bias"](%1140)
  %1142 : Tensor = prim::GetAttr[name="weight"](%1140)
  %1143 : Float(3072:1, 768:3072) = aten::t(%1142), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.122, %1143), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.123 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.72, %1141, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.123, %25, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.124 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states, %input_tensor, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output # transformers/modeling_bert.py:371:0
  %1148 : Tensor = prim::GetAttr[name="bias"](%1139)
  %1149 : Tensor = prim::GetAttr[name="weight"](%1139)
  %1150 : int[] = prim::ListConstruct(%24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm
  %input : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.124, %1150, %1149, %1148, %23, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1152 : int = prim::Constant[value=1](), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %1153 : Tensor = prim::GetAttr[name="bias"](%3)
  %1154 : Tensor = prim::GetAttr[name="weight"](%3)
  %1155 : Float(768:1, 2:768) = aten::t(%1154), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %1155), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %1157 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %1153, %1152), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %7 : int = prim::Constant[value=1]() # torch/tensor.py:371:0
  %8 : int = prim::Constant[value=-1]() # torch/tensor.py:371:0
  %9 : Tensor[] = aten::split(%1157, %7, %8) # torch/tensor.py:371:0
  %start_logits : Float(17:26, 13:2, 1:1), %end_logits : Float(17:26, 13:2, 1:1) = prim::ListUnpack(%9)
  %12 : int = prim::Constant[value=-1]() # transformers/modeling_bert.py:1619:0
  %13 : Float(17:26, 13:2) = aten::squeeze(%start_logits, %12) # transformers/modeling_bert.py:1619:0
  %14 : int = prim::Constant[value=-1]() # transformers/modeling_bert.py:1620:0
  %15 : Float(17:26, 13:2) = aten::squeeze(%end_logits, %14) # transformers/modeling_bert.py:1620:0
  %16 : (Float(17:26, 13:2), Float(17:26, 13:2)) = prim::TupleConstruct(%13, %15)
  return (%16)
