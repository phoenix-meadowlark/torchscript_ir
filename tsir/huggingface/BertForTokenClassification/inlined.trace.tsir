graph(%self.1 : __torch__.transformers.modeling_bert.BertForTokenClassification,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_5487.Linear = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.torch.nn.modules.dropout.___torch_mangle_5486.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %5 : __torch__.transformers.modeling_bert.___torch_mangle_5485.BertModel = prim::GetAttr[name="bert"](%self.1)
  %10 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
  %11 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %12 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %13 : int = prim::Constant[value=12](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %14 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %15 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %16 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %17 : int = prim::Constant[value=768](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %18 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
  %19 : Double() = prim::Constant[value={-10000}](), scope: __module.bert # transformers/modeling_utils.py:258:0
  %20 : float = prim::Constant[value=1.](), scope: __module.bert # torch/tensor.py:396:0
  %21 : None = prim::Constant(), scope: __module.bert
  %22 : int = prim::Constant[value=6](), scope: __module.bert # transformers/modeling_utils.py:257:0
  %23 : int = prim::Constant[value=3](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %24 : int = prim::Constant[value=2](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %25 : int = prim::Constant[value=9223372036854775807](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %26 : bool = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_bert.py:806:0
  %27 : Device = prim::Constant[value="cpu"](), scope: __module.bert # transformers/modeling_bert.py:806:0
  %28 : int = prim::Constant[value=4](), scope: __module.bert # transformers/modeling_bert.py:806:0
  %29 : int = prim::Constant[value=1](), scope: __module.bert # transformers/modeling_bert.py:795:0
  %30 : int = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_bert.py:795:0
  %31 : __torch__.transformers.modeling_bert.___torch_mangle_5484.BertEncoder = prim::GetAttr[name="encoder"](%5)
  %32 : __torch__.transformers.modeling_bert.___torch_mangle_5278.BertEmbeddings = prim::GetAttr[name="embeddings"](%5)
  %33 : int = aten::size(%input_ids, %30), scope: __module.bert # transformers/modeling_bert.py:795:0
  %34 : int = aten::size(%input_ids, %29), scope: __module.bert # transformers/modeling_bert.py:795:0
  %35 : int[] = prim::ListConstruct(%33, %34), scope: __module.bert
  %input.2 : Long(17:13, 13:1) = aten::zeros(%35, %28, %30, %27, %26), scope: __module.bert # transformers/modeling_bert.py:806:0
  %37 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %30, %30, %25, %29), scope: __module.bert # transformers/modeling_utils.py:244:0
  %38 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%37, %29), scope: __module.bert # transformers/modeling_utils.py:244:0
  %39 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%38, %24), scope: __module.bert # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%39, %23, %30, %25, %29), scope: __module.bert # transformers/modeling_utils.py:244:0
  %41 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %22, %26, %26, %21), scope: __module.bert # transformers/modeling_utils.py:257:0
  %42 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%41, %20, %29), scope: __module.bert # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%42, %19), scope: __module.bert # transformers/modeling_utils.py:258:0
  %44 : __torch__.torch.nn.modules.normalization.___torch_mangle_5276.LayerNorm = prim::GetAttr[name="LayerNorm"](%32)
  %45 : __torch__.torch.nn.modules.sparse.___torch_mangle_5275.Embedding = prim::GetAttr[name="token_type_embeddings"](%32)
  %46 : __torch__.torch.nn.modules.sparse.___torch_mangle_5274.Embedding = prim::GetAttr[name="position_embeddings"](%32)
  %47 : __torch__.torch.nn.modules.sparse.___torch_mangle_5273.Embedding = prim::GetAttr[name="word_embeddings"](%32)
  %48 : Tensor = prim::GetAttr[name="position_ids"](%32)
  %49 : int = aten::size(%input_ids, %29), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:184:0
  %50 : Long(1:512, 512:1) = aten::slice(%48, %30, %30, %25, %29), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:191:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%50, %29, %30, %49, %29), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:191:0
  %52 : Tensor = prim::GetAttr[name="weight"](%47)
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::embedding(%52, %input_ids, %30, %26, %26), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %54 : Tensor = prim::GetAttr[name="weight"](%46)
  %position_embeddings : Float(1:9984, 13:768, 768:1) = aten::embedding(%54, %input.1, %14, %26, %26), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %56 : Tensor = prim::GetAttr[name="weight"](%45)
  %token_type_embeddings : Float(17:9984, 13:768, 768:1) = aten::embedding(%56, %input.2, %14, %26, %26), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %58 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %position_embeddings, %29), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:201:0
  %input.3 : Float(17:9984, 13:768, 768:1) = aten::add(%58, %token_type_embeddings, %29), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:201:0
  %60 : Tensor = prim::GetAttr[name="bias"](%44)
  %61 : Tensor = prim::GetAttr[name="weight"](%44)
  %62 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm
  %input.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.3, %62, %61, %60, %16, %15), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.4, %18, %26), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
  %65 : __torch__.torch.nn.modules.container.___torch_mangle_5483.ModuleList = prim::GetAttr[name="layer"](%31)
  %66 : __torch__.transformers.modeling_bert.___torch_mangle_5482.BertLayer = prim::GetAttr[name="11"](%65)
  %67 : __torch__.torch.nn.modules.container.___torch_mangle_5483.ModuleList = prim::GetAttr[name="layer"](%31)
  %68 : __torch__.transformers.modeling_bert.___torch_mangle_5465.BertLayer = prim::GetAttr[name="10"](%67)
  %69 : __torch__.torch.nn.modules.container.___torch_mangle_5483.ModuleList = prim::GetAttr[name="layer"](%31)
  %70 : __torch__.transformers.modeling_bert.___torch_mangle_5448.BertLayer = prim::GetAttr[name="9"](%69)
  %71 : __torch__.torch.nn.modules.container.___torch_mangle_5483.ModuleList = prim::GetAttr[name="layer"](%31)
  %72 : __torch__.transformers.modeling_bert.___torch_mangle_5431.BertLayer = prim::GetAttr[name="8"](%71)
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_5483.ModuleList = prim::GetAttr[name="layer"](%31)
  %74 : __torch__.transformers.modeling_bert.___torch_mangle_5414.BertLayer = prim::GetAttr[name="7"](%73)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_5483.ModuleList = prim::GetAttr[name="layer"](%31)
  %76 : __torch__.transformers.modeling_bert.___torch_mangle_5397.BertLayer = prim::GetAttr[name="6"](%75)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_5483.ModuleList = prim::GetAttr[name="layer"](%31)
  %78 : __torch__.transformers.modeling_bert.___torch_mangle_5380.BertLayer = prim::GetAttr[name="5"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_5483.ModuleList = prim::GetAttr[name="layer"](%31)
  %80 : __torch__.transformers.modeling_bert.___torch_mangle_5363.BertLayer = prim::GetAttr[name="4"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_5483.ModuleList = prim::GetAttr[name="layer"](%31)
  %82 : __torch__.transformers.modeling_bert.___torch_mangle_5346.BertLayer = prim::GetAttr[name="3"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_5483.ModuleList = prim::GetAttr[name="layer"](%31)
  %84 : __torch__.transformers.modeling_bert.___torch_mangle_5329.BertLayer = prim::GetAttr[name="2"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_5483.ModuleList = prim::GetAttr[name="layer"](%31)
  %86 : __torch__.transformers.modeling_bert.___torch_mangle_5312.BertLayer = prim::GetAttr[name="1"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_5483.ModuleList = prim::GetAttr[name="layer"](%31)
  %88 : __torch__.transformers.modeling_bert.___torch_mangle_5295.BertLayer = prim::GetAttr[name="0"](%87)
  %89 : __torch__.transformers.modeling_bert.___torch_mangle_5294.BertOutput = prim::GetAttr[name="output"](%88)
  %90 : __torch__.transformers.modeling_bert.___torch_mangle_5290.BertIntermediate = prim::GetAttr[name="intermediate"](%88)
  %91 : __torch__.transformers.modeling_bert.___torch_mangle_5288.BertAttention = prim::GetAttr[name="attention"](%88)
  %92 : __torch__.transformers.modeling_bert.___torch_mangle_5287.BertSelfOutput = prim::GetAttr[name="output"](%91)
  %93 : __torch__.transformers.modeling_bert.___torch_mangle_5283.BertSelfAttention = prim::GetAttr[name="self"](%91)
  %94 : __torch__.torch.nn.modules.linear.___torch_mangle_5281.Linear = prim::GetAttr[name="value"](%93)
  %95 : __torch__.torch.nn.modules.linear.___torch_mangle_5280.Linear = prim::GetAttr[name="key"](%93)
  %96 : __torch__.torch.nn.modules.linear.___torch_mangle_5279.Linear = prim::GetAttr[name="query"](%93)
  %97 : Tensor = prim::GetAttr[name="bias"](%96)
  %98 : Tensor = prim::GetAttr[name="weight"](%96)
  %99 : Float(768:1, 768:768) = aten::t(%98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %97, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %102 : Tensor = prim::GetAttr[name="bias"](%95)
  %103 : Tensor = prim::GetAttr[name="weight"](%95)
  %104 : Float(768:1, 768:768) = aten::t(%103), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %104), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %102, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %107 : Tensor = prim::GetAttr[name="bias"](%94)
  %108 : Tensor = prim::GetAttr[name="weight"](%94)
  %109 : Float(768:1, 768:768) = aten::t(%108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %107, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %112 : int = aten::size(%x.1, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %113 : int = aten::size(%x.1, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %114 : int[] = prim::ListConstruct(%112, %113, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.1, %114), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %116 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.2, %116), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %118 : int = aten::size(%x.3, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %119 : int = aten::size(%x.3, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %120 : int[] = prim::ListConstruct(%118, %119, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.3, %120), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %122 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.4, %122), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %124 : int = aten::size(%x.5, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %125 : int = aten::size(%x.5, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %126 : int[] = prim::ListConstruct(%124, %125, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.5, %126), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %128 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.6, %128), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %130 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.1, %14, %11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %130), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.1, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:262:0
  %input.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.6, %14, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.7, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:275:0
  %137 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %138 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.1, %137), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%138, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
  %140 : int = aten::size(%context_layer.2, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
  %141 : int = aten::size(%context_layer.2, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
  %142 : int[] = prim::ListConstruct(%140, %141, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.2, %142), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:279:0
  %144 : __torch__.torch.nn.modules.normalization.___torch_mangle_5285.LayerNorm = prim::GetAttr[name="LayerNorm"](%92)
  %145 : __torch__.torch.nn.modules.linear.___torch_mangle_5284.Linear = prim::GetAttr[name="dense"](%92)
  %146 : Tensor = prim::GetAttr[name="bias"](%145)
  %147 : Tensor = prim::GetAttr[name="weight"](%145)
  %148 : Float(768:1, 768:768) = aten::t(%147), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.8, %148), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.4, %146, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.9, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.1, %input.5, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output # transformers/modeling_bert.py:295:0
  %153 : Tensor = prim::GetAttr[name="bias"](%144)
  %154 : Tensor = prim::GetAttr[name="weight"](%144)
  %155 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.10, %155, %154, %153, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %157 : __torch__.torch.nn.modules.linear.___torch_mangle_5289.Linear = prim::GetAttr[name="dense"](%90)
  %158 : Tensor = prim::GetAttr[name="bias"](%157)
  %159 : Tensor = prim::GetAttr[name="weight"](%157)
  %160 : Float(768:1, 3072:768) = aten::t(%159), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.1, %160), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.5, %158, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %164 : __torch__.torch.nn.modules.normalization.___torch_mangle_5292.LayerNorm = prim::GetAttr[name="LayerNorm"](%89)
  %165 : __torch__.torch.nn.modules.linear.___torch_mangle_5291.Linear = prim::GetAttr[name="dense"](%89)
  %166 : Tensor = prim::GetAttr[name="bias"](%165)
  %167 : Tensor = prim::GetAttr[name="weight"](%165)
  %168 : Float(3072:1, 768:3072) = aten::t(%167), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %168), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %166, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.2, %input_tensor.1, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output # transformers/modeling_bert.py:371:0
  %173 : Tensor = prim::GetAttr[name="bias"](%164)
  %174 : Tensor = prim::GetAttr[name="weight"](%164)
  %175 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm
  %input.15 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %175, %174, %173, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %177 : __torch__.transformers.modeling_bert.___torch_mangle_5311.BertOutput = prim::GetAttr[name="output"](%86)
  %178 : __torch__.transformers.modeling_bert.___torch_mangle_5307.BertIntermediate = prim::GetAttr[name="intermediate"](%86)
  %179 : __torch__.transformers.modeling_bert.___torch_mangle_5305.BertAttention = prim::GetAttr[name="attention"](%86)
  %180 : __torch__.transformers.modeling_bert.___torch_mangle_5304.BertSelfOutput = prim::GetAttr[name="output"](%179)
  %181 : __torch__.transformers.modeling_bert.___torch_mangle_5300.BertSelfAttention = prim::GetAttr[name="self"](%179)
  %182 : __torch__.torch.nn.modules.linear.___torch_mangle_5298.Linear = prim::GetAttr[name="value"](%181)
  %183 : __torch__.torch.nn.modules.linear.___torch_mangle_5297.Linear = prim::GetAttr[name="key"](%181)
  %184 : __torch__.torch.nn.modules.linear.___torch_mangle_5296.Linear = prim::GetAttr[name="query"](%181)
  %185 : Tensor = prim::GetAttr[name="bias"](%184)
  %186 : Tensor = prim::GetAttr[name="weight"](%184)
  %187 : Float(768:1, 768:768) = aten::t(%186), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %187), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %185, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %190 : Tensor = prim::GetAttr[name="bias"](%183)
  %191 : Tensor = prim::GetAttr[name="weight"](%183)
  %192 : Float(768:1, 768:768) = aten::t(%191), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %192), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %190, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %195 : Tensor = prim::GetAttr[name="bias"](%182)
  %196 : Tensor = prim::GetAttr[name="weight"](%182)
  %197 : Float(768:1, 768:768) = aten::t(%196), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %197), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.9, %195, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %200 : int = aten::size(%x.7, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %201 : int = aten::size(%x.7, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %202 : int[] = prim::ListConstruct(%200, %201, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.7, %202), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %204 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.8, %204), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %206 : int = aten::size(%x.9, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %207 : int = aten::size(%x.9, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %208 : int[] = prim::ListConstruct(%206, %207, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.9, %208), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %210 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.10, %210), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %212 : int = aten::size(%x.11, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %213 : int = aten::size(%x.11, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %214 : int[] = prim::ListConstruct(%212, %213, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.11, %214), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %216 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.12, %216), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %218 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.2, %14, %11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %218), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.3, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:259:0
  %input.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:262:0
  %input.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.16, %14, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.17, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:275:0
  %225 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %226 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.3, %225), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%226, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
  %228 : int = aten::size(%context_layer.4, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
  %229 : int = aten::size(%context_layer.4, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
  %230 : int[] = prim::ListConstruct(%228, %229, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.4, %230), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:279:0
  %232 : __torch__.torch.nn.modules.normalization.___torch_mangle_5302.LayerNorm = prim::GetAttr[name="LayerNorm"](%180)
  %233 : __torch__.torch.nn.modules.linear.___torch_mangle_5301.Linear = prim::GetAttr[name="dense"](%180)
  %234 : Tensor = prim::GetAttr[name="bias"](%233)
  %235 : Tensor = prim::GetAttr[name="weight"](%233)
  %236 : Float(768:1, 768:768) = aten::t(%235), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.18, %236), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %234, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.19, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.3, %input.15, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output # transformers/modeling_bert.py:295:0
  %241 : Tensor = prim::GetAttr[name="bias"](%232)
  %242 : Tensor = prim::GetAttr[name="weight"](%232)
  %243 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.20, %243, %242, %241, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %245 : __torch__.torch.nn.modules.linear.___torch_mangle_5306.Linear = prim::GetAttr[name="dense"](%178)
  %246 : Tensor = prim::GetAttr[name="bias"](%245)
  %247 : Tensor = prim::GetAttr[name="weight"](%245)
  %248 : Float(768:1, 3072:768) = aten::t(%247), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.2, %248), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.11, %246, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.22 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %252 : __torch__.torch.nn.modules.normalization.___torch_mangle_5309.LayerNorm = prim::GetAttr[name="LayerNorm"](%177)
  %253 : __torch__.torch.nn.modules.linear.___torch_mangle_5308.Linear = prim::GetAttr[name="dense"](%177)
  %254 : Tensor = prim::GetAttr[name="bias"](%253)
  %255 : Tensor = prim::GetAttr[name="weight"](%253)
  %256 : Float(3072:1, 768:3072) = aten::t(%255), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.22, %256), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %254, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.23, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.4, %input_tensor.2, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output # transformers/modeling_bert.py:371:0
  %261 : Tensor = prim::GetAttr[name="bias"](%252)
  %262 : Tensor = prim::GetAttr[name="weight"](%252)
  %263 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.24, %263, %262, %261, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %265 : __torch__.transformers.modeling_bert.___torch_mangle_5328.BertOutput = prim::GetAttr[name="output"](%84)
  %266 : __torch__.transformers.modeling_bert.___torch_mangle_5324.BertIntermediate = prim::GetAttr[name="intermediate"](%84)
  %267 : __torch__.transformers.modeling_bert.___torch_mangle_5322.BertAttention = prim::GetAttr[name="attention"](%84)
  %268 : __torch__.transformers.modeling_bert.___torch_mangle_5321.BertSelfOutput = prim::GetAttr[name="output"](%267)
  %269 : __torch__.transformers.modeling_bert.___torch_mangle_5317.BertSelfAttention = prim::GetAttr[name="self"](%267)
  %270 : __torch__.torch.nn.modules.linear.___torch_mangle_5315.Linear = prim::GetAttr[name="value"](%269)
  %271 : __torch__.torch.nn.modules.linear.___torch_mangle_5314.Linear = prim::GetAttr[name="key"](%269)
  %272 : __torch__.torch.nn.modules.linear.___torch_mangle_5313.Linear = prim::GetAttr[name="query"](%269)
  %273 : Tensor = prim::GetAttr[name="bias"](%272)
  %274 : Tensor = prim::GetAttr[name="weight"](%272)
  %275 : Float(768:1, 768:768) = aten::t(%274), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %275), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %273, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %278 : Tensor = prim::GetAttr[name="bias"](%271)
  %279 : Tensor = prim::GetAttr[name="weight"](%271)
  %280 : Float(768:1, 768:768) = aten::t(%279), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %280), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.14, %278, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %283 : Tensor = prim::GetAttr[name="bias"](%270)
  %284 : Tensor = prim::GetAttr[name="weight"](%270)
  %285 : Float(768:1, 768:768) = aten::t(%284), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %285), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %283, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %288 : int = aten::size(%x.13, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %289 : int = aten::size(%x.13, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %290 : int[] = prim::ListConstruct(%288, %289, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.13, %290), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %292 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.14, %292), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %294 : int = aten::size(%x.15, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %295 : int = aten::size(%x.15, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %296 : int[] = prim::ListConstruct(%294, %295, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.15, %296), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %298 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.16, %298), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %300 : int = aten::size(%x.17, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %301 : int = aten::size(%x.17, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %302 : int[] = prim::ListConstruct(%300, %301, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.17, %302), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %304 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.18, %304), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %306 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.3, %14, %11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %306), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.5, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:259:0
  %input.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:262:0
  %input.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.26, %14, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.27, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:275:0
  %313 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %314 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.5, %313), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%314, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
  %316 : int = aten::size(%context_layer.6, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
  %317 : int = aten::size(%context_layer.6, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
  %318 : int[] = prim::ListConstruct(%316, %317, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.6, %318), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:279:0
  %320 : __torch__.torch.nn.modules.normalization.___torch_mangle_5319.LayerNorm = prim::GetAttr[name="LayerNorm"](%268)
  %321 : __torch__.torch.nn.modules.linear.___torch_mangle_5318.Linear = prim::GetAttr[name="dense"](%268)
  %322 : Tensor = prim::GetAttr[name="bias"](%321)
  %323 : Tensor = prim::GetAttr[name="weight"](%321)
  %324 : Float(768:1, 768:768) = aten::t(%323), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.28, %324), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %322, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.29, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.30 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.5, %input.25, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output # transformers/modeling_bert.py:295:0
  %329 : Tensor = prim::GetAttr[name="bias"](%320)
  %330 : Tensor = prim::GetAttr[name="weight"](%320)
  %331 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.30, %331, %330, %329, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %333 : __torch__.torch.nn.modules.linear.___torch_mangle_5323.Linear = prim::GetAttr[name="dense"](%266)
  %334 : Tensor = prim::GetAttr[name="bias"](%333)
  %335 : Tensor = prim::GetAttr[name="weight"](%333)
  %336 : Float(768:1, 3072:768) = aten::t(%335), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.3, %336), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.31 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.17, %334, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.32 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %340 : __torch__.torch.nn.modules.normalization.___torch_mangle_5326.LayerNorm = prim::GetAttr[name="LayerNorm"](%265)
  %341 : __torch__.torch.nn.modules.linear.___torch_mangle_5325.Linear = prim::GetAttr[name="dense"](%265)
  %342 : Tensor = prim::GetAttr[name="bias"](%341)
  %343 : Tensor = prim::GetAttr[name="weight"](%341)
  %344 : Float(3072:1, 768:3072) = aten::t(%343), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.32, %344), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %342, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.33, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.6, %input_tensor.3, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output # transformers/modeling_bert.py:371:0
  %349 : Tensor = prim::GetAttr[name="bias"](%340)
  %350 : Tensor = prim::GetAttr[name="weight"](%340)
  %351 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.34, %351, %350, %349, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %353 : __torch__.transformers.modeling_bert.___torch_mangle_5345.BertOutput = prim::GetAttr[name="output"](%82)
  %354 : __torch__.transformers.modeling_bert.___torch_mangle_5341.BertIntermediate = prim::GetAttr[name="intermediate"](%82)
  %355 : __torch__.transformers.modeling_bert.___torch_mangle_5339.BertAttention = prim::GetAttr[name="attention"](%82)
  %356 : __torch__.transformers.modeling_bert.___torch_mangle_5338.BertSelfOutput = prim::GetAttr[name="output"](%355)
  %357 : __torch__.transformers.modeling_bert.___torch_mangle_5334.BertSelfAttention = prim::GetAttr[name="self"](%355)
  %358 : __torch__.torch.nn.modules.linear.___torch_mangle_5332.Linear = prim::GetAttr[name="value"](%357)
  %359 : __torch__.torch.nn.modules.linear.___torch_mangle_5331.Linear = prim::GetAttr[name="key"](%357)
  %360 : __torch__.torch.nn.modules.linear.___torch_mangle_5330.Linear = prim::GetAttr[name="query"](%357)
  %361 : Tensor = prim::GetAttr[name="bias"](%360)
  %362 : Tensor = prim::GetAttr[name="weight"](%360)
  %363 : Float(768:1, 768:768) = aten::t(%362), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %363), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.19, %361, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %366 : Tensor = prim::GetAttr[name="bias"](%359)
  %367 : Tensor = prim::GetAttr[name="weight"](%359)
  %368 : Float(768:1, 768:768) = aten::t(%367), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %368), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %366, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %371 : Tensor = prim::GetAttr[name="bias"](%358)
  %372 : Tensor = prim::GetAttr[name="weight"](%358)
  %373 : Float(768:1, 768:768) = aten::t(%372), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %373), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %371, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %376 : int = aten::size(%x.19, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %377 : int = aten::size(%x.19, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %378 : int[] = prim::ListConstruct(%376, %377, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.19, %378), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %380 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.20, %380), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %382 : int = aten::size(%x.21, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %383 : int = aten::size(%x.21, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %384 : int[] = prim::ListConstruct(%382, %383, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.21, %384), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %386 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.22, %386), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %388 : int = aten::size(%x.23, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %389 : int = aten::size(%x.23, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %390 : int[] = prim::ListConstruct(%388, %389, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.24 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.23, %390), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %392 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.24, %392), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %394 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.4, %14, %11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %394), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.7, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:259:0
  %input.36 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:262:0
  %input.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.36, %14, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.37, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:275:0
  %401 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %402 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.7, %401), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%402, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
  %404 : int = aten::size(%context_layer.8, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
  %405 : int = aten::size(%context_layer.8, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
  %406 : int[] = prim::ListConstruct(%404, %405, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %input.38 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.8, %406), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:279:0
  %408 : __torch__.torch.nn.modules.normalization.___torch_mangle_5336.LayerNorm = prim::GetAttr[name="LayerNorm"](%356)
  %409 : __torch__.torch.nn.modules.linear.___torch_mangle_5335.Linear = prim::GetAttr[name="dense"](%356)
  %410 : Tensor = prim::GetAttr[name="bias"](%409)
  %411 : Tensor = prim::GetAttr[name="weight"](%409)
  %412 : Float(768:1, 768:768) = aten::t(%411), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.38, %412), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %410, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.39, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.7, %input.35, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output # transformers/modeling_bert.py:295:0
  %417 : Tensor = prim::GetAttr[name="bias"](%408)
  %418 : Tensor = prim::GetAttr[name="weight"](%408)
  %419 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.40, %419, %418, %417, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %421 : __torch__.torch.nn.modules.linear.___torch_mangle_5340.Linear = prim::GetAttr[name="dense"](%354)
  %422 : Tensor = prim::GetAttr[name="bias"](%421)
  %423 : Tensor = prim::GetAttr[name="weight"](%421)
  %424 : Float(768:1, 3072:768) = aten::t(%423), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.4, %424), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.41 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.23, %422, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.42 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.41), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %428 : __torch__.torch.nn.modules.normalization.___torch_mangle_5343.LayerNorm = prim::GetAttr[name="LayerNorm"](%353)
  %429 : __torch__.torch.nn.modules.linear.___torch_mangle_5342.Linear = prim::GetAttr[name="dense"](%353)
  %430 : Tensor = prim::GetAttr[name="bias"](%429)
  %431 : Tensor = prim::GetAttr[name="weight"](%429)
  %432 : Float(3072:1, 768:3072) = aten::t(%431), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.42, %432), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.24, %430, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.43, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.44 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.8, %input_tensor.4, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output # transformers/modeling_bert.py:371:0
  %437 : Tensor = prim::GetAttr[name="bias"](%428)
  %438 : Tensor = prim::GetAttr[name="weight"](%428)
  %439 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm
  %input.45 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.44, %439, %438, %437, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %441 : __torch__.transformers.modeling_bert.___torch_mangle_5362.BertOutput = prim::GetAttr[name="output"](%80)
  %442 : __torch__.transformers.modeling_bert.___torch_mangle_5358.BertIntermediate = prim::GetAttr[name="intermediate"](%80)
  %443 : __torch__.transformers.modeling_bert.___torch_mangle_5356.BertAttention = prim::GetAttr[name="attention"](%80)
  %444 : __torch__.transformers.modeling_bert.___torch_mangle_5355.BertSelfOutput = prim::GetAttr[name="output"](%443)
  %445 : __torch__.transformers.modeling_bert.___torch_mangle_5351.BertSelfAttention = prim::GetAttr[name="self"](%443)
  %446 : __torch__.torch.nn.modules.linear.___torch_mangle_5349.Linear = prim::GetAttr[name="value"](%445)
  %447 : __torch__.torch.nn.modules.linear.___torch_mangle_5348.Linear = prim::GetAttr[name="key"](%445)
  %448 : __torch__.torch.nn.modules.linear.___torch_mangle_5347.Linear = prim::GetAttr[name="query"](%445)
  %449 : Tensor = prim::GetAttr[name="bias"](%448)
  %450 : Tensor = prim::GetAttr[name="weight"](%448)
  %451 : Float(768:1, 768:768) = aten::t(%450), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %451), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.25, %449, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %454 : Tensor = prim::GetAttr[name="bias"](%447)
  %455 : Tensor = prim::GetAttr[name="weight"](%447)
  %456 : Float(768:1, 768:768) = aten::t(%455), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %456), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.26, %454, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %459 : Tensor = prim::GetAttr[name="bias"](%446)
  %460 : Tensor = prim::GetAttr[name="weight"](%446)
  %461 : Float(768:1, 768:768) = aten::t(%460), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %461), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.27, %459, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %464 : int = aten::size(%x.25, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %465 : int = aten::size(%x.25, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %466 : int[] = prim::ListConstruct(%464, %465, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.25, %466), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %468 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.26, %468), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %470 : int = aten::size(%x.27, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %471 : int = aten::size(%x.27, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %472 : int[] = prim::ListConstruct(%470, %471, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.28 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.27, %472), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %474 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.28, %474), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %476 : int = aten::size(%x.29, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %477 : int = aten::size(%x.29, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %478 : int[] = prim::ListConstruct(%476, %477, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.30 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.29, %478), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %480 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.30, %480), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %482 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.5, %14, %11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %482), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.9, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:259:0
  %input.46 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:262:0
  %input.47 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.46, %14, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.47, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:275:0
  %489 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %490 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.9, %489), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%490, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
  %492 : int = aten::size(%context_layer.10, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
  %493 : int = aten::size(%context_layer.10, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
  %494 : int[] = prim::ListConstruct(%492, %493, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %input.48 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.10, %494), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:279:0
  %496 : __torch__.torch.nn.modules.normalization.___torch_mangle_5353.LayerNorm = prim::GetAttr[name="LayerNorm"](%444)
  %497 : __torch__.torch.nn.modules.linear.___torch_mangle_5352.Linear = prim::GetAttr[name="dense"](%444)
  %498 : Tensor = prim::GetAttr[name="bias"](%497)
  %499 : Tensor = prim::GetAttr[name="weight"](%497)
  %500 : Float(768:1, 768:768) = aten::t(%499), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.48, %500), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.28, %498, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.49, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.9, %input.45, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output # transformers/modeling_bert.py:295:0
  %505 : Tensor = prim::GetAttr[name="bias"](%496)
  %506 : Tensor = prim::GetAttr[name="weight"](%496)
  %507 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.50, %507, %506, %505, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %509 : __torch__.torch.nn.modules.linear.___torch_mangle_5357.Linear = prim::GetAttr[name="dense"](%442)
  %510 : Tensor = prim::GetAttr[name="bias"](%509)
  %511 : Tensor = prim::GetAttr[name="weight"](%509)
  %512 : Float(768:1, 3072:768) = aten::t(%511), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.5, %512), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.51 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.29, %510, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.52 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %516 : __torch__.torch.nn.modules.normalization.___torch_mangle_5360.LayerNorm = prim::GetAttr[name="LayerNorm"](%441)
  %517 : __torch__.torch.nn.modules.linear.___torch_mangle_5359.Linear = prim::GetAttr[name="dense"](%441)
  %518 : Tensor = prim::GetAttr[name="bias"](%517)
  %519 : Tensor = prim::GetAttr[name="weight"](%517)
  %520 : Float(3072:1, 768:3072) = aten::t(%519), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.52, %520), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.30, %518, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.53, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.54 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.10, %input_tensor.5, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output # transformers/modeling_bert.py:371:0
  %525 : Tensor = prim::GetAttr[name="bias"](%516)
  %526 : Tensor = prim::GetAttr[name="weight"](%516)
  %527 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm
  %input.55 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.54, %527, %526, %525, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %529 : __torch__.transformers.modeling_bert.___torch_mangle_5379.BertOutput = prim::GetAttr[name="output"](%78)
  %530 : __torch__.transformers.modeling_bert.___torch_mangle_5375.BertIntermediate = prim::GetAttr[name="intermediate"](%78)
  %531 : __torch__.transformers.modeling_bert.___torch_mangle_5373.BertAttention = prim::GetAttr[name="attention"](%78)
  %532 : __torch__.transformers.modeling_bert.___torch_mangle_5372.BertSelfOutput = prim::GetAttr[name="output"](%531)
  %533 : __torch__.transformers.modeling_bert.___torch_mangle_5368.BertSelfAttention = prim::GetAttr[name="self"](%531)
  %534 : __torch__.torch.nn.modules.linear.___torch_mangle_5366.Linear = prim::GetAttr[name="value"](%533)
  %535 : __torch__.torch.nn.modules.linear.___torch_mangle_5365.Linear = prim::GetAttr[name="key"](%533)
  %536 : __torch__.torch.nn.modules.linear.___torch_mangle_5364.Linear = prim::GetAttr[name="query"](%533)
  %537 : Tensor = prim::GetAttr[name="bias"](%536)
  %538 : Tensor = prim::GetAttr[name="weight"](%536)
  %539 : Float(768:1, 768:768) = aten::t(%538), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %539), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.31, %537, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %542 : Tensor = prim::GetAttr[name="bias"](%535)
  %543 : Tensor = prim::GetAttr[name="weight"](%535)
  %544 : Float(768:1, 768:768) = aten::t(%543), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %544), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.32, %542, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %547 : Tensor = prim::GetAttr[name="bias"](%534)
  %548 : Tensor = prim::GetAttr[name="weight"](%534)
  %549 : Float(768:1, 768:768) = aten::t(%548), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %549), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.33, %547, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %552 : int = aten::size(%x.31, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %553 : int = aten::size(%x.31, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %554 : int[] = prim::ListConstruct(%552, %553, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.32 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.31, %554), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %556 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.32, %556), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %558 : int = aten::size(%x.33, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %559 : int = aten::size(%x.33, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %560 : int[] = prim::ListConstruct(%558, %559, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.34 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.33, %560), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %562 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.34, %562), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %564 : int = aten::size(%x.35, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %565 : int = aten::size(%x.35, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %566 : int[] = prim::ListConstruct(%564, %565, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.36 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.35, %566), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %568 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.36, %568), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %570 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.6, %14, %11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %570), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:259:0
  %input.56 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:262:0
  %input.57 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.56, %14, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.57, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:275:0
  %577 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %578 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.11, %577), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%578, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
  %580 : int = aten::size(%context_layer.12, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
  %581 : int = aten::size(%context_layer.12, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
  %582 : int[] = prim::ListConstruct(%580, %581, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %input.58 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.12, %582), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:279:0
  %584 : __torch__.torch.nn.modules.normalization.___torch_mangle_5370.LayerNorm = prim::GetAttr[name="LayerNorm"](%532)
  %585 : __torch__.torch.nn.modules.linear.___torch_mangle_5369.Linear = prim::GetAttr[name="dense"](%532)
  %586 : Tensor = prim::GetAttr[name="bias"](%585)
  %587 : Tensor = prim::GetAttr[name="weight"](%585)
  %588 : Float(768:1, 768:768) = aten::t(%587), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.58, %588), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.34, %586, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.59, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.11, %input.55, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output # transformers/modeling_bert.py:295:0
  %593 : Tensor = prim::GetAttr[name="bias"](%584)
  %594 : Tensor = prim::GetAttr[name="weight"](%584)
  %595 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.60, %595, %594, %593, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %597 : __torch__.torch.nn.modules.linear.___torch_mangle_5374.Linear = prim::GetAttr[name="dense"](%530)
  %598 : Tensor = prim::GetAttr[name="bias"](%597)
  %599 : Tensor = prim::GetAttr[name="weight"](%597)
  %600 : Float(768:1, 3072:768) = aten::t(%599), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.6, %600), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.61 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.35, %598, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.62 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.61), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %604 : __torch__.torch.nn.modules.normalization.___torch_mangle_5377.LayerNorm = prim::GetAttr[name="LayerNorm"](%529)
  %605 : __torch__.torch.nn.modules.linear.___torch_mangle_5376.Linear = prim::GetAttr[name="dense"](%529)
  %606 : Tensor = prim::GetAttr[name="bias"](%605)
  %607 : Tensor = prim::GetAttr[name="weight"](%605)
  %608 : Float(3072:1, 768:3072) = aten::t(%607), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.62, %608), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.36, %606, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.63, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.12, %input_tensor.6, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output # transformers/modeling_bert.py:371:0
  %613 : Tensor = prim::GetAttr[name="bias"](%604)
  %614 : Tensor = prim::GetAttr[name="weight"](%604)
  %615 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm
  %input.65 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.64, %615, %614, %613, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %617 : __torch__.transformers.modeling_bert.___torch_mangle_5396.BertOutput = prim::GetAttr[name="output"](%76)
  %618 : __torch__.transformers.modeling_bert.___torch_mangle_5392.BertIntermediate = prim::GetAttr[name="intermediate"](%76)
  %619 : __torch__.transformers.modeling_bert.___torch_mangle_5390.BertAttention = prim::GetAttr[name="attention"](%76)
  %620 : __torch__.transformers.modeling_bert.___torch_mangle_5389.BertSelfOutput = prim::GetAttr[name="output"](%619)
  %621 : __torch__.transformers.modeling_bert.___torch_mangle_5385.BertSelfAttention = prim::GetAttr[name="self"](%619)
  %622 : __torch__.torch.nn.modules.linear.___torch_mangle_5383.Linear = prim::GetAttr[name="value"](%621)
  %623 : __torch__.torch.nn.modules.linear.___torch_mangle_5382.Linear = prim::GetAttr[name="key"](%621)
  %624 : __torch__.torch.nn.modules.linear.___torch_mangle_5381.Linear = prim::GetAttr[name="query"](%621)
  %625 : Tensor = prim::GetAttr[name="bias"](%624)
  %626 : Tensor = prim::GetAttr[name="weight"](%624)
  %627 : Float(768:1, 768:768) = aten::t(%626), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %627), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.37, %625, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %630 : Tensor = prim::GetAttr[name="bias"](%623)
  %631 : Tensor = prim::GetAttr[name="weight"](%623)
  %632 : Float(768:1, 768:768) = aten::t(%631), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %632), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.38, %630, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %635 : Tensor = prim::GetAttr[name="bias"](%622)
  %636 : Tensor = prim::GetAttr[name="weight"](%622)
  %637 : Float(768:1, 768:768) = aten::t(%636), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %637), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.39, %635, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %640 : int = aten::size(%x.37, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %641 : int = aten::size(%x.37, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %642 : int[] = prim::ListConstruct(%640, %641, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.38 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.37, %642), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %644 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.38, %644), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %646 : int = aten::size(%x.39, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %647 : int = aten::size(%x.39, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %648 : int[] = prim::ListConstruct(%646, %647, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.40 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.39, %648), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %650 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.40, %650), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %652 : int = aten::size(%x.41, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %653 : int = aten::size(%x.41, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %654 : int[] = prim::ListConstruct(%652, %653, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.42 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.41, %654), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %656 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.42, %656), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %658 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.7, %14, %11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %658), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.14 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.13, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:259:0
  %input.66 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:262:0
  %input.67 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.66, %14, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.67, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:275:0
  %665 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %666 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.13, %665), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%666, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
  %668 : int = aten::size(%context_layer.14, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
  %669 : int = aten::size(%context_layer.14, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
  %670 : int[] = prim::ListConstruct(%668, %669, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %input.68 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.14, %670), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:279:0
  %672 : __torch__.torch.nn.modules.normalization.___torch_mangle_5387.LayerNorm = prim::GetAttr[name="LayerNorm"](%620)
  %673 : __torch__.torch.nn.modules.linear.___torch_mangle_5386.Linear = prim::GetAttr[name="dense"](%620)
  %674 : Tensor = prim::GetAttr[name="bias"](%673)
  %675 : Tensor = prim::GetAttr[name="weight"](%673)
  %676 : Float(768:1, 768:768) = aten::t(%675), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.68, %676), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.40, %674, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.69, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.70 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.13, %input.65, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output # transformers/modeling_bert.py:295:0
  %681 : Tensor = prim::GetAttr[name="bias"](%672)
  %682 : Tensor = prim::GetAttr[name="weight"](%672)
  %683 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.70, %683, %682, %681, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %685 : __torch__.torch.nn.modules.linear.___torch_mangle_5391.Linear = prim::GetAttr[name="dense"](%618)
  %686 : Tensor = prim::GetAttr[name="bias"](%685)
  %687 : Tensor = prim::GetAttr[name="weight"](%685)
  %688 : Float(768:1, 3072:768) = aten::t(%687), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.7, %688), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.71 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.41, %686, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.72 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %692 : __torch__.torch.nn.modules.normalization.___torch_mangle_5394.LayerNorm = prim::GetAttr[name="LayerNorm"](%617)
  %693 : __torch__.torch.nn.modules.linear.___torch_mangle_5393.Linear = prim::GetAttr[name="dense"](%617)
  %694 : Tensor = prim::GetAttr[name="bias"](%693)
  %695 : Tensor = prim::GetAttr[name="weight"](%693)
  %696 : Float(3072:1, 768:3072) = aten::t(%695), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.72, %696), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.73 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.42, %694, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.73, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.14, %input_tensor.7, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output # transformers/modeling_bert.py:371:0
  %701 : Tensor = prim::GetAttr[name="bias"](%692)
  %702 : Tensor = prim::GetAttr[name="weight"](%692)
  %703 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm
  %input.75 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.74, %703, %702, %701, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %705 : __torch__.transformers.modeling_bert.___torch_mangle_5413.BertOutput = prim::GetAttr[name="output"](%74)
  %706 : __torch__.transformers.modeling_bert.___torch_mangle_5409.BertIntermediate = prim::GetAttr[name="intermediate"](%74)
  %707 : __torch__.transformers.modeling_bert.___torch_mangle_5407.BertAttention = prim::GetAttr[name="attention"](%74)
  %708 : __torch__.transformers.modeling_bert.___torch_mangle_5406.BertSelfOutput = prim::GetAttr[name="output"](%707)
  %709 : __torch__.transformers.modeling_bert.___torch_mangle_5402.BertSelfAttention = prim::GetAttr[name="self"](%707)
  %710 : __torch__.torch.nn.modules.linear.___torch_mangle_5400.Linear = prim::GetAttr[name="value"](%709)
  %711 : __torch__.torch.nn.modules.linear.___torch_mangle_5399.Linear = prim::GetAttr[name="key"](%709)
  %712 : __torch__.torch.nn.modules.linear.___torch_mangle_5398.Linear = prim::GetAttr[name="query"](%709)
  %713 : Tensor = prim::GetAttr[name="bias"](%712)
  %714 : Tensor = prim::GetAttr[name="weight"](%712)
  %715 : Float(768:1, 768:768) = aten::t(%714), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %715), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.43, %713, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %718 : Tensor = prim::GetAttr[name="bias"](%711)
  %719 : Tensor = prim::GetAttr[name="weight"](%711)
  %720 : Float(768:1, 768:768) = aten::t(%719), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %720), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.44, %718, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %723 : Tensor = prim::GetAttr[name="bias"](%710)
  %724 : Tensor = prim::GetAttr[name="weight"](%710)
  %725 : Float(768:1, 768:768) = aten::t(%724), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %725), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.45, %723, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %728 : int = aten::size(%x.43, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %729 : int = aten::size(%x.43, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %730 : int[] = prim::ListConstruct(%728, %729, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.44 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.43, %730), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %732 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.44, %732), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %734 : int = aten::size(%x.45, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %735 : int = aten::size(%x.45, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %736 : int[] = prim::ListConstruct(%734, %735, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.46 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.45, %736), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %738 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.46, %738), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %740 : int = aten::size(%x.47, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %741 : int = aten::size(%x.47, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %742 : int[] = prim::ListConstruct(%740, %741, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.48 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.47, %742), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %744 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.48, %744), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %746 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.8, %14, %11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %746), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.15, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:259:0
  %input.76 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:262:0
  %input.77 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.76, %14, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.77, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:275:0
  %753 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %754 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.15, %753), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%754, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
  %756 : int = aten::size(%context_layer.16, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
  %757 : int = aten::size(%context_layer.16, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
  %758 : int[] = prim::ListConstruct(%756, %757, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %input.78 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.16, %758), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:279:0
  %760 : __torch__.torch.nn.modules.normalization.___torch_mangle_5404.LayerNorm = prim::GetAttr[name="LayerNorm"](%708)
  %761 : __torch__.torch.nn.modules.linear.___torch_mangle_5403.Linear = prim::GetAttr[name="dense"](%708)
  %762 : Tensor = prim::GetAttr[name="bias"](%761)
  %763 : Tensor = prim::GetAttr[name="weight"](%761)
  %764 : Float(768:1, 768:768) = aten::t(%763), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.78, %764), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.79 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.46, %762, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.79, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.80 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.15, %input.75, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output # transformers/modeling_bert.py:295:0
  %769 : Tensor = prim::GetAttr[name="bias"](%760)
  %770 : Tensor = prim::GetAttr[name="weight"](%760)
  %771 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.80, %771, %770, %769, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %773 : __torch__.torch.nn.modules.linear.___torch_mangle_5408.Linear = prim::GetAttr[name="dense"](%706)
  %774 : Tensor = prim::GetAttr[name="bias"](%773)
  %775 : Tensor = prim::GetAttr[name="weight"](%773)
  %776 : Float(768:1, 3072:768) = aten::t(%775), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.8, %776), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.81 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.47, %774, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.82 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.81), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %780 : __torch__.torch.nn.modules.normalization.___torch_mangle_5411.LayerNorm = prim::GetAttr[name="LayerNorm"](%705)
  %781 : __torch__.torch.nn.modules.linear.___torch_mangle_5410.Linear = prim::GetAttr[name="dense"](%705)
  %782 : Tensor = prim::GetAttr[name="bias"](%781)
  %783 : Tensor = prim::GetAttr[name="weight"](%781)
  %784 : Float(3072:1, 768:3072) = aten::t(%783), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.82, %784), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.83 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.48, %782, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.83, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.84 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.16, %input_tensor.8, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output # transformers/modeling_bert.py:371:0
  %789 : Tensor = prim::GetAttr[name="bias"](%780)
  %790 : Tensor = prim::GetAttr[name="weight"](%780)
  %791 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm
  %input.85 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.84, %791, %790, %789, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %793 : __torch__.transformers.modeling_bert.___torch_mangle_5430.BertOutput = prim::GetAttr[name="output"](%72)
  %794 : __torch__.transformers.modeling_bert.___torch_mangle_5426.BertIntermediate = prim::GetAttr[name="intermediate"](%72)
  %795 : __torch__.transformers.modeling_bert.___torch_mangle_5424.BertAttention = prim::GetAttr[name="attention"](%72)
  %796 : __torch__.transformers.modeling_bert.___torch_mangle_5423.BertSelfOutput = prim::GetAttr[name="output"](%795)
  %797 : __torch__.transformers.modeling_bert.___torch_mangle_5419.BertSelfAttention = prim::GetAttr[name="self"](%795)
  %798 : __torch__.torch.nn.modules.linear.___torch_mangle_5417.Linear = prim::GetAttr[name="value"](%797)
  %799 : __torch__.torch.nn.modules.linear.___torch_mangle_5416.Linear = prim::GetAttr[name="key"](%797)
  %800 : __torch__.torch.nn.modules.linear.___torch_mangle_5415.Linear = prim::GetAttr[name="query"](%797)
  %801 : Tensor = prim::GetAttr[name="bias"](%800)
  %802 : Tensor = prim::GetAttr[name="weight"](%800)
  %803 : Float(768:1, 768:768) = aten::t(%802), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %803), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.49, %801, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %806 : Tensor = prim::GetAttr[name="bias"](%799)
  %807 : Tensor = prim::GetAttr[name="weight"](%799)
  %808 : Float(768:1, 768:768) = aten::t(%807), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %808), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.50, %806, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %811 : Tensor = prim::GetAttr[name="bias"](%798)
  %812 : Tensor = prim::GetAttr[name="weight"](%798)
  %813 : Float(768:1, 768:768) = aten::t(%812), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %813), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.51, %811, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %816 : int = aten::size(%x.49, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %817 : int = aten::size(%x.49, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %818 : int[] = prim::ListConstruct(%816, %817, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.50 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.49, %818), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %820 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.50, %820), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %822 : int = aten::size(%x.51, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %823 : int = aten::size(%x.51, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %824 : int[] = prim::ListConstruct(%822, %823, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.52 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.51, %824), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %826 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.52, %826), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %828 : int = aten::size(%x.53, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %829 : int = aten::size(%x.53, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %830 : int[] = prim::ListConstruct(%828, %829, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.53, %830), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %832 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.54, %832), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %834 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.9, %14, %11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %834), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.18 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.17, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:259:0
  %input.86 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:262:0
  %input.87 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.86, %14, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.87, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:275:0
  %841 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %842 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.17, %841), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%842, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
  %844 : int = aten::size(%context_layer.18, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
  %845 : int = aten::size(%context_layer.18, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
  %846 : int[] = prim::ListConstruct(%844, %845, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %input.88 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.18, %846), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:279:0
  %848 : __torch__.torch.nn.modules.normalization.___torch_mangle_5421.LayerNorm = prim::GetAttr[name="LayerNorm"](%796)
  %849 : __torch__.torch.nn.modules.linear.___torch_mangle_5420.Linear = prim::GetAttr[name="dense"](%796)
  %850 : Tensor = prim::GetAttr[name="bias"](%849)
  %851 : Tensor = prim::GetAttr[name="weight"](%849)
  %852 : Float(768:1, 768:768) = aten::t(%851), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.88, %852), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.89 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.52, %850, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.89, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.90 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.17, %input.85, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output # transformers/modeling_bert.py:295:0
  %857 : Tensor = prim::GetAttr[name="bias"](%848)
  %858 : Tensor = prim::GetAttr[name="weight"](%848)
  %859 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.90, %859, %858, %857, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %861 : __torch__.torch.nn.modules.linear.___torch_mangle_5425.Linear = prim::GetAttr[name="dense"](%794)
  %862 : Tensor = prim::GetAttr[name="bias"](%861)
  %863 : Tensor = prim::GetAttr[name="weight"](%861)
  %864 : Float(768:1, 3072:768) = aten::t(%863), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.9, %864), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.91 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.53, %862, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.92 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.91), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %868 : __torch__.torch.nn.modules.normalization.___torch_mangle_5428.LayerNorm = prim::GetAttr[name="LayerNorm"](%793)
  %869 : __torch__.torch.nn.modules.linear.___torch_mangle_5427.Linear = prim::GetAttr[name="dense"](%793)
  %870 : Tensor = prim::GetAttr[name="bias"](%869)
  %871 : Tensor = prim::GetAttr[name="weight"](%869)
  %872 : Float(3072:1, 768:3072) = aten::t(%871), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.92, %872), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.93 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.54, %870, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.93, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.94 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.18, %input_tensor.9, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output # transformers/modeling_bert.py:371:0
  %877 : Tensor = prim::GetAttr[name="bias"](%868)
  %878 : Tensor = prim::GetAttr[name="weight"](%868)
  %879 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm
  %input.95 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.94, %879, %878, %877, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %881 : __torch__.transformers.modeling_bert.___torch_mangle_5447.BertOutput = prim::GetAttr[name="output"](%70)
  %882 : __torch__.transformers.modeling_bert.___torch_mangle_5443.BertIntermediate = prim::GetAttr[name="intermediate"](%70)
  %883 : __torch__.transformers.modeling_bert.___torch_mangle_5441.BertAttention = prim::GetAttr[name="attention"](%70)
  %884 : __torch__.transformers.modeling_bert.___torch_mangle_5440.BertSelfOutput = prim::GetAttr[name="output"](%883)
  %885 : __torch__.transformers.modeling_bert.___torch_mangle_5436.BertSelfAttention = prim::GetAttr[name="self"](%883)
  %886 : __torch__.torch.nn.modules.linear.___torch_mangle_5434.Linear = prim::GetAttr[name="value"](%885)
  %887 : __torch__.torch.nn.modules.linear.___torch_mangle_5433.Linear = prim::GetAttr[name="key"](%885)
  %888 : __torch__.torch.nn.modules.linear.___torch_mangle_5432.Linear = prim::GetAttr[name="query"](%885)
  %889 : Tensor = prim::GetAttr[name="bias"](%888)
  %890 : Tensor = prim::GetAttr[name="weight"](%888)
  %891 : Float(768:1, 768:768) = aten::t(%890), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %891), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.55, %889, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %894 : Tensor = prim::GetAttr[name="bias"](%887)
  %895 : Tensor = prim::GetAttr[name="weight"](%887)
  %896 : Float(768:1, 768:768) = aten::t(%895), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %896), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.56, %894, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %899 : Tensor = prim::GetAttr[name="bias"](%886)
  %900 : Tensor = prim::GetAttr[name="weight"](%886)
  %901 : Float(768:1, 768:768) = aten::t(%900), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %901), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.57, %899, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %904 : int = aten::size(%x.55, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %905 : int = aten::size(%x.55, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %906 : int[] = prim::ListConstruct(%904, %905, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.56 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.55, %906), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %908 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.56, %908), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %910 : int = aten::size(%x.57, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %911 : int = aten::size(%x.57, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %912 : int[] = prim::ListConstruct(%910, %911, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.58 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.57, %912), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %914 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.58, %914), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %916 : int = aten::size(%x.59, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %917 : int = aten::size(%x.59, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %918 : int[] = prim::ListConstruct(%916, %917, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.60 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.59, %918), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %920 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.60, %920), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %922 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.10, %14, %11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.19 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %922), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.20 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.19, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:259:0
  %input.96 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:262:0
  %input.97 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.96, %14, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.97, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:275:0
  %929 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %930 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.19, %929), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%930, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
  %932 : int = aten::size(%context_layer.20, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
  %933 : int = aten::size(%context_layer.20, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
  %934 : int[] = prim::ListConstruct(%932, %933, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %input.98 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.20, %934), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:279:0
  %936 : __torch__.torch.nn.modules.normalization.___torch_mangle_5438.LayerNorm = prim::GetAttr[name="LayerNorm"](%884)
  %937 : __torch__.torch.nn.modules.linear.___torch_mangle_5437.Linear = prim::GetAttr[name="dense"](%884)
  %938 : Tensor = prim::GetAttr[name="bias"](%937)
  %939 : Tensor = prim::GetAttr[name="weight"](%937)
  %940 : Float(768:1, 768:768) = aten::t(%939), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.98, %940), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.99 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.58, %938, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.99, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.100 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.19, %input.95, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output # transformers/modeling_bert.py:295:0
  %945 : Tensor = prim::GetAttr[name="bias"](%936)
  %946 : Tensor = prim::GetAttr[name="weight"](%936)
  %947 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.100, %947, %946, %945, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %949 : __torch__.torch.nn.modules.linear.___torch_mangle_5442.Linear = prim::GetAttr[name="dense"](%882)
  %950 : Tensor = prim::GetAttr[name="bias"](%949)
  %951 : Tensor = prim::GetAttr[name="weight"](%949)
  %952 : Float(768:1, 3072:768) = aten::t(%951), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.10, %952), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.101 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.59, %950, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.102 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.101), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %956 : __torch__.torch.nn.modules.normalization.___torch_mangle_5445.LayerNorm = prim::GetAttr[name="LayerNorm"](%881)
  %957 : __torch__.torch.nn.modules.linear.___torch_mangle_5444.Linear = prim::GetAttr[name="dense"](%881)
  %958 : Tensor = prim::GetAttr[name="bias"](%957)
  %959 : Tensor = prim::GetAttr[name="weight"](%957)
  %960 : Float(3072:1, 768:3072) = aten::t(%959), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.102, %960), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.103 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.60, %958, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.103, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.104 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.20, %input_tensor.10, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output # transformers/modeling_bert.py:371:0
  %965 : Tensor = prim::GetAttr[name="bias"](%956)
  %966 : Tensor = prim::GetAttr[name="weight"](%956)
  %967 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm
  %input.105 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.104, %967, %966, %965, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %969 : __torch__.transformers.modeling_bert.___torch_mangle_5464.BertOutput = prim::GetAttr[name="output"](%68)
  %970 : __torch__.transformers.modeling_bert.___torch_mangle_5460.BertIntermediate = prim::GetAttr[name="intermediate"](%68)
  %971 : __torch__.transformers.modeling_bert.___torch_mangle_5458.BertAttention = prim::GetAttr[name="attention"](%68)
  %972 : __torch__.transformers.modeling_bert.___torch_mangle_5457.BertSelfOutput = prim::GetAttr[name="output"](%971)
  %973 : __torch__.transformers.modeling_bert.___torch_mangle_5453.BertSelfAttention = prim::GetAttr[name="self"](%971)
  %974 : __torch__.torch.nn.modules.linear.___torch_mangle_5451.Linear = prim::GetAttr[name="value"](%973)
  %975 : __torch__.torch.nn.modules.linear.___torch_mangle_5450.Linear = prim::GetAttr[name="key"](%973)
  %976 : __torch__.torch.nn.modules.linear.___torch_mangle_5449.Linear = prim::GetAttr[name="query"](%973)
  %977 : Tensor = prim::GetAttr[name="bias"](%976)
  %978 : Tensor = prim::GetAttr[name="weight"](%976)
  %979 : Float(768:1, 768:768) = aten::t(%978), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %979), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.61, %977, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %982 : Tensor = prim::GetAttr[name="bias"](%975)
  %983 : Tensor = prim::GetAttr[name="weight"](%975)
  %984 : Float(768:1, 768:768) = aten::t(%983), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %984), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.62, %982, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %987 : Tensor = prim::GetAttr[name="bias"](%974)
  %988 : Tensor = prim::GetAttr[name="weight"](%974)
  %989 : Float(768:1, 768:768) = aten::t(%988), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %989), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.63, %987, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %992 : int = aten::size(%x.61, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %993 : int = aten::size(%x.61, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %994 : int[] = prim::ListConstruct(%992, %993, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.62 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.61, %994), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %996 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.62, %996), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %998 : int = aten::size(%x.63, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %999 : int = aten::size(%x.63, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1000 : int[] = prim::ListConstruct(%998, %999, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.64 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.63, %1000), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1002 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.64, %1002), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1004 : int = aten::size(%x.65, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1005 : int = aten::size(%x.65, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1006 : int[] = prim::ListConstruct(%1004, %1005, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.66 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.65, %1006), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1008 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.66, %1008), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1010 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.11, %14, %11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.21 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1010), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.22 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.21, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:259:0
  %input.106 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:262:0
  %input.107 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.106, %14, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.107, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:275:0
  %1017 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %1018 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.21, %1017), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1018, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
  %1020 : int = aten::size(%context_layer.22, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
  %1021 : int = aten::size(%context_layer.22, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
  %1022 : int[] = prim::ListConstruct(%1020, %1021, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %input.108 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.22, %1022), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:279:0
  %1024 : __torch__.torch.nn.modules.normalization.___torch_mangle_5455.LayerNorm = prim::GetAttr[name="LayerNorm"](%972)
  %1025 : __torch__.torch.nn.modules.linear.___torch_mangle_5454.Linear = prim::GetAttr[name="dense"](%972)
  %1026 : Tensor = prim::GetAttr[name="bias"](%1025)
  %1027 : Tensor = prim::GetAttr[name="weight"](%1025)
  %1028 : Float(768:1, 768:768) = aten::t(%1027), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.108, %1028), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.109 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.64, %1026, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.109, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.21, %input.105, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output # transformers/modeling_bert.py:295:0
  %1033 : Tensor = prim::GetAttr[name="bias"](%1024)
  %1034 : Tensor = prim::GetAttr[name="weight"](%1024)
  %1035 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.110, %1035, %1034, %1033, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1037 : __torch__.torch.nn.modules.linear.___torch_mangle_5459.Linear = prim::GetAttr[name="dense"](%970)
  %1038 : Tensor = prim::GetAttr[name="bias"](%1037)
  %1039 : Tensor = prim::GetAttr[name="weight"](%1037)
  %1040 : Float(768:1, 3072:768) = aten::t(%1039), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.11, %1040), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.111 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.65, %1038, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.112 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.111), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1044 : __torch__.torch.nn.modules.normalization.___torch_mangle_5462.LayerNorm = prim::GetAttr[name="LayerNorm"](%969)
  %1045 : __torch__.torch.nn.modules.linear.___torch_mangle_5461.Linear = prim::GetAttr[name="dense"](%969)
  %1046 : Tensor = prim::GetAttr[name="bias"](%1045)
  %1047 : Tensor = prim::GetAttr[name="weight"](%1045)
  %1048 : Float(3072:1, 768:3072) = aten::t(%1047), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.112, %1048), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.113 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.66, %1046, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.113, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.22, %input_tensor.11, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output # transformers/modeling_bert.py:371:0
  %1053 : Tensor = prim::GetAttr[name="bias"](%1044)
  %1054 : Tensor = prim::GetAttr[name="weight"](%1044)
  %1055 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm
  %input.115 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.114, %1055, %1054, %1053, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1057 : __torch__.transformers.modeling_bert.___torch_mangle_5481.BertOutput = prim::GetAttr[name="output"](%66)
  %1058 : __torch__.transformers.modeling_bert.___torch_mangle_5477.BertIntermediate = prim::GetAttr[name="intermediate"](%66)
  %1059 : __torch__.transformers.modeling_bert.___torch_mangle_5475.BertAttention = prim::GetAttr[name="attention"](%66)
  %1060 : __torch__.transformers.modeling_bert.___torch_mangle_5474.BertSelfOutput = prim::GetAttr[name="output"](%1059)
  %1061 : __torch__.transformers.modeling_bert.___torch_mangle_5470.BertSelfAttention = prim::GetAttr[name="self"](%1059)
  %1062 : __torch__.torch.nn.modules.linear.___torch_mangle_5468.Linear = prim::GetAttr[name="value"](%1061)
  %1063 : __torch__.torch.nn.modules.linear.___torch_mangle_5467.Linear = prim::GetAttr[name="key"](%1061)
  %1064 : __torch__.torch.nn.modules.linear.___torch_mangle_5466.Linear = prim::GetAttr[name="query"](%1061)
  %1065 : Tensor = prim::GetAttr[name="bias"](%1064)
  %1066 : Tensor = prim::GetAttr[name="weight"](%1064)
  %1067 : Float(768:1, 768:768) = aten::t(%1066), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1067), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.67, %1065, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1070 : Tensor = prim::GetAttr[name="bias"](%1063)
  %1071 : Tensor = prim::GetAttr[name="weight"](%1063)
  %1072 : Float(768:1, 768:768) = aten::t(%1071), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1072), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.68, %1070, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1075 : Tensor = prim::GetAttr[name="bias"](%1062)
  %1076 : Tensor = prim::GetAttr[name="weight"](%1062)
  %1077 : Float(768:1, 768:768) = aten::t(%1076), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1077), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.69, %1075, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1080 : int = aten::size(%x.67, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1081 : int = aten::size(%x.67, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1082 : int[] = prim::ListConstruct(%1080, %1081, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x.68 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.67, %1082), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1084 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %query_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.68, %1084), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1086 : int = aten::size(%x.69, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1087 : int = aten::size(%x.69, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1088 : int[] = prim::ListConstruct(%1086, %1087, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x.70 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.69, %1088), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1090 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %key_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.70, %1090), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1092 : int = aten::size(%x.71, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1093 : int = aten::size(%x.71, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1094 : int[] = prim::ListConstruct(%1092, %1093, %13, %12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.71, %1094), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1096 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %value_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x, %1096), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1098 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer, %14, %11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.23 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer, %1098), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.23, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:259:0
  %input.116 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:262:0
  %input.117 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.116, %14, %21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.117, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:275:0
  %1105 : int[] = prim::ListConstruct(%30, %24, %29, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %1106 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.23, %1105), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
  %context_layer : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1106, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
  %1108 : int = aten::size(%context_layer, %30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
  %1109 : int = aten::size(%context_layer, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
  %1110 : int[] = prim::ListConstruct(%1108, %1109, %17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %input.118 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer, %1110), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:279:0
  %1112 : __torch__.torch.nn.modules.normalization.___torch_mangle_5472.LayerNorm = prim::GetAttr[name="LayerNorm"](%1060)
  %1113 : __torch__.torch.nn.modules.linear.___torch_mangle_5471.Linear = prim::GetAttr[name="dense"](%1060)
  %1114 : Tensor = prim::GetAttr[name="bias"](%1113)
  %1115 : Tensor = prim::GetAttr[name="weight"](%1113)
  %1116 : Float(768:1, 768:768) = aten::t(%1115), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.118, %1116), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.119 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.70, %1114, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.119, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.120 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.23, %input.115, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output # transformers/modeling_bert.py:295:0
  %1121 : Tensor = prim::GetAttr[name="bias"](%1112)
  %1122 : Tensor = prim::GetAttr[name="weight"](%1112)
  %1123 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.120, %1123, %1122, %1121, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1125 : __torch__.torch.nn.modules.linear.___torch_mangle_5476.Linear = prim::GetAttr[name="dense"](%1058)
  %1126 : Tensor = prim::GetAttr[name="bias"](%1125)
  %1127 : Tensor = prim::GetAttr[name="weight"](%1125)
  %1128 : Float(768:1, 3072:768) = aten::t(%1127), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor, %1128), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.121 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.71, %1126, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.122 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.121), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1132 : __torch__.torch.nn.modules.normalization.___torch_mangle_5479.LayerNorm = prim::GetAttr[name="LayerNorm"](%1057)
  %1133 : __torch__.torch.nn.modules.linear.___torch_mangle_5478.Linear = prim::GetAttr[name="dense"](%1057)
  %1134 : Tensor = prim::GetAttr[name="bias"](%1133)
  %1135 : Tensor = prim::GetAttr[name="weight"](%1133)
  %1136 : Float(3072:1, 768:3072) = aten::t(%1135), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.122, %1136), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.123 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.72, %1134, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.123, %18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.124 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states, %input_tensor, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output # transformers/modeling_bert.py:371:0
  %1141 : Tensor = prim::GetAttr[name="bias"](%1132)
  %1142 : Tensor = prim::GetAttr[name="weight"](%1132)
  %1143 : int[] = prim::ListConstruct(%17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm
  %input.125 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.124, %1143, %1142, %1141, %16, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1145 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %1146 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %input : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.125, %1146, %1145), scope: __module.dropout # torch/nn/functional.py:973:0
  %1148 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1678:0
  %1149 : Tensor = prim::GetAttr[name="bias"](%3)
  %1150 : Tensor = prim::GetAttr[name="weight"](%3)
  %1151 : Float(768:1, 2:768) = aten::t(%1150), scope: __module.classifier # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %1151), scope: __module.classifier # torch/nn/functional.py:1676:0
  %1153 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %1149, %1148), scope: __module.classifier # torch/nn/functional.py:1678:0
  %9 : (Float(17:26, 13:2, 2:1)) = prim::TupleConstruct(%1153)
  return (%9)
