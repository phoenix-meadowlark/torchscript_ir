graph(%self.1 : __torch__.transformers.modeling_camembert.CamembertForMaskedLM,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_roberta.___torch_mangle_10232.RobertaLMHead = prim::GetAttr[name="lm_head"](%self.1)
  %4 : __torch__.transformers.modeling_roberta.___torch_mangle_10228.RobertaModel = prim::GetAttr[name="roberta"](%self.1)
  %8 : Double() = prim::Constant[value={8}](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:195:0
  %9 : int = prim::Constant[value=-2](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %10 : int = prim::Constant[value=64](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %11 : int = prim::Constant[value=12](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %12 : Long() = prim::Constant[value={1}](), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %13 : int = prim::Constant[value=-1](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %14 : bool = prim::Constant[value=1](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %15 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %16 : int = prim::Constant[value=768](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %17 : float = prim::Constant[value=0.10000000000000001](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.dropout # torch/nn/functional.py:973:0
  %18 : Double() = prim::Constant[value={-10000}](), scope: __module.roberta # transformers/modeling_utils.py:258:0
  %19 : float = prim::Constant[value=1.](), scope: __module.roberta # torch/tensor.py:396:0
  %20 : None = prim::Constant(), scope: __module.roberta
  %21 : int = prim::Constant[value=6](), scope: __module.roberta # transformers/modeling_utils.py:257:0
  %22 : int = prim::Constant[value=3](), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %23 : int = prim::Constant[value=2](), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %24 : int = prim::Constant[value=9223372036854775807](), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %25 : bool = prim::Constant[value=0](), scope: __module.roberta # transformers/modeling_roberta.py:650:0
  %26 : Device = prim::Constant[value="cpu"](), scope: __module.roberta # transformers/modeling_roberta.py:650:0
  %27 : int = prim::Constant[value=4](), scope: __module.roberta # transformers/modeling_roberta.py:650:0
  %28 : int = prim::Constant[value=1](), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %29 : int = prim::Constant[value=0](), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %30 : __torch__.transformers.modeling_roberta.___torch_mangle_10227.RobertaEncoder = prim::GetAttr[name="encoder"](%4)
  %31 : __torch__.transformers.modeling_roberta.___torch_mangle_10021.RobertaEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %32 : int = aten::size(%input_ids, %29), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %33 : int = aten::size(%input_ids, %28), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %34 : int[] = prim::ListConstruct(%32, %33), scope: __module.roberta
  %input.2 : Long(17:13, 13:1) = aten::zeros(%34, %27, %29, %26, %25), scope: __module.roberta # transformers/modeling_roberta.py:650:0
  %36 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %29, %29, %24, %28), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %37 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%36, %28), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %38 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%37, %23), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%38, %22, %29, %24, %28), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %40 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %21, %25, %25, %20), scope: __module.roberta # transformers/modeling_utils.py:257:0
  %41 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%40, %19, %28), scope: __module.roberta # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%41, %18), scope: __module.roberta # transformers/modeling_utils.py:258:0
  %43 : __torch__.torch.nn.modules.normalization.___torch_mangle_10019.LayerNorm = prim::GetAttr[name="LayerNorm"](%31)
  %44 : __torch__.torch.nn.modules.sparse.___torch_mangle_10018.Embedding = prim::GetAttr[name="token_type_embeddings"](%31)
  %45 : __torch__.torch.nn.modules.sparse.___torch_mangle_10017.Embedding = prim::GetAttr[name="position_embeddings"](%31)
  %46 : __torch__.torch.nn.modules.sparse.___torch_mangle_10016.Embedding = prim::GetAttr[name="word_embeddings"](%31)
  %47 : Bool(17:13, 13:1) = aten::ne(%input_ids, %28), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1332:0
  %mask : Int(17:13, 13:1) = aten::to(%47, %22, %25, %25, %20), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1332:0
  %49 : Long(17:13, 13:1) = aten::cumsum(%mask, %28, %20), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1333:0
  %50 : Int(17:13, 13:1) = aten::type_as(%49, %mask), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1333:0
  %incremental_indices : Int(17:13, 13:1) = aten::mul(%50, %mask), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1333:0
  %52 : Long(17:13, 13:1) = aten::to(%incremental_indices, %27, %25, %25, %20), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %53 : Long(17:13, 13:1) = aten::add(%52, %12, %28), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %input.1 : Long(17:13, 13:1) = aten::to(%53, %27, %29, %26, %25, %25, %25, %20), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:98:0
  %55 : Tensor = prim::GetAttr[name="weight"](%46)
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::embedding(%55, %input_ids, %28, %25, %25), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %57 : Tensor = prim::GetAttr[name="weight"](%45)
  %position_embeddings : Float(17:9984, 13:768, 768:1) = aten::embedding(%57, %input.1, %28, %25, %25), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %59 : Tensor = prim::GetAttr[name="weight"](%44)
  %token_type_embeddings : Float(17:9984, 13:768, 768:1) = aten::embedding(%59, %input.2, %13, %25, %25), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %61 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %position_embeddings, %28), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:121:0
  %input.3 : Float(17:9984, 13:768, 768:1) = aten::add(%61, %token_type_embeddings, %28), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:121:0
  %63 : Tensor = prim::GetAttr[name="bias"](%43)
  %64 : Tensor = prim::GetAttr[name="weight"](%43)
  %65 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm
  %input.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.3, %65, %64, %63, %15, %14), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.4, %17, %25), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.dropout # torch/nn/functional.py:973:0
  %68 : __torch__.torch.nn.modules.container.___torch_mangle_10226.ModuleList = prim::GetAttr[name="layer"](%30)
  %69 : __torch__.transformers.modeling_roberta.___torch_mangle_10225.RobertaLayer = prim::GetAttr[name="11"](%68)
  %70 : __torch__.torch.nn.modules.container.___torch_mangle_10226.ModuleList = prim::GetAttr[name="layer"](%30)
  %71 : __torch__.transformers.modeling_roberta.___torch_mangle_10208.RobertaLayer = prim::GetAttr[name="10"](%70)
  %72 : __torch__.torch.nn.modules.container.___torch_mangle_10226.ModuleList = prim::GetAttr[name="layer"](%30)
  %73 : __torch__.transformers.modeling_roberta.___torch_mangle_10191.RobertaLayer = prim::GetAttr[name="9"](%72)
  %74 : __torch__.torch.nn.modules.container.___torch_mangle_10226.ModuleList = prim::GetAttr[name="layer"](%30)
  %75 : __torch__.transformers.modeling_roberta.___torch_mangle_10174.RobertaLayer = prim::GetAttr[name="8"](%74)
  %76 : __torch__.torch.nn.modules.container.___torch_mangle_10226.ModuleList = prim::GetAttr[name="layer"](%30)
  %77 : __torch__.transformers.modeling_roberta.___torch_mangle_10157.RobertaLayer = prim::GetAttr[name="7"](%76)
  %78 : __torch__.torch.nn.modules.container.___torch_mangle_10226.ModuleList = prim::GetAttr[name="layer"](%30)
  %79 : __torch__.transformers.modeling_roberta.___torch_mangle_10140.RobertaLayer = prim::GetAttr[name="6"](%78)
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_10226.ModuleList = prim::GetAttr[name="layer"](%30)
  %81 : __torch__.transformers.modeling_roberta.___torch_mangle_10123.RobertaLayer = prim::GetAttr[name="5"](%80)
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_10226.ModuleList = prim::GetAttr[name="layer"](%30)
  %83 : __torch__.transformers.modeling_roberta.___torch_mangle_10106.RobertaLayer = prim::GetAttr[name="4"](%82)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_10226.ModuleList = prim::GetAttr[name="layer"](%30)
  %85 : __torch__.transformers.modeling_roberta.___torch_mangle_10089.RobertaLayer = prim::GetAttr[name="3"](%84)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_10226.ModuleList = prim::GetAttr[name="layer"](%30)
  %87 : __torch__.transformers.modeling_roberta.___torch_mangle_10072.RobertaLayer = prim::GetAttr[name="2"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_10226.ModuleList = prim::GetAttr[name="layer"](%30)
  %89 : __torch__.transformers.modeling_roberta.___torch_mangle_10055.RobertaLayer = prim::GetAttr[name="1"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_10226.ModuleList = prim::GetAttr[name="layer"](%30)
  %91 : __torch__.transformers.modeling_roberta.___torch_mangle_10038.RobertaLayer = prim::GetAttr[name="0"](%90)
  %92 : __torch__.transformers.modeling_roberta.___torch_mangle_10037.RobertaOutput = prim::GetAttr[name="output"](%91)
  %93 : __torch__.transformers.modeling_roberta.___torch_mangle_10033.RobertaIntermediate = prim::GetAttr[name="intermediate"](%91)
  %94 : __torch__.transformers.modeling_roberta.___torch_mangle_10031.RobertaAttention = prim::GetAttr[name="attention"](%91)
  %95 : __torch__.transformers.modeling_roberta.___torch_mangle_10030.RobertaSelfOutput = prim::GetAttr[name="output"](%94)
  %96 : __torch__.transformers.modeling_roberta.___torch_mangle_10026.RobertaSelfAttention = prim::GetAttr[name="self"](%94)
  %97 : __torch__.torch.nn.modules.linear.___torch_mangle_10024.Linear = prim::GetAttr[name="value"](%96)
  %98 : __torch__.torch.nn.modules.linear.___torch_mangle_10023.Linear = prim::GetAttr[name="key"](%96)
  %99 : __torch__.torch.nn.modules.linear.___torch_mangle_10022.Linear = prim::GetAttr[name="query"](%96)
  %100 : Tensor = prim::GetAttr[name="bias"](%99)
  %101 : Tensor = prim::GetAttr[name="weight"](%99)
  %102 : Float(768:1, 768:768) = aten::t(%101), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %102), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %100, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %105 : Tensor = prim::GetAttr[name="bias"](%98)
  %106 : Tensor = prim::GetAttr[name="weight"](%98)
  %107 : Float(768:1, 768:768) = aten::t(%106), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %107), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %105, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %110 : Tensor = prim::GetAttr[name="bias"](%97)
  %111 : Tensor = prim::GetAttr[name="weight"](%97)
  %112 : Float(768:1, 768:768) = aten::t(%111), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %112), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %110, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %115 : int = aten::size(%x.1, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %116 : int = aten::size(%x.1, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %117 : int[] = prim::ListConstruct(%115, %116, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %x.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.1, %117), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %119 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.2, %119), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %121 : int = aten::size(%x.3, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %122 : int = aten::size(%x.3, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %123 : int[] = prim::ListConstruct(%121, %122, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %x.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.3, %123), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %125 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.4, %125), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %127 : int = aten::size(%x.5, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %128 : int = aten::size(%x.5, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %129 : int[] = prim::ListConstruct(%127, %128, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %x.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.5, %129), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %131 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.6, %131), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %133 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.1, %13, %9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %133), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.1, %8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:195:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:198:0
  %input.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.6, %13, %20), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.7, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:211:0
  %140 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %141 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.1, %140), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%141, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:213:0
  %143 : int = aten::size(%context_layer.2, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:214:0
  %144 : int = aten::size(%context_layer.2, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:214:0
  %145 : int[] = prim::ListConstruct(%143, %144, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.2, %145), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:215:0
  %147 : __torch__.torch.nn.modules.normalization.___torch_mangle_10028.LayerNorm = prim::GetAttr[name="LayerNorm"](%95)
  %148 : __torch__.torch.nn.modules.linear.___torch_mangle_10027.Linear = prim::GetAttr[name="dense"](%95)
  %149 : Tensor = prim::GetAttr[name="bias"](%148)
  %150 : Tensor = prim::GetAttr[name="weight"](%148)
  %151 : Float(768:1, 768:768) = aten::t(%150), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.8, %151), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.4, %149, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.9, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.1, %input.5, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output # transformers/modeling_roberta.py:232:0
  %156 : Tensor = prim::GetAttr[name="bias"](%147)
  %157 : Tensor = prim::GetAttr[name="weight"](%147)
  %158 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.10, %158, %157, %156, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %160 : __torch__.torch.nn.modules.linear.___torch_mangle_10032.Linear = prim::GetAttr[name="dense"](%93)
  %161 : Tensor = prim::GetAttr[name="bias"](%160)
  %162 : Tensor = prim::GetAttr[name="weight"](%160)
  %163 : Float(768:1, 3072:768) = aten::t(%162), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate/__module.roberta.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.1, %163), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate/__module.roberta.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.5, %161, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate/__module.roberta.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %167 : __torch__.torch.nn.modules.normalization.___torch_mangle_10035.LayerNorm = prim::GetAttr[name="LayerNorm"](%92)
  %168 : __torch__.torch.nn.modules.linear.___torch_mangle_10034.Linear = prim::GetAttr[name="dense"](%92)
  %169 : Tensor = prim::GetAttr[name="bias"](%168)
  %170 : Tensor = prim::GetAttr[name="weight"](%168)
  %171 : Float(3072:1, 768:3072) = aten::t(%170), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %171), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %169, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.2, %input_tensor.1, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output # transformers/modeling_roberta.py:311:0
  %176 : Tensor = prim::GetAttr[name="bias"](%167)
  %177 : Tensor = prim::GetAttr[name="weight"](%167)
  %178 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.LayerNorm
  %input.15 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %178, %177, %176, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %180 : __torch__.transformers.modeling_roberta.___torch_mangle_10054.RobertaOutput = prim::GetAttr[name="output"](%89)
  %181 : __torch__.transformers.modeling_roberta.___torch_mangle_10050.RobertaIntermediate = prim::GetAttr[name="intermediate"](%89)
  %182 : __torch__.transformers.modeling_roberta.___torch_mangle_10048.RobertaAttention = prim::GetAttr[name="attention"](%89)
  %183 : __torch__.transformers.modeling_roberta.___torch_mangle_10047.RobertaSelfOutput = prim::GetAttr[name="output"](%182)
  %184 : __torch__.transformers.modeling_roberta.___torch_mangle_10043.RobertaSelfAttention = prim::GetAttr[name="self"](%182)
  %185 : __torch__.torch.nn.modules.linear.___torch_mangle_10041.Linear = prim::GetAttr[name="value"](%184)
  %186 : __torch__.torch.nn.modules.linear.___torch_mangle_10040.Linear = prim::GetAttr[name="key"](%184)
  %187 : __torch__.torch.nn.modules.linear.___torch_mangle_10039.Linear = prim::GetAttr[name="query"](%184)
  %188 : Tensor = prim::GetAttr[name="bias"](%187)
  %189 : Tensor = prim::GetAttr[name="weight"](%187)
  %190 : Float(768:1, 768:768) = aten::t(%189), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %190), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %188, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %193 : Tensor = prim::GetAttr[name="bias"](%186)
  %194 : Tensor = prim::GetAttr[name="weight"](%186)
  %195 : Float(768:1, 768:768) = aten::t(%194), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %195), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %193, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %198 : Tensor = prim::GetAttr[name="bias"](%185)
  %199 : Tensor = prim::GetAttr[name="weight"](%185)
  %200 : Float(768:1, 768:768) = aten::t(%199), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %200), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.9, %198, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %203 : int = aten::size(%x.7, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %204 : int = aten::size(%x.7, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %205 : int[] = prim::ListConstruct(%203, %204, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %x.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.7, %205), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %207 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.8, %207), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %209 : int = aten::size(%x.9, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %210 : int = aten::size(%x.9, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %211 : int[] = prim::ListConstruct(%209, %210, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %x.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.9, %211), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %213 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.10, %213), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %215 : int = aten::size(%x.11, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %216 : int = aten::size(%x.11, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %217 : int[] = prim::ListConstruct(%215, %216, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %x.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.11, %217), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %219 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.12, %219), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %221 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.2, %13, %9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %221), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.3, %8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:195:0
  %input.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:198:0
  %input.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.16, %13, %20), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.17, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:211:0
  %228 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %229 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.3, %228), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%229, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:213:0
  %231 : int = aten::size(%context_layer.4, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:214:0
  %232 : int = aten::size(%context_layer.4, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:214:0
  %233 : int[] = prim::ListConstruct(%231, %232, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.4, %233), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:215:0
  %235 : __torch__.torch.nn.modules.normalization.___torch_mangle_10045.LayerNorm = prim::GetAttr[name="LayerNorm"](%183)
  %236 : __torch__.torch.nn.modules.linear.___torch_mangle_10044.Linear = prim::GetAttr[name="dense"](%183)
  %237 : Tensor = prim::GetAttr[name="bias"](%236)
  %238 : Tensor = prim::GetAttr[name="weight"](%236)
  %239 : Float(768:1, 768:768) = aten::t(%238), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.18, %239), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %237, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.19, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.3, %input.15, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output # transformers/modeling_roberta.py:232:0
  %244 : Tensor = prim::GetAttr[name="bias"](%235)
  %245 : Tensor = prim::GetAttr[name="weight"](%235)
  %246 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.20, %246, %245, %244, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %248 : __torch__.torch.nn.modules.linear.___torch_mangle_10049.Linear = prim::GetAttr[name="dense"](%181)
  %249 : Tensor = prim::GetAttr[name="bias"](%248)
  %250 : Tensor = prim::GetAttr[name="weight"](%248)
  %251 : Float(768:1, 3072:768) = aten::t(%250), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate/__module.roberta.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.2, %251), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate/__module.roberta.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.11, %249, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate/__module.roberta.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.22 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.21), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %255 : __torch__.torch.nn.modules.normalization.___torch_mangle_10052.LayerNorm = prim::GetAttr[name="LayerNorm"](%180)
  %256 : __torch__.torch.nn.modules.linear.___torch_mangle_10051.Linear = prim::GetAttr[name="dense"](%180)
  %257 : Tensor = prim::GetAttr[name="bias"](%256)
  %258 : Tensor = prim::GetAttr[name="weight"](%256)
  %259 : Float(3072:1, 768:3072) = aten::t(%258), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.22, %259), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %257, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.23, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.4, %input_tensor.2, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output # transformers/modeling_roberta.py:311:0
  %264 : Tensor = prim::GetAttr[name="bias"](%255)
  %265 : Tensor = prim::GetAttr[name="weight"](%255)
  %266 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.LayerNorm
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.24, %266, %265, %264, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %268 : __torch__.transformers.modeling_roberta.___torch_mangle_10071.RobertaOutput = prim::GetAttr[name="output"](%87)
  %269 : __torch__.transformers.modeling_roberta.___torch_mangle_10067.RobertaIntermediate = prim::GetAttr[name="intermediate"](%87)
  %270 : __torch__.transformers.modeling_roberta.___torch_mangle_10065.RobertaAttention = prim::GetAttr[name="attention"](%87)
  %271 : __torch__.transformers.modeling_roberta.___torch_mangle_10064.RobertaSelfOutput = prim::GetAttr[name="output"](%270)
  %272 : __torch__.transformers.modeling_roberta.___torch_mangle_10060.RobertaSelfAttention = prim::GetAttr[name="self"](%270)
  %273 : __torch__.torch.nn.modules.linear.___torch_mangle_10058.Linear = prim::GetAttr[name="value"](%272)
  %274 : __torch__.torch.nn.modules.linear.___torch_mangle_10057.Linear = prim::GetAttr[name="key"](%272)
  %275 : __torch__.torch.nn.modules.linear.___torch_mangle_10056.Linear = prim::GetAttr[name="query"](%272)
  %276 : Tensor = prim::GetAttr[name="bias"](%275)
  %277 : Tensor = prim::GetAttr[name="weight"](%275)
  %278 : Float(768:1, 768:768) = aten::t(%277), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %278), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %276, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %281 : Tensor = prim::GetAttr[name="bias"](%274)
  %282 : Tensor = prim::GetAttr[name="weight"](%274)
  %283 : Float(768:1, 768:768) = aten::t(%282), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %283), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.14, %281, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %286 : Tensor = prim::GetAttr[name="bias"](%273)
  %287 : Tensor = prim::GetAttr[name="weight"](%273)
  %288 : Float(768:1, 768:768) = aten::t(%287), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %288), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %286, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %291 : int = aten::size(%x.13, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %292 : int = aten::size(%x.13, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %293 : int[] = prim::ListConstruct(%291, %292, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %x.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.13, %293), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %295 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.14, %295), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %297 : int = aten::size(%x.15, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %298 : int = aten::size(%x.15, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %299 : int[] = prim::ListConstruct(%297, %298, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %x.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.15, %299), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %301 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.16, %301), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %303 : int = aten::size(%x.17, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %304 : int = aten::size(%x.17, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %305 : int[] = prim::ListConstruct(%303, %304, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %x.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.17, %305), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %307 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.18, %307), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %309 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.3, %13, %9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %309), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.5, %8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:195:0
  %input.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:198:0
  %input.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.26, %13, %20), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.27, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:211:0
  %316 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %317 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.5, %316), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%317, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:213:0
  %319 : int = aten::size(%context_layer.6, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:214:0
  %320 : int = aten::size(%context_layer.6, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:214:0
  %321 : int[] = prim::ListConstruct(%319, %320, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.6, %321), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:215:0
  %323 : __torch__.torch.nn.modules.normalization.___torch_mangle_10062.LayerNorm = prim::GetAttr[name="LayerNorm"](%271)
  %324 : __torch__.torch.nn.modules.linear.___torch_mangle_10061.Linear = prim::GetAttr[name="dense"](%271)
  %325 : Tensor = prim::GetAttr[name="bias"](%324)
  %326 : Tensor = prim::GetAttr[name="weight"](%324)
  %327 : Float(768:1, 768:768) = aten::t(%326), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.28, %327), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %325, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.29, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.30 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.5, %input.25, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output # transformers/modeling_roberta.py:232:0
  %332 : Tensor = prim::GetAttr[name="bias"](%323)
  %333 : Tensor = prim::GetAttr[name="weight"](%323)
  %334 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.30, %334, %333, %332, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %336 : __torch__.torch.nn.modules.linear.___torch_mangle_10066.Linear = prim::GetAttr[name="dense"](%269)
  %337 : Tensor = prim::GetAttr[name="bias"](%336)
  %338 : Tensor = prim::GetAttr[name="weight"](%336)
  %339 : Float(768:1, 3072:768) = aten::t(%338), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate/__module.roberta.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.3, %339), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate/__module.roberta.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.31 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.17, %337, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate/__module.roberta.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.32 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %343 : __torch__.torch.nn.modules.normalization.___torch_mangle_10069.LayerNorm = prim::GetAttr[name="LayerNorm"](%268)
  %344 : __torch__.torch.nn.modules.linear.___torch_mangle_10068.Linear = prim::GetAttr[name="dense"](%268)
  %345 : Tensor = prim::GetAttr[name="bias"](%344)
  %346 : Tensor = prim::GetAttr[name="weight"](%344)
  %347 : Float(3072:1, 768:3072) = aten::t(%346), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.32, %347), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %345, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.33, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.6, %input_tensor.3, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output # transformers/modeling_roberta.py:311:0
  %352 : Tensor = prim::GetAttr[name="bias"](%343)
  %353 : Tensor = prim::GetAttr[name="weight"](%343)
  %354 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.LayerNorm
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.34, %354, %353, %352, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %356 : __torch__.transformers.modeling_roberta.___torch_mangle_10088.RobertaOutput = prim::GetAttr[name="output"](%85)
  %357 : __torch__.transformers.modeling_roberta.___torch_mangle_10084.RobertaIntermediate = prim::GetAttr[name="intermediate"](%85)
  %358 : __torch__.transformers.modeling_roberta.___torch_mangle_10082.RobertaAttention = prim::GetAttr[name="attention"](%85)
  %359 : __torch__.transformers.modeling_roberta.___torch_mangle_10081.RobertaSelfOutput = prim::GetAttr[name="output"](%358)
  %360 : __torch__.transformers.modeling_roberta.___torch_mangle_10077.RobertaSelfAttention = prim::GetAttr[name="self"](%358)
  %361 : __torch__.torch.nn.modules.linear.___torch_mangle_10075.Linear = prim::GetAttr[name="value"](%360)
  %362 : __torch__.torch.nn.modules.linear.___torch_mangle_10074.Linear = prim::GetAttr[name="key"](%360)
  %363 : __torch__.torch.nn.modules.linear.___torch_mangle_10073.Linear = prim::GetAttr[name="query"](%360)
  %364 : Tensor = prim::GetAttr[name="bias"](%363)
  %365 : Tensor = prim::GetAttr[name="weight"](%363)
  %366 : Float(768:1, 768:768) = aten::t(%365), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %366), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.19, %364, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %369 : Tensor = prim::GetAttr[name="bias"](%362)
  %370 : Tensor = prim::GetAttr[name="weight"](%362)
  %371 : Float(768:1, 768:768) = aten::t(%370), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %371), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %369, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %374 : Tensor = prim::GetAttr[name="bias"](%361)
  %375 : Tensor = prim::GetAttr[name="weight"](%361)
  %376 : Float(768:1, 768:768) = aten::t(%375), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %376), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %374, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %379 : int = aten::size(%x.19, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %380 : int = aten::size(%x.19, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %381 : int[] = prim::ListConstruct(%379, %380, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %x.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.19, %381), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %383 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.20, %383), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %385 : int = aten::size(%x.21, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %386 : int = aten::size(%x.21, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %387 : int[] = prim::ListConstruct(%385, %386, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %x.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.21, %387), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %389 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.22, %389), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %391 : int = aten::size(%x.23, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %392 : int = aten::size(%x.23, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %393 : int[] = prim::ListConstruct(%391, %392, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %x.24 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.23, %393), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %395 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.24, %395), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %397 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.4, %13, %9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %397), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.7, %8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:195:0
  %input.36 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:198:0
  %input.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.36, %13, %20), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.37, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:211:0
  %404 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %405 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.7, %404), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%405, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:213:0
  %407 : int = aten::size(%context_layer.8, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:214:0
  %408 : int = aten::size(%context_layer.8, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:214:0
  %409 : int[] = prim::ListConstruct(%407, %408, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %input.38 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.8, %409), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:215:0
  %411 : __torch__.torch.nn.modules.normalization.___torch_mangle_10079.LayerNorm = prim::GetAttr[name="LayerNorm"](%359)
  %412 : __torch__.torch.nn.modules.linear.___torch_mangle_10078.Linear = prim::GetAttr[name="dense"](%359)
  %413 : Tensor = prim::GetAttr[name="bias"](%412)
  %414 : Tensor = prim::GetAttr[name="weight"](%412)
  %415 : Float(768:1, 768:768) = aten::t(%414), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.38, %415), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %413, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.39, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.7, %input.35, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output # transformers/modeling_roberta.py:232:0
  %420 : Tensor = prim::GetAttr[name="bias"](%411)
  %421 : Tensor = prim::GetAttr[name="weight"](%411)
  %422 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.40, %422, %421, %420, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %424 : __torch__.torch.nn.modules.linear.___torch_mangle_10083.Linear = prim::GetAttr[name="dense"](%357)
  %425 : Tensor = prim::GetAttr[name="bias"](%424)
  %426 : Tensor = prim::GetAttr[name="weight"](%424)
  %427 : Float(768:1, 3072:768) = aten::t(%426), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate/__module.roberta.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.4, %427), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate/__module.roberta.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.41 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.23, %425, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate/__module.roberta.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.42 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.41), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %431 : __torch__.torch.nn.modules.normalization.___torch_mangle_10086.LayerNorm = prim::GetAttr[name="LayerNorm"](%356)
  %432 : __torch__.torch.nn.modules.linear.___torch_mangle_10085.Linear = prim::GetAttr[name="dense"](%356)
  %433 : Tensor = prim::GetAttr[name="bias"](%432)
  %434 : Tensor = prim::GetAttr[name="weight"](%432)
  %435 : Float(3072:1, 768:3072) = aten::t(%434), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.42, %435), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.24, %433, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.43, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.44 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.8, %input_tensor.4, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output # transformers/modeling_roberta.py:311:0
  %440 : Tensor = prim::GetAttr[name="bias"](%431)
  %441 : Tensor = prim::GetAttr[name="weight"](%431)
  %442 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.LayerNorm
  %input.45 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.44, %442, %441, %440, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %444 : __torch__.transformers.modeling_roberta.___torch_mangle_10105.RobertaOutput = prim::GetAttr[name="output"](%83)
  %445 : __torch__.transformers.modeling_roberta.___torch_mangle_10101.RobertaIntermediate = prim::GetAttr[name="intermediate"](%83)
  %446 : __torch__.transformers.modeling_roberta.___torch_mangle_10099.RobertaAttention = prim::GetAttr[name="attention"](%83)
  %447 : __torch__.transformers.modeling_roberta.___torch_mangle_10098.RobertaSelfOutput = prim::GetAttr[name="output"](%446)
  %448 : __torch__.transformers.modeling_roberta.___torch_mangle_10094.RobertaSelfAttention = prim::GetAttr[name="self"](%446)
  %449 : __torch__.torch.nn.modules.linear.___torch_mangle_10092.Linear = prim::GetAttr[name="value"](%448)
  %450 : __torch__.torch.nn.modules.linear.___torch_mangle_10091.Linear = prim::GetAttr[name="key"](%448)
  %451 : __torch__.torch.nn.modules.linear.___torch_mangle_10090.Linear = prim::GetAttr[name="query"](%448)
  %452 : Tensor = prim::GetAttr[name="bias"](%451)
  %453 : Tensor = prim::GetAttr[name="weight"](%451)
  %454 : Float(768:1, 768:768) = aten::t(%453), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %454), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.25, %452, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %457 : Tensor = prim::GetAttr[name="bias"](%450)
  %458 : Tensor = prim::GetAttr[name="weight"](%450)
  %459 : Float(768:1, 768:768) = aten::t(%458), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %459), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.26, %457, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %462 : Tensor = prim::GetAttr[name="bias"](%449)
  %463 : Tensor = prim::GetAttr[name="weight"](%449)
  %464 : Float(768:1, 768:768) = aten::t(%463), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %464), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.27, %462, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %467 : int = aten::size(%x.25, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %468 : int = aten::size(%x.25, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %469 : int[] = prim::ListConstruct(%467, %468, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %x.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.25, %469), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %471 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.26, %471), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %473 : int = aten::size(%x.27, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %474 : int = aten::size(%x.27, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %475 : int[] = prim::ListConstruct(%473, %474, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %x.28 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.27, %475), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %477 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.28, %477), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %479 : int = aten::size(%x.29, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %480 : int = aten::size(%x.29, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %481 : int[] = prim::ListConstruct(%479, %480, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %x.30 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.29, %481), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %483 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.30, %483), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %485 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.5, %13, %9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %485), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.9, %8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:195:0
  %input.46 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:198:0
  %input.47 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.46, %13, %20), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.47, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:211:0
  %492 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %493 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.9, %492), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%493, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:213:0
  %495 : int = aten::size(%context_layer.10, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:214:0
  %496 : int = aten::size(%context_layer.10, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:214:0
  %497 : int[] = prim::ListConstruct(%495, %496, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %input.48 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.10, %497), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:215:0
  %499 : __torch__.torch.nn.modules.normalization.___torch_mangle_10096.LayerNorm = prim::GetAttr[name="LayerNorm"](%447)
  %500 : __torch__.torch.nn.modules.linear.___torch_mangle_10095.Linear = prim::GetAttr[name="dense"](%447)
  %501 : Tensor = prim::GetAttr[name="bias"](%500)
  %502 : Tensor = prim::GetAttr[name="weight"](%500)
  %503 : Float(768:1, 768:768) = aten::t(%502), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.48, %503), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.28, %501, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.49, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.9, %input.45, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output # transformers/modeling_roberta.py:232:0
  %508 : Tensor = prim::GetAttr[name="bias"](%499)
  %509 : Tensor = prim::GetAttr[name="weight"](%499)
  %510 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.50, %510, %509, %508, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %512 : __torch__.torch.nn.modules.linear.___torch_mangle_10100.Linear = prim::GetAttr[name="dense"](%445)
  %513 : Tensor = prim::GetAttr[name="bias"](%512)
  %514 : Tensor = prim::GetAttr[name="weight"](%512)
  %515 : Float(768:1, 3072:768) = aten::t(%514), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate/__module.roberta.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.5, %515), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate/__module.roberta.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.51 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.29, %513, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate/__module.roberta.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.52 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %519 : __torch__.torch.nn.modules.normalization.___torch_mangle_10103.LayerNorm = prim::GetAttr[name="LayerNorm"](%444)
  %520 : __torch__.torch.nn.modules.linear.___torch_mangle_10102.Linear = prim::GetAttr[name="dense"](%444)
  %521 : Tensor = prim::GetAttr[name="bias"](%520)
  %522 : Tensor = prim::GetAttr[name="weight"](%520)
  %523 : Float(3072:1, 768:3072) = aten::t(%522), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.52, %523), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.30, %521, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.53, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.54 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.10, %input_tensor.5, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output # transformers/modeling_roberta.py:311:0
  %528 : Tensor = prim::GetAttr[name="bias"](%519)
  %529 : Tensor = prim::GetAttr[name="weight"](%519)
  %530 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.LayerNorm
  %input.55 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.54, %530, %529, %528, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %532 : __torch__.transformers.modeling_roberta.___torch_mangle_10122.RobertaOutput = prim::GetAttr[name="output"](%81)
  %533 : __torch__.transformers.modeling_roberta.___torch_mangle_10118.RobertaIntermediate = prim::GetAttr[name="intermediate"](%81)
  %534 : __torch__.transformers.modeling_roberta.___torch_mangle_10116.RobertaAttention = prim::GetAttr[name="attention"](%81)
  %535 : __torch__.transformers.modeling_roberta.___torch_mangle_10115.RobertaSelfOutput = prim::GetAttr[name="output"](%534)
  %536 : __torch__.transformers.modeling_roberta.___torch_mangle_10111.RobertaSelfAttention = prim::GetAttr[name="self"](%534)
  %537 : __torch__.torch.nn.modules.linear.___torch_mangle_10109.Linear = prim::GetAttr[name="value"](%536)
  %538 : __torch__.torch.nn.modules.linear.___torch_mangle_10108.Linear = prim::GetAttr[name="key"](%536)
  %539 : __torch__.torch.nn.modules.linear.___torch_mangle_10107.Linear = prim::GetAttr[name="query"](%536)
  %540 : Tensor = prim::GetAttr[name="bias"](%539)
  %541 : Tensor = prim::GetAttr[name="weight"](%539)
  %542 : Float(768:1, 768:768) = aten::t(%541), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %542), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.31, %540, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %545 : Tensor = prim::GetAttr[name="bias"](%538)
  %546 : Tensor = prim::GetAttr[name="weight"](%538)
  %547 : Float(768:1, 768:768) = aten::t(%546), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %547), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.32, %545, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %550 : Tensor = prim::GetAttr[name="bias"](%537)
  %551 : Tensor = prim::GetAttr[name="weight"](%537)
  %552 : Float(768:1, 768:768) = aten::t(%551), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %552), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.33, %550, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %555 : int = aten::size(%x.31, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %556 : int = aten::size(%x.31, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %557 : int[] = prim::ListConstruct(%555, %556, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %x.32 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.31, %557), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %559 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.32, %559), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %561 : int = aten::size(%x.33, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %562 : int = aten::size(%x.33, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %563 : int[] = prim::ListConstruct(%561, %562, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %x.34 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.33, %563), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %565 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.34, %565), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %567 : int = aten::size(%x.35, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %568 : int = aten::size(%x.35, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %569 : int[] = prim::ListConstruct(%567, %568, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %x.36 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.35, %569), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %571 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.36, %571), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %573 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.6, %13, %9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %573), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.11, %8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:195:0
  %input.56 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:198:0
  %input.57 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.56, %13, %20), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.57, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:211:0
  %580 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %581 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.11, %580), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%581, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:213:0
  %583 : int = aten::size(%context_layer.12, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:214:0
  %584 : int = aten::size(%context_layer.12, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:214:0
  %585 : int[] = prim::ListConstruct(%583, %584, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %input.58 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.12, %585), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:215:0
  %587 : __torch__.torch.nn.modules.normalization.___torch_mangle_10113.LayerNorm = prim::GetAttr[name="LayerNorm"](%535)
  %588 : __torch__.torch.nn.modules.linear.___torch_mangle_10112.Linear = prim::GetAttr[name="dense"](%535)
  %589 : Tensor = prim::GetAttr[name="bias"](%588)
  %590 : Tensor = prim::GetAttr[name="weight"](%588)
  %591 : Float(768:1, 768:768) = aten::t(%590), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.58, %591), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.34, %589, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.59, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.11, %input.55, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output # transformers/modeling_roberta.py:232:0
  %596 : Tensor = prim::GetAttr[name="bias"](%587)
  %597 : Tensor = prim::GetAttr[name="weight"](%587)
  %598 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.60, %598, %597, %596, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %600 : __torch__.torch.nn.modules.linear.___torch_mangle_10117.Linear = prim::GetAttr[name="dense"](%533)
  %601 : Tensor = prim::GetAttr[name="bias"](%600)
  %602 : Tensor = prim::GetAttr[name="weight"](%600)
  %603 : Float(768:1, 3072:768) = aten::t(%602), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate/__module.roberta.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.6, %603), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate/__module.roberta.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.61 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.35, %601, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate/__module.roberta.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.62 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.61), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %607 : __torch__.torch.nn.modules.normalization.___torch_mangle_10120.LayerNorm = prim::GetAttr[name="LayerNorm"](%532)
  %608 : __torch__.torch.nn.modules.linear.___torch_mangle_10119.Linear = prim::GetAttr[name="dense"](%532)
  %609 : Tensor = prim::GetAttr[name="bias"](%608)
  %610 : Tensor = prim::GetAttr[name="weight"](%608)
  %611 : Float(3072:1, 768:3072) = aten::t(%610), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.62, %611), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.36, %609, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.63, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.12, %input_tensor.6, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output # transformers/modeling_roberta.py:311:0
  %616 : Tensor = prim::GetAttr[name="bias"](%607)
  %617 : Tensor = prim::GetAttr[name="weight"](%607)
  %618 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.LayerNorm
  %input.65 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.64, %618, %617, %616, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %620 : __torch__.transformers.modeling_roberta.___torch_mangle_10139.RobertaOutput = prim::GetAttr[name="output"](%79)
  %621 : __torch__.transformers.modeling_roberta.___torch_mangle_10135.RobertaIntermediate = prim::GetAttr[name="intermediate"](%79)
  %622 : __torch__.transformers.modeling_roberta.___torch_mangle_10133.RobertaAttention = prim::GetAttr[name="attention"](%79)
  %623 : __torch__.transformers.modeling_roberta.___torch_mangle_10132.RobertaSelfOutput = prim::GetAttr[name="output"](%622)
  %624 : __torch__.transformers.modeling_roberta.___torch_mangle_10128.RobertaSelfAttention = prim::GetAttr[name="self"](%622)
  %625 : __torch__.torch.nn.modules.linear.___torch_mangle_10126.Linear = prim::GetAttr[name="value"](%624)
  %626 : __torch__.torch.nn.modules.linear.___torch_mangle_10125.Linear = prim::GetAttr[name="key"](%624)
  %627 : __torch__.torch.nn.modules.linear.___torch_mangle_10124.Linear = prim::GetAttr[name="query"](%624)
  %628 : Tensor = prim::GetAttr[name="bias"](%627)
  %629 : Tensor = prim::GetAttr[name="weight"](%627)
  %630 : Float(768:1, 768:768) = aten::t(%629), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %630), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.37, %628, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %633 : Tensor = prim::GetAttr[name="bias"](%626)
  %634 : Tensor = prim::GetAttr[name="weight"](%626)
  %635 : Float(768:1, 768:768) = aten::t(%634), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %635), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.38, %633, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %638 : Tensor = prim::GetAttr[name="bias"](%625)
  %639 : Tensor = prim::GetAttr[name="weight"](%625)
  %640 : Float(768:1, 768:768) = aten::t(%639), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %640), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.39, %638, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %643 : int = aten::size(%x.37, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %644 : int = aten::size(%x.37, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %645 : int[] = prim::ListConstruct(%643, %644, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %x.38 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.37, %645), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %647 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.38, %647), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %649 : int = aten::size(%x.39, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %650 : int = aten::size(%x.39, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %651 : int[] = prim::ListConstruct(%649, %650, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %x.40 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.39, %651), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %653 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.40, %653), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %655 : int = aten::size(%x.41, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %656 : int = aten::size(%x.41, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %657 : int[] = prim::ListConstruct(%655, %656, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %x.42 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.41, %657), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %659 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.42, %659), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %661 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.7, %13, %9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %661), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.14 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.13, %8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:195:0
  %input.66 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:198:0
  %input.67 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.66, %13, %20), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.67, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:211:0
  %668 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %669 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.13, %668), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%669, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:213:0
  %671 : int = aten::size(%context_layer.14, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:214:0
  %672 : int = aten::size(%context_layer.14, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:214:0
  %673 : int[] = prim::ListConstruct(%671, %672, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %input.68 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.14, %673), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:215:0
  %675 : __torch__.torch.nn.modules.normalization.___torch_mangle_10130.LayerNorm = prim::GetAttr[name="LayerNorm"](%623)
  %676 : __torch__.torch.nn.modules.linear.___torch_mangle_10129.Linear = prim::GetAttr[name="dense"](%623)
  %677 : Tensor = prim::GetAttr[name="bias"](%676)
  %678 : Tensor = prim::GetAttr[name="weight"](%676)
  %679 : Float(768:1, 768:768) = aten::t(%678), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.68, %679), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.40, %677, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.69, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.70 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.13, %input.65, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output # transformers/modeling_roberta.py:232:0
  %684 : Tensor = prim::GetAttr[name="bias"](%675)
  %685 : Tensor = prim::GetAttr[name="weight"](%675)
  %686 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.70, %686, %685, %684, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %688 : __torch__.torch.nn.modules.linear.___torch_mangle_10134.Linear = prim::GetAttr[name="dense"](%621)
  %689 : Tensor = prim::GetAttr[name="bias"](%688)
  %690 : Tensor = prim::GetAttr[name="weight"](%688)
  %691 : Float(768:1, 3072:768) = aten::t(%690), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate/__module.roberta.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.7, %691), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate/__module.roberta.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.71 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.41, %689, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate/__module.roberta.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.72 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.71), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %695 : __torch__.torch.nn.modules.normalization.___torch_mangle_10137.LayerNorm = prim::GetAttr[name="LayerNorm"](%620)
  %696 : __torch__.torch.nn.modules.linear.___torch_mangle_10136.Linear = prim::GetAttr[name="dense"](%620)
  %697 : Tensor = prim::GetAttr[name="bias"](%696)
  %698 : Tensor = prim::GetAttr[name="weight"](%696)
  %699 : Float(3072:1, 768:3072) = aten::t(%698), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.72, %699), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.73 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.42, %697, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.73, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.14, %input_tensor.7, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output # transformers/modeling_roberta.py:311:0
  %704 : Tensor = prim::GetAttr[name="bias"](%695)
  %705 : Tensor = prim::GetAttr[name="weight"](%695)
  %706 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.LayerNorm
  %input.75 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.74, %706, %705, %704, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %708 : __torch__.transformers.modeling_roberta.___torch_mangle_10156.RobertaOutput = prim::GetAttr[name="output"](%77)
  %709 : __torch__.transformers.modeling_roberta.___torch_mangle_10152.RobertaIntermediate = prim::GetAttr[name="intermediate"](%77)
  %710 : __torch__.transformers.modeling_roberta.___torch_mangle_10150.RobertaAttention = prim::GetAttr[name="attention"](%77)
  %711 : __torch__.transformers.modeling_roberta.___torch_mangle_10149.RobertaSelfOutput = prim::GetAttr[name="output"](%710)
  %712 : __torch__.transformers.modeling_roberta.___torch_mangle_10145.RobertaSelfAttention = prim::GetAttr[name="self"](%710)
  %713 : __torch__.torch.nn.modules.linear.___torch_mangle_10143.Linear = prim::GetAttr[name="value"](%712)
  %714 : __torch__.torch.nn.modules.linear.___torch_mangle_10142.Linear = prim::GetAttr[name="key"](%712)
  %715 : __torch__.torch.nn.modules.linear.___torch_mangle_10141.Linear = prim::GetAttr[name="query"](%712)
  %716 : Tensor = prim::GetAttr[name="bias"](%715)
  %717 : Tensor = prim::GetAttr[name="weight"](%715)
  %718 : Float(768:1, 768:768) = aten::t(%717), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %718), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.43, %716, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %721 : Tensor = prim::GetAttr[name="bias"](%714)
  %722 : Tensor = prim::GetAttr[name="weight"](%714)
  %723 : Float(768:1, 768:768) = aten::t(%722), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %723), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.44, %721, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %726 : Tensor = prim::GetAttr[name="bias"](%713)
  %727 : Tensor = prim::GetAttr[name="weight"](%713)
  %728 : Float(768:1, 768:768) = aten::t(%727), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %728), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.45, %726, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %731 : int = aten::size(%x.43, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %732 : int = aten::size(%x.43, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %733 : int[] = prim::ListConstruct(%731, %732, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %x.44 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.43, %733), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %735 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.44, %735), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %737 : int = aten::size(%x.45, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %738 : int = aten::size(%x.45, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %739 : int[] = prim::ListConstruct(%737, %738, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %x.46 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.45, %739), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %741 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.46, %741), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %743 : int = aten::size(%x.47, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %744 : int = aten::size(%x.47, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %745 : int[] = prim::ListConstruct(%743, %744, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %x.48 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.47, %745), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %747 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.48, %747), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %749 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.8, %13, %9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %749), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.15, %8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:195:0
  %input.76 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:198:0
  %input.77 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.76, %13, %20), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.77, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:211:0
  %756 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %757 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.15, %756), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%757, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:213:0
  %759 : int = aten::size(%context_layer.16, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:214:0
  %760 : int = aten::size(%context_layer.16, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:214:0
  %761 : int[] = prim::ListConstruct(%759, %760, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %input.78 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.16, %761), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:215:0
  %763 : __torch__.torch.nn.modules.normalization.___torch_mangle_10147.LayerNorm = prim::GetAttr[name="LayerNorm"](%711)
  %764 : __torch__.torch.nn.modules.linear.___torch_mangle_10146.Linear = prim::GetAttr[name="dense"](%711)
  %765 : Tensor = prim::GetAttr[name="bias"](%764)
  %766 : Tensor = prim::GetAttr[name="weight"](%764)
  %767 : Float(768:1, 768:768) = aten::t(%766), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.78, %767), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.79 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.46, %765, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.79, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.80 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.15, %input.75, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output # transformers/modeling_roberta.py:232:0
  %772 : Tensor = prim::GetAttr[name="bias"](%763)
  %773 : Tensor = prim::GetAttr[name="weight"](%763)
  %774 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.80, %774, %773, %772, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %776 : __torch__.torch.nn.modules.linear.___torch_mangle_10151.Linear = prim::GetAttr[name="dense"](%709)
  %777 : Tensor = prim::GetAttr[name="bias"](%776)
  %778 : Tensor = prim::GetAttr[name="weight"](%776)
  %779 : Float(768:1, 3072:768) = aten::t(%778), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate/__module.roberta.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.8, %779), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate/__module.roberta.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.81 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.47, %777, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate/__module.roberta.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.82 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.81), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %783 : __torch__.torch.nn.modules.normalization.___torch_mangle_10154.LayerNorm = prim::GetAttr[name="LayerNorm"](%708)
  %784 : __torch__.torch.nn.modules.linear.___torch_mangle_10153.Linear = prim::GetAttr[name="dense"](%708)
  %785 : Tensor = prim::GetAttr[name="bias"](%784)
  %786 : Tensor = prim::GetAttr[name="weight"](%784)
  %787 : Float(3072:1, 768:3072) = aten::t(%786), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.82, %787), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.83 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.48, %785, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.83, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.84 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.16, %input_tensor.8, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output # transformers/modeling_roberta.py:311:0
  %792 : Tensor = prim::GetAttr[name="bias"](%783)
  %793 : Tensor = prim::GetAttr[name="weight"](%783)
  %794 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.LayerNorm
  %input.85 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.84, %794, %793, %792, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %796 : __torch__.transformers.modeling_roberta.___torch_mangle_10173.RobertaOutput = prim::GetAttr[name="output"](%75)
  %797 : __torch__.transformers.modeling_roberta.___torch_mangle_10169.RobertaIntermediate = prim::GetAttr[name="intermediate"](%75)
  %798 : __torch__.transformers.modeling_roberta.___torch_mangle_10167.RobertaAttention = prim::GetAttr[name="attention"](%75)
  %799 : __torch__.transformers.modeling_roberta.___torch_mangle_10166.RobertaSelfOutput = prim::GetAttr[name="output"](%798)
  %800 : __torch__.transformers.modeling_roberta.___torch_mangle_10162.RobertaSelfAttention = prim::GetAttr[name="self"](%798)
  %801 : __torch__.torch.nn.modules.linear.___torch_mangle_10160.Linear = prim::GetAttr[name="value"](%800)
  %802 : __torch__.torch.nn.modules.linear.___torch_mangle_10159.Linear = prim::GetAttr[name="key"](%800)
  %803 : __torch__.torch.nn.modules.linear.___torch_mangle_10158.Linear = prim::GetAttr[name="query"](%800)
  %804 : Tensor = prim::GetAttr[name="bias"](%803)
  %805 : Tensor = prim::GetAttr[name="weight"](%803)
  %806 : Float(768:1, 768:768) = aten::t(%805), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %806), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.49, %804, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %809 : Tensor = prim::GetAttr[name="bias"](%802)
  %810 : Tensor = prim::GetAttr[name="weight"](%802)
  %811 : Float(768:1, 768:768) = aten::t(%810), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %811), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.50, %809, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %814 : Tensor = prim::GetAttr[name="bias"](%801)
  %815 : Tensor = prim::GetAttr[name="weight"](%801)
  %816 : Float(768:1, 768:768) = aten::t(%815), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %816), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.51, %814, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %819 : int = aten::size(%x.49, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %820 : int = aten::size(%x.49, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %821 : int[] = prim::ListConstruct(%819, %820, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %x.50 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.49, %821), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %823 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.50, %823), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %825 : int = aten::size(%x.51, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %826 : int = aten::size(%x.51, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %827 : int[] = prim::ListConstruct(%825, %826, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %x.52 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.51, %827), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %829 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.52, %829), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %831 : int = aten::size(%x.53, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %832 : int = aten::size(%x.53, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %833 : int[] = prim::ListConstruct(%831, %832, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %x.54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.53, %833), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %835 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.54, %835), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %837 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.9, %13, %9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %837), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.18 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.17, %8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:195:0
  %input.86 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:198:0
  %input.87 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.86, %13, %20), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.87, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:211:0
  %844 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %845 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.17, %844), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%845, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:213:0
  %847 : int = aten::size(%context_layer.18, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:214:0
  %848 : int = aten::size(%context_layer.18, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:214:0
  %849 : int[] = prim::ListConstruct(%847, %848, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %input.88 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.18, %849), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:215:0
  %851 : __torch__.torch.nn.modules.normalization.___torch_mangle_10164.LayerNorm = prim::GetAttr[name="LayerNorm"](%799)
  %852 : __torch__.torch.nn.modules.linear.___torch_mangle_10163.Linear = prim::GetAttr[name="dense"](%799)
  %853 : Tensor = prim::GetAttr[name="bias"](%852)
  %854 : Tensor = prim::GetAttr[name="weight"](%852)
  %855 : Float(768:1, 768:768) = aten::t(%854), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.88, %855), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.89 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.52, %853, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.89, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.90 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.17, %input.85, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output # transformers/modeling_roberta.py:232:0
  %860 : Tensor = prim::GetAttr[name="bias"](%851)
  %861 : Tensor = prim::GetAttr[name="weight"](%851)
  %862 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.90, %862, %861, %860, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %864 : __torch__.torch.nn.modules.linear.___torch_mangle_10168.Linear = prim::GetAttr[name="dense"](%797)
  %865 : Tensor = prim::GetAttr[name="bias"](%864)
  %866 : Tensor = prim::GetAttr[name="weight"](%864)
  %867 : Float(768:1, 3072:768) = aten::t(%866), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate/__module.roberta.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.9, %867), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate/__module.roberta.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.91 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.53, %865, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate/__module.roberta.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.92 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.91), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %871 : __torch__.torch.nn.modules.normalization.___torch_mangle_10171.LayerNorm = prim::GetAttr[name="LayerNorm"](%796)
  %872 : __torch__.torch.nn.modules.linear.___torch_mangle_10170.Linear = prim::GetAttr[name="dense"](%796)
  %873 : Tensor = prim::GetAttr[name="bias"](%872)
  %874 : Tensor = prim::GetAttr[name="weight"](%872)
  %875 : Float(3072:1, 768:3072) = aten::t(%874), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.92, %875), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.93 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.54, %873, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.93, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.94 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.18, %input_tensor.9, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output # transformers/modeling_roberta.py:311:0
  %880 : Tensor = prim::GetAttr[name="bias"](%871)
  %881 : Tensor = prim::GetAttr[name="weight"](%871)
  %882 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.LayerNorm
  %input.95 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.94, %882, %881, %880, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %884 : __torch__.transformers.modeling_roberta.___torch_mangle_10190.RobertaOutput = prim::GetAttr[name="output"](%73)
  %885 : __torch__.transformers.modeling_roberta.___torch_mangle_10186.RobertaIntermediate = prim::GetAttr[name="intermediate"](%73)
  %886 : __torch__.transformers.modeling_roberta.___torch_mangle_10184.RobertaAttention = prim::GetAttr[name="attention"](%73)
  %887 : __torch__.transformers.modeling_roberta.___torch_mangle_10183.RobertaSelfOutput = prim::GetAttr[name="output"](%886)
  %888 : __torch__.transformers.modeling_roberta.___torch_mangle_10179.RobertaSelfAttention = prim::GetAttr[name="self"](%886)
  %889 : __torch__.torch.nn.modules.linear.___torch_mangle_10177.Linear = prim::GetAttr[name="value"](%888)
  %890 : __torch__.torch.nn.modules.linear.___torch_mangle_10176.Linear = prim::GetAttr[name="key"](%888)
  %891 : __torch__.torch.nn.modules.linear.___torch_mangle_10175.Linear = prim::GetAttr[name="query"](%888)
  %892 : Tensor = prim::GetAttr[name="bias"](%891)
  %893 : Tensor = prim::GetAttr[name="weight"](%891)
  %894 : Float(768:1, 768:768) = aten::t(%893), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %894), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.55, %892, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %897 : Tensor = prim::GetAttr[name="bias"](%890)
  %898 : Tensor = prim::GetAttr[name="weight"](%890)
  %899 : Float(768:1, 768:768) = aten::t(%898), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %899), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.56, %897, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %902 : Tensor = prim::GetAttr[name="bias"](%889)
  %903 : Tensor = prim::GetAttr[name="weight"](%889)
  %904 : Float(768:1, 768:768) = aten::t(%903), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %904), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.57, %902, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %907 : int = aten::size(%x.55, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %908 : int = aten::size(%x.55, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %909 : int[] = prim::ListConstruct(%907, %908, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %x.56 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.55, %909), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %911 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.56, %911), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %913 : int = aten::size(%x.57, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %914 : int = aten::size(%x.57, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %915 : int[] = prim::ListConstruct(%913, %914, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %x.58 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.57, %915), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %917 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.58, %917), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %919 : int = aten::size(%x.59, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %920 : int = aten::size(%x.59, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %921 : int[] = prim::ListConstruct(%919, %920, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %x.60 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.59, %921), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %923 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.60, %923), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %925 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.10, %13, %9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.19 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %925), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.20 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.19, %8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:195:0
  %input.96 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:198:0
  %input.97 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.96, %13, %20), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.97, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:211:0
  %932 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %933 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.19, %932), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%933, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:213:0
  %935 : int = aten::size(%context_layer.20, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:214:0
  %936 : int = aten::size(%context_layer.20, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:214:0
  %937 : int[] = prim::ListConstruct(%935, %936, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %input.98 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.20, %937), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:215:0
  %939 : __torch__.torch.nn.modules.normalization.___torch_mangle_10181.LayerNorm = prim::GetAttr[name="LayerNorm"](%887)
  %940 : __torch__.torch.nn.modules.linear.___torch_mangle_10180.Linear = prim::GetAttr[name="dense"](%887)
  %941 : Tensor = prim::GetAttr[name="bias"](%940)
  %942 : Tensor = prim::GetAttr[name="weight"](%940)
  %943 : Float(768:1, 768:768) = aten::t(%942), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.98, %943), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.99 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.58, %941, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.99, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.100 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.19, %input.95, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output # transformers/modeling_roberta.py:232:0
  %948 : Tensor = prim::GetAttr[name="bias"](%939)
  %949 : Tensor = prim::GetAttr[name="weight"](%939)
  %950 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.100, %950, %949, %948, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %952 : __torch__.torch.nn.modules.linear.___torch_mangle_10185.Linear = prim::GetAttr[name="dense"](%885)
  %953 : Tensor = prim::GetAttr[name="bias"](%952)
  %954 : Tensor = prim::GetAttr[name="weight"](%952)
  %955 : Float(768:1, 3072:768) = aten::t(%954), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate/__module.roberta.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.10, %955), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate/__module.roberta.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.101 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.59, %953, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate/__module.roberta.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.102 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.101), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %959 : __torch__.torch.nn.modules.normalization.___torch_mangle_10188.LayerNorm = prim::GetAttr[name="LayerNorm"](%884)
  %960 : __torch__.torch.nn.modules.linear.___torch_mangle_10187.Linear = prim::GetAttr[name="dense"](%884)
  %961 : Tensor = prim::GetAttr[name="bias"](%960)
  %962 : Tensor = prim::GetAttr[name="weight"](%960)
  %963 : Float(3072:1, 768:3072) = aten::t(%962), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.102, %963), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.103 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.60, %961, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.103, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.104 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.20, %input_tensor.10, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output # transformers/modeling_roberta.py:311:0
  %968 : Tensor = prim::GetAttr[name="bias"](%959)
  %969 : Tensor = prim::GetAttr[name="weight"](%959)
  %970 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.LayerNorm
  %input.105 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.104, %970, %969, %968, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %972 : __torch__.transformers.modeling_roberta.___torch_mangle_10207.RobertaOutput = prim::GetAttr[name="output"](%71)
  %973 : __torch__.transformers.modeling_roberta.___torch_mangle_10203.RobertaIntermediate = prim::GetAttr[name="intermediate"](%71)
  %974 : __torch__.transformers.modeling_roberta.___torch_mangle_10201.RobertaAttention = prim::GetAttr[name="attention"](%71)
  %975 : __torch__.transformers.modeling_roberta.___torch_mangle_10200.RobertaSelfOutput = prim::GetAttr[name="output"](%974)
  %976 : __torch__.transformers.modeling_roberta.___torch_mangle_10196.RobertaSelfAttention = prim::GetAttr[name="self"](%974)
  %977 : __torch__.torch.nn.modules.linear.___torch_mangle_10194.Linear = prim::GetAttr[name="value"](%976)
  %978 : __torch__.torch.nn.modules.linear.___torch_mangle_10193.Linear = prim::GetAttr[name="key"](%976)
  %979 : __torch__.torch.nn.modules.linear.___torch_mangle_10192.Linear = prim::GetAttr[name="query"](%976)
  %980 : Tensor = prim::GetAttr[name="bias"](%979)
  %981 : Tensor = prim::GetAttr[name="weight"](%979)
  %982 : Float(768:1, 768:768) = aten::t(%981), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %982), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.61, %980, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %985 : Tensor = prim::GetAttr[name="bias"](%978)
  %986 : Tensor = prim::GetAttr[name="weight"](%978)
  %987 : Float(768:1, 768:768) = aten::t(%986), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %987), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.62, %985, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %990 : Tensor = prim::GetAttr[name="bias"](%977)
  %991 : Tensor = prim::GetAttr[name="weight"](%977)
  %992 : Float(768:1, 768:768) = aten::t(%991), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %992), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.63, %990, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %995 : int = aten::size(%x.61, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %996 : int = aten::size(%x.61, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %997 : int[] = prim::ListConstruct(%995, %996, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %x.62 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.61, %997), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %999 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.62, %999), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1001 : int = aten::size(%x.63, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1002 : int = aten::size(%x.63, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1003 : int[] = prim::ListConstruct(%1001, %1002, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %x.64 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.63, %1003), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1005 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.64, %1005), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1007 : int = aten::size(%x.65, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1008 : int = aten::size(%x.65, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1009 : int[] = prim::ListConstruct(%1007, %1008, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %x.66 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.65, %1009), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1011 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.66, %1011), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1013 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.11, %13, %9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.21 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1013), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.22 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.21, %8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:195:0
  %input.106 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:198:0
  %input.107 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.106, %13, %20), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.107, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:211:0
  %1020 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %1021 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.21, %1020), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1021, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:213:0
  %1023 : int = aten::size(%context_layer.22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:214:0
  %1024 : int = aten::size(%context_layer.22, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:214:0
  %1025 : int[] = prim::ListConstruct(%1023, %1024, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %input.108 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.22, %1025), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:215:0
  %1027 : __torch__.torch.nn.modules.normalization.___torch_mangle_10198.LayerNorm = prim::GetAttr[name="LayerNorm"](%975)
  %1028 : __torch__.torch.nn.modules.linear.___torch_mangle_10197.Linear = prim::GetAttr[name="dense"](%975)
  %1029 : Tensor = prim::GetAttr[name="bias"](%1028)
  %1030 : Tensor = prim::GetAttr[name="weight"](%1028)
  %1031 : Float(768:1, 768:768) = aten::t(%1030), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.108, %1031), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.109 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.64, %1029, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.109, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.21, %input.105, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output # transformers/modeling_roberta.py:232:0
  %1036 : Tensor = prim::GetAttr[name="bias"](%1027)
  %1037 : Tensor = prim::GetAttr[name="weight"](%1027)
  %1038 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.110, %1038, %1037, %1036, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1040 : __torch__.torch.nn.modules.linear.___torch_mangle_10202.Linear = prim::GetAttr[name="dense"](%973)
  %1041 : Tensor = prim::GetAttr[name="bias"](%1040)
  %1042 : Tensor = prim::GetAttr[name="weight"](%1040)
  %1043 : Float(768:1, 3072:768) = aten::t(%1042), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate/__module.roberta.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.11, %1043), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate/__module.roberta.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.111 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.65, %1041, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate/__module.roberta.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.112 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.111), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1047 : __torch__.torch.nn.modules.normalization.___torch_mangle_10205.LayerNorm = prim::GetAttr[name="LayerNorm"](%972)
  %1048 : __torch__.torch.nn.modules.linear.___torch_mangle_10204.Linear = prim::GetAttr[name="dense"](%972)
  %1049 : Tensor = prim::GetAttr[name="bias"](%1048)
  %1050 : Tensor = prim::GetAttr[name="weight"](%1048)
  %1051 : Float(3072:1, 768:3072) = aten::t(%1050), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.112, %1051), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.113 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.66, %1049, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.113, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.22, %input_tensor.11, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output # transformers/modeling_roberta.py:311:0
  %1056 : Tensor = prim::GetAttr[name="bias"](%1047)
  %1057 : Tensor = prim::GetAttr[name="weight"](%1047)
  %1058 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.LayerNorm
  %input.115 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.114, %1058, %1057, %1056, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1060 : __torch__.transformers.modeling_roberta.___torch_mangle_10224.RobertaOutput = prim::GetAttr[name="output"](%69)
  %1061 : __torch__.transformers.modeling_roberta.___torch_mangle_10220.RobertaIntermediate = prim::GetAttr[name="intermediate"](%69)
  %1062 : __torch__.transformers.modeling_roberta.___torch_mangle_10218.RobertaAttention = prim::GetAttr[name="attention"](%69)
  %1063 : __torch__.transformers.modeling_roberta.___torch_mangle_10217.RobertaSelfOutput = prim::GetAttr[name="output"](%1062)
  %1064 : __torch__.transformers.modeling_roberta.___torch_mangle_10213.RobertaSelfAttention = prim::GetAttr[name="self"](%1062)
  %1065 : __torch__.torch.nn.modules.linear.___torch_mangle_10211.Linear = prim::GetAttr[name="value"](%1064)
  %1066 : __torch__.torch.nn.modules.linear.___torch_mangle_10210.Linear = prim::GetAttr[name="key"](%1064)
  %1067 : __torch__.torch.nn.modules.linear.___torch_mangle_10209.Linear = prim::GetAttr[name="query"](%1064)
  %1068 : Tensor = prim::GetAttr[name="bias"](%1067)
  %1069 : Tensor = prim::GetAttr[name="weight"](%1067)
  %1070 : Float(768:1, 768:768) = aten::t(%1069), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1070), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.67, %1068, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1073 : Tensor = prim::GetAttr[name="bias"](%1066)
  %1074 : Tensor = prim::GetAttr[name="weight"](%1066)
  %1075 : Float(768:1, 768:768) = aten::t(%1074), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1075), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.68, %1073, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1078 : Tensor = prim::GetAttr[name="bias"](%1065)
  %1079 : Tensor = prim::GetAttr[name="weight"](%1065)
  %1080 : Float(768:1, 768:768) = aten::t(%1079), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1080), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.69, %1078, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1083 : int = aten::size(%x.67, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1084 : int = aten::size(%x.67, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1085 : int[] = prim::ListConstruct(%1083, %1084, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %x.68 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.67, %1085), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1087 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %query_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.68, %1087), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1089 : int = aten::size(%x.69, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1090 : int = aten::size(%x.69, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1091 : int[] = prim::ListConstruct(%1089, %1090, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %x.70 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.69, %1091), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1093 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %key_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.70, %1093), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1095 : int = aten::size(%x.71, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1096 : int = aten::size(%x.71, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1097 : int[] = prim::ListConstruct(%1095, %1096, %11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %x : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.71, %1097), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1099 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %value_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x, %1099), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1101 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer, %13, %9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.23 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer, %1101), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.23, %8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:195:0
  %input.116 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:198:0
  %input.117 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.116, %13, %20), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.117, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:211:0
  %1108 : int[] = prim::ListConstruct(%29, %23, %28, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %1109 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.23, %1108), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1109, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:213:0
  %1111 : int = aten::size(%context_layer, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:214:0
  %1112 : int = aten::size(%context_layer, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:214:0
  %1113 : int[] = prim::ListConstruct(%1111, %1112, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %input.118 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer, %1113), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:215:0
  %1115 : __torch__.torch.nn.modules.normalization.___torch_mangle_10215.LayerNorm = prim::GetAttr[name="LayerNorm"](%1063)
  %1116 : __torch__.torch.nn.modules.linear.___torch_mangle_10214.Linear = prim::GetAttr[name="dense"](%1063)
  %1117 : Tensor = prim::GetAttr[name="bias"](%1116)
  %1118 : Tensor = prim::GetAttr[name="weight"](%1116)
  %1119 : Float(768:1, 768:768) = aten::t(%1118), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.118, %1119), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.119 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.70, %1117, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.119, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.120 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.23, %input.115, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output # transformers/modeling_roberta.py:232:0
  %1124 : Tensor = prim::GetAttr[name="bias"](%1115)
  %1125 : Tensor = prim::GetAttr[name="weight"](%1115)
  %1126 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.120, %1126, %1125, %1124, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1128 : __torch__.torch.nn.modules.linear.___torch_mangle_10219.Linear = prim::GetAttr[name="dense"](%1061)
  %1129 : Tensor = prim::GetAttr[name="bias"](%1128)
  %1130 : Tensor = prim::GetAttr[name="weight"](%1128)
  %1131 : Float(768:1, 3072:768) = aten::t(%1130), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate/__module.roberta.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor, %1131), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate/__module.roberta.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.121 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.71, %1129, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate/__module.roberta.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.122 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.121), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1135 : __torch__.torch.nn.modules.normalization.___torch_mangle_10222.LayerNorm = prim::GetAttr[name="LayerNorm"](%1060)
  %1136 : __torch__.torch.nn.modules.linear.___torch_mangle_10221.Linear = prim::GetAttr[name="dense"](%1060)
  %1137 : Tensor = prim::GetAttr[name="bias"](%1136)
  %1138 : Tensor = prim::GetAttr[name="weight"](%1136)
  %1139 : Float(3072:1, 768:3072) = aten::t(%1138), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.122, %1139), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.123 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.72, %1137, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.123, %17, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.124 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states, %input_tensor, %28), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output # transformers/modeling_roberta.py:311:0
  %1144 : Tensor = prim::GetAttr[name="bias"](%1135)
  %1145 : Tensor = prim::GetAttr[name="weight"](%1135)
  %1146 : int[] = prim::ListConstruct(%16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.LayerNorm
  %input.125 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.124, %1146, %1145, %1144, %15, %14), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1148 : int = prim::Constant[value=768](), scope: __module.lm_head/__module.lm_head.layer_norm # torch/nn/functional.py:2048:0
  %1149 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.lm_head/__module.lm_head.layer_norm # torch/nn/functional.py:2048:0
  %1150 : bool = prim::Constant[value=1](), scope: __module.lm_head/__module.lm_head.layer_norm # torch/nn/functional.py:2048:0
  %1151 : int = prim::Constant[value=1](), scope: __module.lm_head/__module.lm_head.dense # torch/nn/functional.py:1678:0
  %1152 : Tensor = prim::GetAttr[name="bias"](%3)
  %1153 : __torch__.torch.nn.modules.linear.___torch_mangle_10231.Linear = prim::GetAttr[name="decoder"](%3)
  %1154 : __torch__.torch.nn.modules.normalization.___torch_mangle_10230.LayerNorm = prim::GetAttr[name="layer_norm"](%3)
  %1155 : __torch__.torch.nn.modules.linear.___torch_mangle_10229.Linear = prim::GetAttr[name="dense"](%3)
  %1156 : Tensor = prim::GetAttr[name="bias"](%1155)
  %1157 : Tensor = prim::GetAttr[name="weight"](%1155)
  %1158 : Float(768:1, 768:768) = aten::t(%1157), scope: __module.lm_head/__module.lm_head.dense # torch/nn/functional.py:1676:0
  %output.73 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.125, %1158), scope: __module.lm_head/__module.lm_head.dense # torch/nn/functional.py:1676:0
  %input.126 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.73, %1156, %1151), scope: __module.lm_head/__module.lm_head.dense # torch/nn/functional.py:1678:0
  %input.127 : Float(17:9984, 13:768, 768:1) = aten::gelu(%input.126), scope: __module.lm_head # torch/nn/functional.py:1369:0
  %1162 : Tensor = prim::GetAttr[name="bias"](%1154)
  %1163 : Tensor = prim::GetAttr[name="weight"](%1154)
  %1164 : int[] = prim::ListConstruct(%1148), scope: __module.lm_head/__module.lm_head.layer_norm
  %input : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.127, %1164, %1163, %1162, %1149, %1150), scope: __module.lm_head/__module.lm_head.layer_norm # torch/nn/functional.py:2048:0
  %1166 : Tensor = prim::GetAttr[name="weight"](%1153)
  %1167 : Float(768:1, 30522:768) = aten::t(%1166), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1676:0
  %output : Float(17:396786, 13:30522, 30522:1) = aten::matmul(%input, %1167), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1676:0
  %1169 : Float(17:396786, 13:30522, 30522:1) = aten::add_(%output, %1152, %1151), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1678:0
  %7 : (Float(17:396786, 13:30522, 30522:1)) = prim::TupleConstruct(%1169)
  return (%7)
