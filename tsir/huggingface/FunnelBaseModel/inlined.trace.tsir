graph(%self.1 : __torch__.transformers.modeling_funnel.FunnelBaseModel,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_funnel.FunnelEncoder = prim::GetAttr[name="encoder"](%self.1)
  %4 : __torch__.transformers.modeling_funnel.FunnelEmbeddings = prim::GetAttr[name="embeddings"](%self.1)
  %5 : int = prim::Constant[value=0]() # transformers/modeling_funnel.py:934:0
  %6 : int = aten::size(%input_ids, %5) # transformers/modeling_funnel.py:934:0
  %7 : Long() = prim::NumToTensor(%6)
  %8 : int = aten::Int(%7)
  %9 : int = prim::Constant[value=1]() # transformers/modeling_funnel.py:934:0
  %10 : int = aten::size(%input_ids, %9) # transformers/modeling_funnel.py:934:0
  %11 : Long() = prim::NumToTensor(%10)
  %12 : int = aten::Int(%11)
  %13 : int[] = prim::ListConstruct(%8, %12)
  %14 : int = prim::Constant[value=4]() # transformers/modeling_funnel.py:945:0
  %15 : int = prim::Constant[value=0]() # transformers/modeling_funnel.py:945:0
  %16 : Device = prim::Constant[value="cpu"]() # transformers/modeling_funnel.py:945:0
  %17 : bool = prim::Constant[value=0]() # transformers/modeling_funnel.py:945:0
  %token_type_ids : Long(17:13, 13:1) = aten::zeros(%13, %14, %15, %16, %17) # transformers/modeling_funnel.py:945:0
  %22 : float = prim::Constant[value=0.10000000000000001](), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
  %23 : int = prim::Constant[value=768](), scope: __module.embeddings/__module.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %24 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.embeddings/__module.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %25 : bool = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %26 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %27 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %28 : __torch__.torch.nn.modules.normalization.___torch_mangle_1563.LayerNorm = prim::GetAttr[name="layer_norm"](%4)
  %29 : __torch__.torch.nn.modules.sparse.___torch_mangle_1562.Embedding = prim::GetAttr[name="word_embeddings"](%4)
  %30 : Tensor = prim::GetAttr[name="weight"](%29)
  %input.1 : Float(17:9984, 13:768, 768:1) = aten::embedding(%30, %input_ids, %26, %27, %27), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %32 : Tensor = prim::GetAttr[name="bias"](%28)
  %33 : Tensor = prim::GetAttr[name="weight"](%28)
  %34 : int[] = prim::ListConstruct(%23), scope: __module.embeddings/__module.embeddings.layer_norm
  %input.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.1, %34, %33, %32, %24, %25), scope: __module.embeddings/__module.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.2, %22, %27), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
  %37 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %38 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %39 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %40 : Double() = prim::Constant[value={1e+06}](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %41 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %42 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %43 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %44 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %45 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %46 : Double() = prim::Constant[value={0.125}](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %47 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %48 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %49 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %50 : Double() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %51 : Double() = prim::Constant[value={0.797885}](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %52 : Double() = prim::Constant[value={0.044715}](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %53 : float = prim::Constant[value=3.](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %54 : Double() = prim::Constant[value={0.5}](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %55 : int = prim::Constant[value=-4](), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %56 : Long() = prim::Constant[value={16}](), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %57 : Float(1:1) = prim::Constant[value={-3}](), scope: __module.encoder # transformers/modeling_funnel.py:300:0
  %58 : int = prim::Constant[value=-2](), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %59 : Long() = prim::Constant[value={14}](), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %60 : int = prim::Constant[value=2](), scope: __module.encoder # transformers/modeling_funnel.py:302:0
  %61 : Float(1:1) = prim::Constant[value={-1}](), scope: __module.encoder # transformers/modeling_funnel.py:300:0
  %62 : int = prim::Constant[value=768](), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %63 : int = prim::Constant[value=4](), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %64 : Long() = prim::Constant[value={13}](), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %65 : int = prim::Constant[value=-1](), scope: __module.encoder # transformers/modeling_funnel.py:255:0
  %66 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %67 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder # transformers/modeling_funnel.py:252:0
  %68 : Long() = prim::Constant[value={2}](), scope: __module.encoder # transformers/modeling_funnel.py:250:0
  %69 : Long() = prim::Constant[value={1}](), scope: __module.encoder # torch/tensor.py:400:0
  %70 : None = prim::Constant(), scope: __module.encoder
  %71 : Float() = prim::Constant[value={10000}](), scope: __module.encoder # torch/tensor.py:420:0
  %72 : Long() = prim::Constant[value={384}](), scope: __module.encoder # transformers/modeling_funnel.py:248:0
  %73 : bool = prim::Constant[value=0](), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %74 : Device = prim::Constant[value="cpu"](), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %75 : int = prim::Constant[value=6](), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %76 : float = prim::Constant[value=1.](), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %77 : int = prim::Constant[value=384](), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %78 : int = prim::Constant[value=0](), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %79 : int = prim::Constant[value=1](), scope: __module.encoder # transformers/modeling_funnel.py:195:0
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_1747.ModuleList = prim::GetAttr[name="blocks"](%3)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_1746.ModuleList = prim::GetAttr[name="2"](%80)
  %82 : __torch__.transformers.modeling_funnel.___torch_mangle_1745.FunnelLayer = prim::GetAttr[name="3"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_1747.ModuleList = prim::GetAttr[name="blocks"](%3)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_1746.ModuleList = prim::GetAttr[name="2"](%83)
  %85 : __torch__.transformers.modeling_funnel.___torch_mangle_1730.FunnelLayer = prim::GetAttr[name="2"](%84)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_1747.ModuleList = prim::GetAttr[name="blocks"](%3)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_1746.ModuleList = prim::GetAttr[name="2"](%86)
  %88 : __torch__.transformers.modeling_funnel.___torch_mangle_1715.FunnelLayer = prim::GetAttr[name="1"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_1747.ModuleList = prim::GetAttr[name="blocks"](%3)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_1746.ModuleList = prim::GetAttr[name="2"](%89)
  %91 : __torch__.transformers.modeling_funnel.___torch_mangle_1700.FunnelLayer = prim::GetAttr[name="0"](%90)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_1747.ModuleList = prim::GetAttr[name="blocks"](%3)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_1685.ModuleList = prim::GetAttr[name="1"](%92)
  %94 : __torch__.transformers.modeling_funnel.___torch_mangle_1684.FunnelLayer = prim::GetAttr[name="3"](%93)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_1747.ModuleList = prim::GetAttr[name="blocks"](%3)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_1685.ModuleList = prim::GetAttr[name="1"](%95)
  %97 : __torch__.transformers.modeling_funnel.___torch_mangle_1669.FunnelLayer = prim::GetAttr[name="2"](%96)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_1747.ModuleList = prim::GetAttr[name="blocks"](%3)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_1685.ModuleList = prim::GetAttr[name="1"](%98)
  %100 : __torch__.transformers.modeling_funnel.___torch_mangle_1654.FunnelLayer = prim::GetAttr[name="1"](%99)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_1747.ModuleList = prim::GetAttr[name="blocks"](%3)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_1685.ModuleList = prim::GetAttr[name="1"](%101)
  %103 : __torch__.transformers.modeling_funnel.___torch_mangle_1639.FunnelLayer = prim::GetAttr[name="0"](%102)
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_1747.ModuleList = prim::GetAttr[name="blocks"](%3)
  %105 : __torch__.torch.nn.modules.container.___torch_mangle_1624.ModuleList = prim::GetAttr[name="0"](%104)
  %106 : __torch__.transformers.modeling_funnel.___torch_mangle_1623.FunnelLayer = prim::GetAttr[name="3"](%105)
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_1747.ModuleList = prim::GetAttr[name="blocks"](%3)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_1624.ModuleList = prim::GetAttr[name="0"](%107)
  %109 : __torch__.transformers.modeling_funnel.___torch_mangle_1608.FunnelLayer = prim::GetAttr[name="2"](%108)
  %110 : __torch__.torch.nn.modules.container.___torch_mangle_1747.ModuleList = prim::GetAttr[name="blocks"](%3)
  %111 : __torch__.torch.nn.modules.container.___torch_mangle_1624.ModuleList = prim::GetAttr[name="0"](%110)
  %112 : __torch__.transformers.modeling_funnel.___torch_mangle_1593.FunnelLayer = prim::GetAttr[name="1"](%111)
  %113 : __torch__.torch.nn.modules.container.___torch_mangle_1747.ModuleList = prim::GetAttr[name="blocks"](%3)
  %114 : __torch__.torch.nn.modules.container.___torch_mangle_1624.ModuleList = prim::GetAttr[name="0"](%113)
  %115 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="0"](%114)
  %attention_mask.2 : Float(17:13, 13:1) = aten::type_as(%attention_mask.1, %inputs_embeds), scope: __module.encoder # transformers/modeling_funnel.py:625:0
  %117 : int = aten::size(%inputs_embeds, %79), scope: __module.encoder # transformers/modeling_funnel.py:195:0
  %seq_len.1 : Long() = prim::NumToTensor(%117), scope: __module.encoder
  %freq_seq : Float(384:1) = aten::arange(%78, %77, %76, %75, %78, %74, %73), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %120 : Float(384:1) = aten::div(%freq_seq, %72), scope: __module.encoder # transformers/modeling_funnel.py:248:0
  %121 : Float() = aten::to(%71, %74, %75, %73, %73, %70), scope: __module.encoder # torch/tensor.py:420:0
  %122 : Float() = aten::detach(%121), scope: __module.encoder # torch/tensor.py:420:0
  %123 : Float(384:1) = aten::pow(%122, %120), scope: __module.encoder # torch/tensor.py:420:0
  %124 : Float(384:1) = aten::reciprocal(%123), scope: __module.encoder # torch/tensor.py:400:0
  %inv_freq : Float(384:1) = aten::mul(%124, %69), scope: __module.encoder # torch/tensor.py:400:0
  %126 : Long() = aten::neg(%seq_len.1), scope: __module.encoder # transformers/modeling_funnel.py:250:0
  %127 : Long() = aten::mul(%126, %68), scope: __module.encoder # transformers/modeling_funnel.py:250:0
  %128 : Scalar = aten::ScalarImplicit(%127), scope: __module.encoder
  %129 : Long() = aten::mul(%seq_len.1, %68), scope: __module.encoder # transformers/modeling_funnel.py:250:0
  %130 : Scalar = aten::ScalarImplicit(%129), scope: __module.encoder
  %rel_pos_id : Float(52:1) = aten::arange(%128, %130, %76, %75, %78, %74, %73), scope: __module.encoder # transformers/modeling_funnel.py:250:0
  %zero_offset : Long() = aten::mul(%seq_len.1, %68), scope: __module.encoder # transformers/modeling_funnel.py:251:0
  %133 : Float(52:1) = aten::slice(%rel_pos_id, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:252:0
  %134 : Float(52:1, 1:1) = aten::unsqueeze(%133, %79), scope: __module.encoder # transformers/modeling_funnel.py:252:0
  %135 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq, %78), scope: __module.encoder # transformers/modeling_funnel.py:252:0
  %sinusoid : Float(52:384, 384:1) = aten::mul(%134, %135), scope: __module.encoder # transformers/modeling_funnel.py:252:0
  %input.3 : Float(52:384, 384:1) = aten::sin(%sinusoid), scope: __module.encoder # transformers/modeling_funnel.py:253:0
  %sin_embed : Float(52:384, 384:1) = aten::dropout(%input.3, %66, %73), scope: __module.encoder/__module.encoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %input.4 : Float(52:384, 384:1) = aten::cos(%sinusoid), scope: __module.encoder # transformers/modeling_funnel.py:254:0
  %cos_embed : Float(52:384, 384:1) = aten::dropout(%input.4, %66, %73), scope: __module.encoder/__module.encoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
  %141 : Tensor[] = prim::ListConstruct(%sin_embed, %cos_embed), scope: __module.encoder
  %pos_embed : Float(52:768, 768:1) = aten::cat(%141, %65), scope: __module.encoder # transformers/modeling_funnel.py:255:0
  %pos : Float(13:1) = aten::arange(%78, %117, %79, %75, %78, %74, %73), scope: __module.encoder # transformers/modeling_funnel.py:257:0
  %144 : Float() = aten::select(%pos, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %145 : Float() = aten::select(%pos, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.1 : Float() = aten::sub(%144, %145, %79), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.1 : Float() = aten::add(%ref_point.1, %64, %79), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %148 : Scalar = aten::ScalarImplicit(%max_dist.1), scope: __module.encoder
  %149 : Float() = aten::select(%pos, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %150 : Float() = aten::select(%pos, %78, %65), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.1 : Float() = aten::sub(%149, %150, %79), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %152 : Float() = aten::sub(%min_dist.1, %69, %79), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %153 : Scalar = aten::ScalarImplicit(%152), scope: __module.encoder
  %rel_pos.1 : Long(26:1) = aten::arange(%148, %153, %65, %63, %78, %74, %73), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %155 : Long(26:1) = aten::slice(%rel_pos.1, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %156 : Long(26:1, 1:1) = aten::unsqueeze(%155, %79), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.2 : Long(26:1, 1:1) = aten::add(%156, %zero_offset, %79), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %158 : int = aten::size(%rel_pos.2, %78), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %159 : int[] = prim::ListConstruct(%158, %62), scope: __module.encoder
  %rel_pos.3 : Long(26:1, 768:0) = aten::expand(%rel_pos.2, %159, %73), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %161 : Float(26:768, 768:1) = aten::gather(%pos_embed, %78, %rel_pos.3, %73), scope: __module.encoder # transformers/modeling_funnel.py:286:0
  %162 : Float(1:1) = aten::to(%61, %74, %75, %73, %73, %70), scope: __module.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos.1 : Float(1:1) = aten::detach(%162), scope: __module.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id.1 : Float(11:1) = aten::slice(%pos, %78, %79, %65, %79), scope: __module.encoder # transformers/modeling_funnel.py:301:0
  %165 : Float(6:2) = aten::slice(%pooled_pos_id.1, %78, %78, %67, %60), scope: __module.encoder # transformers/modeling_funnel.py:302:0
  %166 : Tensor[] = prim::ListConstruct(%cls_pos.1, %165), scope: __module.encoder
  %pooled_pos.1 : Float(7:1) = aten::cat(%166, %78), scope: __module.encoder # transformers/modeling_funnel.py:302:0
  %168 : Float() = aten::select(%pooled_pos.1, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %169 : Float() = aten::select(%pos, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.2 : Float() = aten::sub(%168, %169, %79), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.2 : Float() = aten::add(%ref_point.2, %59, %79), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %172 : Scalar = aten::ScalarImplicit(%max_dist.2), scope: __module.encoder
  %173 : Float() = aten::select(%pooled_pos.1, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %174 : Float() = aten::select(%pos, %78, %65), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.2 : Float() = aten::sub(%173, %174, %79), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %176 : Float() = aten::sub(%min_dist.2, %69, %79), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %177 : Scalar = aten::ScalarImplicit(%176), scope: __module.encoder
  %rel_pos.4 : Long(27:1) = aten::arange(%172, %177, %65, %63, %78, %74, %73), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %179 : Long(27:1) = aten::slice(%rel_pos.4, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:275:0
  %180 : Long(27:1, 1:1) = aten::unsqueeze(%179, %79), scope: __module.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.5 : Long(27:1, 1:1) = aten::add(%180, %zero_offset, %79), scope: __module.encoder # transformers/modeling_funnel.py:275:0
  %182 : int = aten::size(%rel_pos.5, %78), scope: __module.encoder # transformers/modeling_funnel.py:276:0
  %183 : int[] = prim::ListConstruct(%182, %62), scope: __module.encoder
  %rel_pos.6 : Long(27:1, 768:0) = aten::expand(%rel_pos.5, %183, %73), scope: __module.encoder # transformers/modeling_funnel.py:276:0
  %185 : Float(27:768, 768:1) = aten::gather(%pos_embed, %78, %rel_pos.6, %73), scope: __module.encoder # transformers/modeling_funnel.py:277:0
  %186 : Float() = aten::select(%pooled_pos.1, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %187 : Float() = aten::select(%pooled_pos.1, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.3 : Float() = aten::sub(%186, %187, %79), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.3 : Float() = aten::add(%ref_point.3, %59, %79), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %190 : Scalar = aten::ScalarImplicit(%max_dist.3), scope: __module.encoder
  %191 : Float() = aten::select(%pooled_pos.1, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %192 : Float() = aten::select(%pooled_pos.1, %78, %65), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.3 : Float() = aten::sub(%191, %192, %79), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %194 : Float() = aten::sub(%min_dist.3, %69, %79), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %195 : Scalar = aten::ScalarImplicit(%194), scope: __module.encoder
  %rel_pos.7 : Long(14:1) = aten::arange(%190, %195, %58, %63, %78, %74, %73), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %197 : Long(14:1) = aten::slice(%rel_pos.7, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %198 : Long(14:1, 1:1) = aten::unsqueeze(%197, %79), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.8 : Long(14:1, 1:1) = aten::add(%198, %zero_offset, %79), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %200 : int = aten::size(%rel_pos.8, %78), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %201 : int[] = prim::ListConstruct(%200, %62), scope: __module.encoder
  %rel_pos.9 : Long(14:1, 768:0) = aten::expand(%rel_pos.8, %201, %73), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %203 : Float(14:768, 768:1) = aten::gather(%pos_embed, %78, %rel_pos.9, %73), scope: __module.encoder # transformers/modeling_funnel.py:286:0
  %204 : Float(1:1) = aten::to(%57, %74, %75, %73, %73, %70), scope: __module.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos : Float(1:1) = aten::detach(%204), scope: __module.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id : Float(5:1) = aten::slice(%pooled_pos.1, %78, %79, %65, %79), scope: __module.encoder # transformers/modeling_funnel.py:301:0
  %207 : Float(3:2) = aten::slice(%pooled_pos_id, %78, %78, %67, %60), scope: __module.encoder # transformers/modeling_funnel.py:302:0
  %208 : Tensor[] = prim::ListConstruct(%cls_pos, %207), scope: __module.encoder
  %pooled_pos : Float(4:1) = aten::cat(%208, %78), scope: __module.encoder # transformers/modeling_funnel.py:302:0
  %210 : Float() = aten::select(%pooled_pos, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %211 : Float() = aten::select(%pooled_pos.1, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.4 : Float() = aten::sub(%210, %211, %79), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.4 : Float() = aten::add(%ref_point.4, %56, %79), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %214 : Scalar = aten::ScalarImplicit(%max_dist.4), scope: __module.encoder
  %215 : Float() = aten::select(%pooled_pos, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %216 : Float() = aten::select(%pooled_pos.1, %78, %65), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.4 : Float() = aten::sub(%215, %216, %79), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %218 : Float() = aten::sub(%min_dist.4, %69, %79), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %219 : Scalar = aten::ScalarImplicit(%218), scope: __module.encoder
  %rel_pos.10 : Long(15:1) = aten::arange(%214, %219, %58, %63, %78, %74, %73), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %221 : Long(15:1) = aten::slice(%rel_pos.10, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:275:0
  %222 : Long(15:1, 1:1) = aten::unsqueeze(%221, %79), scope: __module.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.11 : Long(15:1, 1:1) = aten::add(%222, %zero_offset, %79), scope: __module.encoder # transformers/modeling_funnel.py:275:0
  %224 : int = aten::size(%rel_pos.11, %78), scope: __module.encoder # transformers/modeling_funnel.py:276:0
  %225 : int[] = prim::ListConstruct(%224, %62), scope: __module.encoder
  %rel_pos.12 : Long(15:1, 768:0) = aten::expand(%rel_pos.11, %225, %73), scope: __module.encoder # transformers/modeling_funnel.py:276:0
  %227 : Float(15:768, 768:1) = aten::gather(%pos_embed, %78, %rel_pos.12, %73), scope: __module.encoder # transformers/modeling_funnel.py:277:0
  %228 : Float() = aten::select(%pooled_pos, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %229 : Float() = aten::select(%pooled_pos, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %ref_point : Float() = aten::sub(%228, %229, %79), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %max_dist : Float() = aten::add(%ref_point, %56, %79), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %232 : Scalar = aten::ScalarImplicit(%max_dist), scope: __module.encoder
  %233 : Float() = aten::select(%pooled_pos, %78, %78), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %234 : Float() = aten::select(%pooled_pos, %78, %65), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %min_dist : Float() = aten::sub(%233, %234, %79), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %236 : Float() = aten::sub(%min_dist, %69, %79), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %237 : Scalar = aten::ScalarImplicit(%236), scope: __module.encoder
  %rel_pos.13 : Long(8:1) = aten::arange(%232, %237, %55, %63, %78, %74, %73), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %239 : Long(8:1) = aten::slice(%rel_pos.13, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %240 : Long(8:1, 1:1) = aten::unsqueeze(%239, %79), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.14 : Long(8:1, 1:1) = aten::add(%240, %zero_offset, %79), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %242 : int = aten::size(%rel_pos.14, %78), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %243 : int[] = prim::ListConstruct(%242, %62), scope: __module.encoder
  %rel_pos : Long(8:1, 768:0) = aten::expand(%rel_pos.14, %243, %73), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %245 : Float(8:768, 768:1) = aten::gather(%pos_embed, %78, %rel_pos, %73), scope: __module.encoder # transformers/modeling_funnel.py:286:0
  %246 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:207:0
  %247 : Long(17:13, 13:1) = aten::slice(%246, %79, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:207:0
  %248 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%247, %60), scope: __module.encoder # transformers/modeling_funnel.py:207:0
  %249 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:207:0
  %250 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%249, %79), scope: __module.encoder # transformers/modeling_funnel.py:207:0
  %token_type_mat.1 : Bool(17:169, 13:13, 13:1) = aten::eq(%248, %250), scope: __module.encoder # torch/tensor.py:22:0
  %cls_ids : Bool(17:13, 13:1) = aten::eq(%token_type_ids, %60), scope: __module.encoder # torch/tensor.py:22:0
  %253 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:210:0
  %254 : Bool(17:13, 13:1) = aten::slice(%253, %79, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:210:0
  %255 : Bool(17:13, 13:1, 1:1) = aten::unsqueeze(%254, %60), scope: __module.encoder # transformers/modeling_funnel.py:210:0
  %256 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:210:0
  %257 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%256, %79), scope: __module.encoder # transformers/modeling_funnel.py:210:0
  %cls_mat : Bool(17:169, 13:13, 13:1) = aten::__or__(%255, %257), scope: __module.encoder # transformers/modeling_funnel.py:210:0
  %token_type_mat.2 : Bool(17:169, 13:13, 13:1) = aten::__or__(%cls_mat, %token_type_mat.1), scope: __module.encoder # transformers/modeling_funnel.py:211:0
  %260 : Long() = aten::sub(%seq_len.1, %69, %79), scope: __module.encoder # transformers/modeling_funnel.py:199:0
  %261 : int = aten::Int(%260), scope: __module.encoder
  %262 : Long() = aten::sub(%seq_len.1, %69, %79), scope: __module.encoder # transformers/modeling_funnel.py:199:0
  %263 : int = aten::Int(%262), scope: __module.encoder
  %264 : int[] = prim::ListConstruct(%261, %263), scope: __module.encoder
  %input.5 : Float(12:12, 12:1) = aten::ones(%264, %75, %78, %74, %73), scope: __module.encoder # transformers/modeling_funnel.py:199:0
  %266 : int[] = prim::ListConstruct(%79, %78, %79, %78), scope: __module.encoder
  %cls_mask.1 : Float(13:13, 13:1) = aten::constant_pad_nd(%input.5, %266, %78), scope: __module.encoder # torch/nn/functional.py:3552:0
  %268 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%115)
  %269 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%115)
  %270 : __torch__.torch.nn.modules.normalization.___torch_mangle_1573.LayerNorm = prim::GetAttr[name="layer_norm"](%269)
  %271 : __torch__.torch.nn.modules.linear.___torch_mangle_1572.Linear = prim::GetAttr[name="post_proj"](%269)
  %272 : Tensor = prim::GetAttr[name="seg_embed"](%269)
  %273 : Tensor = prim::GetAttr[name="r_s_bias"](%269)
  %274 : Tensor = prim::GetAttr[name="r_kernel"](%269)
  %275 : Tensor = prim::GetAttr[name="r_r_bias"](%269)
  %276 : Tensor = prim::GetAttr[name="r_w_bias"](%269)
  %277 : __torch__.torch.nn.modules.linear.___torch_mangle_1571.Linear = prim::GetAttr[name="v_head"](%269)
  %278 : __torch__.torch.nn.modules.linear.___torch_mangle_1570.Linear = prim::GetAttr[name="k_head"](%269)
  %279 : __torch__.torch.nn.modules.linear.___torch_mangle_1569.Linear = prim::GetAttr[name="q_head"](%269)
  %280 : int = aten::size(%inputs_embeds, %78), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %281 : int = aten::size(%inputs_embeds, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %282 : int = aten::size(%inputs_embeds, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:531:0
  %283 : Tensor = prim::GetAttr[name="weight"](%279)
  %284 : Float(768:1, 768:768) = aten::t(%283), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %285 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %284), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %286 : int[] = prim::ListConstruct(%280, %281, %48, %47), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %q_head.1 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%285, %286), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %288 : Tensor = prim::GetAttr[name="bias"](%278)
  %289 : Tensor = prim::GetAttr[name="weight"](%278)
  %290 : Float(768:1, 768:768) = aten::t(%289), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %290), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %292 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %288, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1678:0
  %293 : int[] = prim::ListConstruct(%280, %282, %48, %47), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %294 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%292, %293), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:537:0
  %295 : Tensor = prim::GetAttr[name="bias"](%277)
  %296 : Tensor = prim::GetAttr[name="weight"](%277)
  %297 : Float(768:1, 768:768) = aten::t(%296), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %297), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %299 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %295, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1678:0
  %300 : int[] = prim::ListConstruct(%280, %282, %48, %47), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %301 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%299, %300), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.1, %46), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.1 : Float(12:64, 64:1) = aten::mul(%276, %46), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:542:0
  %304 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_w_bias.1, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:544:0
  %305 : Tensor[] = prim::ListConstruct(%304, %294), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %content_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%45, %305), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %v.1 : Float(12:64, 64:1) = aten::mul(%275, %46), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:486:0
  %308 : Tensor[] = prim::ListConstruct(%161, %274), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %309 : Float(26:768, 12:64, 64:1) = aten::einsum(%44, %308), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %310 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %v.1, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:493:0
  %311 : Tensor[] = prim::ListConstruct(%310, %309), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %positional_attn.1 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%43, %311), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %313 : int = aten::size(%positional_attn.1, %78), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %314 : int = aten::size(%positional_attn.1, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %315 : int = aten::size(%positional_attn.1, %60), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %316 : int = aten::size(%positional_attn.1, %42), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.1 : Long() = prim::NumToTensor(%316), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %318 : int[] = prim::ListConstruct(%313, %314, %316, %315), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %positional_attn.2 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.1, %318), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:428:0
  %320 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.2, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %321 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%320, %79, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %322 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%321, %60, %79, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.3 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%322, %42, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %324 : Long() = aten::sub(%max_rel_len.1, %69, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %325 : int = aten::Int(%324), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %326 : int[] = prim::ListConstruct(%313, %314, %315, %325), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %positional_attn.4 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.3, %326), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.5 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.4, %42, %78, %282, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.6 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.5, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:498:0
  %330 : int = aten::size(%token_type_mat.2, %78), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %331 : int = aten::size(%token_type_mat.2, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %332 : int = aten::size(%token_type_mat.2, %60), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.1 : Float(12:64, 64:1) = aten::mul(%273, %46), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:508:0
  %334 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_s_bias.1, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:511:0
  %335 : Tensor[] = prim::ListConstruct(%334, %272), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %336 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%41, %335), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %337 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %338 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%337, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %339 : int = aten::size(%q_head.2, %60), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %340 : int[] = prim::ListConstruct(%330, %339, %331, %332), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %token_type_mat.3 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%338, %340, %73), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %342 : Tensor[] = aten::split(%336, %79, %65), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/tensor.py:371:0
  %diff_token_type.1 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.1 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%342), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %345 : int = aten::size(%token_type_mat.3, %78), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %346 : int = aten::size(%token_type_mat.3, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %347 : int = aten::size(%token_type_mat.3, %60), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %348 : int = aten::size(%token_type_mat.3, %42), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %349 : int[] = prim::ListConstruct(%345, %346, %347, %348), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %350 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.1, %349, %73), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %351 : int = aten::size(%token_type_mat.3, %78), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %352 : int = aten::size(%token_type_mat.3, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %353 : int = aten::size(%token_type_mat.3, %60), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %354 : int = aten::size(%token_type_mat.3, %42), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %355 : int[] = prim::ListConstruct(%351, %352, %353, %354), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %356 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.1, %355, %73), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.3, %350, %356), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.1, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:522:0
  %359 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.1, %positional_attn.6, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%359, %token_type_attn.2, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.1, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:553:0
  %362 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %363 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%362, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %364 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%363, %60), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %365 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%364, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %366 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%365, %79, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/tensor.py:396:0
  %367 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%366, %40), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.2, %367, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.3, %65, %75), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:558:0
  %370 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.6, %66, %73), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %371 : Tensor[] = prim::ListConstruct(%370, %301), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %attn_vec.1 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%39, %371), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %373 : int[] = prim::ListConstruct(%280, %281, %62), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %input.7 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.1, %373), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:565:0
  %375 : Tensor = prim::GetAttr[name="bias"](%271)
  %376 : Tensor = prim::GetAttr[name="weight"](%271)
  %377 : Float(768:1, 768:768) = aten::t(%376), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.7, %377), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %375, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.8, %66, %73), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %attn_out.1, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:568:0
  %382 : Tensor = prim::GetAttr[name="bias"](%270)
  %383 : Tensor = prim::GetAttr[name="weight"](%270)
  %384 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.layer_norm
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.9, %384, %383, %382, %37, %38), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %386 : __torch__.torch.nn.modules.normalization.___torch_mangle_1578.LayerNorm = prim::GetAttr[name="layer_norm"](%268)
  %387 : __torch__.torch.nn.modules.linear.___torch_mangle_1576.Linear = prim::GetAttr[name="linear_2"](%268)
  %388 : __torch__.torch.nn.modules.linear.___torch_mangle_1574.Linear = prim::GetAttr[name="linear_1"](%268)
  %389 : Tensor = prim::GetAttr[name="bias"](%388)
  %390 : Tensor = prim::GetAttr[name="weight"](%388)
  %391 : Float(768:1, 3072:768) = aten::t(%390), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.10, %391), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.1 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.4, %389, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %394 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.1, %54), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %395 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.1, %53), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %396 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%395, %52), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %397 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.1, %396, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %398 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%397, %51), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %399 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%398), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %400 : Float(17:39936, 13:3072, 3072:1) = aten::add(%399, %50, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%394, %400), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.11, %49, %73), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %403 : Tensor = prim::GetAttr[name="bias"](%387)
  %404 : Tensor = prim::GetAttr[name="weight"](%387)
  %405 : Float(3072:1, 768:3072) = aten::t(%404), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %405), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.5, %403, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %66, %73), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%input.10, %h.1, %79), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/modeling_funnel.py:588:0
  %410 : Tensor = prim::GetAttr[name="bias"](%386)
  %411 : Tensor = prim::GetAttr[name="weight"](%386)
  %412 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.layer_norm
  %query.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %412, %411, %410, %37, %38), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %414 : __torch__.transformers.modeling_funnel.___torch_mangle_1592.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%112)
  %415 : __torch__.transformers.modeling_funnel.___torch_mangle_1586.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%112)
  %416 : __torch__.torch.nn.modules.normalization.___torch_mangle_1585.LayerNorm = prim::GetAttr[name="layer_norm"](%415)
  %417 : __torch__.torch.nn.modules.linear.___torch_mangle_1584.Linear = prim::GetAttr[name="post_proj"](%415)
  %418 : Tensor = prim::GetAttr[name="seg_embed"](%415)
  %419 : Tensor = prim::GetAttr[name="r_s_bias"](%415)
  %420 : Tensor = prim::GetAttr[name="r_kernel"](%415)
  %421 : Tensor = prim::GetAttr[name="r_r_bias"](%415)
  %422 : Tensor = prim::GetAttr[name="r_w_bias"](%415)
  %423 : __torch__.torch.nn.modules.linear.___torch_mangle_1583.Linear = prim::GetAttr[name="v_head"](%415)
  %424 : __torch__.torch.nn.modules.linear.___torch_mangle_1582.Linear = prim::GetAttr[name="k_head"](%415)
  %425 : __torch__.torch.nn.modules.linear.___torch_mangle_1581.Linear = prim::GetAttr[name="q_head"](%415)
  %426 : int = aten::size(%query.1, %78), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %427 : int = aten::size(%query.1, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %428 : int = aten::size(%query.1, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:531:0
  %429 : Tensor = prim::GetAttr[name="weight"](%425)
  %430 : Float(768:1, 768:768) = aten::t(%429), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %431 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %430), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %432 : int[] = prim::ListConstruct(%426, %427, %48, %47), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %q_head.3 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%431, %432), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:535:0
  %434 : Tensor = prim::GetAttr[name="bias"](%424)
  %435 : Tensor = prim::GetAttr[name="weight"](%424)
  %436 : Float(768:1, 768:768) = aten::t(%435), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %436), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %438 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %434, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1678:0
  %439 : int[] = prim::ListConstruct(%426, %428, %48, %47), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %440 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%438, %439), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:537:0
  %441 : Tensor = prim::GetAttr[name="bias"](%423)
  %442 : Tensor = prim::GetAttr[name="weight"](%423)
  %443 : Float(768:1, 768:768) = aten::t(%442), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %443), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %445 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %441, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1678:0
  %446 : int[] = prim::ListConstruct(%426, %428, %48, %47), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %447 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%445, %446), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.3, %46), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.2 : Float(12:64, 64:1) = aten::mul(%422, %46), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:542:0
  %450 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_w_bias.2, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:544:0
  %451 : Tensor[] = prim::ListConstruct(%450, %440), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %content_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%45, %451), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %v.2 : Float(12:64, 64:1) = aten::mul(%421, %46), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:486:0
  %454 : Tensor[] = prim::ListConstruct(%161, %420), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %455 : Float(26:768, 12:64, 64:1) = aten::einsum(%44, %454), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %456 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %v.2, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:493:0
  %457 : Tensor[] = prim::ListConstruct(%456, %455), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %positional_attn.7 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%43, %457), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %459 : int = aten::size(%positional_attn.7, %78), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %460 : int = aten::size(%positional_attn.7, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %461 : int = aten::size(%positional_attn.7, %60), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %462 : int = aten::size(%positional_attn.7, %42), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.2 : Long() = prim::NumToTensor(%462), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %464 : int[] = prim::ListConstruct(%459, %460, %462, %461), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %positional_attn.8 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.7, %464), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:428:0
  %466 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.8, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %467 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%466, %79, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %468 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%467, %60, %79, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.9 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%468, %42, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %470 : Long() = aten::sub(%max_rel_len.2, %69, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %471 : int = aten::Int(%470), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %472 : int[] = prim::ListConstruct(%459, %460, %461, %471), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %positional_attn.10 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.9, %472), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.11 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.10, %42, %78, %428, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.12 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.11, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:498:0
  %476 : int = aten::size(%token_type_mat.2, %78), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %477 : int = aten::size(%token_type_mat.2, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %478 : int = aten::size(%token_type_mat.2, %60), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.2 : Float(12:64, 64:1) = aten::mul(%419, %46), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:508:0
  %480 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_s_bias.2, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:511:0
  %481 : Tensor[] = prim::ListConstruct(%480, %418), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %482 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%41, %481), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %483 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %484 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%483, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %485 : int = aten::size(%q_head.4, %60), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %486 : int[] = prim::ListConstruct(%476, %485, %477, %478), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %token_type_mat.4 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%484, %486, %73), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %488 : Tensor[] = aten::split(%482, %79, %65), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/tensor.py:371:0
  %diff_token_type.2 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.2 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%488), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %491 : int = aten::size(%token_type_mat.4, %78), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %492 : int = aten::size(%token_type_mat.4, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %493 : int = aten::size(%token_type_mat.4, %60), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %494 : int = aten::size(%token_type_mat.4, %42), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %495 : int[] = prim::ListConstruct(%491, %492, %493, %494), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %496 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.2, %495, %73), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %497 : int = aten::size(%token_type_mat.4, %78), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %498 : int = aten::size(%token_type_mat.4, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %499 : int = aten::size(%token_type_mat.4, %60), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %500 : int = aten::size(%token_type_mat.4, %42), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %501 : int[] = prim::ListConstruct(%497, %498, %499, %500), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %502 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.2, %501, %73), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.4, %496, %502), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.3, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:522:0
  %505 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.2, %positional_attn.12, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%505, %token_type_attn.4, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.4, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:553:0
  %508 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %509 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%508, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %510 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%509, %60), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %511 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%510, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %512 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%511, %79, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/tensor.py:396:0
  %513 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%512, %40), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.5, %513, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %input.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.6, %65, %75), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:558:0
  %516 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.15, %66, %73), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %517 : Tensor[] = prim::ListConstruct(%516, %447), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %attn_vec.2 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%39, %517), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %519 : int[] = prim::ListConstruct(%426, %427, %62), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %input.16 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.2, %519), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:565:0
  %521 : Tensor = prim::GetAttr[name="bias"](%417)
  %522 : Tensor = prim::GetAttr[name="weight"](%417)
  %523 : Float(768:1, 768:768) = aten::t(%522), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.16, %523), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %521, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.17, %66, %73), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::add(%query.1, %attn_out.2, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:568:0
  %528 : Tensor = prim::GetAttr[name="bias"](%416)
  %529 : Tensor = prim::GetAttr[name="weight"](%416)
  %530 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.layer_norm
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.18, %530, %529, %528, %37, %38), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %532 : __torch__.torch.nn.modules.normalization.___torch_mangle_1591.LayerNorm = prim::GetAttr[name="layer_norm"](%414)
  %533 : __torch__.torch.nn.modules.linear.___torch_mangle_1589.Linear = prim::GetAttr[name="linear_2"](%414)
  %534 : __torch__.torch.nn.modules.linear.___torch_mangle_1587.Linear = prim::GetAttr[name="linear_1"](%414)
  %535 : Tensor = prim::GetAttr[name="bias"](%534)
  %536 : Tensor = prim::GetAttr[name="weight"](%534)
  %537 : Float(768:1, 3072:768) = aten::t(%536), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.9 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.19, %537), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.2 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.9, %535, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %540 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.2, %54), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %541 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.2, %53), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %542 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%541, %52), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %543 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.2, %542, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %544 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%543, %51), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %545 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%544), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %546 : Float(17:39936, 13:3072, 3072:1) = aten::add(%545, %50, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.20 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%540, %546), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.20, %49, %73), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %549 : Tensor = prim::GetAttr[name="bias"](%533)
  %550 : Tensor = prim::GetAttr[name="weight"](%533)
  %551 : Float(3072:1, 768:3072) = aten::t(%550), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.21, %551), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %549, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.22, %66, %73), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add(%input.19, %h.2, %79), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/modeling_funnel.py:588:0
  %556 : Tensor = prim::GetAttr[name="bias"](%532)
  %557 : Tensor = prim::GetAttr[name="weight"](%532)
  %558 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.layer_norm
  %query.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.23, %558, %557, %556, %37, %38), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %560 : __torch__.transformers.modeling_funnel.___torch_mangle_1607.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%109)
  %561 : __torch__.transformers.modeling_funnel.___torch_mangle_1601.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%109)
  %562 : __torch__.torch.nn.modules.normalization.___torch_mangle_1600.LayerNorm = prim::GetAttr[name="layer_norm"](%561)
  %563 : __torch__.torch.nn.modules.linear.___torch_mangle_1599.Linear = prim::GetAttr[name="post_proj"](%561)
  %564 : Tensor = prim::GetAttr[name="seg_embed"](%561)
  %565 : Tensor = prim::GetAttr[name="r_s_bias"](%561)
  %566 : Tensor = prim::GetAttr[name="r_kernel"](%561)
  %567 : Tensor = prim::GetAttr[name="r_r_bias"](%561)
  %568 : Tensor = prim::GetAttr[name="r_w_bias"](%561)
  %569 : __torch__.torch.nn.modules.linear.___torch_mangle_1598.Linear = prim::GetAttr[name="v_head"](%561)
  %570 : __torch__.torch.nn.modules.linear.___torch_mangle_1597.Linear = prim::GetAttr[name="k_head"](%561)
  %571 : __torch__.torch.nn.modules.linear.___torch_mangle_1596.Linear = prim::GetAttr[name="q_head"](%561)
  %572 : int = aten::size(%query.2, %78), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %573 : int = aten::size(%query.2, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %574 : int = aten::size(%query.2, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:531:0
  %575 : Tensor = prim::GetAttr[name="weight"](%571)
  %576 : Float(768:1, 768:768) = aten::t(%575), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %577 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %576), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %578 : int[] = prim::ListConstruct(%572, %573, %48, %47), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %q_head.5 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%577, %578), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:535:0
  %580 : Tensor = prim::GetAttr[name="bias"](%570)
  %581 : Tensor = prim::GetAttr[name="weight"](%570)
  %582 : Float(768:1, 768:768) = aten::t(%581), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.11 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %582), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %584 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.11, %580, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1678:0
  %585 : int[] = prim::ListConstruct(%572, %574, %48, %47), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %586 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%584, %585), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:537:0
  %587 : Tensor = prim::GetAttr[name="bias"](%569)
  %588 : Tensor = prim::GetAttr[name="weight"](%569)
  %589 : Float(768:1, 768:768) = aten::t(%588), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %589), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %591 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %587, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1678:0
  %592 : int[] = prim::ListConstruct(%572, %574, %48, %47), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %593 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%591, %592), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.5, %46), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.3 : Float(12:64, 64:1) = aten::mul(%568, %46), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:542:0
  %596 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_w_bias.3, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:544:0
  %597 : Tensor[] = prim::ListConstruct(%596, %586), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %content_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%45, %597), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %v.3 : Float(12:64, 64:1) = aten::mul(%567, %46), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:486:0
  %600 : Tensor[] = prim::ListConstruct(%161, %566), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %601 : Float(26:768, 12:64, 64:1) = aten::einsum(%44, %600), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %602 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %v.3, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:493:0
  %603 : Tensor[] = prim::ListConstruct(%602, %601), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %positional_attn.13 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%43, %603), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %605 : int = aten::size(%positional_attn.13, %78), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %606 : int = aten::size(%positional_attn.13, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %607 : int = aten::size(%positional_attn.13, %60), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %608 : int = aten::size(%positional_attn.13, %42), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.3 : Long() = prim::NumToTensor(%608), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %610 : int[] = prim::ListConstruct(%605, %606, %608, %607), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %positional_attn.14 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.13, %610), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:428:0
  %612 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.14, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %613 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%612, %79, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %614 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%613, %60, %79, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.15 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%614, %42, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %616 : Long() = aten::sub(%max_rel_len.3, %69, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %617 : int = aten::Int(%616), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %618 : int[] = prim::ListConstruct(%605, %606, %607, %617), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %positional_attn.16 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.15, %618), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.17 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.16, %42, %78, %574, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.18 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.17, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:498:0
  %622 : int = aten::size(%token_type_mat.2, %78), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %623 : int = aten::size(%token_type_mat.2, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %624 : int = aten::size(%token_type_mat.2, %60), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.3 : Float(12:64, 64:1) = aten::mul(%565, %46), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:508:0
  %626 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_s_bias.3, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:511:0
  %627 : Tensor[] = prim::ListConstruct(%626, %564), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %628 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%41, %627), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %629 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %630 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%629, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %631 : int = aten::size(%q_head.6, %60), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %632 : int[] = prim::ListConstruct(%622, %631, %623, %624), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %token_type_mat.5 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%630, %632, %73), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %634 : Tensor[] = aten::split(%628, %79, %65), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/tensor.py:371:0
  %diff_token_type.3 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.3 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%634), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %637 : int = aten::size(%token_type_mat.5, %78), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %638 : int = aten::size(%token_type_mat.5, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %639 : int = aten::size(%token_type_mat.5, %60), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %640 : int = aten::size(%token_type_mat.5, %42), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %641 : int[] = prim::ListConstruct(%637, %638, %639, %640), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %642 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.3, %641, %73), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %643 : int = aten::size(%token_type_mat.5, %78), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %644 : int = aten::size(%token_type_mat.5, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %645 : int = aten::size(%token_type_mat.5, %60), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %646 : int = aten::size(%token_type_mat.5, %42), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %647 : int[] = prim::ListConstruct(%643, %644, %645, %646), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %648 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.3, %647, %73), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.5, %642, %648), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.5, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:522:0
  %651 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.3, %positional_attn.18, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%651, %token_type_attn.6, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.7, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:553:0
  %654 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %655 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%654, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %656 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%655, %60), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %657 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%656, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %658 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%657, %79, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/tensor.py:396:0
  %659 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%658, %40), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.8, %659, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %input.24 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.9, %65, %75), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:558:0
  %662 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.24, %66, %73), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %663 : Tensor[] = prim::ListConstruct(%662, %593), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %attn_vec.3 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%39, %663), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %665 : int[] = prim::ListConstruct(%572, %573, %62), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.3, %665), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:565:0
  %667 : Tensor = prim::GetAttr[name="bias"](%563)
  %668 : Tensor = prim::GetAttr[name="weight"](%563)
  %669 : Float(768:1, 768:768) = aten::t(%668), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %669), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.26 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %667, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.26, %66, %73), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.27 : Float(17:9984, 13:768, 768:1) = aten::add(%query.2, %attn_out.3, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:568:0
  %674 : Tensor = prim::GetAttr[name="bias"](%562)
  %675 : Tensor = prim::GetAttr[name="weight"](%562)
  %676 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.layer_norm
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.27, %676, %675, %674, %37, %38), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %678 : __torch__.torch.nn.modules.normalization.___torch_mangle_1606.LayerNorm = prim::GetAttr[name="layer_norm"](%560)
  %679 : __torch__.torch.nn.modules.linear.___torch_mangle_1604.Linear = prim::GetAttr[name="linear_2"](%560)
  %680 : __torch__.torch.nn.modules.linear.___torch_mangle_1602.Linear = prim::GetAttr[name="linear_1"](%560)
  %681 : Tensor = prim::GetAttr[name="bias"](%680)
  %682 : Tensor = prim::GetAttr[name="weight"](%680)
  %683 : Float(768:1, 3072:768) = aten::t(%682), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.14 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.28, %683), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.3 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.14, %681, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %686 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.3, %54), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %687 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.3, %53), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %688 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%687, %52), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %689 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.3, %688, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %690 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%689, %51), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %691 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%690), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %692 : Float(17:39936, 13:3072, 3072:1) = aten::add(%691, %50, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.29 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%686, %692), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.30 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.29, %49, %73), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %695 : Tensor = prim::GetAttr[name="bias"](%679)
  %696 : Tensor = prim::GetAttr[name="weight"](%679)
  %697 : Float(3072:1, 768:3072) = aten::t(%696), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.30, %697), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %695, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.31, %66, %73), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.32 : Float(17:9984, 13:768, 768:1) = aten::add(%input.28, %h.3, %79), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/modeling_funnel.py:588:0
  %702 : Tensor = prim::GetAttr[name="bias"](%678)
  %703 : Tensor = prim::GetAttr[name="weight"](%678)
  %704 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.layer_norm
  %query.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.32, %704, %703, %702, %37, %38), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %706 : __torch__.transformers.modeling_funnel.___torch_mangle_1622.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%106)
  %707 : __torch__.transformers.modeling_funnel.___torch_mangle_1616.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%106)
  %708 : __torch__.torch.nn.modules.normalization.___torch_mangle_1615.LayerNorm = prim::GetAttr[name="layer_norm"](%707)
  %709 : __torch__.torch.nn.modules.linear.___torch_mangle_1614.Linear = prim::GetAttr[name="post_proj"](%707)
  %710 : Tensor = prim::GetAttr[name="seg_embed"](%707)
  %711 : Tensor = prim::GetAttr[name="r_s_bias"](%707)
  %712 : Tensor = prim::GetAttr[name="r_kernel"](%707)
  %713 : Tensor = prim::GetAttr[name="r_r_bias"](%707)
  %714 : Tensor = prim::GetAttr[name="r_w_bias"](%707)
  %715 : __torch__.torch.nn.modules.linear.___torch_mangle_1613.Linear = prim::GetAttr[name="v_head"](%707)
  %716 : __torch__.torch.nn.modules.linear.___torch_mangle_1612.Linear = prim::GetAttr[name="k_head"](%707)
  %717 : __torch__.torch.nn.modules.linear.___torch_mangle_1611.Linear = prim::GetAttr[name="q_head"](%707)
  %718 : int = aten::size(%query.3, %78), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %719 : int = aten::size(%query.3, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %720 : int = aten::size(%query.3, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:531:0
  %721 : Tensor = prim::GetAttr[name="weight"](%717)
  %722 : Float(768:1, 768:768) = aten::t(%721), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %723 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %722), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %724 : int[] = prim::ListConstruct(%718, %719, %48, %47), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %q_head.7 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%723, %724), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:535:0
  %726 : Tensor = prim::GetAttr[name="bias"](%716)
  %727 : Tensor = prim::GetAttr[name="weight"](%716)
  %728 : Float(768:1, 768:768) = aten::t(%727), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %728), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %730 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %726, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1678:0
  %731 : int[] = prim::ListConstruct(%718, %720, %48, %47), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %732 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%730, %731), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:537:0
  %733 : Tensor = prim::GetAttr[name="bias"](%715)
  %734 : Tensor = prim::GetAttr[name="weight"](%715)
  %735 : Float(768:1, 768:768) = aten::t(%734), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.17 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %735), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %737 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.17, %733, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1678:0
  %738 : int[] = prim::ListConstruct(%718, %720, %48, %47), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %739 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%737, %738), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.7, %46), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.4 : Float(12:64, 64:1) = aten::mul(%714, %46), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:542:0
  %742 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_w_bias.4, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:544:0
  %743 : Tensor[] = prim::ListConstruct(%742, %732), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %content_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%45, %743), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %v.4 : Float(12:64, 64:1) = aten::mul(%713, %46), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:486:0
  %746 : Tensor[] = prim::ListConstruct(%161, %712), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %747 : Float(26:768, 12:64, 64:1) = aten::einsum(%44, %746), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %748 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %v.4, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:493:0
  %749 : Tensor[] = prim::ListConstruct(%748, %747), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %positional_attn.19 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%43, %749), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %751 : int = aten::size(%positional_attn.19, %78), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %752 : int = aten::size(%positional_attn.19, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %753 : int = aten::size(%positional_attn.19, %60), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %754 : int = aten::size(%positional_attn.19, %42), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.4 : Long() = prim::NumToTensor(%754), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %756 : int[] = prim::ListConstruct(%751, %752, %754, %753), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %positional_attn.20 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.19, %756), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:428:0
  %758 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.20, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %759 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%758, %79, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %760 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%759, %60, %79, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.21 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%760, %42, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %762 : Long() = aten::sub(%max_rel_len.4, %69, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %763 : int = aten::Int(%762), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %764 : int[] = prim::ListConstruct(%751, %752, %753, %763), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %positional_attn.22 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.21, %764), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.23 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.22, %42, %78, %720, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.24 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.23, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:498:0
  %768 : int = aten::size(%token_type_mat.2, %78), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %769 : int = aten::size(%token_type_mat.2, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %770 : int = aten::size(%token_type_mat.2, %60), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.4 : Float(12:64, 64:1) = aten::mul(%711, %46), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:508:0
  %772 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_s_bias.4, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:511:0
  %773 : Tensor[] = prim::ListConstruct(%772, %710), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %774 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%41, %773), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %775 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %776 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%775, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %777 : int = aten::size(%q_head.8, %60), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %778 : int[] = prim::ListConstruct(%768, %777, %769, %770), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %token_type_mat.6 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%776, %778, %73), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %780 : Tensor[] = aten::split(%774, %79, %65), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/tensor.py:371:0
  %diff_token_type.4 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.4 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%780), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %783 : int = aten::size(%token_type_mat.6, %78), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %784 : int = aten::size(%token_type_mat.6, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %785 : int = aten::size(%token_type_mat.6, %60), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %786 : int = aten::size(%token_type_mat.6, %42), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %787 : int[] = prim::ListConstruct(%783, %784, %785, %786), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %788 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.4, %787, %73), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %789 : int = aten::size(%token_type_mat.6, %78), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %790 : int = aten::size(%token_type_mat.6, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %791 : int = aten::size(%token_type_mat.6, %60), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %792 : int = aten::size(%token_type_mat.6, %42), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %793 : int[] = prim::ListConstruct(%789, %790, %791, %792), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %794 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.4, %793, %73), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.6, %788, %794), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.7, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:522:0
  %797 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.4, %positional_attn.24, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%797, %token_type_attn.8, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.10, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:553:0
  %800 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %801 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%800, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %802 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%801, %60), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %803 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%802, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %804 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%803, %79, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/tensor.py:396:0
  %805 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%804, %40), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.11, %805, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %input.33 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.12, %65, %75), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:558:0
  %808 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.33, %66, %73), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %809 : Tensor[] = prim::ListConstruct(%808, %739), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %attn_vec.4 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%39, %809), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %811 : int[] = prim::ListConstruct(%718, %719, %62), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.4, %811), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:565:0
  %813 : Tensor = prim::GetAttr[name="bias"](%709)
  %814 : Tensor = prim::GetAttr[name="weight"](%709)
  %815 : Float(768:1, 768:768) = aten::t(%814), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.34, %815), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %813, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.35, %66, %73), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.36 : Float(17:9984, 13:768, 768:1) = aten::add(%query.3, %attn_out.4, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:568:0
  %820 : Tensor = prim::GetAttr[name="bias"](%708)
  %821 : Tensor = prim::GetAttr[name="weight"](%708)
  %822 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.layer_norm
  %input.37 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.36, %822, %821, %820, %37, %38), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %824 : __torch__.torch.nn.modules.normalization.___torch_mangle_1621.LayerNorm = prim::GetAttr[name="layer_norm"](%706)
  %825 : __torch__.torch.nn.modules.linear.___torch_mangle_1619.Linear = prim::GetAttr[name="linear_2"](%706)
  %826 : __torch__.torch.nn.modules.linear.___torch_mangle_1617.Linear = prim::GetAttr[name="linear_1"](%706)
  %827 : Tensor = prim::GetAttr[name="bias"](%826)
  %828 : Tensor = prim::GetAttr[name="weight"](%826)
  %829 : Float(768:1, 3072:768) = aten::t(%828), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.37, %829), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.4 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.19, %827, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %832 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.4, %54), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %833 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.4, %53), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %834 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%833, %52), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %835 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.4, %834, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %836 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%835, %51), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %837 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%836), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %838 : Float(17:39936, 13:3072, 3072:1) = aten::add(%837, %50, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.38 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%832, %838), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.39 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.38, %49, %73), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %841 : Tensor = prim::GetAttr[name="bias"](%825)
  %842 : Tensor = prim::GetAttr[name="weight"](%825)
  %843 : Float(3072:1, 768:3072) = aten::t(%842), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.39, %843), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %841, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.40, %66, %73), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.41 : Float(17:9984, 13:768, 768:1) = aten::add(%input.37, %h.4, %79), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/modeling_funnel.py:588:0
  %848 : Tensor = prim::GetAttr[name="bias"](%824)
  %849 : Tensor = prim::GetAttr[name="weight"](%824)
  %850 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.layer_norm
  %hidden.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.41, %850, %849, %848, %37, %38), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %852 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %853 : Bool(17:169, 1:13, 13:1) = aten::slice(%852, %79, %78, %79, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %854 : Tensor[] = prim::ListConstruct(%853, %token_type_mat.2), scope: __module.encoder
  %tensor.1 : Bool(17:182, 14:13, 13:1) = aten::cat(%854, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %856 : Bool(17:182, 14:13, 13:1) = aten::slice(%tensor.1, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.7 : Bool(17:182, 7:26, 13:1) = aten::slice(%856, %79, %78, %65, %60), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %858 : Float(1:13, 13:1) = aten::slice(%cls_mask.1, %78, %78, %79, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %859 : Tensor[] = prim::ListConstruct(%858, %cls_mask.1), scope: __module.encoder
  %tensor.2 : Float(14:13, 13:1) = aten::cat(%859, %78), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.2 : Float(7:26, 13:1) = aten::slice(%tensor.2, %78, %78, %65, %60), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %862 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden.1, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %suffix.1 : Float(17:9984, 12:768, 768:1) = aten::slice(%862, %79, %78, %65, %79), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %864 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden.1, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %865 : Float(17:9984, 1:768, 768:1) = aten::slice(%864, %79, %78, %79, %79), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %866 : Tensor[] = prim::ListConstruct(%865, %suffix.1), scope: __module.encoder
  %tensor.3 : Float(17:9984, 13:768, 768:1) = aten::cat(%866, %79), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %868 : Float(17:9984, 13:768, 768:1) = aten::slice(%tensor.3, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %869 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::unsqueeze(%868, %79), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %870 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%869, %60, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %tensor.4 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%870, %42, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %872 : int[] = prim::ListConstruct(%60, %79), scope: __module.encoder
  %873 : int[] = prim::ListConstruct(%60, %79), scope: __module.encoder
  %874 : int[] = prim::ListConstruct(%78, %78), scope: __module.encoder
  %tensor.5 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::avg_pool2d(%tensor.4, %872, %873, %874, %38, %38, %70), scope: __module.encoder # transformers/modeling_funnel.py:371:0
  %876 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%tensor.5, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:382:0
  %query.4 : Float(17:5376, 7:768, 768:1) = aten::select(%876, %79, %78), scope: __module.encoder # transformers/modeling_funnel.py:382:0
  %878 : __torch__.transformers.modeling_funnel.___torch_mangle_1638.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%103)
  %879 : __torch__.transformers.modeling_funnel.___torch_mangle_1632.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%103)
  %880 : __torch__.torch.nn.modules.normalization.___torch_mangle_1631.LayerNorm = prim::GetAttr[name="layer_norm"](%879)
  %881 : __torch__.torch.nn.modules.linear.___torch_mangle_1630.Linear = prim::GetAttr[name="post_proj"](%879)
  %882 : Tensor = prim::GetAttr[name="seg_embed"](%879)
  %883 : Tensor = prim::GetAttr[name="r_s_bias"](%879)
  %884 : Tensor = prim::GetAttr[name="r_kernel"](%879)
  %885 : Tensor = prim::GetAttr[name="r_r_bias"](%879)
  %886 : Tensor = prim::GetAttr[name="r_w_bias"](%879)
  %887 : __torch__.torch.nn.modules.linear.___torch_mangle_1629.Linear = prim::GetAttr[name="v_head"](%879)
  %888 : __torch__.torch.nn.modules.linear.___torch_mangle_1628.Linear = prim::GetAttr[name="k_head"](%879)
  %889 : __torch__.torch.nn.modules.linear.___torch_mangle_1627.Linear = prim::GetAttr[name="q_head"](%879)
  %890 : int = aten::size(%query.4, %78), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %891 : int = aten::size(%query.4, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %892 : int = aten::size(%hidden.1, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:531:0
  %893 : Tensor = prim::GetAttr[name="weight"](%889)
  %894 : Float(768:1, 768:768) = aten::t(%893), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %895 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.4, %894), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %896 : int[] = prim::ListConstruct(%890, %891, %48, %47), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %q_head.9 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%895, %896), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:535:0
  %898 : Tensor = prim::GetAttr[name="bias"](%888)
  %899 : Tensor = prim::GetAttr[name="weight"](%888)
  %900 : Float(768:1, 768:768) = aten::t(%899), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %900), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %902 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %898, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1678:0
  %903 : int[] = prim::ListConstruct(%890, %892, %48, %47), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %904 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%902, %903), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:537:0
  %905 : Tensor = prim::GetAttr[name="bias"](%887)
  %906 : Tensor = prim::GetAttr[name="weight"](%887)
  %907 : Float(768:1, 768:768) = aten::t(%906), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %907), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %909 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %905, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1678:0
  %910 : int[] = prim::ListConstruct(%890, %892, %48, %47), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %911 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%909, %910), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.10 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.9, %46), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.5 : Float(12:64, 64:1) = aten::mul(%886, %46), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:542:0
  %914 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_w_bias.5, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:544:0
  %915 : Tensor[] = prim::ListConstruct(%914, %904), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %content_score.5 : Float(17:1092, 12:91, 7:13, 13:1) = aten::einsum(%45, %915), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %v.5 : Float(12:64, 64:1) = aten::mul(%885, %46), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:486:0
  %918 : Tensor[] = prim::ListConstruct(%185, %884), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %919 : Float(27:768, 12:64, 64:1) = aten::einsum(%44, %918), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %920 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %v.5, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:493:0
  %921 : Tensor[] = prim::ListConstruct(%920, %919), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %positional_attn.25 : Float(17:189, 12:3213, 7:27, 27:1) = aten::einsum(%43, %921), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %923 : int = aten::size(%positional_attn.25, %78), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %924 : int = aten::size(%positional_attn.25, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %925 : int = aten::size(%positional_attn.25, %60), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %926 : int = aten::size(%positional_attn.25, %42), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.5 : Long() = prim::NumToTensor(%926), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %928 : int[] = prim::ListConstruct(%923, %924, %926, %925), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %positional_attn.26 : Float(17:189, 12:3213, 27:7, 7:1) = aten::reshape(%positional_attn.25, %928), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:428:0
  %930 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%positional_attn.26, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %931 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%930, %79, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %932 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%931, %60, %60, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.27 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%932, %42, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %934 : Long() = aten::sub(%max_rel_len.5, %68, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %935 : int = aten::Int(%934), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %936 : int[] = prim::ListConstruct(%923, %924, %925, %935), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %positional_attn.28 : Float(17:189, 12:3213, 7:25, 25:1) = aten::reshape(%positional_attn.27, %936), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.29 : Float(17:189, 12:3213, 7:25, 13:1) = aten::slice(%positional_attn.28, %42, %78, %892, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.30 : Float(17:189, 12:3213, 7:25, 13:1) = aten::mul_(%positional_attn.29, %cls_mask.2), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:498:0
  %940 : int = aten::size(%token_type_mat.7, %78), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %941 : int = aten::size(%token_type_mat.7, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %942 : int = aten::size(%token_type_mat.7, %60), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.5 : Float(12:64, 64:1) = aten::mul(%883, %46), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:508:0
  %944 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_s_bias.5, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:511:0
  %945 : Tensor[] = prim::ListConstruct(%944, %882), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %946 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%41, %945), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %947 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %948 : Bool(17:182, 1:182, 7:26, 13:1) = aten::unsqueeze(%947, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %949 : int = aten::size(%q_head.10, %60), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %950 : int[] = prim::ListConstruct(%940, %949, %941, %942), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %token_type_mat.8 : Bool(17:182, 12:0, 7:26, 13:1) = aten::expand(%948, %950, %73), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %952 : Tensor[] = aten::split(%946, %79, %65), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/tensor.py:371:0
  %diff_token_type.5 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.5 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%952), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %955 : int = aten::size(%token_type_mat.8, %78), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %956 : int = aten::size(%token_type_mat.8, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %957 : int = aten::size(%token_type_mat.8, %60), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %958 : int = aten::size(%token_type_mat.8, %42), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %959 : int[] = prim::ListConstruct(%955, %956, %957, %958), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %960 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%same_token_type.5, %959, %73), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %961 : int = aten::size(%token_type_mat.8, %78), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %962 : int = aten::size(%token_type_mat.8, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %963 : int = aten::size(%token_type_mat.8, %60), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %964 : int = aten::size(%token_type_mat.8, %42), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %965 : int[] = prim::ListConstruct(%961, %962, %963, %964), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %966 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%diff_token_type.5, %965, %73), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.9 : Float(17:1092, 12:91, 7:13, 13:1) = aten::where(%token_type_mat.8, %960, %966), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.10 : Float(17:1092, 12:91, 7:13, 13:1) = aten::mul_(%token_type_attn.9, %cls_mask.2), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:522:0
  %969 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%content_score.5, %positional_attn.30, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.13 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%969, %token_type_attn.10, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.14 : Float(17:1092, 12:91, 7:13, 13:1) = aten::to(%attn_score.13, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:553:0
  %972 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %973 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%972, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %974 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%973, %60), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %975 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%974, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %976 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%975, %79, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/tensor.py:396:0
  %977 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%976, %40), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.15 : Float(17:1092, 12:91, 7:13, 13:1) = aten::sub(%attn_score.14, %977, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %input.42 : Float(17:1092, 12:91, 7:13, 13:1) = aten::softmax(%attn_score.15, %65, %75), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:558:0
  %980 : Float(17:1092, 12:91, 7:13, 13:1) = aten::dropout(%input.42, %66, %73), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %981 : Tensor[] = prim::ListConstruct(%980, %911), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %attn_vec.5 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%39, %981), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %983 : int[] = prim::ListConstruct(%890, %891, %62), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %input.43 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.5, %983), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:565:0
  %985 : Tensor = prim::GetAttr[name="bias"](%881)
  %986 : Tensor = prim::GetAttr[name="weight"](%881)
  %987 : Float(768:1, 768:768) = aten::t(%986), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.23 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.43, %987), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.44 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.23, %985, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.44, %66, %73), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:5376, 7:768, 768:1) = aten::add(%query.4, %attn_out.5, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:568:0
  %992 : Tensor = prim::GetAttr[name="bias"](%880)
  %993 : Tensor = prim::GetAttr[name="weight"](%880)
  %994 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.layer_norm
  %input.46 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.45, %994, %993, %992, %37, %38), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %996 : __torch__.torch.nn.modules.normalization.___torch_mangle_1637.LayerNorm = prim::GetAttr[name="layer_norm"](%878)
  %997 : __torch__.torch.nn.modules.linear.___torch_mangle_1635.Linear = prim::GetAttr[name="linear_2"](%878)
  %998 : __torch__.torch.nn.modules.linear.___torch_mangle_1633.Linear = prim::GetAttr[name="linear_1"](%878)
  %999 : Tensor = prim::GetAttr[name="bias"](%998)
  %1000 : Tensor = prim::GetAttr[name="weight"](%998)
  %1001 : Float(768:1, 3072:768) = aten::t(%1000), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.46, %1001), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.5 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.24, %999, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1004 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.5, %54), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1005 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.5, %53), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1006 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1005, %52), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1007 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.5, %1006, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1008 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1007, %51), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1009 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1008), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1010 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1009, %50, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.47 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1004, %1010), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.48 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.47, %49, %73), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1013 : Tensor = prim::GetAttr[name="bias"](%997)
  %1014 : Tensor = prim::GetAttr[name="weight"](%997)
  %1015 : Float(3072:1, 768:3072) = aten::t(%1014), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.25 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.48, %1015), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.49 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.25, %1013, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.49, %66, %73), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:5376, 7:768, 768:1) = aten::add(%input.46, %h.5, %79), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/modeling_funnel.py:588:0
  %1020 : Tensor = prim::GetAttr[name="bias"](%996)
  %1021 : Tensor = prim::GetAttr[name="weight"](%996)
  %1022 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.layer_norm
  %query.5 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.50, %1022, %1021, %1020, %37, %38), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1024 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1025 : Bool(17:182, 7:26, 13:1) = aten::slice(%1024, %79, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1026 : Bool(17:182, 7:26, 1:1) = aten::slice(%1025, %60, %78, %79, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1027 : Tensor[] = prim::ListConstruct(%1026, %token_type_mat.7), scope: __module.encoder
  %tensor.6 : Bool(17:98, 7:14, 14:1) = aten::cat(%1027, %60), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1029 : Bool(17:98, 7:14, 14:1) = aten::slice(%tensor.6, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1030 : Bool(17:98, 7:14, 14:1) = aten::slice(%1029, %79, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.9 : Bool(17:98, 7:14, 7:2) = aten::slice(%1030, %60, %78, %65, %60), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1032 : Float(7:26, 13:1) = aten::slice(%cls_mask.2, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1033 : Float(7:26, 1:1) = aten::slice(%1032, %79, %78, %79, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1034 : Tensor[] = prim::ListConstruct(%1033, %cls_mask.2), scope: __module.encoder
  %tensor.7 : Float(7:14, 14:1) = aten::cat(%1034, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1036 : Float(7:14, 14:1) = aten::slice(%tensor.7, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask.3 : Float(7:14, 7:2) = aten::slice(%1036, %79, %78, %65, %60), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1038 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %suffix.2 : Float(17:13, 12:1) = aten::slice(%1038, %79, %78, %65, %79), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %1040 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1041 : Float(17:13, 1:1) = aten::slice(%1040, %79, %78, %79, %79), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1042 : Tensor[] = prim::ListConstruct(%1041, %suffix.2), scope: __module.encoder
  %tensor.8 : Float(17:13, 13:1) = aten::cat(%1042, %79), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1044 : Float(17:13, 13:1) = aten::slice(%tensor.8, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %1045 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%1044, %79), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %1046 : Float(17:13, 1:13, 13:1) = aten::slice(%1045, %60, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %tensor.9 : Float(17:13, 1:13, 13:1, 1:1) = aten::unsqueeze(%1046, %42), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %input.51 : Float(17:13, 1:13, 13:1, 1:1) = aten::neg(%tensor.9), scope: __module.encoder # transformers/modeling_funnel.py:375:0
  %1049 : int[] = prim::ListConstruct(%60, %79), scope: __module.encoder
  %1050 : int[] = prim::ListConstruct(%60, %79), scope: __module.encoder
  %1051 : int[] = prim::ListConstruct(%78, %78), scope: __module.encoder
  %1052 : int[] = prim::ListConstruct(%79, %79), scope: __module.encoder
  %1053 : Float(17:7, 1:7, 7:1, 1:1) = aten::max_pool2d(%input.51, %1049, %1050, %1051, %1052, %38), scope: __module.encoder # torch/nn/functional.py:575:0
  %tensor.10 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%1053), scope: __module.encoder # transformers/modeling_funnel.py:375:0
  %1055 : Float(17:7, 1:7, 7:1, 1:1) = aten::slice(%tensor.10, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %1056 : Float(17:7, 7:1, 1:1) = aten::select(%1055, %79, %78), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %1057 : Float(17:7, 7:1, 1:1) = aten::slice(%1056, %79, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask.3 : Float(17:7, 7:1) = aten::select(%1057, %60, %78), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %1059 : __torch__.transformers.modeling_funnel.___torch_mangle_1653.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%100)
  %1060 : __torch__.transformers.modeling_funnel.___torch_mangle_1647.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%100)
  %1061 : __torch__.torch.nn.modules.normalization.___torch_mangle_1646.LayerNorm = prim::GetAttr[name="layer_norm"](%1060)
  %1062 : __torch__.torch.nn.modules.linear.___torch_mangle_1645.Linear = prim::GetAttr[name="post_proj"](%1060)
  %1063 : Tensor = prim::GetAttr[name="seg_embed"](%1060)
  %1064 : Tensor = prim::GetAttr[name="r_s_bias"](%1060)
  %1065 : Tensor = prim::GetAttr[name="r_kernel"](%1060)
  %1066 : Tensor = prim::GetAttr[name="r_r_bias"](%1060)
  %1067 : Tensor = prim::GetAttr[name="r_w_bias"](%1060)
  %1068 : __torch__.torch.nn.modules.linear.___torch_mangle_1644.Linear = prim::GetAttr[name="v_head"](%1060)
  %1069 : __torch__.torch.nn.modules.linear.___torch_mangle_1643.Linear = prim::GetAttr[name="k_head"](%1060)
  %1070 : __torch__.torch.nn.modules.linear.___torch_mangle_1642.Linear = prim::GetAttr[name="q_head"](%1060)
  %1071 : int = aten::size(%query.5, %78), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1072 : int = aten::size(%query.5, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1073 : int = aten::size(%query.5, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:531:0
  %1074 : Tensor = prim::GetAttr[name="weight"](%1070)
  %1075 : Float(768:1, 768:768) = aten::t(%1074), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1076 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1075), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1077 : int[] = prim::ListConstruct(%1071, %1072, %48, %47), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %q_head.11 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1076, %1077), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:535:0
  %1079 : Tensor = prim::GetAttr[name="bias"](%1069)
  %1080 : Tensor = prim::GetAttr[name="weight"](%1069)
  %1081 : Float(768:1, 768:768) = aten::t(%1080), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.26 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1081), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %1083 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.26, %1079, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1678:0
  %1084 : int[] = prim::ListConstruct(%1071, %1073, %48, %47), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1085 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1083, %1084), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:537:0
  %1086 : Tensor = prim::GetAttr[name="bias"](%1068)
  %1087 : Tensor = prim::GetAttr[name="weight"](%1068)
  %1088 : Float(768:1, 768:768) = aten::t(%1087), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.27 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1088), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %1090 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.27, %1086, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1678:0
  %1091 : int[] = prim::ListConstruct(%1071, %1073, %48, %47), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1092 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1090, %1091), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.12 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.11, %46), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.6 : Float(12:64, 64:1) = aten::mul(%1067, %46), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:542:0
  %1095 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_w_bias.6, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:544:0
  %1096 : Tensor[] = prim::ListConstruct(%1095, %1085), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %content_score.6 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%45, %1096), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %v.6 : Float(12:64, 64:1) = aten::mul(%1066, %46), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:486:0
  %1099 : Tensor[] = prim::ListConstruct(%203, %1065), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1100 : Float(14:768, 12:64, 64:1) = aten::einsum(%44, %1099), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1101 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %v.6, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:493:0
  %1102 : Tensor[] = prim::ListConstruct(%1101, %1100), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %positional_attn.31 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%43, %1102), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1104 : int = aten::size(%positional_attn.31, %78), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1105 : int = aten::size(%positional_attn.31, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1106 : int = aten::size(%positional_attn.31, %60), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1107 : int = aten::size(%positional_attn.31, %42), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.6 : Long() = prim::NumToTensor(%1107), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1109 : int[] = prim::ListConstruct(%1104, %1105, %1107, %1106), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %positional_attn.32 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.31, %1109), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:428:0
  %1111 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.32, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1112 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1111, %79, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1113 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1112, %60, %79, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.33 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1113, %42, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1115 : Long() = aten::sub(%max_rel_len.6, %69, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %1116 : int = aten::Int(%1115), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1117 : int[] = prim::ListConstruct(%1104, %1105, %1106, %1116), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %positional_attn.34 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.33, %1117), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.35 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.34, %42, %78, %1073, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.36 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.35, %cls_mask.3), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:498:0
  %1121 : int = aten::size(%token_type_mat.9, %78), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1122 : int = aten::size(%token_type_mat.9, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1123 : int = aten::size(%token_type_mat.9, %60), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.6 : Float(12:64, 64:1) = aten::mul(%1064, %46), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:508:0
  %1125 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_s_bias.6, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:511:0
  %1126 : Tensor[] = prim::ListConstruct(%1125, %1063), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1127 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%41, %1126), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1128 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1129 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1128, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1130 : int = aten::size(%q_head.12, %60), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1131 : int[] = prim::ListConstruct(%1121, %1130, %1122, %1123), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %token_type_mat.10 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1129, %1131, %73), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1133 : Tensor[] = aten::split(%1127, %79, %65), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/tensor.py:371:0
  %diff_token_type.6 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.6 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1133), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1136 : int = aten::size(%token_type_mat.10, %78), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1137 : int = aten::size(%token_type_mat.10, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1138 : int = aten::size(%token_type_mat.10, %60), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1139 : int = aten::size(%token_type_mat.10, %42), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1140 : int[] = prim::ListConstruct(%1136, %1137, %1138, %1139), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1141 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.6, %1140, %73), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1142 : int = aten::size(%token_type_mat.10, %78), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1143 : int = aten::size(%token_type_mat.10, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1144 : int = aten::size(%token_type_mat.10, %60), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1145 : int = aten::size(%token_type_mat.10, %42), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1146 : int[] = prim::ListConstruct(%1142, %1143, %1144, %1145), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1147 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.6, %1146, %73), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.11 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.10, %1141, %1147), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.12 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.11, %cls_mask.3), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:522:0
  %1150 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.6, %positional_attn.36, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1150, %token_type_attn.12, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.17 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.16, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:553:0
  %1153 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1154 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1153, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1155 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1154, %60), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1156 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1155, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1157 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1156, %79, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/tensor.py:396:0
  %1158 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1157, %40), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.18 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.17, %1158, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %input.52 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.18, %65, %75), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:558:0
  %1161 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.52, %66, %73), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1162 : Tensor[] = prim::ListConstruct(%1161, %1092), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %attn_vec.6 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%39, %1162), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1164 : int[] = prim::ListConstruct(%1071, %1072, %62), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %input.53 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.6, %1164), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:565:0
  %1166 : Tensor = prim::GetAttr[name="bias"](%1062)
  %1167 : Tensor = prim::GetAttr[name="weight"](%1062)
  %1168 : Float(768:1, 768:768) = aten::t(%1167), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.28 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.53, %1168), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.54 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.28, %1166, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.54, %66, %73), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.55 : Float(17:5376, 7:768, 768:1) = aten::add(%query.5, %attn_out.6, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:568:0
  %1173 : Tensor = prim::GetAttr[name="bias"](%1061)
  %1174 : Tensor = prim::GetAttr[name="weight"](%1061)
  %1175 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.layer_norm
  %input.56 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.55, %1175, %1174, %1173, %37, %38), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1177 : __torch__.torch.nn.modules.normalization.___torch_mangle_1652.LayerNorm = prim::GetAttr[name="layer_norm"](%1059)
  %1178 : __torch__.torch.nn.modules.linear.___torch_mangle_1650.Linear = prim::GetAttr[name="linear_2"](%1059)
  %1179 : __torch__.torch.nn.modules.linear.___torch_mangle_1648.Linear = prim::GetAttr[name="linear_1"](%1059)
  %1180 : Tensor = prim::GetAttr[name="bias"](%1179)
  %1181 : Tensor = prim::GetAttr[name="weight"](%1179)
  %1182 : Float(768:1, 3072:768) = aten::t(%1181), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.56, %1182), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.6 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.29, %1180, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1185 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.6, %54), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1186 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.6, %53), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1187 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1186, %52), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1188 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.6, %1187, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1189 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1188, %51), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1190 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1189), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1191 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1190, %50, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.57 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1185, %1191), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.58 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.57, %49, %73), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1194 : Tensor = prim::GetAttr[name="bias"](%1178)
  %1195 : Tensor = prim::GetAttr[name="weight"](%1178)
  %1196 : Float(3072:1, 768:3072) = aten::t(%1195), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.58, %1196), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.59 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.30, %1194, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.59, %66, %73), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:5376, 7:768, 768:1) = aten::add(%input.56, %h.6, %79), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/modeling_funnel.py:588:0
  %1201 : Tensor = prim::GetAttr[name="bias"](%1177)
  %1202 : Tensor = prim::GetAttr[name="weight"](%1177)
  %1203 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.layer_norm
  %query.6 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.60, %1203, %1202, %1201, %37, %38), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1205 : __torch__.transformers.modeling_funnel.___torch_mangle_1668.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%97)
  %1206 : __torch__.transformers.modeling_funnel.___torch_mangle_1662.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%97)
  %1207 : __torch__.torch.nn.modules.normalization.___torch_mangle_1661.LayerNorm = prim::GetAttr[name="layer_norm"](%1206)
  %1208 : __torch__.torch.nn.modules.linear.___torch_mangle_1660.Linear = prim::GetAttr[name="post_proj"](%1206)
  %1209 : Tensor = prim::GetAttr[name="seg_embed"](%1206)
  %1210 : Tensor = prim::GetAttr[name="r_s_bias"](%1206)
  %1211 : Tensor = prim::GetAttr[name="r_kernel"](%1206)
  %1212 : Tensor = prim::GetAttr[name="r_r_bias"](%1206)
  %1213 : Tensor = prim::GetAttr[name="r_w_bias"](%1206)
  %1214 : __torch__.torch.nn.modules.linear.___torch_mangle_1659.Linear = prim::GetAttr[name="v_head"](%1206)
  %1215 : __torch__.torch.nn.modules.linear.___torch_mangle_1658.Linear = prim::GetAttr[name="k_head"](%1206)
  %1216 : __torch__.torch.nn.modules.linear.___torch_mangle_1657.Linear = prim::GetAttr[name="q_head"](%1206)
  %1217 : int = aten::size(%query.6, %78), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1218 : int = aten::size(%query.6, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1219 : int = aten::size(%query.6, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:531:0
  %1220 : Tensor = prim::GetAttr[name="weight"](%1216)
  %1221 : Float(768:1, 768:768) = aten::t(%1220), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1222 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1221), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1223 : int[] = prim::ListConstruct(%1217, %1218, %48, %47), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %q_head.13 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1222, %1223), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:535:0
  %1225 : Tensor = prim::GetAttr[name="bias"](%1215)
  %1226 : Tensor = prim::GetAttr[name="weight"](%1215)
  %1227 : Float(768:1, 768:768) = aten::t(%1226), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.31 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1227), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %1229 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.31, %1225, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1678:0
  %1230 : int[] = prim::ListConstruct(%1217, %1219, %48, %47), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1231 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1229, %1230), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:537:0
  %1232 : Tensor = prim::GetAttr[name="bias"](%1214)
  %1233 : Tensor = prim::GetAttr[name="weight"](%1214)
  %1234 : Float(768:1, 768:768) = aten::t(%1233), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.32 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1234), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %1236 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.32, %1232, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1678:0
  %1237 : int[] = prim::ListConstruct(%1217, %1219, %48, %47), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1238 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1236, %1237), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.14 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.13, %46), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.7 : Float(12:64, 64:1) = aten::mul(%1213, %46), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:542:0
  %1241 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_w_bias.7, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:544:0
  %1242 : Tensor[] = prim::ListConstruct(%1241, %1231), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %content_score.7 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%45, %1242), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %v.7 : Float(12:64, 64:1) = aten::mul(%1212, %46), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:486:0
  %1245 : Tensor[] = prim::ListConstruct(%203, %1211), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1246 : Float(14:768, 12:64, 64:1) = aten::einsum(%44, %1245), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1247 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %v.7, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:493:0
  %1248 : Tensor[] = prim::ListConstruct(%1247, %1246), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %positional_attn.37 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%43, %1248), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1250 : int = aten::size(%positional_attn.37, %78), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1251 : int = aten::size(%positional_attn.37, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1252 : int = aten::size(%positional_attn.37, %60), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1253 : int = aten::size(%positional_attn.37, %42), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.7 : Long() = prim::NumToTensor(%1253), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1255 : int[] = prim::ListConstruct(%1250, %1251, %1253, %1252), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %positional_attn.38 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.37, %1255), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:428:0
  %1257 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.38, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1258 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1257, %79, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1259 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1258, %60, %79, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.39 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1259, %42, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1261 : Long() = aten::sub(%max_rel_len.7, %69, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %1262 : int = aten::Int(%1261), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1263 : int[] = prim::ListConstruct(%1250, %1251, %1252, %1262), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %positional_attn.40 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.39, %1263), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.41 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.40, %42, %78, %1219, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.42 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.41, %cls_mask.3), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:498:0
  %1267 : int = aten::size(%token_type_mat.9, %78), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1268 : int = aten::size(%token_type_mat.9, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1269 : int = aten::size(%token_type_mat.9, %60), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.7 : Float(12:64, 64:1) = aten::mul(%1210, %46), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:508:0
  %1271 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_s_bias.7, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:511:0
  %1272 : Tensor[] = prim::ListConstruct(%1271, %1209), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1273 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%41, %1272), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1274 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1275 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1274, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1276 : int = aten::size(%q_head.14, %60), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1277 : int[] = prim::ListConstruct(%1267, %1276, %1268, %1269), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %token_type_mat.11 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1275, %1277, %73), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1279 : Tensor[] = aten::split(%1273, %79, %65), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/tensor.py:371:0
  %diff_token_type.7 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.7 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1279), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1282 : int = aten::size(%token_type_mat.11, %78), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1283 : int = aten::size(%token_type_mat.11, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1284 : int = aten::size(%token_type_mat.11, %60), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1285 : int = aten::size(%token_type_mat.11, %42), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1286 : int[] = prim::ListConstruct(%1282, %1283, %1284, %1285), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1287 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.7, %1286, %73), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1288 : int = aten::size(%token_type_mat.11, %78), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1289 : int = aten::size(%token_type_mat.11, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1290 : int = aten::size(%token_type_mat.11, %60), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1291 : int = aten::size(%token_type_mat.11, %42), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1292 : int[] = prim::ListConstruct(%1288, %1289, %1290, %1291), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1293 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.7, %1292, %73), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.13 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.11, %1287, %1293), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.14 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.13, %cls_mask.3), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:522:0
  %1296 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.7, %positional_attn.42, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.19 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1296, %token_type_attn.14, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.20 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.19, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:553:0
  %1299 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1300 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1299, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1301 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1300, %60), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1302 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1301, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1303 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1302, %79, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/tensor.py:396:0
  %1304 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1303, %40), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.21 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.20, %1304, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %input.61 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.21, %65, %75), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:558:0
  %1307 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.61, %66, %73), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1308 : Tensor[] = prim::ListConstruct(%1307, %1238), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %attn_vec.7 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%39, %1308), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1310 : int[] = prim::ListConstruct(%1217, %1218, %62), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %input.62 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.7, %1310), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:565:0
  %1312 : Tensor = prim::GetAttr[name="bias"](%1208)
  %1313 : Tensor = prim::GetAttr[name="weight"](%1208)
  %1314 : Float(768:1, 768:768) = aten::t(%1313), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.33 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.62, %1314), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.63 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.33, %1312, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.63, %66, %73), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:5376, 7:768, 768:1) = aten::add(%query.6, %attn_out.7, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:568:0
  %1319 : Tensor = prim::GetAttr[name="bias"](%1207)
  %1320 : Tensor = prim::GetAttr[name="weight"](%1207)
  %1321 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.layer_norm
  %input.65 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.64, %1321, %1320, %1319, %37, %38), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1323 : __torch__.torch.nn.modules.normalization.___torch_mangle_1667.LayerNorm = prim::GetAttr[name="layer_norm"](%1205)
  %1324 : __torch__.torch.nn.modules.linear.___torch_mangle_1665.Linear = prim::GetAttr[name="linear_2"](%1205)
  %1325 : __torch__.torch.nn.modules.linear.___torch_mangle_1663.Linear = prim::GetAttr[name="linear_1"](%1205)
  %1326 : Tensor = prim::GetAttr[name="bias"](%1325)
  %1327 : Tensor = prim::GetAttr[name="weight"](%1325)
  %1328 : Float(768:1, 3072:768) = aten::t(%1327), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.65, %1328), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.7 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.34, %1326, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1331 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.7, %54), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1332 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.7, %53), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1333 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1332, %52), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1334 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.7, %1333, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1335 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1334, %51), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1336 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1335), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1337 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1336, %50, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.66 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1331, %1337), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.67 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.66, %49, %73), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1340 : Tensor = prim::GetAttr[name="bias"](%1324)
  %1341 : Tensor = prim::GetAttr[name="weight"](%1324)
  %1342 : Float(3072:1, 768:3072) = aten::t(%1341), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.67, %1342), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.68 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.35, %1340, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.68, %66, %73), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(17:5376, 7:768, 768:1) = aten::add(%input.65, %h.7, %79), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/modeling_funnel.py:588:0
  %1347 : Tensor = prim::GetAttr[name="bias"](%1323)
  %1348 : Tensor = prim::GetAttr[name="weight"](%1323)
  %1349 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.layer_norm
  %query.7 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.69, %1349, %1348, %1347, %37, %38), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1351 : __torch__.transformers.modeling_funnel.___torch_mangle_1683.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%94)
  %1352 : __torch__.transformers.modeling_funnel.___torch_mangle_1677.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%94)
  %1353 : __torch__.torch.nn.modules.normalization.___torch_mangle_1676.LayerNorm = prim::GetAttr[name="layer_norm"](%1352)
  %1354 : __torch__.torch.nn.modules.linear.___torch_mangle_1675.Linear = prim::GetAttr[name="post_proj"](%1352)
  %1355 : Tensor = prim::GetAttr[name="seg_embed"](%1352)
  %1356 : Tensor = prim::GetAttr[name="r_s_bias"](%1352)
  %1357 : Tensor = prim::GetAttr[name="r_kernel"](%1352)
  %1358 : Tensor = prim::GetAttr[name="r_r_bias"](%1352)
  %1359 : Tensor = prim::GetAttr[name="r_w_bias"](%1352)
  %1360 : __torch__.torch.nn.modules.linear.___torch_mangle_1674.Linear = prim::GetAttr[name="v_head"](%1352)
  %1361 : __torch__.torch.nn.modules.linear.___torch_mangle_1673.Linear = prim::GetAttr[name="k_head"](%1352)
  %1362 : __torch__.torch.nn.modules.linear.___torch_mangle_1672.Linear = prim::GetAttr[name="q_head"](%1352)
  %1363 : int = aten::size(%query.7, %78), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1364 : int = aten::size(%query.7, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1365 : int = aten::size(%query.7, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:531:0
  %1366 : Tensor = prim::GetAttr[name="weight"](%1362)
  %1367 : Float(768:1, 768:768) = aten::t(%1366), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1368 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1367), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1369 : int[] = prim::ListConstruct(%1363, %1364, %48, %47), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %q_head.15 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1368, %1369), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:535:0
  %1371 : Tensor = prim::GetAttr[name="bias"](%1361)
  %1372 : Tensor = prim::GetAttr[name="weight"](%1361)
  %1373 : Float(768:1, 768:768) = aten::t(%1372), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.36 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1373), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %1375 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.36, %1371, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1678:0
  %1376 : int[] = prim::ListConstruct(%1363, %1365, %48, %47), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1377 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1375, %1376), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:537:0
  %1378 : Tensor = prim::GetAttr[name="bias"](%1360)
  %1379 : Tensor = prim::GetAttr[name="weight"](%1360)
  %1380 : Float(768:1, 768:768) = aten::t(%1379), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.37 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1380), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %1382 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.37, %1378, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1678:0
  %1383 : int[] = prim::ListConstruct(%1363, %1365, %48, %47), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1384 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1382, %1383), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.16 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.15, %46), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.8 : Float(12:64, 64:1) = aten::mul(%1359, %46), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:542:0
  %1387 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_w_bias.8, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:544:0
  %1388 : Tensor[] = prim::ListConstruct(%1387, %1377), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %content_score.8 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%45, %1388), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %v.8 : Float(12:64, 64:1) = aten::mul(%1358, %46), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:486:0
  %1391 : Tensor[] = prim::ListConstruct(%203, %1357), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1392 : Float(14:768, 12:64, 64:1) = aten::einsum(%44, %1391), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1393 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %v.8, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:493:0
  %1394 : Tensor[] = prim::ListConstruct(%1393, %1392), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %positional_attn.43 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%43, %1394), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1396 : int = aten::size(%positional_attn.43, %78), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1397 : int = aten::size(%positional_attn.43, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1398 : int = aten::size(%positional_attn.43, %60), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1399 : int = aten::size(%positional_attn.43, %42), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.8 : Long() = prim::NumToTensor(%1399), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1401 : int[] = prim::ListConstruct(%1396, %1397, %1399, %1398), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %positional_attn.44 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.43, %1401), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:428:0
  %1403 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.44, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1404 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1403, %79, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1405 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1404, %60, %79, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.45 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1405, %42, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1407 : Long() = aten::sub(%max_rel_len.8, %69, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %1408 : int = aten::Int(%1407), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1409 : int[] = prim::ListConstruct(%1396, %1397, %1398, %1408), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %positional_attn.46 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.45, %1409), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.47 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.46, %42, %78, %1365, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.48 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.47, %cls_mask.3), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:498:0
  %1413 : int = aten::size(%token_type_mat.9, %78), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1414 : int = aten::size(%token_type_mat.9, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1415 : int = aten::size(%token_type_mat.9, %60), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.8 : Float(12:64, 64:1) = aten::mul(%1356, %46), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:508:0
  %1417 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_s_bias.8, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:511:0
  %1418 : Tensor[] = prim::ListConstruct(%1417, %1355), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1419 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%41, %1418), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1420 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1421 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1420, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1422 : int = aten::size(%q_head.16, %60), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1423 : int[] = prim::ListConstruct(%1413, %1422, %1414, %1415), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %token_type_mat.12 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1421, %1423, %73), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1425 : Tensor[] = aten::split(%1419, %79, %65), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/tensor.py:371:0
  %diff_token_type.8 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.8 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1425), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1428 : int = aten::size(%token_type_mat.12, %78), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1429 : int = aten::size(%token_type_mat.12, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1430 : int = aten::size(%token_type_mat.12, %60), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1431 : int = aten::size(%token_type_mat.12, %42), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1432 : int[] = prim::ListConstruct(%1428, %1429, %1430, %1431), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1433 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.8, %1432, %73), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1434 : int = aten::size(%token_type_mat.12, %78), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1435 : int = aten::size(%token_type_mat.12, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1436 : int = aten::size(%token_type_mat.12, %60), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1437 : int = aten::size(%token_type_mat.12, %42), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1438 : int[] = prim::ListConstruct(%1434, %1435, %1436, %1437), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1439 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.8, %1438, %73), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.15 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.12, %1433, %1439), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.15, %cls_mask.3), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:522:0
  %1442 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.8, %positional_attn.48, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.22 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1442, %token_type_attn.16, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.23 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.22, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:553:0
  %1445 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1446 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1445, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1447 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1446, %60), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1448 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1447, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1449 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1448, %79, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/tensor.py:396:0
  %1450 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1449, %40), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.24 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.23, %1450, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %input.70 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.24, %65, %75), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:558:0
  %1453 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.70, %66, %73), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %1454 : Tensor[] = prim::ListConstruct(%1453, %1384), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %attn_vec.8 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%39, %1454), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1456 : int[] = prim::ListConstruct(%1363, %1364, %62), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %input.71 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.8, %1456), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:565:0
  %1458 : Tensor = prim::GetAttr[name="bias"](%1354)
  %1459 : Tensor = prim::GetAttr[name="weight"](%1354)
  %1460 : Float(768:1, 768:768) = aten::t(%1459), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.38 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.71, %1460), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.72 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.38, %1458, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.72, %66, %73), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:5376, 7:768, 768:1) = aten::add(%query.7, %attn_out.8, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:568:0
  %1465 : Tensor = prim::GetAttr[name="bias"](%1353)
  %1466 : Tensor = prim::GetAttr[name="weight"](%1353)
  %1467 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.layer_norm
  %input.74 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.73, %1467, %1466, %1465, %37, %38), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %1469 : __torch__.torch.nn.modules.normalization.___torch_mangle_1682.LayerNorm = prim::GetAttr[name="layer_norm"](%1351)
  %1470 : __torch__.torch.nn.modules.linear.___torch_mangle_1680.Linear = prim::GetAttr[name="linear_2"](%1351)
  %1471 : __torch__.torch.nn.modules.linear.___torch_mangle_1678.Linear = prim::GetAttr[name="linear_1"](%1351)
  %1472 : Tensor = prim::GetAttr[name="bias"](%1471)
  %1473 : Tensor = prim::GetAttr[name="weight"](%1471)
  %1474 : Float(768:1, 3072:768) = aten::t(%1473), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.39 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.74, %1474), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.8 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.39, %1472, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1477 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.8, %54), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1478 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.8, %53), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1479 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1478, %52), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1480 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.8, %1479, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1481 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1480, %51), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1482 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1481), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1483 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1482, %50, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.75 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1477, %1483), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.76 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.75, %49, %73), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1486 : Tensor = prim::GetAttr[name="bias"](%1470)
  %1487 : Tensor = prim::GetAttr[name="weight"](%1470)
  %1488 : Float(3072:1, 768:3072) = aten::t(%1487), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.40 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.76, %1488), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.77 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.40, %1486, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.77, %66, %73), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(17:5376, 7:768, 768:1) = aten::add(%input.74, %h.8, %79), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/modeling_funnel.py:588:0
  %1493 : Tensor = prim::GetAttr[name="bias"](%1469)
  %1494 : Tensor = prim::GetAttr[name="weight"](%1469)
  %1495 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.layer_norm
  %hidden : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.78, %1495, %1494, %1493, %37, %38), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1497 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1498 : Bool(17:98, 1:14, 7:2) = aten::slice(%1497, %79, %78, %79, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1499 : Tensor[] = prim::ListConstruct(%1498, %token_type_mat.9), scope: __module.encoder
  %tensor.11 : Bool(17:56, 8:7, 7:1) = aten::cat(%1499, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1501 : Bool(17:56, 8:7, 7:1) = aten::slice(%tensor.11, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.13 : Bool(17:56, 4:14, 7:1) = aten::slice(%1501, %79, %78, %65, %60), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1503 : Float(1:14, 7:2) = aten::slice(%cls_mask.3, %78, %78, %79, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1504 : Tensor[] = prim::ListConstruct(%1503, %cls_mask.3), scope: __module.encoder
  %tensor.12 : Float(8:7, 7:1) = aten::cat(%1504, %78), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.4 : Float(4:14, 7:1) = aten::slice(%tensor.12, %78, %78, %65, %60), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1507 : Float(17:5376, 7:768, 768:1) = aten::slice(%hidden, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %suffix.3 : Float(17:5376, 6:768, 768:1) = aten::slice(%1507, %79, %78, %65, %79), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %1509 : Float(17:5376, 7:768, 768:1) = aten::slice(%hidden, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1510 : Float(17:5376, 1:768, 768:1) = aten::slice(%1509, %79, %78, %79, %79), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1511 : Tensor[] = prim::ListConstruct(%1510, %suffix.3), scope: __module.encoder
  %tensor.13 : Float(17:5376, 7:768, 768:1) = aten::cat(%1511, %79), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1513 : Float(17:5376, 7:768, 768:1) = aten::slice(%tensor.13, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %1514 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::unsqueeze(%1513, %79), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %1515 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%1514, %60, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %tensor.14 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%1515, %42, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %1517 : int[] = prim::ListConstruct(%60, %79), scope: __module.encoder
  %1518 : int[] = prim::ListConstruct(%60, %79), scope: __module.encoder
  %1519 : int[] = prim::ListConstruct(%78, %78), scope: __module.encoder
  %tensor.15 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::avg_pool2d(%tensor.14, %1517, %1518, %1519, %38, %38, %70), scope: __module.encoder # transformers/modeling_funnel.py:371:0
  %1521 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::slice(%tensor.15, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:382:0
  %query.8 : Float(17:3072, 4:768, 768:1) = aten::select(%1521, %79, %78), scope: __module.encoder # transformers/modeling_funnel.py:382:0
  %1523 : __torch__.transformers.modeling_funnel.___torch_mangle_1699.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%91)
  %1524 : __torch__.transformers.modeling_funnel.___torch_mangle_1693.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%91)
  %1525 : __torch__.torch.nn.modules.normalization.___torch_mangle_1692.LayerNorm = prim::GetAttr[name="layer_norm"](%1524)
  %1526 : __torch__.torch.nn.modules.linear.___torch_mangle_1691.Linear = prim::GetAttr[name="post_proj"](%1524)
  %1527 : Tensor = prim::GetAttr[name="seg_embed"](%1524)
  %1528 : Tensor = prim::GetAttr[name="r_s_bias"](%1524)
  %1529 : Tensor = prim::GetAttr[name="r_kernel"](%1524)
  %1530 : Tensor = prim::GetAttr[name="r_r_bias"](%1524)
  %1531 : Tensor = prim::GetAttr[name="r_w_bias"](%1524)
  %1532 : __torch__.torch.nn.modules.linear.___torch_mangle_1690.Linear = prim::GetAttr[name="v_head"](%1524)
  %1533 : __torch__.torch.nn.modules.linear.___torch_mangle_1689.Linear = prim::GetAttr[name="k_head"](%1524)
  %1534 : __torch__.torch.nn.modules.linear.___torch_mangle_1688.Linear = prim::GetAttr[name="q_head"](%1524)
  %1535 : int = aten::size(%query.8, %78), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1536 : int = aten::size(%query.8, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1537 : int = aten::size(%hidden, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:531:0
  %1538 : Tensor = prim::GetAttr[name="weight"](%1534)
  %1539 : Float(768:1, 768:768) = aten::t(%1538), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1540 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.8, %1539), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1541 : int[] = prim::ListConstruct(%1535, %1536, %48, %47), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %q_head.17 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1540, %1541), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:535:0
  %1543 : Tensor = prim::GetAttr[name="bias"](%1533)
  %1544 : Tensor = prim::GetAttr[name="weight"](%1533)
  %1545 : Float(768:1, 768:768) = aten::t(%1544), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.41 : Float(17:5376, 7:768, 768:1) = aten::matmul(%hidden, %1545), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %1547 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.41, %1543, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1678:0
  %1548 : int[] = prim::ListConstruct(%1535, %1537, %48, %47), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1549 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1547, %1548), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:537:0
  %1550 : Tensor = prim::GetAttr[name="bias"](%1532)
  %1551 : Tensor = prim::GetAttr[name="weight"](%1532)
  %1552 : Float(768:1, 768:768) = aten::t(%1551), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.42 : Float(17:5376, 7:768, 768:1) = aten::matmul(%hidden, %1552), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %1554 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.42, %1550, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1678:0
  %1555 : int[] = prim::ListConstruct(%1535, %1537, %48, %47), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1556 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1554, %1555), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.18 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.17, %46), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.9 : Float(12:64, 64:1) = aten::mul(%1531, %46), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:542:0
  %1559 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_w_bias.9, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:544:0
  %1560 : Tensor[] = prim::ListConstruct(%1559, %1549), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %content_score.9 : Float(17:336, 12:28, 4:7, 7:1) = aten::einsum(%45, %1560), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %v.9 : Float(12:64, 64:1) = aten::mul(%1530, %46), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:486:0
  %1563 : Tensor[] = prim::ListConstruct(%227, %1529), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1564 : Float(15:768, 12:64, 64:1) = aten::einsum(%44, %1563), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1565 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %v.9, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:493:0
  %1566 : Tensor[] = prim::ListConstruct(%1565, %1564), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %positional_attn.49 : Float(17:60, 12:1020, 4:15, 15:1) = aten::einsum(%43, %1566), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1568 : int = aten::size(%positional_attn.49, %78), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1569 : int = aten::size(%positional_attn.49, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1570 : int = aten::size(%positional_attn.49, %60), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1571 : int = aten::size(%positional_attn.49, %42), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.9 : Long() = prim::NumToTensor(%1571), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1573 : int[] = prim::ListConstruct(%1568, %1569, %1571, %1570), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %positional_attn.50 : Float(17:60, 12:1020, 15:4, 4:1) = aten::reshape(%positional_attn.49, %1573), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:428:0
  %1575 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%positional_attn.50, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1576 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%1575, %79, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1577 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%1576, %60, %60, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.51 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%1577, %42, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1579 : Long() = aten::sub(%max_rel_len.9, %68, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %1580 : int = aten::Int(%1579), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1581 : int[] = prim::ListConstruct(%1568, %1569, %1570, %1580), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %positional_attn.52 : Float(17:60, 12:1020, 4:13, 13:1) = aten::reshape(%positional_attn.51, %1581), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.53 : Float(17:60, 12:1020, 4:13, 7:1) = aten::slice(%positional_attn.52, %42, %78, %1537, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.54 : Float(17:60, 12:1020, 4:13, 7:1) = aten::mul_(%positional_attn.53, %cls_mask.4), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:498:0
  %1585 : int = aten::size(%token_type_mat.13, %78), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1586 : int = aten::size(%token_type_mat.13, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1587 : int = aten::size(%token_type_mat.13, %60), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.9 : Float(12:64, 64:1) = aten::mul(%1528, %46), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:508:0
  %1589 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_s_bias.9, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:511:0
  %1590 : Tensor[] = prim::ListConstruct(%1589, %1527), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1591 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%41, %1590), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1592 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1593 : Bool(17:56, 1:56, 4:14, 7:1) = aten::unsqueeze(%1592, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1594 : int = aten::size(%q_head.18, %60), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1595 : int[] = prim::ListConstruct(%1585, %1594, %1586, %1587), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %token_type_mat.14 : Bool(17:56, 12:0, 4:14, 7:1) = aten::expand(%1593, %1595, %73), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1597 : Tensor[] = aten::split(%1591, %79, %65), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/tensor.py:371:0
  %diff_token_type.9 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.9 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1597), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1600 : int = aten::size(%token_type_mat.14, %78), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1601 : int = aten::size(%token_type_mat.14, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1602 : int = aten::size(%token_type_mat.14, %60), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1603 : int = aten::size(%token_type_mat.14, %42), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1604 : int[] = prim::ListConstruct(%1600, %1601, %1602, %1603), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1605 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%same_token_type.9, %1604, %73), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1606 : int = aten::size(%token_type_mat.14, %78), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1607 : int = aten::size(%token_type_mat.14, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1608 : int = aten::size(%token_type_mat.14, %60), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1609 : int = aten::size(%token_type_mat.14, %42), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1610 : int[] = prim::ListConstruct(%1606, %1607, %1608, %1609), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1611 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%diff_token_type.9, %1610, %73), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.17 : Float(17:336, 12:28, 4:7, 7:1) = aten::where(%token_type_mat.14, %1605, %1611), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.18 : Float(17:336, 12:28, 4:7, 7:1) = aten::mul_(%token_type_attn.17, %cls_mask.4), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:522:0
  %1614 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%content_score.9, %positional_attn.54, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.25 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%1614, %token_type_attn.18, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.26 : Float(17:336, 12:28, 4:7, 7:1) = aten::to(%attn_score.25, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:553:0
  %1617 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1618 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1617, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1619 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1618, %60), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1620 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1619, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1621 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1620, %79, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/tensor.py:396:0
  %1622 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1621, %40), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.27 : Float(17:336, 12:28, 4:7, 7:1) = aten::sub(%attn_score.26, %1622, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %input.79 : Float(17:336, 12:28, 4:7, 7:1) = aten::softmax(%attn_score.27, %65, %75), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:558:0
  %1625 : Float(17:336, 12:28, 4:7, 7:1) = aten::dropout(%input.79, %66, %73), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %1626 : Tensor[] = prim::ListConstruct(%1625, %1556), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %attn_vec.9 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%39, %1626), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1628 : int[] = prim::ListConstruct(%1535, %1536, %62), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %input.80 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.9, %1628), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:565:0
  %1630 : Tensor = prim::GetAttr[name="bias"](%1526)
  %1631 : Tensor = prim::GetAttr[name="weight"](%1526)
  %1632 : Float(768:1, 768:768) = aten::t(%1631), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.43 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.80, %1632), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.81 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.43, %1630, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.81, %66, %73), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.82 : Float(17:3072, 4:768, 768:1) = aten::add(%query.8, %attn_out.9, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:568:0
  %1637 : Tensor = prim::GetAttr[name="bias"](%1525)
  %1638 : Tensor = prim::GetAttr[name="weight"](%1525)
  %1639 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.layer_norm
  %input.83 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.82, %1639, %1638, %1637, %37, %38), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %1641 : __torch__.torch.nn.modules.normalization.___torch_mangle_1698.LayerNorm = prim::GetAttr[name="layer_norm"](%1523)
  %1642 : __torch__.torch.nn.modules.linear.___torch_mangle_1696.Linear = prim::GetAttr[name="linear_2"](%1523)
  %1643 : __torch__.torch.nn.modules.linear.___torch_mangle_1694.Linear = prim::GetAttr[name="linear_1"](%1523)
  %1644 : Tensor = prim::GetAttr[name="bias"](%1643)
  %1645 : Tensor = prim::GetAttr[name="weight"](%1643)
  %1646 : Float(768:1, 3072:768) = aten::t(%1645), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.44 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.83, %1646), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.9 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.44, %1644, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1649 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.9, %54), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1650 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.9, %53), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1651 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1650, %52), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1652 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.9, %1651, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1653 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1652, %51), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1654 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1653), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1655 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1654, %50, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.84 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1649, %1655), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.85 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.84, %49, %73), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1658 : Tensor = prim::GetAttr[name="bias"](%1642)
  %1659 : Tensor = prim::GetAttr[name="weight"](%1642)
  %1660 : Float(3072:1, 768:3072) = aten::t(%1659), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.45 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.85, %1660), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.86 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.45, %1658, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.86, %66, %73), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(17:3072, 4:768, 768:1) = aten::add(%input.83, %h.9, %79), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/modeling_funnel.py:588:0
  %1665 : Tensor = prim::GetAttr[name="bias"](%1641)
  %1666 : Tensor = prim::GetAttr[name="weight"](%1641)
  %1667 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.layer_norm
  %query.9 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.87, %1667, %1666, %1665, %37, %38), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1669 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1670 : Bool(17:56, 4:14, 7:1) = aten::slice(%1669, %79, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1671 : Bool(17:56, 4:14, 1:1) = aten::slice(%1670, %60, %78, %79, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1672 : Tensor[] = prim::ListConstruct(%1671, %token_type_mat.13), scope: __module.encoder
  %tensor.16 : Bool(17:32, 4:8, 8:1) = aten::cat(%1672, %60), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1674 : Bool(17:32, 4:8, 8:1) = aten::slice(%tensor.16, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1675 : Bool(17:32, 4:8, 8:1) = aten::slice(%1674, %79, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.15 : Bool(17:32, 4:8, 4:2) = aten::slice(%1675, %60, %78, %65, %60), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1677 : Float(4:14, 7:1) = aten::slice(%cls_mask.4, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1678 : Float(4:14, 1:1) = aten::slice(%1677, %79, %78, %79, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1679 : Tensor[] = prim::ListConstruct(%1678, %cls_mask.4), scope: __module.encoder
  %tensor.17 : Float(4:8, 8:1) = aten::cat(%1679, %79), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1681 : Float(4:8, 8:1) = aten::slice(%tensor.17, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask : Float(4:8, 4:2) = aten::slice(%1681, %79, %78, %65, %60), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1683 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %suffix : Float(17:7, 6:1) = aten::slice(%1683, %79, %78, %65, %79), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %1685 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1686 : Float(17:7, 1:1) = aten::slice(%1685, %79, %78, %79, %79), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1687 : Tensor[] = prim::ListConstruct(%1686, %suffix), scope: __module.encoder
  %tensor.18 : Float(17:7, 7:1) = aten::cat(%1687, %79), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1689 : Float(17:7, 7:1) = aten::slice(%tensor.18, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %1690 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1689, %79), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %1691 : Float(17:7, 1:7, 7:1) = aten::slice(%1690, %60, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %tensor.19 : Float(17:7, 1:7, 7:1, 1:1) = aten::unsqueeze(%1691, %42), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %input.88 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%tensor.19), scope: __module.encoder # transformers/modeling_funnel.py:375:0
  %1694 : int[] = prim::ListConstruct(%60, %79), scope: __module.encoder
  %1695 : int[] = prim::ListConstruct(%60, %79), scope: __module.encoder
  %1696 : int[] = prim::ListConstruct(%78, %78), scope: __module.encoder
  %1697 : int[] = prim::ListConstruct(%79, %79), scope: __module.encoder
  %1698 : Float(17:4, 1:4, 4:1, 1:1) = aten::max_pool2d(%input.88, %1694, %1695, %1696, %1697, %38), scope: __module.encoder # torch/nn/functional.py:575:0
  %tensor : Float(17:4, 1:4, 4:1, 1:1) = aten::neg(%1698), scope: __module.encoder # transformers/modeling_funnel.py:375:0
  %1700 : Float(17:4, 1:4, 4:1, 1:1) = aten::slice(%tensor, %78, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %1701 : Float(17:4, 4:1, 1:1) = aten::select(%1700, %79, %78), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %1702 : Float(17:4, 4:1, 1:1) = aten::slice(%1701, %79, %78, %67, %79), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask : Float(17:4, 4:1) = aten::select(%1702, %60, %78), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %1704 : __torch__.transformers.modeling_funnel.___torch_mangle_1714.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%88)
  %1705 : __torch__.transformers.modeling_funnel.___torch_mangle_1708.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%88)
  %1706 : __torch__.torch.nn.modules.normalization.___torch_mangle_1707.LayerNorm = prim::GetAttr[name="layer_norm"](%1705)
  %1707 : __torch__.torch.nn.modules.linear.___torch_mangle_1706.Linear = prim::GetAttr[name="post_proj"](%1705)
  %1708 : Tensor = prim::GetAttr[name="seg_embed"](%1705)
  %1709 : Tensor = prim::GetAttr[name="r_s_bias"](%1705)
  %1710 : Tensor = prim::GetAttr[name="r_kernel"](%1705)
  %1711 : Tensor = prim::GetAttr[name="r_r_bias"](%1705)
  %1712 : Tensor = prim::GetAttr[name="r_w_bias"](%1705)
  %1713 : __torch__.torch.nn.modules.linear.___torch_mangle_1705.Linear = prim::GetAttr[name="v_head"](%1705)
  %1714 : __torch__.torch.nn.modules.linear.___torch_mangle_1704.Linear = prim::GetAttr[name="k_head"](%1705)
  %1715 : __torch__.torch.nn.modules.linear.___torch_mangle_1703.Linear = prim::GetAttr[name="q_head"](%1705)
  %1716 : int = aten::size(%query.9, %78), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1717 : int = aten::size(%query.9, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1718 : int = aten::size(%query.9, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:531:0
  %1719 : Tensor = prim::GetAttr[name="weight"](%1715)
  %1720 : Float(768:1, 768:768) = aten::t(%1719), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1721 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1720), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1722 : int[] = prim::ListConstruct(%1716, %1717, %48, %47), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %q_head.19 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1721, %1722), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:535:0
  %1724 : Tensor = prim::GetAttr[name="bias"](%1714)
  %1725 : Tensor = prim::GetAttr[name="weight"](%1714)
  %1726 : Float(768:1, 768:768) = aten::t(%1725), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.46 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1726), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %1728 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.46, %1724, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1678:0
  %1729 : int[] = prim::ListConstruct(%1716, %1718, %48, %47), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1730 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1728, %1729), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:537:0
  %1731 : Tensor = prim::GetAttr[name="bias"](%1713)
  %1732 : Tensor = prim::GetAttr[name="weight"](%1713)
  %1733 : Float(768:1, 768:768) = aten::t(%1732), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.47 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1733), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %1735 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.47, %1731, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1678:0
  %1736 : int[] = prim::ListConstruct(%1716, %1718, %48, %47), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1737 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1735, %1736), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.20 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.19, %46), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.10 : Float(12:64, 64:1) = aten::mul(%1712, %46), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:542:0
  %1740 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_w_bias.10, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:544:0
  %1741 : Tensor[] = prim::ListConstruct(%1740, %1730), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %content_score.10 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%45, %1741), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %v.10 : Float(12:64, 64:1) = aten::mul(%1711, %46), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:486:0
  %1744 : Tensor[] = prim::ListConstruct(%245, %1710), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1745 : Float(8:768, 12:64, 64:1) = aten::einsum(%44, %1744), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1746 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %v.10, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:493:0
  %1747 : Tensor[] = prim::ListConstruct(%1746, %1745), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %positional_attn.55 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%43, %1747), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1749 : int = aten::size(%positional_attn.55, %78), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1750 : int = aten::size(%positional_attn.55, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1751 : int = aten::size(%positional_attn.55, %60), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1752 : int = aten::size(%positional_attn.55, %42), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.10 : Long() = prim::NumToTensor(%1752), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1754 : int[] = prim::ListConstruct(%1749, %1750, %1752, %1751), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %positional_attn.56 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.55, %1754), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:428:0
  %1756 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.56, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1757 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%1756, %79, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1758 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1757, %60, %79, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.57 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1758, %42, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1760 : Long() = aten::sub(%max_rel_len.10, %69, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %1761 : int = aten::Int(%1760), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1762 : int[] = prim::ListConstruct(%1749, %1750, %1751, %1761), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %positional_attn.58 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.57, %1762), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.59 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.58, %42, %78, %1718, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.60 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.59, %cls_mask), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:498:0
  %1766 : int = aten::size(%token_type_mat.15, %78), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1767 : int = aten::size(%token_type_mat.15, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1768 : int = aten::size(%token_type_mat.15, %60), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.10 : Float(12:64, 64:1) = aten::mul(%1709, %46), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:508:0
  %1770 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_s_bias.10, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:511:0
  %1771 : Tensor[] = prim::ListConstruct(%1770, %1708), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1772 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%41, %1771), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1773 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1774 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1773, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1775 : int = aten::size(%q_head.20, %60), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1776 : int[] = prim::ListConstruct(%1766, %1775, %1767, %1768), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %token_type_mat.16 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%1774, %1776, %73), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1778 : Tensor[] = aten::split(%1772, %79, %65), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/tensor.py:371:0
  %diff_token_type.10 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.10 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1778), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1781 : int = aten::size(%token_type_mat.16, %78), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1782 : int = aten::size(%token_type_mat.16, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1783 : int = aten::size(%token_type_mat.16, %60), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1784 : int = aten::size(%token_type_mat.16, %42), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1785 : int[] = prim::ListConstruct(%1781, %1782, %1783, %1784), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1786 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.10, %1785, %73), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1787 : int = aten::size(%token_type_mat.16, %78), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1788 : int = aten::size(%token_type_mat.16, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1789 : int = aten::size(%token_type_mat.16, %60), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1790 : int = aten::size(%token_type_mat.16, %42), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1791 : int[] = prim::ListConstruct(%1787, %1788, %1789, %1790), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1792 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.10, %1791, %73), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.19 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.16, %1786, %1792), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.20 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.19, %cls_mask), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:522:0
  %1795 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.10, %positional_attn.60, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.28 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%1795, %token_type_attn.20, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.29 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.28, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:553:0
  %1798 : Float(17:4, 4:1) = aten::slice(%attention_mask, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1799 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%1798, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1800 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1799, %60), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1801 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%1800, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1802 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%1801, %79, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/tensor.py:396:0
  %1803 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%1802, %40), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.30 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.29, %1803, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %input.89 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.30, %65, %75), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:558:0
  %1806 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.89, %66, %73), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1807 : Tensor[] = prim::ListConstruct(%1806, %1737), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %attn_vec.10 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%39, %1807), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1809 : int[] = prim::ListConstruct(%1716, %1717, %62), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %input.90 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.10, %1809), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:565:0
  %1811 : Tensor = prim::GetAttr[name="bias"](%1707)
  %1812 : Tensor = prim::GetAttr[name="weight"](%1707)
  %1813 : Float(768:1, 768:768) = aten::t(%1812), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.48 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.90, %1813), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.91 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.48, %1811, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.91, %66, %73), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.92 : Float(17:3072, 4:768, 768:1) = aten::add(%query.9, %attn_out.10, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:568:0
  %1818 : Tensor = prim::GetAttr[name="bias"](%1706)
  %1819 : Tensor = prim::GetAttr[name="weight"](%1706)
  %1820 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.layer_norm
  %input.93 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.92, %1820, %1819, %1818, %37, %38), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1822 : __torch__.torch.nn.modules.normalization.___torch_mangle_1713.LayerNorm = prim::GetAttr[name="layer_norm"](%1704)
  %1823 : __torch__.torch.nn.modules.linear.___torch_mangle_1711.Linear = prim::GetAttr[name="linear_2"](%1704)
  %1824 : __torch__.torch.nn.modules.linear.___torch_mangle_1709.Linear = prim::GetAttr[name="linear_1"](%1704)
  %1825 : Tensor = prim::GetAttr[name="bias"](%1824)
  %1826 : Tensor = prim::GetAttr[name="weight"](%1824)
  %1827 : Float(768:1, 3072:768) = aten::t(%1826), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.93, %1827), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.10 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.49, %1825, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1830 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.10, %54), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1831 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.10, %53), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1832 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1831, %52), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1833 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.10, %1832, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1834 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1833, %51), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1835 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1834), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1836 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1835, %50, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.94 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1830, %1836), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.95 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.94, %49, %73), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1839 : Tensor = prim::GetAttr[name="bias"](%1823)
  %1840 : Tensor = prim::GetAttr[name="weight"](%1823)
  %1841 : Float(3072:1, 768:3072) = aten::t(%1840), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.95, %1841), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.96 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.50, %1839, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.96, %66, %73), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.97 : Float(17:3072, 4:768, 768:1) = aten::add(%input.93, %h.10, %79), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/modeling_funnel.py:588:0
  %1846 : Tensor = prim::GetAttr[name="bias"](%1822)
  %1847 : Tensor = prim::GetAttr[name="weight"](%1822)
  %1848 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.layer_norm
  %query.10 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.97, %1848, %1847, %1846, %37, %38), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1850 : __torch__.transformers.modeling_funnel.___torch_mangle_1729.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%85)
  %1851 : __torch__.transformers.modeling_funnel.___torch_mangle_1723.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%85)
  %1852 : __torch__.torch.nn.modules.normalization.___torch_mangle_1722.LayerNorm = prim::GetAttr[name="layer_norm"](%1851)
  %1853 : __torch__.torch.nn.modules.linear.___torch_mangle_1721.Linear = prim::GetAttr[name="post_proj"](%1851)
  %1854 : Tensor = prim::GetAttr[name="seg_embed"](%1851)
  %1855 : Tensor = prim::GetAttr[name="r_s_bias"](%1851)
  %1856 : Tensor = prim::GetAttr[name="r_kernel"](%1851)
  %1857 : Tensor = prim::GetAttr[name="r_r_bias"](%1851)
  %1858 : Tensor = prim::GetAttr[name="r_w_bias"](%1851)
  %1859 : __torch__.torch.nn.modules.linear.___torch_mangle_1720.Linear = prim::GetAttr[name="v_head"](%1851)
  %1860 : __torch__.torch.nn.modules.linear.___torch_mangle_1719.Linear = prim::GetAttr[name="k_head"](%1851)
  %1861 : __torch__.torch.nn.modules.linear.___torch_mangle_1718.Linear = prim::GetAttr[name="q_head"](%1851)
  %1862 : int = aten::size(%query.10, %78), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1863 : int = aten::size(%query.10, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1864 : int = aten::size(%query.10, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:531:0
  %1865 : Tensor = prim::GetAttr[name="weight"](%1861)
  %1866 : Float(768:1, 768:768) = aten::t(%1865), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1867 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1866), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1868 : int[] = prim::ListConstruct(%1862, %1863, %48, %47), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %q_head.21 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1867, %1868), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:535:0
  %1870 : Tensor = prim::GetAttr[name="bias"](%1860)
  %1871 : Tensor = prim::GetAttr[name="weight"](%1860)
  %1872 : Float(768:1, 768:768) = aten::t(%1871), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.51 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1872), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %1874 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.51, %1870, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1678:0
  %1875 : int[] = prim::ListConstruct(%1862, %1864, %48, %47), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1876 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1874, %1875), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:537:0
  %1877 : Tensor = prim::GetAttr[name="bias"](%1859)
  %1878 : Tensor = prim::GetAttr[name="weight"](%1859)
  %1879 : Float(768:1, 768:768) = aten::t(%1878), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.52 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1879), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %1881 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.52, %1877, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1678:0
  %1882 : int[] = prim::ListConstruct(%1862, %1864, %48, %47), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1883 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1881, %1882), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.22 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.21, %46), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.11 : Float(12:64, 64:1) = aten::mul(%1858, %46), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:542:0
  %1886 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_w_bias.11, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:544:0
  %1887 : Tensor[] = prim::ListConstruct(%1886, %1876), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %content_score.11 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%45, %1887), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %v.11 : Float(12:64, 64:1) = aten::mul(%1857, %46), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:486:0
  %1890 : Tensor[] = prim::ListConstruct(%245, %1856), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1891 : Float(8:768, 12:64, 64:1) = aten::einsum(%44, %1890), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1892 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %v.11, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:493:0
  %1893 : Tensor[] = prim::ListConstruct(%1892, %1891), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %positional_attn.61 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%43, %1893), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1895 : int = aten::size(%positional_attn.61, %78), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1896 : int = aten::size(%positional_attn.61, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1897 : int = aten::size(%positional_attn.61, %60), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1898 : int = aten::size(%positional_attn.61, %42), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.11 : Long() = prim::NumToTensor(%1898), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1900 : int[] = prim::ListConstruct(%1895, %1896, %1898, %1897), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %positional_attn.62 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.61, %1900), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:428:0
  %1902 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.62, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1903 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%1902, %79, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1904 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1903, %60, %79, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.63 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1904, %42, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1906 : Long() = aten::sub(%max_rel_len.11, %69, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %1907 : int = aten::Int(%1906), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1908 : int[] = prim::ListConstruct(%1895, %1896, %1897, %1907), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %positional_attn.64 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.63, %1908), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.65 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.64, %42, %78, %1864, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.66 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.65, %cls_mask), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:498:0
  %1912 : int = aten::size(%token_type_mat.15, %78), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1913 : int = aten::size(%token_type_mat.15, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1914 : int = aten::size(%token_type_mat.15, %60), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.11 : Float(12:64, 64:1) = aten::mul(%1855, %46), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:508:0
  %1916 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_s_bias.11, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:511:0
  %1917 : Tensor[] = prim::ListConstruct(%1916, %1854), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1918 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%41, %1917), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1919 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1920 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1919, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1921 : int = aten::size(%q_head.22, %60), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1922 : int[] = prim::ListConstruct(%1912, %1921, %1913, %1914), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %token_type_mat.17 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%1920, %1922, %73), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1924 : Tensor[] = aten::split(%1918, %79, %65), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/tensor.py:371:0
  %diff_token_type.11 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.11 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1924), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1927 : int = aten::size(%token_type_mat.17, %78), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1928 : int = aten::size(%token_type_mat.17, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1929 : int = aten::size(%token_type_mat.17, %60), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1930 : int = aten::size(%token_type_mat.17, %42), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1931 : int[] = prim::ListConstruct(%1927, %1928, %1929, %1930), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1932 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.11, %1931, %73), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1933 : int = aten::size(%token_type_mat.17, %78), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1934 : int = aten::size(%token_type_mat.17, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1935 : int = aten::size(%token_type_mat.17, %60), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1936 : int = aten::size(%token_type_mat.17, %42), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1937 : int[] = prim::ListConstruct(%1933, %1934, %1935, %1936), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1938 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.11, %1937, %73), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.21 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.17, %1932, %1938), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.22 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.21, %cls_mask), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:522:0
  %1941 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.11, %positional_attn.66, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.31 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%1941, %token_type_attn.22, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.32 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.31, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:553:0
  %1944 : Float(17:4, 4:1) = aten::slice(%attention_mask, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1945 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%1944, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1946 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1945, %60), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1947 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%1946, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1948 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%1947, %79, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/tensor.py:396:0
  %1949 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%1948, %40), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.33 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.32, %1949, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %input.98 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.33, %65, %75), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:558:0
  %1952 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.98, %66, %73), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1953 : Tensor[] = prim::ListConstruct(%1952, %1883), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %attn_vec.11 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%39, %1953), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1955 : int[] = prim::ListConstruct(%1862, %1863, %62), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %input.99 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.11, %1955), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:565:0
  %1957 : Tensor = prim::GetAttr[name="bias"](%1853)
  %1958 : Tensor = prim::GetAttr[name="weight"](%1853)
  %1959 : Float(768:1, 768:768) = aten::t(%1958), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.53 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.99, %1959), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.100 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.53, %1957, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.100, %66, %73), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.101 : Float(17:3072, 4:768, 768:1) = aten::add(%query.10, %attn_out.11, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:568:0
  %1964 : Tensor = prim::GetAttr[name="bias"](%1852)
  %1965 : Tensor = prim::GetAttr[name="weight"](%1852)
  %1966 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.layer_norm
  %input.102 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.101, %1966, %1965, %1964, %37, %38), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1968 : __torch__.torch.nn.modules.normalization.___torch_mangle_1728.LayerNorm = prim::GetAttr[name="layer_norm"](%1850)
  %1969 : __torch__.torch.nn.modules.linear.___torch_mangle_1726.Linear = prim::GetAttr[name="linear_2"](%1850)
  %1970 : __torch__.torch.nn.modules.linear.___torch_mangle_1724.Linear = prim::GetAttr[name="linear_1"](%1850)
  %1971 : Tensor = prim::GetAttr[name="bias"](%1970)
  %1972 : Tensor = prim::GetAttr[name="weight"](%1970)
  %1973 : Float(768:1, 3072:768) = aten::t(%1972), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.102, %1973), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.11 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.54, %1971, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1976 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.11, %54), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1977 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.11, %53), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1978 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1977, %52), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1979 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.11, %1978, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1980 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1979, %51), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1981 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1980), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1982 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1981, %50, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.103 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1976, %1982), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.104 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.103, %49, %73), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1985 : Tensor = prim::GetAttr[name="bias"](%1969)
  %1986 : Tensor = prim::GetAttr[name="weight"](%1969)
  %1987 : Float(3072:1, 768:3072) = aten::t(%1986), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.55 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.104, %1987), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.105 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.55, %1985, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.105, %66, %73), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.106 : Float(17:3072, 4:768, 768:1) = aten::add(%input.102, %h.11, %79), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/modeling_funnel.py:588:0
  %1992 : Tensor = prim::GetAttr[name="bias"](%1968)
  %1993 : Tensor = prim::GetAttr[name="weight"](%1968)
  %1994 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.layer_norm
  %query : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.106, %1994, %1993, %1992, %37, %38), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1996 : __torch__.transformers.modeling_funnel.___torch_mangle_1744.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%82)
  %1997 : __torch__.transformers.modeling_funnel.___torch_mangle_1738.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%82)
  %1998 : __torch__.torch.nn.modules.normalization.___torch_mangle_1737.LayerNorm = prim::GetAttr[name="layer_norm"](%1997)
  %1999 : __torch__.torch.nn.modules.linear.___torch_mangle_1736.Linear = prim::GetAttr[name="post_proj"](%1997)
  %2000 : Tensor = prim::GetAttr[name="seg_embed"](%1997)
  %2001 : Tensor = prim::GetAttr[name="r_s_bias"](%1997)
  %2002 : Tensor = prim::GetAttr[name="r_kernel"](%1997)
  %2003 : Tensor = prim::GetAttr[name="r_r_bias"](%1997)
  %2004 : Tensor = prim::GetAttr[name="r_w_bias"](%1997)
  %2005 : __torch__.torch.nn.modules.linear.___torch_mangle_1735.Linear = prim::GetAttr[name="v_head"](%1997)
  %2006 : __torch__.torch.nn.modules.linear.___torch_mangle_1734.Linear = prim::GetAttr[name="k_head"](%1997)
  %2007 : __torch__.torch.nn.modules.linear.___torch_mangle_1733.Linear = prim::GetAttr[name="q_head"](%1997)
  %2008 : int = aten::size(%query, %78), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %2009 : int = aten::size(%query, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %2010 : int = aten::size(%query, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:531:0
  %2011 : Tensor = prim::GetAttr[name="weight"](%2007)
  %2012 : Float(768:1, 768:768) = aten::t(%2011), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2013 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query, %2012), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2014 : int[] = prim::ListConstruct(%2008, %2009, %48, %47), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %q_head.23 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2013, %2014), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:535:0
  %2016 : Tensor = prim::GetAttr[name="bias"](%2006)
  %2017 : Tensor = prim::GetAttr[name="weight"](%2006)
  %2018 : Float(768:1, 768:768) = aten::t(%2017), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.56 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query, %2018), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %2020 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.56, %2016, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1678:0
  %2021 : int[] = prim::ListConstruct(%2008, %2010, %48, %47), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2022 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2020, %2021), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:537:0
  %2023 : Tensor = prim::GetAttr[name="bias"](%2005)
  %2024 : Tensor = prim::GetAttr[name="weight"](%2005)
  %2025 : Float(768:1, 768:768) = aten::t(%2024), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.57 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query, %2025), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %2027 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.57, %2023, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1678:0
  %2028 : int[] = prim::ListConstruct(%2008, %2010, %48, %47), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2029 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2027, %2028), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:538:0
  %q_head : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.23, %46), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias : Float(12:64, 64:1) = aten::mul(%2004, %46), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:542:0
  %2032 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head, %r_w_bias, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:544:0
  %2033 : Tensor[] = prim::ListConstruct(%2032, %2022), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %content_score : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%45, %2033), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %v : Float(12:64, 64:1) = aten::mul(%2003, %46), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:486:0
  %2036 : Tensor[] = prim::ListConstruct(%245, %2002), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2037 : Float(8:768, 12:64, 64:1) = aten::einsum(%44, %2036), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2038 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head, %v, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:493:0
  %2039 : Tensor[] = prim::ListConstruct(%2038, %2037), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %positional_attn.67 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%43, %2039), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2041 : int = aten::size(%positional_attn.67, %78), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2042 : int = aten::size(%positional_attn.67, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2043 : int = aten::size(%positional_attn.67, %60), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2044 : int = aten::size(%positional_attn.67, %42), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len : Long() = prim::NumToTensor(%2044), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2046 : int[] = prim::ListConstruct(%2041, %2042, %2044, %2043), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %positional_attn.68 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.67, %2046), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:428:0
  %2048 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.68, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2049 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%2048, %79, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2050 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%2049, %60, %79, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.69 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%2050, %42, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2052 : Long() = aten::sub(%max_rel_len, %69, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %2053 : int = aten::Int(%2052), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2054 : int[] = prim::ListConstruct(%2041, %2042, %2043, %2053), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %positional_attn.70 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.69, %2054), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.71 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.70, %42, %78, %2010, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.71, %cls_mask), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:498:0
  %2058 : int = aten::size(%token_type_mat.15, %78), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2059 : int = aten::size(%token_type_mat.15, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2060 : int = aten::size(%token_type_mat.15, %60), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias : Float(12:64, 64:1) = aten::mul(%2001, %46), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:508:0
  %2062 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head, %r_s_bias, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:511:0
  %2063 : Tensor[] = prim::ListConstruct(%2062, %2000), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2064 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%41, %2063), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2065 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2066 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%2065, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2067 : int = aten::size(%q_head, %60), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2068 : int[] = prim::ListConstruct(%2058, %2067, %2059, %2060), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %token_type_mat : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%2066, %2068, %73), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2070 : Tensor[] = aten::split(%2064, %79, %65), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/tensor.py:371:0
  %diff_token_type : Float(17:8, 12:136, 4:2, 1:1), %same_token_type : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%2070), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2073 : int = aten::size(%token_type_mat, %78), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2074 : int = aten::size(%token_type_mat, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2075 : int = aten::size(%token_type_mat, %60), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2076 : int = aten::size(%token_type_mat, %42), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2077 : int[] = prim::ListConstruct(%2073, %2074, %2075, %2076), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2078 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type, %2077, %73), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2079 : int = aten::size(%token_type_mat, %78), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2080 : int = aten::size(%token_type_mat, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2081 : int = aten::size(%token_type_mat, %60), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2082 : int = aten::size(%token_type_mat, %42), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2083 : int[] = prim::ListConstruct(%2079, %2080, %2081, %2082), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2084 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type, %2083, %73), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.23 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat, %2078, %2084), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.23, %cls_mask), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:522:0
  %2087 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score, %positional_attn, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.34 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%2087, %token_type_attn, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.35 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.34, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:553:0
  %2090 : Float(17:4, 4:1) = aten::slice(%attention_mask, %78, %78, %67, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2091 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%2090, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2092 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%2091, %60), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2093 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%2092, %75, %73, %73, %70), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2094 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%2093, %79, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/tensor.py:396:0
  %2095 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%2094, %40), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.35, %2095, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %input.107 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score, %65, %75), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:558:0
  %2098 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.107, %66, %73), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %2099 : Tensor[] = prim::ListConstruct(%2098, %2029), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %attn_vec : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%39, %2099), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2101 : int[] = prim::ListConstruct(%2008, %2009, %62), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %input.108 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec, %2101), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:565:0
  %2103 : Tensor = prim::GetAttr[name="bias"](%1999)
  %2104 : Tensor = prim::GetAttr[name="weight"](%1999)
  %2105 : Float(768:1, 768:768) = aten::t(%2104), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.58 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.108, %2105), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.109 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.58, %2103, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.109, %66, %73), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:3072, 4:768, 768:1) = aten::add(%query, %attn_out, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:568:0
  %2110 : Tensor = prim::GetAttr[name="bias"](%1998)
  %2111 : Tensor = prim::GetAttr[name="weight"](%1998)
  %2112 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.layer_norm
  %input.111 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.110, %2112, %2111, %2110, %37, %38), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %2114 : __torch__.torch.nn.modules.normalization.___torch_mangle_1743.LayerNorm = prim::GetAttr[name="layer_norm"](%1996)
  %2115 : __torch__.torch.nn.modules.linear.___torch_mangle_1741.Linear = prim::GetAttr[name="linear_2"](%1996)
  %2116 : __torch__.torch.nn.modules.linear.___torch_mangle_1739.Linear = prim::GetAttr[name="linear_1"](%1996)
  %2117 : Tensor = prim::GetAttr[name="bias"](%2116)
  %2118 : Tensor = prim::GetAttr[name="weight"](%2116)
  %2119 : Float(768:1, 3072:768) = aten::t(%2118), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.111, %2119), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.59, %2117, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2122 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x, %54), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2123 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x, %53), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2124 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2123, %52), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2125 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x, %2124, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2126 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2125, %51), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2127 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%2126), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2128 : Float(17:12288, 4:3072, 3072:1) = aten::add(%2127, %50, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.112 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2122, %2128), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.113 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.112, %49, %73), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2131 : Tensor = prim::GetAttr[name="bias"](%2115)
  %2132 : Tensor = prim::GetAttr[name="weight"](%2115)
  %2133 : Float(3072:1, 768:3072) = aten::t(%2132), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.113, %2133), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.114 : Float(17:3072, 4:768, 768:1) = aten::add_(%output, %2131, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.114, %66, %73), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.dropout # torch/nn/functional.py:973:0
  %input : Float(17:3072, 4:768, 768:1) = aten::add(%input.111, %h, %79), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/modeling_funnel.py:588:0
  %2138 : Tensor = prim::GetAttr[name="bias"](%2114)
  %2139 : Tensor = prim::GetAttr[name="weight"](%2114)
  %2140 : int[] = prim::ListConstruct(%62), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.layer_norm
  %2141 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input, %2140, %2139, %2138, %37, %38), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %21 : (Float(17:3072, 4:768, 768:1)) = prim::TupleConstruct(%2141)
  return (%21)
