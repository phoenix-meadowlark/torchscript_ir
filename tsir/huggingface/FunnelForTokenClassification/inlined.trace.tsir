graph(%self.1 : __torch__.transformers.modeling_funnel.FunnelForTokenClassification,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_4337.Linear = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.torch.nn.modules.dropout.___torch_mangle_4336.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %5 : __torch__.transformers.modeling_funnel.___torch_mangle_4335.FunnelModel = prim::GetAttr[name="funnel"](%self.1)
  %10 : int = prim::Constant[value=384](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %11 : float = prim::Constant[value=1.](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %12 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %13 : Long() = prim::Constant[value={384}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:248:0
  %14 : Float() = prim::Constant[value={10000}](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %15 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder
  %16 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %17 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %18 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %19 : Long() = prim::Constant[value={13}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %20 : Float(1:1) = prim::Constant[value={-1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %21 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %22 : Long() = prim::Constant[value={14}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %23 : int = prim::Constant[value=-2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %24 : Float(1:1) = prim::Constant[value={-3}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %25 : Long() = prim::Constant[value={16}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %26 : int = prim::Constant[value=-4](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %27 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %28 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %29 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %30 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %31 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %32 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %33 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %34 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %35 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %36 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %37 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %38 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %39 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %40 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %41 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %42 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %43 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %44 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %45 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %46 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %47 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
  %48 : bool = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
  %49 : Device = prim::Constant[value="cpu"](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
  %50 : int = prim::Constant[value=4](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
  %51 : int = prim::Constant[value=1](), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
  %52 : int = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
  %53 : __torch__.transformers.modeling_funnel.___torch_mangle_4334.FunnelDecoder = prim::GetAttr[name="decoder"](%5)
  %54 : __torch__.transformers.modeling_funnel.___torch_mangle_4299.FunnelEncoder = prim::GetAttr[name="encoder"](%5)
  %55 : __torch__.transformers.modeling_funnel.___torch_mangle_4111.FunnelEmbeddings = prim::GetAttr[name="embeddings"](%5)
  %56 : int = aten::size(%input_ids, %52), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
  %57 : int = aten::size(%input_ids, %51), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
  %58 : int[] = prim::ListConstruct(%56, %57), scope: __module.funnel
  %token_type_ids : Long(17:13, 13:1) = aten::zeros(%58, %50, %52, %49, %48), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
  %60 : __torch__.torch.nn.modules.normalization.___torch_mangle_4109.LayerNorm = prim::GetAttr[name="layer_norm"](%55)
  %61 : __torch__.torch.nn.modules.sparse.___torch_mangle_4108.Embedding = prim::GetAttr[name="word_embeddings"](%55)
  %62 : Tensor = prim::GetAttr[name="weight"](%61)
  %input.1 : Float(17:9984, 13:768, 768:1) = aten::embedding(%62, %input_ids, %43, %48, %48), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %64 : Tensor = prim::GetAttr[name="bias"](%60)
  %65 : Tensor = prim::GetAttr[name="weight"](%60)
  %66 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm
  %input.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.1, %66, %65, %64, %45, %44), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.2, %47, %48), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
  %69 : __torch__.torch.nn.modules.container.___torch_mangle_4298.ModuleList = prim::GetAttr[name="blocks"](%54)
  %70 : __torch__.torch.nn.modules.container.___torch_mangle_4297.ModuleList = prim::GetAttr[name="2"](%69)
  %71 : __torch__.transformers.modeling_funnel.___torch_mangle_4296.FunnelLayer = prim::GetAttr[name="3"](%70)
  %72 : __torch__.torch.nn.modules.container.___torch_mangle_4298.ModuleList = prim::GetAttr[name="blocks"](%54)
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_4297.ModuleList = prim::GetAttr[name="2"](%72)
  %74 : __torch__.transformers.modeling_funnel.___torch_mangle_4281.FunnelLayer = prim::GetAttr[name="2"](%73)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_4298.ModuleList = prim::GetAttr[name="blocks"](%54)
  %76 : __torch__.torch.nn.modules.container.___torch_mangle_4297.ModuleList = prim::GetAttr[name="2"](%75)
  %77 : __torch__.transformers.modeling_funnel.___torch_mangle_4266.FunnelLayer = prim::GetAttr[name="1"](%76)
  %78 : __torch__.torch.nn.modules.container.___torch_mangle_4298.ModuleList = prim::GetAttr[name="blocks"](%54)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_4297.ModuleList = prim::GetAttr[name="2"](%78)
  %80 : __torch__.transformers.modeling_funnel.___torch_mangle_4251.FunnelLayer = prim::GetAttr[name="0"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_4298.ModuleList = prim::GetAttr[name="blocks"](%54)
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_4236.ModuleList = prim::GetAttr[name="1"](%81)
  %83 : __torch__.transformers.modeling_funnel.___torch_mangle_4235.FunnelLayer = prim::GetAttr[name="3"](%82)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_4298.ModuleList = prim::GetAttr[name="blocks"](%54)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_4236.ModuleList = prim::GetAttr[name="1"](%84)
  %86 : __torch__.transformers.modeling_funnel.___torch_mangle_4220.FunnelLayer = prim::GetAttr[name="2"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_4298.ModuleList = prim::GetAttr[name="blocks"](%54)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_4236.ModuleList = prim::GetAttr[name="1"](%87)
  %89 : __torch__.transformers.modeling_funnel.___torch_mangle_4205.FunnelLayer = prim::GetAttr[name="1"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_4298.ModuleList = prim::GetAttr[name="blocks"](%54)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_4236.ModuleList = prim::GetAttr[name="1"](%90)
  %92 : __torch__.transformers.modeling_funnel.___torch_mangle_4190.FunnelLayer = prim::GetAttr[name="0"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_4298.ModuleList = prim::GetAttr[name="blocks"](%54)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_4175.ModuleList = prim::GetAttr[name="0"](%93)
  %95 : __torch__.transformers.modeling_funnel.___torch_mangle_4174.FunnelLayer = prim::GetAttr[name="3"](%94)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_4298.ModuleList = prim::GetAttr[name="blocks"](%54)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_4175.ModuleList = prim::GetAttr[name="0"](%96)
  %98 : __torch__.transformers.modeling_funnel.___torch_mangle_4159.FunnelLayer = prim::GetAttr[name="2"](%97)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_4298.ModuleList = prim::GetAttr[name="blocks"](%54)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_4175.ModuleList = prim::GetAttr[name="0"](%99)
  %101 : __torch__.transformers.modeling_funnel.___torch_mangle_4144.FunnelLayer = prim::GetAttr[name="1"](%100)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_4298.ModuleList = prim::GetAttr[name="blocks"](%54)
  %103 : __torch__.torch.nn.modules.container.___torch_mangle_4175.ModuleList = prim::GetAttr[name="0"](%102)
  %104 : __torch__.transformers.modeling_funnel.___torch_mangle_4129.FunnelLayer = prim::GetAttr[name="0"](%103)
  %attention_mask.2 : Float(17:13, 13:1) = aten::type_as(%attention_mask.1, %inputs_embeds), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:625:0
  %106 : int = aten::size(%inputs_embeds, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:195:0
  %seq_len.1 : Long() = prim::NumToTensor(%106), scope: __module.funnel/__module.funnel.encoder
  %freq_seq.1 : Float(384:1) = aten::arange(%52, %10, %11, %12, %52, %49, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %109 : Float(384:1) = aten::div(%freq_seq.1, %13), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:248:0
  %110 : Float() = aten::to(%14, %49, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %111 : Float() = aten::detach(%110), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %112 : Float(384:1) = aten::pow(%111, %109), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %113 : Float(384:1) = aten::reciprocal(%112), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %inv_freq.1 : Float(384:1) = aten::mul(%113, %16), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %115 : Long() = aten::neg(%seq_len.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %116 : Long() = aten::mul(%115, %17), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %117 : Scalar = aten::ScalarImplicit(%116), scope: __module.funnel/__module.funnel.encoder
  %118 : Long() = aten::mul(%seq_len.1, %17), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %119 : Scalar = aten::ScalarImplicit(%118), scope: __module.funnel/__module.funnel.encoder
  %rel_pos_id.1 : Float(52:1) = aten::arange(%117, %119, %11, %12, %52, %49, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %zero_offset.1 : Long() = aten::mul(%seq_len.1, %17), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:251:0
  %122 : Float(52:1) = aten::slice(%rel_pos_id.1, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %123 : Float(52:1, 1:1) = aten::unsqueeze(%122, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %124 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq.1, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %sinusoid.1 : Float(52:384, 384:1) = aten::mul(%123, %124), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %input.3 : Float(52:384, 384:1) = aten::sin(%sinusoid.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:253:0
  %sin_embed.1 : Float(52:384, 384:1) = aten::dropout(%input.3, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %input.4 : Float(52:384, 384:1) = aten::cos(%sinusoid.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:254:0
  %cos_embed.1 : Float(52:384, 384:1) = aten::dropout(%input.4, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
  %130 : Tensor[] = prim::ListConstruct(%sin_embed.1, %cos_embed.1), scope: __module.funnel/__module.funnel.encoder
  %pos_embed.1 : Float(52:768, 768:1) = aten::cat(%130, %43), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:255:0
  %pos.1 : Float(13:1) = aten::arange(%52, %106, %51, %12, %52, %49, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:257:0
  %133 : Float() = aten::select(%pos.1, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %134 : Float() = aten::select(%pos.1, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.1 : Float() = aten::sub(%133, %134, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.1 : Float() = aten::add(%ref_point.1, %19, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %137 : Scalar = aten::ScalarImplicit(%max_dist.1), scope: __module.funnel/__module.funnel.encoder
  %138 : Float() = aten::select(%pos.1, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %139 : Float() = aten::select(%pos.1, %52, %43), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.1 : Float() = aten::sub(%138, %139, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %141 : Float() = aten::sub(%min_dist.1, %16, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %142 : Scalar = aten::ScalarImplicit(%141), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.1 : Long(26:1) = aten::arange(%137, %142, %43, %50, %52, %49, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %144 : Long(26:1) = aten::slice(%rel_pos.1, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %145 : Long(26:1, 1:1) = aten::unsqueeze(%144, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.2 : Long(26:1, 1:1) = aten::add(%145, %zero_offset.1, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %147 : int = aten::size(%rel_pos.2, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %148 : int[] = prim::ListConstruct(%147, %46), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.3 : Long(26:1, 768:0) = aten::expand(%rel_pos.2, %148, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %150 : Float(26:768, 768:1) = aten::gather(%pos_embed.1, %52, %rel_pos.3, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %151 : Float(1:1) = aten::to(%20, %49, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos.1 : Float(1:1) = aten::detach(%151), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id.1 : Float(11:1) = aten::slice(%pos.1, %52, %51, %43, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
  %154 : Float(6:2) = aten::slice(%pooled_pos_id.1, %52, %52, %18, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %155 : Tensor[] = prim::ListConstruct(%cls_pos.1, %154), scope: __module.funnel/__module.funnel.encoder
  %pooled_pos.1 : Float(7:1) = aten::cat(%155, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %157 : Float() = aten::select(%pooled_pos.1, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %158 : Float() = aten::select(%pos.1, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.2 : Float() = aten::sub(%157, %158, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.2 : Float() = aten::add(%ref_point.2, %22, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %161 : Scalar = aten::ScalarImplicit(%max_dist.2), scope: __module.funnel/__module.funnel.encoder
  %162 : Float() = aten::select(%pooled_pos.1, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %163 : Float() = aten::select(%pos.1, %52, %43), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.2 : Float() = aten::sub(%162, %163, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %165 : Float() = aten::sub(%min_dist.2, %16, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %166 : Scalar = aten::ScalarImplicit(%165), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.4 : Long(27:1) = aten::arange(%161, %166, %43, %50, %52, %49, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %168 : Long(27:1) = aten::slice(%rel_pos.4, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %169 : Long(27:1, 1:1) = aten::unsqueeze(%168, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.5 : Long(27:1, 1:1) = aten::add(%169, %zero_offset.1, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %171 : int = aten::size(%rel_pos.5, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %172 : int[] = prim::ListConstruct(%171, %46), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.6 : Long(27:1, 768:0) = aten::expand(%rel_pos.5, %172, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %174 : Float(27:768, 768:1) = aten::gather(%pos_embed.1, %52, %rel_pos.6, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
  %175 : Float() = aten::select(%pooled_pos.1, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %176 : Float() = aten::select(%pooled_pos.1, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.3 : Float() = aten::sub(%175, %176, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.3 : Float() = aten::add(%ref_point.3, %22, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %179 : Scalar = aten::ScalarImplicit(%max_dist.3), scope: __module.funnel/__module.funnel.encoder
  %180 : Float() = aten::select(%pooled_pos.1, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %181 : Float() = aten::select(%pooled_pos.1, %52, %43), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.3 : Float() = aten::sub(%180, %181, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %183 : Float() = aten::sub(%min_dist.3, %16, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %184 : Scalar = aten::ScalarImplicit(%183), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.7 : Long(14:1) = aten::arange(%179, %184, %23, %50, %52, %49, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %186 : Long(14:1) = aten::slice(%rel_pos.7, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %187 : Long(14:1, 1:1) = aten::unsqueeze(%186, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.8 : Long(14:1, 1:1) = aten::add(%187, %zero_offset.1, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %189 : int = aten::size(%rel_pos.8, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %190 : int[] = prim::ListConstruct(%189, %46), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.9 : Long(14:1, 768:0) = aten::expand(%rel_pos.8, %190, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %192 : Float(14:768, 768:1) = aten::gather(%pos_embed.1, %52, %rel_pos.9, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %193 : Float(1:1) = aten::to(%24, %49, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos.2 : Float(1:1) = aten::detach(%193), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id.2 : Float(5:1) = aten::slice(%pooled_pos.1, %52, %51, %43, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
  %196 : Float(3:2) = aten::slice(%pooled_pos_id.2, %52, %52, %18, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %197 : Tensor[] = prim::ListConstruct(%cls_pos.2, %196), scope: __module.funnel/__module.funnel.encoder
  %pooled_pos.2 : Float(4:1) = aten::cat(%197, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %199 : Float() = aten::select(%pooled_pos.2, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %200 : Float() = aten::select(%pooled_pos.1, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.4 : Float() = aten::sub(%199, %200, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.4 : Float() = aten::add(%ref_point.4, %25, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %203 : Scalar = aten::ScalarImplicit(%max_dist.4), scope: __module.funnel/__module.funnel.encoder
  %204 : Float() = aten::select(%pooled_pos.2, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %205 : Float() = aten::select(%pooled_pos.1, %52, %43), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.4 : Float() = aten::sub(%204, %205, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %207 : Float() = aten::sub(%min_dist.4, %16, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %208 : Scalar = aten::ScalarImplicit(%207), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.10 : Long(15:1) = aten::arange(%203, %208, %23, %50, %52, %49, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %210 : Long(15:1) = aten::slice(%rel_pos.10, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %211 : Long(15:1, 1:1) = aten::unsqueeze(%210, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.11 : Long(15:1, 1:1) = aten::add(%211, %zero_offset.1, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %213 : int = aten::size(%rel_pos.11, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %214 : int[] = prim::ListConstruct(%213, %46), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.12 : Long(15:1, 768:0) = aten::expand(%rel_pos.11, %214, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %216 : Float(15:768, 768:1) = aten::gather(%pos_embed.1, %52, %rel_pos.12, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
  %217 : Float() = aten::select(%pooled_pos.2, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %218 : Float() = aten::select(%pooled_pos.2, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.5 : Float() = aten::sub(%217, %218, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.5 : Float() = aten::add(%ref_point.5, %25, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %221 : Scalar = aten::ScalarImplicit(%max_dist.5), scope: __module.funnel/__module.funnel.encoder
  %222 : Float() = aten::select(%pooled_pos.2, %52, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %223 : Float() = aten::select(%pooled_pos.2, %52, %43), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.5 : Float() = aten::sub(%222, %223, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %225 : Float() = aten::sub(%min_dist.5, %16, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %226 : Scalar = aten::ScalarImplicit(%225), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.13 : Long(8:1) = aten::arange(%221, %226, %26, %50, %52, %49, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %228 : Long(8:1) = aten::slice(%rel_pos.13, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %229 : Long(8:1, 1:1) = aten::unsqueeze(%228, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.14 : Long(8:1, 1:1) = aten::add(%229, %zero_offset.1, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %231 : int = aten::size(%rel_pos.14, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %232 : int[] = prim::ListConstruct(%231, %46), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.15 : Long(8:1, 768:0) = aten::expand(%rel_pos.14, %232, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %234 : Float(8:768, 768:1) = aten::gather(%pos_embed.1, %52, %rel_pos.15, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %235 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %236 : Long(17:13, 13:1) = aten::slice(%235, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %237 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%236, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %238 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %239 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%238, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %token_type_mat.1 : Bool(17:169, 13:13, 13:1) = aten::eq(%237, %239), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
  %cls_ids.1 : Bool(17:13, 13:1) = aten::eq(%token_type_ids, %21), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
  %242 : Bool(17:13, 13:1) = aten::slice(%cls_ids.1, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %243 : Bool(17:13, 13:1) = aten::slice(%242, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %244 : Bool(17:13, 13:1, 1:1) = aten::unsqueeze(%243, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %245 : Bool(17:13, 13:1) = aten::slice(%cls_ids.1, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %246 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%245, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %cls_mat.1 : Bool(17:169, 13:13, 13:1) = aten::__or__(%244, %246), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %token_type_mat.2 : Bool(17:169, 13:13, 13:1) = aten::__or__(%cls_mat.1, %token_type_mat.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:211:0
  %249 : Long() = aten::sub(%seq_len.1, %16, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %250 : int = aten::Int(%249), scope: __module.funnel/__module.funnel.encoder
  %251 : Long() = aten::sub(%seq_len.1, %16, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %252 : int = aten::Int(%251), scope: __module.funnel/__module.funnel.encoder
  %253 : int[] = prim::ListConstruct(%250, %252), scope: __module.funnel/__module.funnel.encoder
  %input.5 : Float(12:12, 12:1) = aten::ones(%253, %12, %52, %49, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %255 : int[] = prim::ListConstruct(%51, %52, %51, %52), scope: __module.funnel/__module.funnel.encoder
  %cls_mask.1 : Float(13:13, 13:1) = aten::constant_pad_nd(%input.5, %255, %52), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:3552:0
  %257 : __torch__.transformers.modeling_funnel.___torch_mangle_4128.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%104)
  %258 : __torch__.transformers.modeling_funnel.___torch_mangle_4122.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%104)
  %259 : __torch__.torch.nn.modules.normalization.___torch_mangle_4121.LayerNorm = prim::GetAttr[name="layer_norm"](%258)
  %260 : __torch__.torch.nn.modules.linear.___torch_mangle_4120.Linear = prim::GetAttr[name="post_proj"](%258)
  %261 : Tensor = prim::GetAttr[name="seg_embed"](%258)
  %262 : Tensor = prim::GetAttr[name="r_s_bias"](%258)
  %263 : Tensor = prim::GetAttr[name="r_kernel"](%258)
  %264 : Tensor = prim::GetAttr[name="r_r_bias"](%258)
  %265 : Tensor = prim::GetAttr[name="r_w_bias"](%258)
  %266 : __torch__.torch.nn.modules.linear.___torch_mangle_4119.Linear = prim::GetAttr[name="v_head"](%258)
  %267 : __torch__.torch.nn.modules.linear.___torch_mangle_4118.Linear = prim::GetAttr[name="k_head"](%258)
  %268 : __torch__.torch.nn.modules.linear.___torch_mangle_4117.Linear = prim::GetAttr[name="q_head"](%258)
  %269 : int = aten::size(%inputs_embeds, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %270 : int = aten::size(%inputs_embeds, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %271 : int = aten::size(%inputs_embeds, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:531:0
  %272 : Tensor = prim::GetAttr[name="weight"](%268)
  %273 : Float(768:1, 768:768) = aten::t(%272), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %274 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %275 : int[] = prim::ListConstruct(%269, %270, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %q_head.1 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%274, %275), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %277 : Tensor = prim::GetAttr[name="bias"](%267)
  %278 : Tensor = prim::GetAttr[name="weight"](%267)
  %279 : Float(768:1, 768:768) = aten::t(%278), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %279), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %281 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %277, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1678:0
  %282 : int[] = prim::ListConstruct(%269, %271, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %283 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%281, %282), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:537:0
  %284 : Tensor = prim::GetAttr[name="bias"](%266)
  %285 : Tensor = prim::GetAttr[name="weight"](%266)
  %286 : Float(768:1, 768:768) = aten::t(%285), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %286), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %288 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %284, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1678:0
  %289 : int[] = prim::ListConstruct(%269, %271, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %290 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%288, %289), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.1, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.1 : Float(12:64, 64:1) = aten::mul(%265, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:542:0
  %293 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_w_bias.1, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:544:0
  %294 : Tensor[] = prim::ListConstruct(%293, %283), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %content_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%36, %294), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %v.1 : Float(12:64, 64:1) = aten::mul(%264, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:486:0
  %297 : Tensor[] = prim::ListConstruct(%150, %263), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %298 : Float(26:768, 12:64, 64:1) = aten::einsum(%37, %297), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %299 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %v.1, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:493:0
  %300 : Tensor[] = prim::ListConstruct(%299, %298), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.1 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%38, %300), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %302 : int = aten::size(%positional_attn.1, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %303 : int = aten::size(%positional_attn.1, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %304 : int = aten::size(%positional_attn.1, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %305 : int = aten::size(%positional_attn.1, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.1 : Long() = prim::NumToTensor(%305), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %307 : int[] = prim::ListConstruct(%302, %303, %305, %304), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.2 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.1, %307), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:428:0
  %309 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %310 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%309, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %311 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%310, %21, %51, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.3 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%311, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %313 : Long() = aten::sub(%max_rel_len.1, %16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %314 : int = aten::Int(%313), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %315 : int[] = prim::ListConstruct(%302, %303, %304, %314), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.4 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.3, %315), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.5 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.4, %39, %52, %271, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.6 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.5, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:498:0
  %319 : int = aten::size(%token_type_mat.2, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %320 : int = aten::size(%token_type_mat.2, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %321 : int = aten::size(%token_type_mat.2, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.1 : Float(12:64, 64:1) = aten::mul(%262, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:508:0
  %323 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_s_bias.1, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:511:0
  %324 : Tensor[] = prim::ListConstruct(%323, %261), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %325 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%40, %324), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %326 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %327 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%326, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %328 : int = aten::size(%q_head.2, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %329 : int[] = prim::ListConstruct(%319, %328, %320, %321), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %token_type_mat.3 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%327, %329, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %331 : Tensor[] = aten::split(%325, %51, %43), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:371:0
  %diff_token_type.1 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.1 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%331), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %334 : int = aten::size(%token_type_mat.3, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %335 : int = aten::size(%token_type_mat.3, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %336 : int = aten::size(%token_type_mat.3, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %337 : int = aten::size(%token_type_mat.3, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %338 : int[] = prim::ListConstruct(%334, %335, %336, %337), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %339 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.1, %338, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %340 : int = aten::size(%token_type_mat.3, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %341 : int = aten::size(%token_type_mat.3, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %342 : int = aten::size(%token_type_mat.3, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %343 : int = aten::size(%token_type_mat.3, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %344 : int[] = prim::ListConstruct(%340, %341, %342, %343), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %345 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.1, %344, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.3, %339, %345), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.1, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:522:0
  %348 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.1, %positional_attn.6, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%348, %token_type_attn.2, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.1, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:553:0
  %351 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %352 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%351, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %353 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%352, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %354 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%353, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %355 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%354, %51, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:396:0
  %356 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%355, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.2, %356, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.3, %43, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:558:0
  %359 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.6, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %360 : Tensor[] = prim::ListConstruct(%359, %290), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %attn_vec.1 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%42, %360), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %362 : int[] = prim::ListConstruct(%269, %270, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %input.7 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.1, %362), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:565:0
  %364 : Tensor = prim::GetAttr[name="bias"](%260)
  %365 : Tensor = prim::GetAttr[name="weight"](%260)
  %366 : Float(768:1, 768:768) = aten::t(%365), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.7, %366), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %364, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.8, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %attn_out.1, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:568:0
  %371 : Tensor = prim::GetAttr[name="bias"](%259)
  %372 : Tensor = prim::GetAttr[name="weight"](%259)
  %373 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.9, %373, %372, %371, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %375 : __torch__.torch.nn.modules.normalization.___torch_mangle_4127.LayerNorm = prim::GetAttr[name="layer_norm"](%257)
  %376 : __torch__.torch.nn.modules.linear.___torch_mangle_4125.Linear = prim::GetAttr[name="linear_2"](%257)
  %377 : __torch__.torch.nn.modules.linear.___torch_mangle_4123.Linear = prim::GetAttr[name="linear_1"](%257)
  %378 : Tensor = prim::GetAttr[name="bias"](%377)
  %379 : Tensor = prim::GetAttr[name="weight"](%377)
  %380 : Float(768:1, 3072:768) = aten::t(%379), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.10, %380), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.1 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.4, %378, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %383 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.1, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %384 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.1, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %385 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%384, %29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %386 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.1, %385, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %387 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%386, %30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %388 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%387), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %389 : Float(17:39936, 13:3072, 3072:1) = aten::add(%388, %31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%383, %389), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.11, %32, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %392 : Tensor = prim::GetAttr[name="bias"](%376)
  %393 : Tensor = prim::GetAttr[name="weight"](%376)
  %394 : Float(3072:1, 768:3072) = aten::t(%393), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %394), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.5, %392, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%input.10, %h.1, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/modeling_funnel.py:588:0
  %399 : Tensor = prim::GetAttr[name="bias"](%375)
  %400 : Tensor = prim::GetAttr[name="weight"](%375)
  %401 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm
  %query.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %401, %400, %399, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %403 : __torch__.transformers.modeling_funnel.___torch_mangle_4143.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%101)
  %404 : __torch__.transformers.modeling_funnel.___torch_mangle_4137.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%101)
  %405 : __torch__.torch.nn.modules.normalization.___torch_mangle_4136.LayerNorm = prim::GetAttr[name="layer_norm"](%404)
  %406 : __torch__.torch.nn.modules.linear.___torch_mangle_4135.Linear = prim::GetAttr[name="post_proj"](%404)
  %407 : Tensor = prim::GetAttr[name="seg_embed"](%404)
  %408 : Tensor = prim::GetAttr[name="r_s_bias"](%404)
  %409 : Tensor = prim::GetAttr[name="r_kernel"](%404)
  %410 : Tensor = prim::GetAttr[name="r_r_bias"](%404)
  %411 : Tensor = prim::GetAttr[name="r_w_bias"](%404)
  %412 : __torch__.torch.nn.modules.linear.___torch_mangle_4134.Linear = prim::GetAttr[name="v_head"](%404)
  %413 : __torch__.torch.nn.modules.linear.___torch_mangle_4133.Linear = prim::GetAttr[name="k_head"](%404)
  %414 : __torch__.torch.nn.modules.linear.___torch_mangle_4132.Linear = prim::GetAttr[name="q_head"](%404)
  %415 : int = aten::size(%query.1, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %416 : int = aten::size(%query.1, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %417 : int = aten::size(%query.1, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:531:0
  %418 : Tensor = prim::GetAttr[name="weight"](%414)
  %419 : Float(768:1, 768:768) = aten::t(%418), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %420 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %419), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %421 : int[] = prim::ListConstruct(%415, %416, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %q_head.3 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%420, %421), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:535:0
  %423 : Tensor = prim::GetAttr[name="bias"](%413)
  %424 : Tensor = prim::GetAttr[name="weight"](%413)
  %425 : Float(768:1, 768:768) = aten::t(%424), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %425), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %427 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %423, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1678:0
  %428 : int[] = prim::ListConstruct(%415, %417, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %429 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%427, %428), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:537:0
  %430 : Tensor = prim::GetAttr[name="bias"](%412)
  %431 : Tensor = prim::GetAttr[name="weight"](%412)
  %432 : Float(768:1, 768:768) = aten::t(%431), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %432), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %434 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %430, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1678:0
  %435 : int[] = prim::ListConstruct(%415, %417, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %436 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%434, %435), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.3, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.2 : Float(12:64, 64:1) = aten::mul(%411, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:542:0
  %439 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_w_bias.2, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:544:0
  %440 : Tensor[] = prim::ListConstruct(%439, %429), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %content_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%36, %440), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %v.2 : Float(12:64, 64:1) = aten::mul(%410, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:486:0
  %443 : Tensor[] = prim::ListConstruct(%150, %409), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %444 : Float(26:768, 12:64, 64:1) = aten::einsum(%37, %443), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %445 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %v.2, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:493:0
  %446 : Tensor[] = prim::ListConstruct(%445, %444), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.7 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%38, %446), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %448 : int = aten::size(%positional_attn.7, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %449 : int = aten::size(%positional_attn.7, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %450 : int = aten::size(%positional_attn.7, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %451 : int = aten::size(%positional_attn.7, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.2 : Long() = prim::NumToTensor(%451), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %453 : int[] = prim::ListConstruct(%448, %449, %451, %450), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.8 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.7, %453), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:428:0
  %455 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.8, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %456 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%455, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %457 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%456, %21, %51, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.9 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%457, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %459 : Long() = aten::sub(%max_rel_len.2, %16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %460 : int = aten::Int(%459), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %461 : int[] = prim::ListConstruct(%448, %449, %450, %460), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.10 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.9, %461), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.11 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.10, %39, %52, %417, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.12 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.11, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:498:0
  %465 : int = aten::size(%token_type_mat.2, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %466 : int = aten::size(%token_type_mat.2, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %467 : int = aten::size(%token_type_mat.2, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.2 : Float(12:64, 64:1) = aten::mul(%408, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:508:0
  %469 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_s_bias.2, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:511:0
  %470 : Tensor[] = prim::ListConstruct(%469, %407), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %471 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%40, %470), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %472 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %473 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%472, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %474 : int = aten::size(%q_head.4, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %475 : int[] = prim::ListConstruct(%465, %474, %466, %467), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %token_type_mat.4 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%473, %475, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %477 : Tensor[] = aten::split(%471, %51, %43), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:371:0
  %diff_token_type.2 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.2 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%477), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %480 : int = aten::size(%token_type_mat.4, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %481 : int = aten::size(%token_type_mat.4, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %482 : int = aten::size(%token_type_mat.4, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %483 : int = aten::size(%token_type_mat.4, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %484 : int[] = prim::ListConstruct(%480, %481, %482, %483), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %485 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.2, %484, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %486 : int = aten::size(%token_type_mat.4, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %487 : int = aten::size(%token_type_mat.4, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %488 : int = aten::size(%token_type_mat.4, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %489 : int = aten::size(%token_type_mat.4, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %490 : int[] = prim::ListConstruct(%486, %487, %488, %489), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %491 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.2, %490, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.4, %485, %491), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.3, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:522:0
  %494 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.2, %positional_attn.12, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%494, %token_type_attn.4, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.4, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:553:0
  %497 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %498 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%497, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %499 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%498, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %500 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%499, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %501 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%500, %51, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:396:0
  %502 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%501, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.5, %502, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %input.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.6, %43, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:558:0
  %505 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.15, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %506 : Tensor[] = prim::ListConstruct(%505, %436), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %attn_vec.2 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%42, %506), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %508 : int[] = prim::ListConstruct(%415, %416, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %input.16 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.2, %508), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:565:0
  %510 : Tensor = prim::GetAttr[name="bias"](%406)
  %511 : Tensor = prim::GetAttr[name="weight"](%406)
  %512 : Float(768:1, 768:768) = aten::t(%511), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.16, %512), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %510, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.17, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::add(%query.1, %attn_out.2, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:568:0
  %517 : Tensor = prim::GetAttr[name="bias"](%405)
  %518 : Tensor = prim::GetAttr[name="weight"](%405)
  %519 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.18, %519, %518, %517, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %521 : __torch__.torch.nn.modules.normalization.___torch_mangle_4142.LayerNorm = prim::GetAttr[name="layer_norm"](%403)
  %522 : __torch__.torch.nn.modules.linear.___torch_mangle_4140.Linear = prim::GetAttr[name="linear_2"](%403)
  %523 : __torch__.torch.nn.modules.linear.___torch_mangle_4138.Linear = prim::GetAttr[name="linear_1"](%403)
  %524 : Tensor = prim::GetAttr[name="bias"](%523)
  %525 : Tensor = prim::GetAttr[name="weight"](%523)
  %526 : Float(768:1, 3072:768) = aten::t(%525), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.9 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.19, %526), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.2 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.9, %524, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %529 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.2, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %530 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.2, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %531 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%530, %29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %532 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.2, %531, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %533 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%532, %30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %534 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%533), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %535 : Float(17:39936, 13:3072, 3072:1) = aten::add(%534, %31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.20 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%529, %535), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.20, %32, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %538 : Tensor = prim::GetAttr[name="bias"](%522)
  %539 : Tensor = prim::GetAttr[name="weight"](%522)
  %540 : Float(3072:1, 768:3072) = aten::t(%539), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.21, %540), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %538, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.22, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add(%input.19, %h.2, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/modeling_funnel.py:588:0
  %545 : Tensor = prim::GetAttr[name="bias"](%521)
  %546 : Tensor = prim::GetAttr[name="weight"](%521)
  %547 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm
  %query.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.23, %547, %546, %545, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %549 : __torch__.transformers.modeling_funnel.___torch_mangle_4158.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%98)
  %550 : __torch__.transformers.modeling_funnel.___torch_mangle_4152.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%98)
  %551 : __torch__.torch.nn.modules.normalization.___torch_mangle_4151.LayerNorm = prim::GetAttr[name="layer_norm"](%550)
  %552 : __torch__.torch.nn.modules.linear.___torch_mangle_4150.Linear = prim::GetAttr[name="post_proj"](%550)
  %553 : Tensor = prim::GetAttr[name="seg_embed"](%550)
  %554 : Tensor = prim::GetAttr[name="r_s_bias"](%550)
  %555 : Tensor = prim::GetAttr[name="r_kernel"](%550)
  %556 : Tensor = prim::GetAttr[name="r_r_bias"](%550)
  %557 : Tensor = prim::GetAttr[name="r_w_bias"](%550)
  %558 : __torch__.torch.nn.modules.linear.___torch_mangle_4149.Linear = prim::GetAttr[name="v_head"](%550)
  %559 : __torch__.torch.nn.modules.linear.___torch_mangle_4148.Linear = prim::GetAttr[name="k_head"](%550)
  %560 : __torch__.torch.nn.modules.linear.___torch_mangle_4147.Linear = prim::GetAttr[name="q_head"](%550)
  %561 : int = aten::size(%query.2, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %562 : int = aten::size(%query.2, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %563 : int = aten::size(%query.2, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:531:0
  %564 : Tensor = prim::GetAttr[name="weight"](%560)
  %565 : Float(768:1, 768:768) = aten::t(%564), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %566 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %565), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %567 : int[] = prim::ListConstruct(%561, %562, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %q_head.5 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%566, %567), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:535:0
  %569 : Tensor = prim::GetAttr[name="bias"](%559)
  %570 : Tensor = prim::GetAttr[name="weight"](%559)
  %571 : Float(768:1, 768:768) = aten::t(%570), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.11 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %571), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %573 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.11, %569, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1678:0
  %574 : int[] = prim::ListConstruct(%561, %563, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %575 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%573, %574), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:537:0
  %576 : Tensor = prim::GetAttr[name="bias"](%558)
  %577 : Tensor = prim::GetAttr[name="weight"](%558)
  %578 : Float(768:1, 768:768) = aten::t(%577), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %578), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %580 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %576, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1678:0
  %581 : int[] = prim::ListConstruct(%561, %563, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %582 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%580, %581), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.5, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.3 : Float(12:64, 64:1) = aten::mul(%557, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:542:0
  %585 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_w_bias.3, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:544:0
  %586 : Tensor[] = prim::ListConstruct(%585, %575), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %content_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%36, %586), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %v.3 : Float(12:64, 64:1) = aten::mul(%556, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:486:0
  %589 : Tensor[] = prim::ListConstruct(%150, %555), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %590 : Float(26:768, 12:64, 64:1) = aten::einsum(%37, %589), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %591 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %v.3, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:493:0
  %592 : Tensor[] = prim::ListConstruct(%591, %590), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.13 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%38, %592), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %594 : int = aten::size(%positional_attn.13, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %595 : int = aten::size(%positional_attn.13, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %596 : int = aten::size(%positional_attn.13, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %597 : int = aten::size(%positional_attn.13, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.3 : Long() = prim::NumToTensor(%597), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %599 : int[] = prim::ListConstruct(%594, %595, %597, %596), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.14 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.13, %599), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:428:0
  %601 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.14, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %602 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%601, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %603 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%602, %21, %51, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.15 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%603, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %605 : Long() = aten::sub(%max_rel_len.3, %16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %606 : int = aten::Int(%605), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %607 : int[] = prim::ListConstruct(%594, %595, %596, %606), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.16 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.15, %607), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.17 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.16, %39, %52, %563, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.18 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.17, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:498:0
  %611 : int = aten::size(%token_type_mat.2, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %612 : int = aten::size(%token_type_mat.2, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %613 : int = aten::size(%token_type_mat.2, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.3 : Float(12:64, 64:1) = aten::mul(%554, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:508:0
  %615 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_s_bias.3, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:511:0
  %616 : Tensor[] = prim::ListConstruct(%615, %553), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %617 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%40, %616), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %618 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %619 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%618, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %620 : int = aten::size(%q_head.6, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %621 : int[] = prim::ListConstruct(%611, %620, %612, %613), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %token_type_mat.5 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%619, %621, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %623 : Tensor[] = aten::split(%617, %51, %43), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:371:0
  %diff_token_type.3 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.3 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%623), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %626 : int = aten::size(%token_type_mat.5, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %627 : int = aten::size(%token_type_mat.5, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %628 : int = aten::size(%token_type_mat.5, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %629 : int = aten::size(%token_type_mat.5, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %630 : int[] = prim::ListConstruct(%626, %627, %628, %629), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %631 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.3, %630, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %632 : int = aten::size(%token_type_mat.5, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %633 : int = aten::size(%token_type_mat.5, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %634 : int = aten::size(%token_type_mat.5, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %635 : int = aten::size(%token_type_mat.5, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %636 : int[] = prim::ListConstruct(%632, %633, %634, %635), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %637 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.3, %636, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.5, %631, %637), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.5, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:522:0
  %640 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.3, %positional_attn.18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%640, %token_type_attn.6, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.7, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:553:0
  %643 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %644 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%643, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %645 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%644, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %646 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%645, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %647 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%646, %51, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:396:0
  %648 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%647, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.8, %648, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %input.24 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.9, %43, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:558:0
  %651 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.24, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %652 : Tensor[] = prim::ListConstruct(%651, %582), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %attn_vec.3 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%42, %652), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %654 : int[] = prim::ListConstruct(%561, %562, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.3, %654), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:565:0
  %656 : Tensor = prim::GetAttr[name="bias"](%552)
  %657 : Tensor = prim::GetAttr[name="weight"](%552)
  %658 : Float(768:1, 768:768) = aten::t(%657), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %658), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.26 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %656, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.26, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.27 : Float(17:9984, 13:768, 768:1) = aten::add(%query.2, %attn_out.3, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:568:0
  %663 : Tensor = prim::GetAttr[name="bias"](%551)
  %664 : Tensor = prim::GetAttr[name="weight"](%551)
  %665 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.27, %665, %664, %663, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %667 : __torch__.torch.nn.modules.normalization.___torch_mangle_4157.LayerNorm = prim::GetAttr[name="layer_norm"](%549)
  %668 : __torch__.torch.nn.modules.linear.___torch_mangle_4155.Linear = prim::GetAttr[name="linear_2"](%549)
  %669 : __torch__.torch.nn.modules.linear.___torch_mangle_4153.Linear = prim::GetAttr[name="linear_1"](%549)
  %670 : Tensor = prim::GetAttr[name="bias"](%669)
  %671 : Tensor = prim::GetAttr[name="weight"](%669)
  %672 : Float(768:1, 3072:768) = aten::t(%671), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.14 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.28, %672), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.3 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.14, %670, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %675 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.3, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %676 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.3, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %677 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%676, %29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %678 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.3, %677, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %679 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%678, %30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %680 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%679), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %681 : Float(17:39936, 13:3072, 3072:1) = aten::add(%680, %31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.29 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%675, %681), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.30 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.29, %32, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %684 : Tensor = prim::GetAttr[name="bias"](%668)
  %685 : Tensor = prim::GetAttr[name="weight"](%668)
  %686 : Float(3072:1, 768:3072) = aten::t(%685), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.30, %686), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %684, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.31, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.32 : Float(17:9984, 13:768, 768:1) = aten::add(%input.28, %h.3, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/modeling_funnel.py:588:0
  %691 : Tensor = prim::GetAttr[name="bias"](%667)
  %692 : Tensor = prim::GetAttr[name="weight"](%667)
  %693 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm
  %query.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.32, %693, %692, %691, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %695 : __torch__.transformers.modeling_funnel.___torch_mangle_4173.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%95)
  %696 : __torch__.transformers.modeling_funnel.___torch_mangle_4167.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%95)
  %697 : __torch__.torch.nn.modules.normalization.___torch_mangle_4166.LayerNorm = prim::GetAttr[name="layer_norm"](%696)
  %698 : __torch__.torch.nn.modules.linear.___torch_mangle_4165.Linear = prim::GetAttr[name="post_proj"](%696)
  %699 : Tensor = prim::GetAttr[name="seg_embed"](%696)
  %700 : Tensor = prim::GetAttr[name="r_s_bias"](%696)
  %701 : Tensor = prim::GetAttr[name="r_kernel"](%696)
  %702 : Tensor = prim::GetAttr[name="r_r_bias"](%696)
  %703 : Tensor = prim::GetAttr[name="r_w_bias"](%696)
  %704 : __torch__.torch.nn.modules.linear.___torch_mangle_4164.Linear = prim::GetAttr[name="v_head"](%696)
  %705 : __torch__.torch.nn.modules.linear.___torch_mangle_4163.Linear = prim::GetAttr[name="k_head"](%696)
  %706 : __torch__.torch.nn.modules.linear.___torch_mangle_4162.Linear = prim::GetAttr[name="q_head"](%696)
  %707 : int = aten::size(%query.3, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %708 : int = aten::size(%query.3, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %709 : int = aten::size(%query.3, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:531:0
  %710 : Tensor = prim::GetAttr[name="weight"](%706)
  %711 : Float(768:1, 768:768) = aten::t(%710), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %712 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %711), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %713 : int[] = prim::ListConstruct(%707, %708, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %q_head.7 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%712, %713), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:535:0
  %715 : Tensor = prim::GetAttr[name="bias"](%705)
  %716 : Tensor = prim::GetAttr[name="weight"](%705)
  %717 : Float(768:1, 768:768) = aten::t(%716), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %717), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %719 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %715, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1678:0
  %720 : int[] = prim::ListConstruct(%707, %709, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %721 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%719, %720), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:537:0
  %722 : Tensor = prim::GetAttr[name="bias"](%704)
  %723 : Tensor = prim::GetAttr[name="weight"](%704)
  %724 : Float(768:1, 768:768) = aten::t(%723), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.17 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %724), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %726 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.17, %722, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1678:0
  %727 : int[] = prim::ListConstruct(%707, %709, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %728 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%726, %727), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.7, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.4 : Float(12:64, 64:1) = aten::mul(%703, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:542:0
  %731 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_w_bias.4, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:544:0
  %732 : Tensor[] = prim::ListConstruct(%731, %721), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %content_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%36, %732), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %v.4 : Float(12:64, 64:1) = aten::mul(%702, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:486:0
  %735 : Tensor[] = prim::ListConstruct(%150, %701), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %736 : Float(26:768, 12:64, 64:1) = aten::einsum(%37, %735), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %737 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %v.4, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:493:0
  %738 : Tensor[] = prim::ListConstruct(%737, %736), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.19 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%38, %738), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %740 : int = aten::size(%positional_attn.19, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %741 : int = aten::size(%positional_attn.19, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %742 : int = aten::size(%positional_attn.19, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %743 : int = aten::size(%positional_attn.19, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.4 : Long() = prim::NumToTensor(%743), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %745 : int[] = prim::ListConstruct(%740, %741, %743, %742), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.20 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.19, %745), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:428:0
  %747 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.20, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %748 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%747, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %749 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%748, %21, %51, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.21 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%749, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %751 : Long() = aten::sub(%max_rel_len.4, %16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %752 : int = aten::Int(%751), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %753 : int[] = prim::ListConstruct(%740, %741, %742, %752), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.22 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.21, %753), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.23 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.22, %39, %52, %709, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.24 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.23, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:498:0
  %757 : int = aten::size(%token_type_mat.2, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %758 : int = aten::size(%token_type_mat.2, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %759 : int = aten::size(%token_type_mat.2, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.4 : Float(12:64, 64:1) = aten::mul(%700, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:508:0
  %761 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_s_bias.4, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:511:0
  %762 : Tensor[] = prim::ListConstruct(%761, %699), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %763 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%40, %762), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %764 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %765 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%764, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %766 : int = aten::size(%q_head.8, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %767 : int[] = prim::ListConstruct(%757, %766, %758, %759), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %token_type_mat.6 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%765, %767, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %769 : Tensor[] = aten::split(%763, %51, %43), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:371:0
  %diff_token_type.4 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.4 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%769), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %772 : int = aten::size(%token_type_mat.6, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %773 : int = aten::size(%token_type_mat.6, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %774 : int = aten::size(%token_type_mat.6, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %775 : int = aten::size(%token_type_mat.6, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %776 : int[] = prim::ListConstruct(%772, %773, %774, %775), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %777 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.4, %776, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %778 : int = aten::size(%token_type_mat.6, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %779 : int = aten::size(%token_type_mat.6, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %780 : int = aten::size(%token_type_mat.6, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %781 : int = aten::size(%token_type_mat.6, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %782 : int[] = prim::ListConstruct(%778, %779, %780, %781), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %783 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.4, %782, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.6, %777, %783), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.7, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:522:0
  %786 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.4, %positional_attn.24, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%786, %token_type_attn.8, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.10, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:553:0
  %789 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %790 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%789, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %791 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%790, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %792 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%791, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %793 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%792, %51, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:396:0
  %794 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%793, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.11, %794, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %input.33 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.12, %43, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:558:0
  %797 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.33, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %798 : Tensor[] = prim::ListConstruct(%797, %728), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %attn_vec.4 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%42, %798), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %800 : int[] = prim::ListConstruct(%707, %708, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.4, %800), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:565:0
  %802 : Tensor = prim::GetAttr[name="bias"](%698)
  %803 : Tensor = prim::GetAttr[name="weight"](%698)
  %804 : Float(768:1, 768:768) = aten::t(%803), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.34, %804), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %802, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.35, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.36 : Float(17:9984, 13:768, 768:1) = aten::add(%query.3, %attn_out.4, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:568:0
  %809 : Tensor = prim::GetAttr[name="bias"](%697)
  %810 : Tensor = prim::GetAttr[name="weight"](%697)
  %811 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm
  %input.37 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.36, %811, %810, %809, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %813 : __torch__.torch.nn.modules.normalization.___torch_mangle_4172.LayerNorm = prim::GetAttr[name="layer_norm"](%695)
  %814 : __torch__.torch.nn.modules.linear.___torch_mangle_4170.Linear = prim::GetAttr[name="linear_2"](%695)
  %815 : __torch__.torch.nn.modules.linear.___torch_mangle_4168.Linear = prim::GetAttr[name="linear_1"](%695)
  %816 : Tensor = prim::GetAttr[name="bias"](%815)
  %817 : Tensor = prim::GetAttr[name="weight"](%815)
  %818 : Float(768:1, 3072:768) = aten::t(%817), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.37, %818), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.4 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.19, %816, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %821 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.4, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %822 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.4, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %823 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%822, %29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %824 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.4, %823, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %825 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%824, %30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %826 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%825), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %827 : Float(17:39936, 13:3072, 3072:1) = aten::add(%826, %31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.38 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%821, %827), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.39 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.38, %32, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %830 : Tensor = prim::GetAttr[name="bias"](%814)
  %831 : Tensor = prim::GetAttr[name="weight"](%814)
  %832 : Float(3072:1, 768:3072) = aten::t(%831), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.39, %832), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %830, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.40, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.41 : Float(17:9984, 13:768, 768:1) = aten::add(%input.37, %h.4, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/modeling_funnel.py:588:0
  %837 : Tensor = prim::GetAttr[name="bias"](%813)
  %838 : Tensor = prim::GetAttr[name="weight"](%813)
  %839 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm
  %hidden.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.41, %839, %838, %837, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %841 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %842 : Bool(17:169, 1:13, 13:1) = aten::slice(%841, %51, %52, %51, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %843 : Tensor[] = prim::ListConstruct(%842, %token_type_mat.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.1 : Bool(17:182, 14:13, 13:1) = aten::cat(%843, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %845 : Bool(17:182, 14:13, 13:1) = aten::slice(%tensor.1, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.7 : Bool(17:182, 7:26, 13:1) = aten::slice(%845, %51, %52, %43, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %847 : Float(1:13, 13:1) = aten::slice(%cls_mask.1, %52, %52, %51, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %848 : Tensor[] = prim::ListConstruct(%847, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder
  %tensor.2 : Float(14:13, 13:1) = aten::cat(%848, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.2 : Float(7:26, 13:1) = aten::slice(%tensor.2, %52, %52, %43, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %851 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden.1, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.1 : Float(17:9984, 12:768, 768:1) = aten::slice(%851, %51, %52, %43, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %853 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden.1, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %854 : Float(17:9984, 1:768, 768:1) = aten::slice(%853, %51, %52, %51, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %855 : Tensor[] = prim::ListConstruct(%854, %suffix.1), scope: __module.funnel/__module.funnel.encoder
  %tensor.3 : Float(17:9984, 13:768, 768:1) = aten::cat(%855, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %857 : Float(17:9984, 13:768, 768:1) = aten::slice(%tensor.3, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %858 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::unsqueeze(%857, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %859 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%858, %21, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %tensor.4 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%859, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %861 : int[] = prim::ListConstruct(%21, %51), scope: __module.funnel/__module.funnel.encoder
  %862 : int[] = prim::ListConstruct(%21, %51), scope: __module.funnel/__module.funnel.encoder
  %863 : int[] = prim::ListConstruct(%52, %52), scope: __module.funnel/__module.funnel.encoder
  %tensor.5 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::avg_pool2d(%tensor.4, %861, %862, %863, %44, %44, %15), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
  %865 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%tensor.5, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %query.4 : Float(17:5376, 7:768, 768:1) = aten::select(%865, %51, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %867 : __torch__.transformers.modeling_funnel.___torch_mangle_4189.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%92)
  %868 : __torch__.transformers.modeling_funnel.___torch_mangle_4183.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%92)
  %869 : __torch__.torch.nn.modules.normalization.___torch_mangle_4182.LayerNorm = prim::GetAttr[name="layer_norm"](%868)
  %870 : __torch__.torch.nn.modules.linear.___torch_mangle_4181.Linear = prim::GetAttr[name="post_proj"](%868)
  %871 : Tensor = prim::GetAttr[name="seg_embed"](%868)
  %872 : Tensor = prim::GetAttr[name="r_s_bias"](%868)
  %873 : Tensor = prim::GetAttr[name="r_kernel"](%868)
  %874 : Tensor = prim::GetAttr[name="r_r_bias"](%868)
  %875 : Tensor = prim::GetAttr[name="r_w_bias"](%868)
  %876 : __torch__.torch.nn.modules.linear.___torch_mangle_4180.Linear = prim::GetAttr[name="v_head"](%868)
  %877 : __torch__.torch.nn.modules.linear.___torch_mangle_4179.Linear = prim::GetAttr[name="k_head"](%868)
  %878 : __torch__.torch.nn.modules.linear.___torch_mangle_4178.Linear = prim::GetAttr[name="q_head"](%868)
  %879 : int = aten::size(%query.4, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %880 : int = aten::size(%query.4, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %881 : int = aten::size(%hidden.1, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:531:0
  %882 : Tensor = prim::GetAttr[name="weight"](%878)
  %883 : Float(768:1, 768:768) = aten::t(%882), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %884 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.4, %883), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %885 : int[] = prim::ListConstruct(%879, %880, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %q_head.9 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%884, %885), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:535:0
  %887 : Tensor = prim::GetAttr[name="bias"](%877)
  %888 : Tensor = prim::GetAttr[name="weight"](%877)
  %889 : Float(768:1, 768:768) = aten::t(%888), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %889), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %891 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %887, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1678:0
  %892 : int[] = prim::ListConstruct(%879, %881, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %893 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%891, %892), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:537:0
  %894 : Tensor = prim::GetAttr[name="bias"](%876)
  %895 : Tensor = prim::GetAttr[name="weight"](%876)
  %896 : Float(768:1, 768:768) = aten::t(%895), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %896), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %898 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %894, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1678:0
  %899 : int[] = prim::ListConstruct(%879, %881, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %900 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%898, %899), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.10 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.9, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.5 : Float(12:64, 64:1) = aten::mul(%875, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:542:0
  %903 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_w_bias.5, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:544:0
  %904 : Tensor[] = prim::ListConstruct(%903, %893), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %content_score.5 : Float(17:1092, 12:91, 7:13, 13:1) = aten::einsum(%36, %904), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %v.5 : Float(12:64, 64:1) = aten::mul(%874, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:486:0
  %907 : Tensor[] = prim::ListConstruct(%174, %873), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %908 : Float(27:768, 12:64, 64:1) = aten::einsum(%37, %907), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %909 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %v.5, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:493:0
  %910 : Tensor[] = prim::ListConstruct(%909, %908), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.25 : Float(17:189, 12:3213, 7:27, 27:1) = aten::einsum(%38, %910), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %912 : int = aten::size(%positional_attn.25, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %913 : int = aten::size(%positional_attn.25, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %914 : int = aten::size(%positional_attn.25, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %915 : int = aten::size(%positional_attn.25, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.5 : Long() = prim::NumToTensor(%915), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %917 : int[] = prim::ListConstruct(%912, %913, %915, %914), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.26 : Float(17:189, 12:3213, 27:7, 7:1) = aten::reshape(%positional_attn.25, %917), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:428:0
  %919 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%positional_attn.26, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %920 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%919, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %921 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%920, %21, %21, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.27 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%921, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %923 : Long() = aten::sub(%max_rel_len.5, %17, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %924 : int = aten::Int(%923), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %925 : int[] = prim::ListConstruct(%912, %913, %914, %924), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.28 : Float(17:189, 12:3213, 7:25, 25:1) = aten::reshape(%positional_attn.27, %925), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.29 : Float(17:189, 12:3213, 7:25, 13:1) = aten::slice(%positional_attn.28, %39, %52, %881, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.30 : Float(17:189, 12:3213, 7:25, 13:1) = aten::mul_(%positional_attn.29, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:498:0
  %929 : int = aten::size(%token_type_mat.7, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %930 : int = aten::size(%token_type_mat.7, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %931 : int = aten::size(%token_type_mat.7, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.5 : Float(12:64, 64:1) = aten::mul(%872, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:508:0
  %933 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_s_bias.5, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:511:0
  %934 : Tensor[] = prim::ListConstruct(%933, %871), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %935 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%40, %934), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %936 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %937 : Bool(17:182, 1:182, 7:26, 13:1) = aten::unsqueeze(%936, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %938 : int = aten::size(%q_head.10, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %939 : int[] = prim::ListConstruct(%929, %938, %930, %931), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %token_type_mat.8 : Bool(17:182, 12:0, 7:26, 13:1) = aten::expand(%937, %939, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %941 : Tensor[] = aten::split(%935, %51, %43), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:371:0
  %diff_token_type.5 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.5 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%941), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %944 : int = aten::size(%token_type_mat.8, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %945 : int = aten::size(%token_type_mat.8, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %946 : int = aten::size(%token_type_mat.8, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %947 : int = aten::size(%token_type_mat.8, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %948 : int[] = prim::ListConstruct(%944, %945, %946, %947), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %949 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%same_token_type.5, %948, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %950 : int = aten::size(%token_type_mat.8, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %951 : int = aten::size(%token_type_mat.8, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %952 : int = aten::size(%token_type_mat.8, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %953 : int = aten::size(%token_type_mat.8, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %954 : int[] = prim::ListConstruct(%950, %951, %952, %953), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %955 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%diff_token_type.5, %954, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.9 : Float(17:1092, 12:91, 7:13, 13:1) = aten::where(%token_type_mat.8, %949, %955), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.10 : Float(17:1092, 12:91, 7:13, 13:1) = aten::mul_(%token_type_attn.9, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:522:0
  %958 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%content_score.5, %positional_attn.30, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.13 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%958, %token_type_attn.10, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.14 : Float(17:1092, 12:91, 7:13, 13:1) = aten::to(%attn_score.13, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:553:0
  %961 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %962 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%961, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %963 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%962, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %964 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%963, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %965 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%964, %51, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:396:0
  %966 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%965, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.15 : Float(17:1092, 12:91, 7:13, 13:1) = aten::sub(%attn_score.14, %966, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %input.42 : Float(17:1092, 12:91, 7:13, 13:1) = aten::softmax(%attn_score.15, %43, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:558:0
  %969 : Float(17:1092, 12:91, 7:13, 13:1) = aten::dropout(%input.42, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %970 : Tensor[] = prim::ListConstruct(%969, %900), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %attn_vec.5 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%42, %970), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %972 : int[] = prim::ListConstruct(%879, %880, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %input.43 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.5, %972), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:565:0
  %974 : Tensor = prim::GetAttr[name="bias"](%870)
  %975 : Tensor = prim::GetAttr[name="weight"](%870)
  %976 : Float(768:1, 768:768) = aten::t(%975), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.23 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.43, %976), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.44 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.23, %974, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.44, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:5376, 7:768, 768:1) = aten::add(%query.4, %attn_out.5, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:568:0
  %981 : Tensor = prim::GetAttr[name="bias"](%869)
  %982 : Tensor = prim::GetAttr[name="weight"](%869)
  %983 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm
  %input.46 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.45, %983, %982, %981, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %985 : __torch__.torch.nn.modules.normalization.___torch_mangle_4188.LayerNorm = prim::GetAttr[name="layer_norm"](%867)
  %986 : __torch__.torch.nn.modules.linear.___torch_mangle_4186.Linear = prim::GetAttr[name="linear_2"](%867)
  %987 : __torch__.torch.nn.modules.linear.___torch_mangle_4184.Linear = prim::GetAttr[name="linear_1"](%867)
  %988 : Tensor = prim::GetAttr[name="bias"](%987)
  %989 : Tensor = prim::GetAttr[name="weight"](%987)
  %990 : Float(768:1, 3072:768) = aten::t(%989), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.46, %990), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.5 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.24, %988, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %993 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.5, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %994 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.5, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %995 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%994, %29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %996 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.5, %995, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %997 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%996, %30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %998 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%997), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %999 : Float(17:21504, 7:3072, 3072:1) = aten::add(%998, %31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.47 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%993, %999), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.48 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.47, %32, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1002 : Tensor = prim::GetAttr[name="bias"](%986)
  %1003 : Tensor = prim::GetAttr[name="weight"](%986)
  %1004 : Float(3072:1, 768:3072) = aten::t(%1003), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.25 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.48, %1004), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.49 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.25, %1002, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.49, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:5376, 7:768, 768:1) = aten::add(%input.46, %h.5, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/modeling_funnel.py:588:0
  %1009 : Tensor = prim::GetAttr[name="bias"](%985)
  %1010 : Tensor = prim::GetAttr[name="weight"](%985)
  %1011 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm
  %query.5 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.50, %1011, %1010, %1009, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1013 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1014 : Bool(17:182, 7:26, 13:1) = aten::slice(%1013, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1015 : Bool(17:182, 7:26, 1:1) = aten::slice(%1014, %21, %52, %51, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1016 : Tensor[] = prim::ListConstruct(%1015, %token_type_mat.7), scope: __module.funnel/__module.funnel.encoder
  %tensor.6 : Bool(17:98, 7:14, 14:1) = aten::cat(%1016, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1018 : Bool(17:98, 7:14, 14:1) = aten::slice(%tensor.6, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1019 : Bool(17:98, 7:14, 14:1) = aten::slice(%1018, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.9 : Bool(17:98, 7:14, 7:2) = aten::slice(%1019, %21, %52, %43, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1021 : Float(7:26, 13:1) = aten::slice(%cls_mask.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1022 : Float(7:26, 1:1) = aten::slice(%1021, %51, %52, %51, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1023 : Tensor[] = prim::ListConstruct(%1022, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.7 : Float(7:14, 14:1) = aten::cat(%1023, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1025 : Float(7:14, 14:1) = aten::slice(%tensor.7, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask.3 : Float(7:14, 7:2) = aten::slice(%1025, %51, %52, %43, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1027 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.2 : Float(17:13, 12:1) = aten::slice(%1027, %51, %52, %43, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1029 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1030 : Float(17:13, 1:1) = aten::slice(%1029, %51, %52, %51, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1031 : Tensor[] = prim::ListConstruct(%1030, %suffix.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.8 : Float(17:13, 13:1) = aten::cat(%1031, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1033 : Float(17:13, 13:1) = aten::slice(%tensor.8, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1034 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%1033, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1035 : Float(17:13, 1:13, 13:1) = aten::slice(%1034, %21, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %tensor.9 : Float(17:13, 1:13, 13:1, 1:1) = aten::unsqueeze(%1035, %39), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %input.51 : Float(17:13, 1:13, 13:1, 1:1) = aten::neg(%tensor.9), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1038 : int[] = prim::ListConstruct(%21, %51), scope: __module.funnel/__module.funnel.encoder
  %1039 : int[] = prim::ListConstruct(%21, %51), scope: __module.funnel/__module.funnel.encoder
  %1040 : int[] = prim::ListConstruct(%52, %52), scope: __module.funnel/__module.funnel.encoder
  %1041 : int[] = prim::ListConstruct(%51, %51), scope: __module.funnel/__module.funnel.encoder
  %1042 : Float(17:7, 1:7, 7:1, 1:1) = aten::max_pool2d(%input.51, %1038, %1039, %1040, %1041, %44), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
  %tensor.10 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%1042), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1044 : Float(17:7, 1:7, 7:1, 1:1) = aten::slice(%tensor.10, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1045 : Float(17:7, 7:1, 1:1) = aten::select(%1044, %51, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1046 : Float(17:7, 7:1, 1:1) = aten::slice(%1045, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask.3 : Float(17:7, 7:1) = aten::select(%1046, %21, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1048 : __torch__.transformers.modeling_funnel.___torch_mangle_4204.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%89)
  %1049 : __torch__.transformers.modeling_funnel.___torch_mangle_4198.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%89)
  %1050 : __torch__.torch.nn.modules.normalization.___torch_mangle_4197.LayerNorm = prim::GetAttr[name="layer_norm"](%1049)
  %1051 : __torch__.torch.nn.modules.linear.___torch_mangle_4196.Linear = prim::GetAttr[name="post_proj"](%1049)
  %1052 : Tensor = prim::GetAttr[name="seg_embed"](%1049)
  %1053 : Tensor = prim::GetAttr[name="r_s_bias"](%1049)
  %1054 : Tensor = prim::GetAttr[name="r_kernel"](%1049)
  %1055 : Tensor = prim::GetAttr[name="r_r_bias"](%1049)
  %1056 : Tensor = prim::GetAttr[name="r_w_bias"](%1049)
  %1057 : __torch__.torch.nn.modules.linear.___torch_mangle_4195.Linear = prim::GetAttr[name="v_head"](%1049)
  %1058 : __torch__.torch.nn.modules.linear.___torch_mangle_4194.Linear = prim::GetAttr[name="k_head"](%1049)
  %1059 : __torch__.torch.nn.modules.linear.___torch_mangle_4193.Linear = prim::GetAttr[name="q_head"](%1049)
  %1060 : int = aten::size(%query.5, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1061 : int = aten::size(%query.5, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1062 : int = aten::size(%query.5, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:531:0
  %1063 : Tensor = prim::GetAttr[name="weight"](%1059)
  %1064 : Float(768:1, 768:768) = aten::t(%1063), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1065 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1064), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1066 : int[] = prim::ListConstruct(%1060, %1061, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %q_head.11 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1065, %1066), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:535:0
  %1068 : Tensor = prim::GetAttr[name="bias"](%1058)
  %1069 : Tensor = prim::GetAttr[name="weight"](%1058)
  %1070 : Float(768:1, 768:768) = aten::t(%1069), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.26 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1070), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %1072 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.26, %1068, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1678:0
  %1073 : int[] = prim::ListConstruct(%1060, %1062, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1074 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1072, %1073), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:537:0
  %1075 : Tensor = prim::GetAttr[name="bias"](%1057)
  %1076 : Tensor = prim::GetAttr[name="weight"](%1057)
  %1077 : Float(768:1, 768:768) = aten::t(%1076), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.27 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1077), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %1079 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.27, %1075, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1678:0
  %1080 : int[] = prim::ListConstruct(%1060, %1062, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1081 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1079, %1080), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.12 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.11, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.6 : Float(12:64, 64:1) = aten::mul(%1056, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:542:0
  %1084 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_w_bias.6, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:544:0
  %1085 : Tensor[] = prim::ListConstruct(%1084, %1074), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %content_score.6 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%36, %1085), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %v.6 : Float(12:64, 64:1) = aten::mul(%1055, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:486:0
  %1088 : Tensor[] = prim::ListConstruct(%192, %1054), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1089 : Float(14:768, 12:64, 64:1) = aten::einsum(%37, %1088), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1090 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %v.6, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:493:0
  %1091 : Tensor[] = prim::ListConstruct(%1090, %1089), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.31 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%38, %1091), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1093 : int = aten::size(%positional_attn.31, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1094 : int = aten::size(%positional_attn.31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1095 : int = aten::size(%positional_attn.31, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1096 : int = aten::size(%positional_attn.31, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.6 : Long() = prim::NumToTensor(%1096), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1098 : int[] = prim::ListConstruct(%1093, %1094, %1096, %1095), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.32 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.31, %1098), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:428:0
  %1100 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.32, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1101 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1100, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1102 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1101, %21, %51, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.33 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1102, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1104 : Long() = aten::sub(%max_rel_len.6, %16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %1105 : int = aten::Int(%1104), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1106 : int[] = prim::ListConstruct(%1093, %1094, %1095, %1105), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.34 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.33, %1106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.35 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.34, %39, %52, %1062, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.36 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.35, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:498:0
  %1110 : int = aten::size(%token_type_mat.9, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1111 : int = aten::size(%token_type_mat.9, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1112 : int = aten::size(%token_type_mat.9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.6 : Float(12:64, 64:1) = aten::mul(%1053, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:508:0
  %1114 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_s_bias.6, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:511:0
  %1115 : Tensor[] = prim::ListConstruct(%1114, %1052), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1116 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%40, %1115), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1117 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1118 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1117, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1119 : int = aten::size(%q_head.12, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1120 : int[] = prim::ListConstruct(%1110, %1119, %1111, %1112), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %token_type_mat.10 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1118, %1120, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1122 : Tensor[] = aten::split(%1116, %51, %43), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:371:0
  %diff_token_type.6 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.6 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1122), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1125 : int = aten::size(%token_type_mat.10, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1126 : int = aten::size(%token_type_mat.10, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1127 : int = aten::size(%token_type_mat.10, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1128 : int = aten::size(%token_type_mat.10, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1129 : int[] = prim::ListConstruct(%1125, %1126, %1127, %1128), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1130 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.6, %1129, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1131 : int = aten::size(%token_type_mat.10, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1132 : int = aten::size(%token_type_mat.10, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1133 : int = aten::size(%token_type_mat.10, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1134 : int = aten::size(%token_type_mat.10, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1135 : int[] = prim::ListConstruct(%1131, %1132, %1133, %1134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1136 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.6, %1135, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.11 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.10, %1130, %1136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.12 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.11, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:522:0
  %1139 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.6, %positional_attn.36, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1139, %token_type_attn.12, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.17 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.16, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:553:0
  %1142 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1143 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1142, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1144 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1143, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1145 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1144, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1146 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1145, %51, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:396:0
  %1147 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1146, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.18 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.17, %1147, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %input.52 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.18, %43, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:558:0
  %1150 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.52, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1151 : Tensor[] = prim::ListConstruct(%1150, %1081), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %attn_vec.6 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%42, %1151), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1153 : int[] = prim::ListConstruct(%1060, %1061, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %input.53 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.6, %1153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:565:0
  %1155 : Tensor = prim::GetAttr[name="bias"](%1051)
  %1156 : Tensor = prim::GetAttr[name="weight"](%1051)
  %1157 : Float(768:1, 768:768) = aten::t(%1156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.28 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.53, %1157), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.54 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.28, %1155, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.54, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.55 : Float(17:5376, 7:768, 768:1) = aten::add(%query.5, %attn_out.6, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:568:0
  %1162 : Tensor = prim::GetAttr[name="bias"](%1050)
  %1163 : Tensor = prim::GetAttr[name="weight"](%1050)
  %1164 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm
  %input.56 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.55, %1164, %1163, %1162, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1166 : __torch__.torch.nn.modules.normalization.___torch_mangle_4203.LayerNorm = prim::GetAttr[name="layer_norm"](%1048)
  %1167 : __torch__.torch.nn.modules.linear.___torch_mangle_4201.Linear = prim::GetAttr[name="linear_2"](%1048)
  %1168 : __torch__.torch.nn.modules.linear.___torch_mangle_4199.Linear = prim::GetAttr[name="linear_1"](%1048)
  %1169 : Tensor = prim::GetAttr[name="bias"](%1168)
  %1170 : Tensor = prim::GetAttr[name="weight"](%1168)
  %1171 : Float(768:1, 3072:768) = aten::t(%1170), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.56, %1171), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.6 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.29, %1169, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1174 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.6, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1175 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.6, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1176 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1175, %29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1177 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.6, %1176, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1178 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1177, %30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1179 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1180 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1179, %31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.57 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1174, %1180), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.58 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.57, %32, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1183 : Tensor = prim::GetAttr[name="bias"](%1167)
  %1184 : Tensor = prim::GetAttr[name="weight"](%1167)
  %1185 : Float(3072:1, 768:3072) = aten::t(%1184), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.58, %1185), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.59 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.30, %1183, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.59, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:5376, 7:768, 768:1) = aten::add(%input.56, %h.6, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/modeling_funnel.py:588:0
  %1190 : Tensor = prim::GetAttr[name="bias"](%1166)
  %1191 : Tensor = prim::GetAttr[name="weight"](%1166)
  %1192 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm
  %query.6 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.60, %1192, %1191, %1190, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1194 : __torch__.transformers.modeling_funnel.___torch_mangle_4219.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%86)
  %1195 : __torch__.transformers.modeling_funnel.___torch_mangle_4213.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%86)
  %1196 : __torch__.torch.nn.modules.normalization.___torch_mangle_4212.LayerNorm = prim::GetAttr[name="layer_norm"](%1195)
  %1197 : __torch__.torch.nn.modules.linear.___torch_mangle_4211.Linear = prim::GetAttr[name="post_proj"](%1195)
  %1198 : Tensor = prim::GetAttr[name="seg_embed"](%1195)
  %1199 : Tensor = prim::GetAttr[name="r_s_bias"](%1195)
  %1200 : Tensor = prim::GetAttr[name="r_kernel"](%1195)
  %1201 : Tensor = prim::GetAttr[name="r_r_bias"](%1195)
  %1202 : Tensor = prim::GetAttr[name="r_w_bias"](%1195)
  %1203 : __torch__.torch.nn.modules.linear.___torch_mangle_4210.Linear = prim::GetAttr[name="v_head"](%1195)
  %1204 : __torch__.torch.nn.modules.linear.___torch_mangle_4209.Linear = prim::GetAttr[name="k_head"](%1195)
  %1205 : __torch__.torch.nn.modules.linear.___torch_mangle_4208.Linear = prim::GetAttr[name="q_head"](%1195)
  %1206 : int = aten::size(%query.6, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1207 : int = aten::size(%query.6, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1208 : int = aten::size(%query.6, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:531:0
  %1209 : Tensor = prim::GetAttr[name="weight"](%1205)
  %1210 : Float(768:1, 768:768) = aten::t(%1209), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1211 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1212 : int[] = prim::ListConstruct(%1206, %1207, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %q_head.13 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1211, %1212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:535:0
  %1214 : Tensor = prim::GetAttr[name="bias"](%1204)
  %1215 : Tensor = prim::GetAttr[name="weight"](%1204)
  %1216 : Float(768:1, 768:768) = aten::t(%1215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.31 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %1218 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.31, %1214, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1678:0
  %1219 : int[] = prim::ListConstruct(%1206, %1208, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1220 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1218, %1219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:537:0
  %1221 : Tensor = prim::GetAttr[name="bias"](%1203)
  %1222 : Tensor = prim::GetAttr[name="weight"](%1203)
  %1223 : Float(768:1, 768:768) = aten::t(%1222), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.32 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %1225 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.32, %1221, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1678:0
  %1226 : int[] = prim::ListConstruct(%1206, %1208, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1227 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1225, %1226), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.14 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.13, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.7 : Float(12:64, 64:1) = aten::mul(%1202, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:542:0
  %1230 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_w_bias.7, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:544:0
  %1231 : Tensor[] = prim::ListConstruct(%1230, %1220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %content_score.7 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%36, %1231), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %v.7 : Float(12:64, 64:1) = aten::mul(%1201, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:486:0
  %1234 : Tensor[] = prim::ListConstruct(%192, %1200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1235 : Float(14:768, 12:64, 64:1) = aten::einsum(%37, %1234), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1236 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %v.7, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:493:0
  %1237 : Tensor[] = prim::ListConstruct(%1236, %1235), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.37 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%38, %1237), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1239 : int = aten::size(%positional_attn.37, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1240 : int = aten::size(%positional_attn.37, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1241 : int = aten::size(%positional_attn.37, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1242 : int = aten::size(%positional_attn.37, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.7 : Long() = prim::NumToTensor(%1242), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1244 : int[] = prim::ListConstruct(%1239, %1240, %1242, %1241), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.38 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.37, %1244), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:428:0
  %1246 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.38, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1247 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1246, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1248 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1247, %21, %51, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.39 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1248, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1250 : Long() = aten::sub(%max_rel_len.7, %16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %1251 : int = aten::Int(%1250), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1252 : int[] = prim::ListConstruct(%1239, %1240, %1241, %1251), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.40 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.39, %1252), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.41 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.40, %39, %52, %1208, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.42 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.41, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:498:0
  %1256 : int = aten::size(%token_type_mat.9, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1257 : int = aten::size(%token_type_mat.9, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1258 : int = aten::size(%token_type_mat.9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.7 : Float(12:64, 64:1) = aten::mul(%1199, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:508:0
  %1260 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_s_bias.7, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:511:0
  %1261 : Tensor[] = prim::ListConstruct(%1260, %1198), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1262 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%40, %1261), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1263 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1264 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1263, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1265 : int = aten::size(%q_head.14, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1266 : int[] = prim::ListConstruct(%1256, %1265, %1257, %1258), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %token_type_mat.11 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1264, %1266, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1268 : Tensor[] = aten::split(%1262, %51, %43), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:371:0
  %diff_token_type.7 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.7 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1271 : int = aten::size(%token_type_mat.11, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1272 : int = aten::size(%token_type_mat.11, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1273 : int = aten::size(%token_type_mat.11, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1274 : int = aten::size(%token_type_mat.11, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1275 : int[] = prim::ListConstruct(%1271, %1272, %1273, %1274), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1276 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.7, %1275, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1277 : int = aten::size(%token_type_mat.11, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1278 : int = aten::size(%token_type_mat.11, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1279 : int = aten::size(%token_type_mat.11, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1280 : int = aten::size(%token_type_mat.11, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1281 : int[] = prim::ListConstruct(%1277, %1278, %1279, %1280), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1282 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.7, %1281, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.13 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.11, %1276, %1282), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.14 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.13, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:522:0
  %1285 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.7, %positional_attn.42, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.19 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1285, %token_type_attn.14, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.20 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.19, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:553:0
  %1288 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1289 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1288, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1290 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1289, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1291 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1290, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1292 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1291, %51, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:396:0
  %1293 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1292, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.21 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.20, %1293, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %input.61 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.21, %43, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:558:0
  %1296 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.61, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1297 : Tensor[] = prim::ListConstruct(%1296, %1227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %attn_vec.7 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%42, %1297), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1299 : int[] = prim::ListConstruct(%1206, %1207, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %input.62 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.7, %1299), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:565:0
  %1301 : Tensor = prim::GetAttr[name="bias"](%1197)
  %1302 : Tensor = prim::GetAttr[name="weight"](%1197)
  %1303 : Float(768:1, 768:768) = aten::t(%1302), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.33 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.62, %1303), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.63 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.33, %1301, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.63, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:5376, 7:768, 768:1) = aten::add(%query.6, %attn_out.7, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:568:0
  %1308 : Tensor = prim::GetAttr[name="bias"](%1196)
  %1309 : Tensor = prim::GetAttr[name="weight"](%1196)
  %1310 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm
  %input.65 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.64, %1310, %1309, %1308, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1312 : __torch__.torch.nn.modules.normalization.___torch_mangle_4218.LayerNorm = prim::GetAttr[name="layer_norm"](%1194)
  %1313 : __torch__.torch.nn.modules.linear.___torch_mangle_4216.Linear = prim::GetAttr[name="linear_2"](%1194)
  %1314 : __torch__.torch.nn.modules.linear.___torch_mangle_4214.Linear = prim::GetAttr[name="linear_1"](%1194)
  %1315 : Tensor = prim::GetAttr[name="bias"](%1314)
  %1316 : Tensor = prim::GetAttr[name="weight"](%1314)
  %1317 : Float(768:1, 3072:768) = aten::t(%1316), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.65, %1317), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.7 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.34, %1315, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1320 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.7, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1321 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.7, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1322 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1321, %29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1323 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.7, %1322, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1324 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1323, %30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1325 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1324), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1326 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1325, %31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.66 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1320, %1326), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.67 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.66, %32, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1329 : Tensor = prim::GetAttr[name="bias"](%1313)
  %1330 : Tensor = prim::GetAttr[name="weight"](%1313)
  %1331 : Float(3072:1, 768:3072) = aten::t(%1330), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.67, %1331), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.68 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.35, %1329, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.68, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(17:5376, 7:768, 768:1) = aten::add(%input.65, %h.7, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/modeling_funnel.py:588:0
  %1336 : Tensor = prim::GetAttr[name="bias"](%1312)
  %1337 : Tensor = prim::GetAttr[name="weight"](%1312)
  %1338 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm
  %query.7 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.69, %1338, %1337, %1336, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1340 : __torch__.transformers.modeling_funnel.___torch_mangle_4234.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%83)
  %1341 : __torch__.transformers.modeling_funnel.___torch_mangle_4228.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%83)
  %1342 : __torch__.torch.nn.modules.normalization.___torch_mangle_4227.LayerNorm = prim::GetAttr[name="layer_norm"](%1341)
  %1343 : __torch__.torch.nn.modules.linear.___torch_mangle_4226.Linear = prim::GetAttr[name="post_proj"](%1341)
  %1344 : Tensor = prim::GetAttr[name="seg_embed"](%1341)
  %1345 : Tensor = prim::GetAttr[name="r_s_bias"](%1341)
  %1346 : Tensor = prim::GetAttr[name="r_kernel"](%1341)
  %1347 : Tensor = prim::GetAttr[name="r_r_bias"](%1341)
  %1348 : Tensor = prim::GetAttr[name="r_w_bias"](%1341)
  %1349 : __torch__.torch.nn.modules.linear.___torch_mangle_4225.Linear = prim::GetAttr[name="v_head"](%1341)
  %1350 : __torch__.torch.nn.modules.linear.___torch_mangle_4224.Linear = prim::GetAttr[name="k_head"](%1341)
  %1351 : __torch__.torch.nn.modules.linear.___torch_mangle_4223.Linear = prim::GetAttr[name="q_head"](%1341)
  %1352 : int = aten::size(%query.7, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1353 : int = aten::size(%query.7, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1354 : int = aten::size(%query.7, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:531:0
  %1355 : Tensor = prim::GetAttr[name="weight"](%1351)
  %1356 : Float(768:1, 768:768) = aten::t(%1355), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1357 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1356), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1358 : int[] = prim::ListConstruct(%1352, %1353, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %q_head.15 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1357, %1358), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:535:0
  %1360 : Tensor = prim::GetAttr[name="bias"](%1350)
  %1361 : Tensor = prim::GetAttr[name="weight"](%1350)
  %1362 : Float(768:1, 768:768) = aten::t(%1361), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.36 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1362), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %1364 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.36, %1360, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1678:0
  %1365 : int[] = prim::ListConstruct(%1352, %1354, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1366 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1364, %1365), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:537:0
  %1367 : Tensor = prim::GetAttr[name="bias"](%1349)
  %1368 : Tensor = prim::GetAttr[name="weight"](%1349)
  %1369 : Float(768:1, 768:768) = aten::t(%1368), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.37 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1369), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %1371 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.37, %1367, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1678:0
  %1372 : int[] = prim::ListConstruct(%1352, %1354, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1373 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1371, %1372), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.16 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.15, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.8 : Float(12:64, 64:1) = aten::mul(%1348, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:542:0
  %1376 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_w_bias.8, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:544:0
  %1377 : Tensor[] = prim::ListConstruct(%1376, %1366), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %content_score.8 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%36, %1377), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %v.8 : Float(12:64, 64:1) = aten::mul(%1347, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:486:0
  %1380 : Tensor[] = prim::ListConstruct(%192, %1346), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1381 : Float(14:768, 12:64, 64:1) = aten::einsum(%37, %1380), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1382 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %v.8, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:493:0
  %1383 : Tensor[] = prim::ListConstruct(%1382, %1381), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.43 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%38, %1383), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1385 : int = aten::size(%positional_attn.43, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1386 : int = aten::size(%positional_attn.43, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1387 : int = aten::size(%positional_attn.43, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1388 : int = aten::size(%positional_attn.43, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.8 : Long() = prim::NumToTensor(%1388), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1390 : int[] = prim::ListConstruct(%1385, %1386, %1388, %1387), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.44 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.43, %1390), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:428:0
  %1392 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.44, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1393 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1392, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1394 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1393, %21, %51, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.45 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1394, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1396 : Long() = aten::sub(%max_rel_len.8, %16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %1397 : int = aten::Int(%1396), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1398 : int[] = prim::ListConstruct(%1385, %1386, %1387, %1397), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.46 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.45, %1398), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.47 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.46, %39, %52, %1354, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.48 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.47, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:498:0
  %1402 : int = aten::size(%token_type_mat.9, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1403 : int = aten::size(%token_type_mat.9, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1404 : int = aten::size(%token_type_mat.9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.8 : Float(12:64, 64:1) = aten::mul(%1345, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:508:0
  %1406 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_s_bias.8, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:511:0
  %1407 : Tensor[] = prim::ListConstruct(%1406, %1344), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1408 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%40, %1407), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1409 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1410 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1409, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1411 : int = aten::size(%q_head.16, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1412 : int[] = prim::ListConstruct(%1402, %1411, %1403, %1404), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %token_type_mat.12 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1410, %1412, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1414 : Tensor[] = aten::split(%1408, %51, %43), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:371:0
  %diff_token_type.8 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.8 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1414), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1417 : int = aten::size(%token_type_mat.12, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1418 : int = aten::size(%token_type_mat.12, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1419 : int = aten::size(%token_type_mat.12, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1420 : int = aten::size(%token_type_mat.12, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1421 : int[] = prim::ListConstruct(%1417, %1418, %1419, %1420), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1422 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.8, %1421, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1423 : int = aten::size(%token_type_mat.12, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1424 : int = aten::size(%token_type_mat.12, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1425 : int = aten::size(%token_type_mat.12, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1426 : int = aten::size(%token_type_mat.12, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1427 : int[] = prim::ListConstruct(%1423, %1424, %1425, %1426), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1428 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.8, %1427, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.15 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.12, %1422, %1428), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.15, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:522:0
  %1431 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.8, %positional_attn.48, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.22 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1431, %token_type_attn.16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.23 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.22, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:553:0
  %1434 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1435 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1434, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1436 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1435, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1437 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1436, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1438 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1437, %51, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:396:0
  %1439 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1438, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.24 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.23, %1439, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %input.70 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.24, %43, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:558:0
  %1442 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.70, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %1443 : Tensor[] = prim::ListConstruct(%1442, %1373), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %attn_vec.8 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%42, %1443), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1445 : int[] = prim::ListConstruct(%1352, %1353, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %input.71 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.8, %1445), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:565:0
  %1447 : Tensor = prim::GetAttr[name="bias"](%1343)
  %1448 : Tensor = prim::GetAttr[name="weight"](%1343)
  %1449 : Float(768:1, 768:768) = aten::t(%1448), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.38 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.71, %1449), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.72 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.38, %1447, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.72, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:5376, 7:768, 768:1) = aten::add(%query.7, %attn_out.8, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:568:0
  %1454 : Tensor = prim::GetAttr[name="bias"](%1342)
  %1455 : Tensor = prim::GetAttr[name="weight"](%1342)
  %1456 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm
  %input.74 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.73, %1456, %1455, %1454, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %1458 : __torch__.torch.nn.modules.normalization.___torch_mangle_4233.LayerNorm = prim::GetAttr[name="layer_norm"](%1340)
  %1459 : __torch__.torch.nn.modules.linear.___torch_mangle_4231.Linear = prim::GetAttr[name="linear_2"](%1340)
  %1460 : __torch__.torch.nn.modules.linear.___torch_mangle_4229.Linear = prim::GetAttr[name="linear_1"](%1340)
  %1461 : Tensor = prim::GetAttr[name="bias"](%1460)
  %1462 : Tensor = prim::GetAttr[name="weight"](%1460)
  %1463 : Float(768:1, 3072:768) = aten::t(%1462), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.39 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.74, %1463), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.8 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.39, %1461, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1466 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.8, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1467 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.8, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1468 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1467, %29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1469 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.8, %1468, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1470 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1469, %30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1471 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1470), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1472 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1471, %31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.75 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1466, %1472), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.76 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.75, %32, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1475 : Tensor = prim::GetAttr[name="bias"](%1459)
  %1476 : Tensor = prim::GetAttr[name="weight"](%1459)
  %1477 : Float(3072:1, 768:3072) = aten::t(%1476), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.40 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.76, %1477), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.77 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.40, %1475, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.77, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(17:5376, 7:768, 768:1) = aten::add(%input.74, %h.8, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/modeling_funnel.py:588:0
  %1482 : Tensor = prim::GetAttr[name="bias"](%1458)
  %1483 : Tensor = prim::GetAttr[name="weight"](%1458)
  %1484 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm
  %hidden : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.78, %1484, %1483, %1482, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1486 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1487 : Bool(17:98, 1:14, 7:2) = aten::slice(%1486, %51, %52, %51, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1488 : Tensor[] = prim::ListConstruct(%1487, %token_type_mat.9), scope: __module.funnel/__module.funnel.encoder
  %tensor.11 : Bool(17:56, 8:7, 7:1) = aten::cat(%1488, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1490 : Bool(17:56, 8:7, 7:1) = aten::slice(%tensor.11, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.13 : Bool(17:56, 4:14, 7:1) = aten::slice(%1490, %51, %52, %43, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1492 : Float(1:14, 7:2) = aten::slice(%cls_mask.3, %52, %52, %51, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1493 : Tensor[] = prim::ListConstruct(%1492, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder
  %tensor.12 : Float(8:7, 7:1) = aten::cat(%1493, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.4 : Float(4:14, 7:1) = aten::slice(%tensor.12, %52, %52, %43, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1496 : Float(17:5376, 7:768, 768:1) = aten::slice(%hidden, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.3 : Float(17:5376, 6:768, 768:1) = aten::slice(%1496, %51, %52, %43, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1498 : Float(17:5376, 7:768, 768:1) = aten::slice(%hidden, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1499 : Float(17:5376, 1:768, 768:1) = aten::slice(%1498, %51, %52, %51, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1500 : Tensor[] = prim::ListConstruct(%1499, %suffix.3), scope: __module.funnel/__module.funnel.encoder
  %tensor.13 : Float(17:5376, 7:768, 768:1) = aten::cat(%1500, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1502 : Float(17:5376, 7:768, 768:1) = aten::slice(%tensor.13, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1503 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::unsqueeze(%1502, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1504 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%1503, %21, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %tensor.14 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%1504, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1506 : int[] = prim::ListConstruct(%21, %51), scope: __module.funnel/__module.funnel.encoder
  %1507 : int[] = prim::ListConstruct(%21, %51), scope: __module.funnel/__module.funnel.encoder
  %1508 : int[] = prim::ListConstruct(%52, %52), scope: __module.funnel/__module.funnel.encoder
  %tensor.15 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::avg_pool2d(%tensor.14, %1506, %1507, %1508, %44, %44, %15), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
  %1510 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::slice(%tensor.15, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %query.8 : Float(17:3072, 4:768, 768:1) = aten::select(%1510, %51, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %1512 : __torch__.transformers.modeling_funnel.___torch_mangle_4250.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%80)
  %1513 : __torch__.transformers.modeling_funnel.___torch_mangle_4244.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%80)
  %1514 : __torch__.torch.nn.modules.normalization.___torch_mangle_4243.LayerNorm = prim::GetAttr[name="layer_norm"](%1513)
  %1515 : __torch__.torch.nn.modules.linear.___torch_mangle_4242.Linear = prim::GetAttr[name="post_proj"](%1513)
  %1516 : Tensor = prim::GetAttr[name="seg_embed"](%1513)
  %1517 : Tensor = prim::GetAttr[name="r_s_bias"](%1513)
  %1518 : Tensor = prim::GetAttr[name="r_kernel"](%1513)
  %1519 : Tensor = prim::GetAttr[name="r_r_bias"](%1513)
  %1520 : Tensor = prim::GetAttr[name="r_w_bias"](%1513)
  %1521 : __torch__.torch.nn.modules.linear.___torch_mangle_4241.Linear = prim::GetAttr[name="v_head"](%1513)
  %1522 : __torch__.torch.nn.modules.linear.___torch_mangle_4240.Linear = prim::GetAttr[name="k_head"](%1513)
  %1523 : __torch__.torch.nn.modules.linear.___torch_mangle_4239.Linear = prim::GetAttr[name="q_head"](%1513)
  %1524 : int = aten::size(%query.8, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1525 : int = aten::size(%query.8, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1526 : int = aten::size(%hidden, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:531:0
  %1527 : Tensor = prim::GetAttr[name="weight"](%1523)
  %1528 : Float(768:1, 768:768) = aten::t(%1527), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1529 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.8, %1528), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1530 : int[] = prim::ListConstruct(%1524, %1525, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %q_head.17 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1529, %1530), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:535:0
  %1532 : Tensor = prim::GetAttr[name="bias"](%1522)
  %1533 : Tensor = prim::GetAttr[name="weight"](%1522)
  %1534 : Float(768:1, 768:768) = aten::t(%1533), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.41 : Float(17:5376, 7:768, 768:1) = aten::matmul(%hidden, %1534), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %1536 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.41, %1532, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1678:0
  %1537 : int[] = prim::ListConstruct(%1524, %1526, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1538 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1536, %1537), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:537:0
  %1539 : Tensor = prim::GetAttr[name="bias"](%1521)
  %1540 : Tensor = prim::GetAttr[name="weight"](%1521)
  %1541 : Float(768:1, 768:768) = aten::t(%1540), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.42 : Float(17:5376, 7:768, 768:1) = aten::matmul(%hidden, %1541), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %1543 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.42, %1539, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1678:0
  %1544 : int[] = prim::ListConstruct(%1524, %1526, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1545 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1543, %1544), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.18 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.17, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.9 : Float(12:64, 64:1) = aten::mul(%1520, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:542:0
  %1548 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_w_bias.9, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:544:0
  %1549 : Tensor[] = prim::ListConstruct(%1548, %1538), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %content_score.9 : Float(17:336, 12:28, 4:7, 7:1) = aten::einsum(%36, %1549), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %v.9 : Float(12:64, 64:1) = aten::mul(%1519, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:486:0
  %1552 : Tensor[] = prim::ListConstruct(%216, %1518), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1553 : Float(15:768, 12:64, 64:1) = aten::einsum(%37, %1552), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1554 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %v.9, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:493:0
  %1555 : Tensor[] = prim::ListConstruct(%1554, %1553), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.49 : Float(17:60, 12:1020, 4:15, 15:1) = aten::einsum(%38, %1555), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1557 : int = aten::size(%positional_attn.49, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1558 : int = aten::size(%positional_attn.49, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1559 : int = aten::size(%positional_attn.49, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1560 : int = aten::size(%positional_attn.49, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.9 : Long() = prim::NumToTensor(%1560), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1562 : int[] = prim::ListConstruct(%1557, %1558, %1560, %1559), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.50 : Float(17:60, 12:1020, 15:4, 4:1) = aten::reshape(%positional_attn.49, %1562), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:428:0
  %1564 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%positional_attn.50, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1565 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%1564, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1566 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%1565, %21, %21, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.51 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%1566, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1568 : Long() = aten::sub(%max_rel_len.9, %17, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %1569 : int = aten::Int(%1568), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1570 : int[] = prim::ListConstruct(%1557, %1558, %1559, %1569), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.52 : Float(17:60, 12:1020, 4:13, 13:1) = aten::reshape(%positional_attn.51, %1570), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.53 : Float(17:60, 12:1020, 4:13, 7:1) = aten::slice(%positional_attn.52, %39, %52, %1526, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.54 : Float(17:60, 12:1020, 4:13, 7:1) = aten::mul_(%positional_attn.53, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:498:0
  %1574 : int = aten::size(%token_type_mat.13, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1575 : int = aten::size(%token_type_mat.13, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1576 : int = aten::size(%token_type_mat.13, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.9 : Float(12:64, 64:1) = aten::mul(%1517, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:508:0
  %1578 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_s_bias.9, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:511:0
  %1579 : Tensor[] = prim::ListConstruct(%1578, %1516), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1580 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%40, %1579), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1581 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1582 : Bool(17:56, 1:56, 4:14, 7:1) = aten::unsqueeze(%1581, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1583 : int = aten::size(%q_head.18, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1584 : int[] = prim::ListConstruct(%1574, %1583, %1575, %1576), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %token_type_mat.14 : Bool(17:56, 12:0, 4:14, 7:1) = aten::expand(%1582, %1584, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1586 : Tensor[] = aten::split(%1580, %51, %43), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:371:0
  %diff_token_type.9 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.9 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1586), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1589 : int = aten::size(%token_type_mat.14, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1590 : int = aten::size(%token_type_mat.14, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1591 : int = aten::size(%token_type_mat.14, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1592 : int = aten::size(%token_type_mat.14, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1593 : int[] = prim::ListConstruct(%1589, %1590, %1591, %1592), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1594 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%same_token_type.9, %1593, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1595 : int = aten::size(%token_type_mat.14, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1596 : int = aten::size(%token_type_mat.14, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1597 : int = aten::size(%token_type_mat.14, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1598 : int = aten::size(%token_type_mat.14, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1599 : int[] = prim::ListConstruct(%1595, %1596, %1597, %1598), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1600 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%diff_token_type.9, %1599, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.17 : Float(17:336, 12:28, 4:7, 7:1) = aten::where(%token_type_mat.14, %1594, %1600), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.18 : Float(17:336, 12:28, 4:7, 7:1) = aten::mul_(%token_type_attn.17, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:522:0
  %1603 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%content_score.9, %positional_attn.54, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.25 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%1603, %token_type_attn.18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.26 : Float(17:336, 12:28, 4:7, 7:1) = aten::to(%attn_score.25, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:553:0
  %1606 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1607 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1606, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1608 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1607, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1609 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1608, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1610 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1609, %51, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:396:0
  %1611 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1610, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.27 : Float(17:336, 12:28, 4:7, 7:1) = aten::sub(%attn_score.26, %1611, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %input.79 : Float(17:336, 12:28, 4:7, 7:1) = aten::softmax(%attn_score.27, %43, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:558:0
  %1614 : Float(17:336, 12:28, 4:7, 7:1) = aten::dropout(%input.79, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %1615 : Tensor[] = prim::ListConstruct(%1614, %1545), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %attn_vec.9 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%42, %1615), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1617 : int[] = prim::ListConstruct(%1524, %1525, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %input.80 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.9, %1617), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:565:0
  %1619 : Tensor = prim::GetAttr[name="bias"](%1515)
  %1620 : Tensor = prim::GetAttr[name="weight"](%1515)
  %1621 : Float(768:1, 768:768) = aten::t(%1620), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.43 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.80, %1621), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.81 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.43, %1619, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.81, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.82 : Float(17:3072, 4:768, 768:1) = aten::add(%query.8, %attn_out.9, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:568:0
  %1626 : Tensor = prim::GetAttr[name="bias"](%1514)
  %1627 : Tensor = prim::GetAttr[name="weight"](%1514)
  %1628 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm
  %input.83 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.82, %1628, %1627, %1626, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %1630 : __torch__.torch.nn.modules.normalization.___torch_mangle_4249.LayerNorm = prim::GetAttr[name="layer_norm"](%1512)
  %1631 : __torch__.torch.nn.modules.linear.___torch_mangle_4247.Linear = prim::GetAttr[name="linear_2"](%1512)
  %1632 : __torch__.torch.nn.modules.linear.___torch_mangle_4245.Linear = prim::GetAttr[name="linear_1"](%1512)
  %1633 : Tensor = prim::GetAttr[name="bias"](%1632)
  %1634 : Tensor = prim::GetAttr[name="weight"](%1632)
  %1635 : Float(768:1, 3072:768) = aten::t(%1634), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.44 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.83, %1635), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.9 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.44, %1633, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1638 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.9, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1639 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.9, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1640 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1639, %29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1641 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.9, %1640, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1642 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1641, %30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1643 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1642), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1644 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1643, %31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.84 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1638, %1644), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.85 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.84, %32, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1647 : Tensor = prim::GetAttr[name="bias"](%1631)
  %1648 : Tensor = prim::GetAttr[name="weight"](%1631)
  %1649 : Float(3072:1, 768:3072) = aten::t(%1648), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.45 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.85, %1649), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.86 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.45, %1647, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.86, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(17:3072, 4:768, 768:1) = aten::add(%input.83, %h.9, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/modeling_funnel.py:588:0
  %1654 : Tensor = prim::GetAttr[name="bias"](%1630)
  %1655 : Tensor = prim::GetAttr[name="weight"](%1630)
  %1656 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm
  %query.9 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.87, %1656, %1655, %1654, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1658 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1659 : Bool(17:56, 4:14, 7:1) = aten::slice(%1658, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1660 : Bool(17:56, 4:14, 1:1) = aten::slice(%1659, %21, %52, %51, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1661 : Tensor[] = prim::ListConstruct(%1660, %token_type_mat.13), scope: __module.funnel/__module.funnel.encoder
  %tensor.16 : Bool(17:32, 4:8, 8:1) = aten::cat(%1661, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1663 : Bool(17:32, 4:8, 8:1) = aten::slice(%tensor.16, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1664 : Bool(17:32, 4:8, 8:1) = aten::slice(%1663, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.15 : Bool(17:32, 4:8, 4:2) = aten::slice(%1664, %21, %52, %43, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1666 : Float(4:14, 7:1) = aten::slice(%cls_mask.4, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1667 : Float(4:14, 1:1) = aten::slice(%1666, %51, %52, %51, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1668 : Tensor[] = prim::ListConstruct(%1667, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder
  %tensor.17 : Float(4:8, 8:1) = aten::cat(%1668, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1670 : Float(4:8, 8:1) = aten::slice(%tensor.17, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask.5 : Float(4:8, 4:2) = aten::slice(%1670, %51, %52, %43, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1672 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix : Float(17:7, 6:1) = aten::slice(%1672, %51, %52, %43, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1674 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1675 : Float(17:7, 1:1) = aten::slice(%1674, %51, %52, %51, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1676 : Tensor[] = prim::ListConstruct(%1675, %suffix), scope: __module.funnel/__module.funnel.encoder
  %tensor.18 : Float(17:7, 7:1) = aten::cat(%1676, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1678 : Float(17:7, 7:1) = aten::slice(%tensor.18, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1679 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1678, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1680 : Float(17:7, 1:7, 7:1) = aten::slice(%1679, %21, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %tensor.19 : Float(17:7, 1:7, 7:1, 1:1) = aten::unsqueeze(%1680, %39), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %input.88 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%tensor.19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1683 : int[] = prim::ListConstruct(%21, %51), scope: __module.funnel/__module.funnel.encoder
  %1684 : int[] = prim::ListConstruct(%21, %51), scope: __module.funnel/__module.funnel.encoder
  %1685 : int[] = prim::ListConstruct(%52, %52), scope: __module.funnel/__module.funnel.encoder
  %1686 : int[] = prim::ListConstruct(%51, %51), scope: __module.funnel/__module.funnel.encoder
  %1687 : Float(17:4, 1:4, 4:1, 1:1) = aten::max_pool2d(%input.88, %1683, %1684, %1685, %1686, %44), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
  %tensor : Float(17:4, 1:4, 4:1, 1:1) = aten::neg(%1687), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1689 : Float(17:4, 1:4, 4:1, 1:1) = aten::slice(%tensor, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1690 : Float(17:4, 4:1, 1:1) = aten::select(%1689, %51, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1691 : Float(17:4, 4:1, 1:1) = aten::slice(%1690, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask : Float(17:4, 4:1) = aten::select(%1691, %21, %52), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1693 : __torch__.transformers.modeling_funnel.___torch_mangle_4265.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%77)
  %1694 : __torch__.transformers.modeling_funnel.___torch_mangle_4259.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%77)
  %1695 : __torch__.torch.nn.modules.normalization.___torch_mangle_4258.LayerNorm = prim::GetAttr[name="layer_norm"](%1694)
  %1696 : __torch__.torch.nn.modules.linear.___torch_mangle_4257.Linear = prim::GetAttr[name="post_proj"](%1694)
  %1697 : Tensor = prim::GetAttr[name="seg_embed"](%1694)
  %1698 : Tensor = prim::GetAttr[name="r_s_bias"](%1694)
  %1699 : Tensor = prim::GetAttr[name="r_kernel"](%1694)
  %1700 : Tensor = prim::GetAttr[name="r_r_bias"](%1694)
  %1701 : Tensor = prim::GetAttr[name="r_w_bias"](%1694)
  %1702 : __torch__.torch.nn.modules.linear.___torch_mangle_4256.Linear = prim::GetAttr[name="v_head"](%1694)
  %1703 : __torch__.torch.nn.modules.linear.___torch_mangle_4255.Linear = prim::GetAttr[name="k_head"](%1694)
  %1704 : __torch__.torch.nn.modules.linear.___torch_mangle_4254.Linear = prim::GetAttr[name="q_head"](%1694)
  %1705 : int = aten::size(%query.9, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1706 : int = aten::size(%query.9, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1707 : int = aten::size(%query.9, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:531:0
  %1708 : Tensor = prim::GetAttr[name="weight"](%1704)
  %1709 : Float(768:1, 768:768) = aten::t(%1708), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1710 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1709), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1711 : int[] = prim::ListConstruct(%1705, %1706, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %q_head.19 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1710, %1711), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:535:0
  %1713 : Tensor = prim::GetAttr[name="bias"](%1703)
  %1714 : Tensor = prim::GetAttr[name="weight"](%1703)
  %1715 : Float(768:1, 768:768) = aten::t(%1714), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.46 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1715), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %1717 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.46, %1713, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1678:0
  %1718 : int[] = prim::ListConstruct(%1705, %1707, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1719 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1717, %1718), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:537:0
  %1720 : Tensor = prim::GetAttr[name="bias"](%1702)
  %1721 : Tensor = prim::GetAttr[name="weight"](%1702)
  %1722 : Float(768:1, 768:768) = aten::t(%1721), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.47 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1722), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %1724 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.47, %1720, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1678:0
  %1725 : int[] = prim::ListConstruct(%1705, %1707, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1726 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1724, %1725), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.20 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.19, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.10 : Float(12:64, 64:1) = aten::mul(%1701, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:542:0
  %1729 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_w_bias.10, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:544:0
  %1730 : Tensor[] = prim::ListConstruct(%1729, %1719), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %content_score.10 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%36, %1730), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %v.10 : Float(12:64, 64:1) = aten::mul(%1700, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:486:0
  %1733 : Tensor[] = prim::ListConstruct(%234, %1699), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1734 : Float(8:768, 12:64, 64:1) = aten::einsum(%37, %1733), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1735 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %v.10, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:493:0
  %1736 : Tensor[] = prim::ListConstruct(%1735, %1734), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.55 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%38, %1736), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1738 : int = aten::size(%positional_attn.55, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1739 : int = aten::size(%positional_attn.55, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1740 : int = aten::size(%positional_attn.55, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1741 : int = aten::size(%positional_attn.55, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.10 : Long() = prim::NumToTensor(%1741), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1743 : int[] = prim::ListConstruct(%1738, %1739, %1741, %1740), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.56 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.55, %1743), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:428:0
  %1745 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.56, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1746 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%1745, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1747 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1746, %21, %51, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.57 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1747, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1749 : Long() = aten::sub(%max_rel_len.10, %16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %1750 : int = aten::Int(%1749), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1751 : int[] = prim::ListConstruct(%1738, %1739, %1740, %1750), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.58 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.57, %1751), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.59 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.58, %39, %52, %1707, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.60 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.59, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:498:0
  %1755 : int = aten::size(%token_type_mat.15, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1756 : int = aten::size(%token_type_mat.15, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1757 : int = aten::size(%token_type_mat.15, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.10 : Float(12:64, 64:1) = aten::mul(%1698, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:508:0
  %1759 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_s_bias.10, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:511:0
  %1760 : Tensor[] = prim::ListConstruct(%1759, %1697), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1761 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%40, %1760), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1762 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1763 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1762, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1764 : int = aten::size(%q_head.20, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1765 : int[] = prim::ListConstruct(%1755, %1764, %1756, %1757), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %token_type_mat.16 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%1763, %1765, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1767 : Tensor[] = aten::split(%1761, %51, %43), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:371:0
  %diff_token_type.10 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.10 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1767), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1770 : int = aten::size(%token_type_mat.16, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1771 : int = aten::size(%token_type_mat.16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1772 : int = aten::size(%token_type_mat.16, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1773 : int = aten::size(%token_type_mat.16, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1774 : int[] = prim::ListConstruct(%1770, %1771, %1772, %1773), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1775 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.10, %1774, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1776 : int = aten::size(%token_type_mat.16, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1777 : int = aten::size(%token_type_mat.16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1778 : int = aten::size(%token_type_mat.16, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1779 : int = aten::size(%token_type_mat.16, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1780 : int[] = prim::ListConstruct(%1776, %1777, %1778, %1779), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1781 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.10, %1780, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.19 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.16, %1775, %1781), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.20 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.19, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:522:0
  %1784 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.10, %positional_attn.60, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.28 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%1784, %token_type_attn.20, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.29 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.28, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:553:0
  %1787 : Float(17:4, 4:1) = aten::slice(%attention_mask, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1788 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%1787, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1789 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1788, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1790 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%1789, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1791 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%1790, %51, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:396:0
  %1792 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%1791, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.30 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.29, %1792, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %input.89 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.30, %43, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:558:0
  %1795 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.89, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1796 : Tensor[] = prim::ListConstruct(%1795, %1726), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %attn_vec.10 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%42, %1796), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1798 : int[] = prim::ListConstruct(%1705, %1706, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %input.90 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.10, %1798), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:565:0
  %1800 : Tensor = prim::GetAttr[name="bias"](%1696)
  %1801 : Tensor = prim::GetAttr[name="weight"](%1696)
  %1802 : Float(768:1, 768:768) = aten::t(%1801), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.48 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.90, %1802), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.91 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.48, %1800, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.91, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.92 : Float(17:3072, 4:768, 768:1) = aten::add(%query.9, %attn_out.10, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:568:0
  %1807 : Tensor = prim::GetAttr[name="bias"](%1695)
  %1808 : Tensor = prim::GetAttr[name="weight"](%1695)
  %1809 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm
  %input.93 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.92, %1809, %1808, %1807, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1811 : __torch__.torch.nn.modules.normalization.___torch_mangle_4264.LayerNorm = prim::GetAttr[name="layer_norm"](%1693)
  %1812 : __torch__.torch.nn.modules.linear.___torch_mangle_4262.Linear = prim::GetAttr[name="linear_2"](%1693)
  %1813 : __torch__.torch.nn.modules.linear.___torch_mangle_4260.Linear = prim::GetAttr[name="linear_1"](%1693)
  %1814 : Tensor = prim::GetAttr[name="bias"](%1813)
  %1815 : Tensor = prim::GetAttr[name="weight"](%1813)
  %1816 : Float(768:1, 3072:768) = aten::t(%1815), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.93, %1816), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.10 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.49, %1814, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1819 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.10, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1820 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.10, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1821 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1820, %29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1822 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.10, %1821, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1823 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1822, %30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1824 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1823), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1825 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1824, %31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.94 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1819, %1825), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.95 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.94, %32, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1828 : Tensor = prim::GetAttr[name="bias"](%1812)
  %1829 : Tensor = prim::GetAttr[name="weight"](%1812)
  %1830 : Float(3072:1, 768:3072) = aten::t(%1829), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.95, %1830), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.96 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.50, %1828, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.96, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.97 : Float(17:3072, 4:768, 768:1) = aten::add(%input.93, %h.10, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/modeling_funnel.py:588:0
  %1835 : Tensor = prim::GetAttr[name="bias"](%1811)
  %1836 : Tensor = prim::GetAttr[name="weight"](%1811)
  %1837 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm
  %query.10 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.97, %1837, %1836, %1835, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1839 : __torch__.transformers.modeling_funnel.___torch_mangle_4280.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%74)
  %1840 : __torch__.transformers.modeling_funnel.___torch_mangle_4274.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%74)
  %1841 : __torch__.torch.nn.modules.normalization.___torch_mangle_4273.LayerNorm = prim::GetAttr[name="layer_norm"](%1840)
  %1842 : __torch__.torch.nn.modules.linear.___torch_mangle_4272.Linear = prim::GetAttr[name="post_proj"](%1840)
  %1843 : Tensor = prim::GetAttr[name="seg_embed"](%1840)
  %1844 : Tensor = prim::GetAttr[name="r_s_bias"](%1840)
  %1845 : Tensor = prim::GetAttr[name="r_kernel"](%1840)
  %1846 : Tensor = prim::GetAttr[name="r_r_bias"](%1840)
  %1847 : Tensor = prim::GetAttr[name="r_w_bias"](%1840)
  %1848 : __torch__.torch.nn.modules.linear.___torch_mangle_4271.Linear = prim::GetAttr[name="v_head"](%1840)
  %1849 : __torch__.torch.nn.modules.linear.___torch_mangle_4270.Linear = prim::GetAttr[name="k_head"](%1840)
  %1850 : __torch__.torch.nn.modules.linear.___torch_mangle_4269.Linear = prim::GetAttr[name="q_head"](%1840)
  %1851 : int = aten::size(%query.10, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1852 : int = aten::size(%query.10, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1853 : int = aten::size(%query.10, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:531:0
  %1854 : Tensor = prim::GetAttr[name="weight"](%1850)
  %1855 : Float(768:1, 768:768) = aten::t(%1854), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1856 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1855), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1857 : int[] = prim::ListConstruct(%1851, %1852, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %q_head.21 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1856, %1857), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:535:0
  %1859 : Tensor = prim::GetAttr[name="bias"](%1849)
  %1860 : Tensor = prim::GetAttr[name="weight"](%1849)
  %1861 : Float(768:1, 768:768) = aten::t(%1860), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.51 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1861), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %1863 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.51, %1859, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1678:0
  %1864 : int[] = prim::ListConstruct(%1851, %1853, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1865 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1863, %1864), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:537:0
  %1866 : Tensor = prim::GetAttr[name="bias"](%1848)
  %1867 : Tensor = prim::GetAttr[name="weight"](%1848)
  %1868 : Float(768:1, 768:768) = aten::t(%1867), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.52 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1868), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %1870 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.52, %1866, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1678:0
  %1871 : int[] = prim::ListConstruct(%1851, %1853, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1872 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1870, %1871), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.22 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.21, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.11 : Float(12:64, 64:1) = aten::mul(%1847, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:542:0
  %1875 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_w_bias.11, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:544:0
  %1876 : Tensor[] = prim::ListConstruct(%1875, %1865), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %content_score.11 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%36, %1876), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %v.11 : Float(12:64, 64:1) = aten::mul(%1846, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:486:0
  %1879 : Tensor[] = prim::ListConstruct(%234, %1845), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1880 : Float(8:768, 12:64, 64:1) = aten::einsum(%37, %1879), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1881 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %v.11, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:493:0
  %1882 : Tensor[] = prim::ListConstruct(%1881, %1880), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.61 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%38, %1882), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1884 : int = aten::size(%positional_attn.61, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1885 : int = aten::size(%positional_attn.61, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1886 : int = aten::size(%positional_attn.61, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1887 : int = aten::size(%positional_attn.61, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.11 : Long() = prim::NumToTensor(%1887), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1889 : int[] = prim::ListConstruct(%1884, %1885, %1887, %1886), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.62 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.61, %1889), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:428:0
  %1891 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.62, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1892 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%1891, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1893 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1892, %21, %51, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.63 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1893, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1895 : Long() = aten::sub(%max_rel_len.11, %16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %1896 : int = aten::Int(%1895), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1897 : int[] = prim::ListConstruct(%1884, %1885, %1886, %1896), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.64 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.63, %1897), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.65 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.64, %39, %52, %1853, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.66 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.65, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:498:0
  %1901 : int = aten::size(%token_type_mat.15, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1902 : int = aten::size(%token_type_mat.15, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1903 : int = aten::size(%token_type_mat.15, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.11 : Float(12:64, 64:1) = aten::mul(%1844, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:508:0
  %1905 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_s_bias.11, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:511:0
  %1906 : Tensor[] = prim::ListConstruct(%1905, %1843), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1907 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%40, %1906), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1908 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1909 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1908, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1910 : int = aten::size(%q_head.22, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1911 : int[] = prim::ListConstruct(%1901, %1910, %1902, %1903), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %token_type_mat.17 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%1909, %1911, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1913 : Tensor[] = aten::split(%1907, %51, %43), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:371:0
  %diff_token_type.11 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.11 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1913), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1916 : int = aten::size(%token_type_mat.17, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1917 : int = aten::size(%token_type_mat.17, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1918 : int = aten::size(%token_type_mat.17, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1919 : int = aten::size(%token_type_mat.17, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1920 : int[] = prim::ListConstruct(%1916, %1917, %1918, %1919), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1921 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.11, %1920, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1922 : int = aten::size(%token_type_mat.17, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1923 : int = aten::size(%token_type_mat.17, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1924 : int = aten::size(%token_type_mat.17, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1925 : int = aten::size(%token_type_mat.17, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1926 : int[] = prim::ListConstruct(%1922, %1923, %1924, %1925), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1927 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.11, %1926, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.21 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.17, %1921, %1927), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.22 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.21, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:522:0
  %1930 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.11, %positional_attn.66, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.31 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%1930, %token_type_attn.22, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.32 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.31, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:553:0
  %1933 : Float(17:4, 4:1) = aten::slice(%attention_mask, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1934 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%1933, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1935 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1934, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1936 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%1935, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1937 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%1936, %51, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:396:0
  %1938 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%1937, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.33 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.32, %1938, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %input.98 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.33, %43, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:558:0
  %1941 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.98, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1942 : Tensor[] = prim::ListConstruct(%1941, %1872), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %attn_vec.11 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%42, %1942), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1944 : int[] = prim::ListConstruct(%1851, %1852, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %input.99 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.11, %1944), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:565:0
  %1946 : Tensor = prim::GetAttr[name="bias"](%1842)
  %1947 : Tensor = prim::GetAttr[name="weight"](%1842)
  %1948 : Float(768:1, 768:768) = aten::t(%1947), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.53 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.99, %1948), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.100 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.53, %1946, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.100, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.101 : Float(17:3072, 4:768, 768:1) = aten::add(%query.10, %attn_out.11, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:568:0
  %1953 : Tensor = prim::GetAttr[name="bias"](%1841)
  %1954 : Tensor = prim::GetAttr[name="weight"](%1841)
  %1955 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm
  %input.102 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.101, %1955, %1954, %1953, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1957 : __torch__.torch.nn.modules.normalization.___torch_mangle_4279.LayerNorm = prim::GetAttr[name="layer_norm"](%1839)
  %1958 : __torch__.torch.nn.modules.linear.___torch_mangle_4277.Linear = prim::GetAttr[name="linear_2"](%1839)
  %1959 : __torch__.torch.nn.modules.linear.___torch_mangle_4275.Linear = prim::GetAttr[name="linear_1"](%1839)
  %1960 : Tensor = prim::GetAttr[name="bias"](%1959)
  %1961 : Tensor = prim::GetAttr[name="weight"](%1959)
  %1962 : Float(768:1, 3072:768) = aten::t(%1961), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.102, %1962), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.11 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.54, %1960, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1965 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.11, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1966 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.11, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1967 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1966, %29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1968 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.11, %1967, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1969 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1968, %30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1970 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1969), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1971 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1970, %31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.103 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1965, %1971), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.104 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.103, %32, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1974 : Tensor = prim::GetAttr[name="bias"](%1958)
  %1975 : Tensor = prim::GetAttr[name="weight"](%1958)
  %1976 : Float(3072:1, 768:3072) = aten::t(%1975), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.55 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.104, %1976), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.105 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.55, %1974, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.105, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.106 : Float(17:3072, 4:768, 768:1) = aten::add(%input.102, %h.11, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/modeling_funnel.py:588:0
  %1981 : Tensor = prim::GetAttr[name="bias"](%1957)
  %1982 : Tensor = prim::GetAttr[name="weight"](%1957)
  %1983 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm
  %query.11 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.106, %1983, %1982, %1981, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1985 : __torch__.transformers.modeling_funnel.___torch_mangle_4295.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%71)
  %1986 : __torch__.transformers.modeling_funnel.___torch_mangle_4289.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%71)
  %1987 : __torch__.torch.nn.modules.normalization.___torch_mangle_4288.LayerNorm = prim::GetAttr[name="layer_norm"](%1986)
  %1988 : __torch__.torch.nn.modules.linear.___torch_mangle_4287.Linear = prim::GetAttr[name="post_proj"](%1986)
  %1989 : Tensor = prim::GetAttr[name="seg_embed"](%1986)
  %1990 : Tensor = prim::GetAttr[name="r_s_bias"](%1986)
  %1991 : Tensor = prim::GetAttr[name="r_kernel"](%1986)
  %1992 : Tensor = prim::GetAttr[name="r_r_bias"](%1986)
  %1993 : Tensor = prim::GetAttr[name="r_w_bias"](%1986)
  %1994 : __torch__.torch.nn.modules.linear.___torch_mangle_4286.Linear = prim::GetAttr[name="v_head"](%1986)
  %1995 : __torch__.torch.nn.modules.linear.___torch_mangle_4285.Linear = prim::GetAttr[name="k_head"](%1986)
  %1996 : __torch__.torch.nn.modules.linear.___torch_mangle_4284.Linear = prim::GetAttr[name="q_head"](%1986)
  %1997 : int = aten::size(%query.11, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %1998 : int = aten::size(%query.11, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %1999 : int = aten::size(%query.11, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:531:0
  %2000 : Tensor = prim::GetAttr[name="weight"](%1996)
  %2001 : Float(768:1, 768:768) = aten::t(%2000), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2002 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.11, %2001), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2003 : int[] = prim::ListConstruct(%1997, %1998, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %q_head.23 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2002, %2003), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:535:0
  %2005 : Tensor = prim::GetAttr[name="bias"](%1995)
  %2006 : Tensor = prim::GetAttr[name="weight"](%1995)
  %2007 : Float(768:1, 768:768) = aten::t(%2006), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.56 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.11, %2007), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %2009 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.56, %2005, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1678:0
  %2010 : int[] = prim::ListConstruct(%1997, %1999, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2011 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2009, %2010), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:537:0
  %2012 : Tensor = prim::GetAttr[name="bias"](%1994)
  %2013 : Tensor = prim::GetAttr[name="weight"](%1994)
  %2014 : Float(768:1, 768:768) = aten::t(%2013), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.57 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.11, %2014), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %2016 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.57, %2012, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1678:0
  %2017 : int[] = prim::ListConstruct(%1997, %1999, %33, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2018 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2016, %2017), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.24 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.23, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.12 : Float(12:64, 64:1) = aten::mul(%1993, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:542:0
  %2021 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %r_w_bias.12, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:544:0
  %2022 : Tensor[] = prim::ListConstruct(%2021, %2011), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %content_score.12 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%36, %2022), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %v.12 : Float(12:64, 64:1) = aten::mul(%1992, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:486:0
  %2025 : Tensor[] = prim::ListConstruct(%234, %1991), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2026 : Float(8:768, 12:64, 64:1) = aten::einsum(%37, %2025), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2027 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %v.12, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:493:0
  %2028 : Tensor[] = prim::ListConstruct(%2027, %2026), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.67 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%38, %2028), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2030 : int = aten::size(%positional_attn.67, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2031 : int = aten::size(%positional_attn.67, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2032 : int = aten::size(%positional_attn.67, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2033 : int = aten::size(%positional_attn.67, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.12 : Long() = prim::NumToTensor(%2033), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2035 : int[] = prim::ListConstruct(%2030, %2031, %2033, %2032), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.68 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.67, %2035), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:428:0
  %2037 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.68, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2038 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%2037, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2039 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%2038, %21, %51, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.69 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%2039, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2041 : Long() = aten::sub(%max_rel_len.12, %16, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %2042 : int = aten::Int(%2041), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2043 : int[] = prim::ListConstruct(%2030, %2031, %2032, %2042), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.70 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.69, %2043), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.71 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.70, %39, %52, %1999, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.72 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.71, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:498:0
  %2047 : int = aten::size(%token_type_mat.15, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2048 : int = aten::size(%token_type_mat.15, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2049 : int = aten::size(%token_type_mat.15, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.12 : Float(12:64, 64:1) = aten::mul(%1990, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:508:0
  %2051 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %r_s_bias.12, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:511:0
  %2052 : Tensor[] = prim::ListConstruct(%2051, %1989), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2053 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%40, %2052), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2054 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2055 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%2054, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2056 : int = aten::size(%q_head.24, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2057 : int[] = prim::ListConstruct(%2047, %2056, %2048, %2049), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %token_type_mat.18 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%2055, %2057, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2059 : Tensor[] = aten::split(%2053, %51, %43), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:371:0
  %diff_token_type.12 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.12 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%2059), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2062 : int = aten::size(%token_type_mat.18, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2063 : int = aten::size(%token_type_mat.18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2064 : int = aten::size(%token_type_mat.18, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2065 : int = aten::size(%token_type_mat.18, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2066 : int[] = prim::ListConstruct(%2062, %2063, %2064, %2065), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2067 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.12, %2066, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2068 : int = aten::size(%token_type_mat.18, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2069 : int = aten::size(%token_type_mat.18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2070 : int = aten::size(%token_type_mat.18, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2071 : int = aten::size(%token_type_mat.18, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2072 : int[] = prim::ListConstruct(%2068, %2069, %2070, %2071), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2073 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.12, %2072, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.23 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.18, %2067, %2073), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.24 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.23, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:522:0
  %2076 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.12, %positional_attn.72, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.34 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%2076, %token_type_attn.24, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.35 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.34, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:553:0
  %2079 : Float(17:4, 4:1) = aten::slice(%attention_mask, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2080 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%2079, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2081 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%2080, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2082 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%2081, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2083 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%2082, %51, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:396:0
  %2084 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%2083, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.36 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.35, %2084, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %input.107 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.36, %43, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:558:0
  %2087 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.107, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %2088 : Tensor[] = prim::ListConstruct(%2087, %2018), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %attn_vec.12 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%42, %2088), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2090 : int[] = prim::ListConstruct(%1997, %1998, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %input.108 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.12, %2090), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:565:0
  %2092 : Tensor = prim::GetAttr[name="bias"](%1988)
  %2093 : Tensor = prim::GetAttr[name="weight"](%1988)
  %2094 : Float(768:1, 768:768) = aten::t(%2093), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.58 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.108, %2094), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.109 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.58, %2092, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.12 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.109, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:3072, 4:768, 768:1) = aten::add(%query.11, %attn_out.12, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:568:0
  %2099 : Tensor = prim::GetAttr[name="bias"](%1987)
  %2100 : Tensor = prim::GetAttr[name="weight"](%1987)
  %2101 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm
  %input.111 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.110, %2101, %2100, %2099, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %2103 : __torch__.torch.nn.modules.normalization.___torch_mangle_4294.LayerNorm = prim::GetAttr[name="layer_norm"](%1985)
  %2104 : __torch__.torch.nn.modules.linear.___torch_mangle_4292.Linear = prim::GetAttr[name="linear_2"](%1985)
  %2105 : __torch__.torch.nn.modules.linear.___torch_mangle_4290.Linear = prim::GetAttr[name="linear_1"](%1985)
  %2106 : Tensor = prim::GetAttr[name="bias"](%2105)
  %2107 : Tensor = prim::GetAttr[name="weight"](%2105)
  %2108 : Float(768:1, 3072:768) = aten::t(%2107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.111, %2108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.12 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.59, %2106, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2111 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.12, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2112 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.12, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2113 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2112, %29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2114 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.12, %2113, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2115 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2114, %30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2116 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%2115), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2117 : Float(17:12288, 4:3072, 3072:1) = aten::add(%2116, %31, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.112 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2111, %2117), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.113 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.112, %32, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2120 : Tensor = prim::GetAttr[name="bias"](%2104)
  %2121 : Tensor = prim::GetAttr[name="weight"](%2104)
  %2122 : Float(3072:1, 768:3072) = aten::t(%2121), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.60 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.113, %2122), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.114 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.60, %2120, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.12 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.114, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.115 : Float(17:3072, 4:768, 768:1) = aten::add(%input.111, %h.12, %51), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/modeling_funnel.py:588:0
  %2127 : Tensor = prim::GetAttr[name="bias"](%2103)
  %2128 : Tensor = prim::GetAttr[name="weight"](%2103)
  %2129 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm
  %x.13 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.115, %2129, %2128, %2127, %45, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2131 : (Float(17:9984, 13:768, 768:1), Float(17:9984, 13:768, 768:1), Float(17:9984, 13:768, 768:1), Float(17:3072, 4:768, 768:1), Float(17:9984, 13:768, 768:1)) = prim::TupleConstruct(%hidden.1, %hidden.1, %hidden.1, %x.13, %hidden.1)
  %2132 : Float(17:9984, 13:768, 768:1), %2133 : Float(17:9984, 13:768, 768:1), %2134 : Float(17:9984, 13:768, 768:1), %2135 : Float(17:3072, 4:768, 768:1), %2136 : Float(17:9984, 13:768, 768:1) = prim::TupleUnpack(%2131)
  %2137 : __torch__.torch.nn.modules.container.___torch_mangle_4333.ModuleList = prim::GetAttr[name="layers"](%53)
  %2138 : __torch__.transformers.modeling_funnel.___torch_mangle_4332.FunnelLayer = prim::GetAttr[name="1"](%2137)
  %2139 : __torch__.torch.nn.modules.container.___torch_mangle_4333.ModuleList = prim::GetAttr[name="layers"](%53)
  %2140 : __torch__.transformers.modeling_funnel.___torch_mangle_4317.FunnelLayer = prim::GetAttr[name="0"](%2139)
  %2141 : int = aten::size(%2133, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:705:0
  %target_len : Long() = prim::NumToTensor(%2141), scope: __module.funnel/__module.funnel.decoder
  %2143 : Float(17:3072, 4:768, 768:1) = aten::slice(%2135, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
  %cls : Float(17:3072, 1:768, 768:1) = aten::slice(%2143, %51, %52, %51, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
  %2145 : Float(17:3072, 4:768, 768:1) = aten::slice(%2135, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
  %x.14 : Float(17:3072, 3:768, 768:1) = aten::slice(%2145, %51, %51, %18, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
  %input.116 : Float(17:9216, 12:768, 768:1) = aten::repeat_interleave(%x.14, %50, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:674:0
  %2148 : int[] = prim::ListConstruct(%52, %52, %52, %39, %52, %52), scope: __module.funnel/__module.funnel.decoder
  %output.61 : Float(17:11520, 15:768, 768:1) = aten::constant_pad_nd(%input.116, %2148, %52), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
  %2150 : Long() = aten::sub(%target_len, %16, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
  %2151 : int = aten::Int(%2150), scope: __module.funnel/__module.funnel.decoder
  %2152 : Float(17:11520, 15:768, 768:1) = aten::slice(%output.61, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
  %output.62 : Float(17:11520, 12:768, 768:1) = aten::slice(%2152, %51, %52, %2151, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
  %2154 : Tensor[] = prim::ListConstruct(%cls, %output.62), scope: __module.funnel/__module.funnel.decoder
  %upsampled_hidden : Float(17:9984, 13:768, 768:1) = aten::cat(%2154, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:679:0
  %input_embeds : Float(17:9984, 13:768, 768:1) = aten::add(%upsampled_hidden, %2136, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:710:0
  %2157 : int = aten::size(%input_embeds, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:195:0
  %seq_len.38 : Long() = prim::NumToTensor(%2157), scope: __module.funnel/__module.funnel.decoder
  %freq_seq : Float(384:1) = aten::arange(%52, %10, %11, %12, %52, %49, %48), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:247:0
  %2160 : Float(384:1) = aten::div(%freq_seq, %13), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:248:0
  %2161 : Float() = aten::to(%14, %49, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
  %2162 : Float() = aten::detach(%2161), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
  %2163 : Float(384:1) = aten::pow(%2162, %2160), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
  %2164 : Float(384:1) = aten::reciprocal(%2163), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:400:0
  %inv_freq : Float(384:1) = aten::mul(%2164, %16), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:400:0
  %2166 : Long() = aten::neg(%seq_len.38), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
  %2167 : Long() = aten::mul(%2166, %17), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
  %2168 : Scalar = aten::ScalarImplicit(%2167), scope: __module.funnel/__module.funnel.decoder
  %2169 : Long() = aten::mul(%seq_len.38, %17), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
  %2170 : Scalar = aten::ScalarImplicit(%2169), scope: __module.funnel/__module.funnel.decoder
  %rel_pos_id : Float(52:1) = aten::arange(%2168, %2170, %11, %12, %52, %49, %48), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
  %zero_offset : Long() = aten::mul(%seq_len.38, %17), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:251:0
  %2173 : Float(52:1) = aten::slice(%rel_pos_id, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
  %2174 : Float(52:1, 1:1) = aten::unsqueeze(%2173, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
  %2175 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq, %52), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
  %sinusoid : Float(52:384, 384:1) = aten::mul(%2174, %2175), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
  %input.117 : Float(52:384, 384:1) = aten::sin(%sinusoid), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:253:0
  %sin_embed : Float(52:384, 384:1) = aten::dropout(%input.117, %47, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %input.118 : Float(52:384, 384:1) = aten::cos(%sinusoid), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:254:0
  %cos_embed : Float(52:384, 384:1) = aten::dropout(%input.118, %47, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
  %2181 : Tensor[] = prim::ListConstruct(%sin_embed, %cos_embed), scope: __module.funnel/__module.funnel.decoder
  %pos_embed : Float(52:768, 768:1) = aten::cat(%2181, %43), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:255:0
  %pos : Float(13:1) = aten::arange(%52, %2157, %51, %12, %52, %49, %48), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:257:0
  %2184 : Float() = aten::select(%pos, %52, %52), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
  %2185 : Float() = aten::select(%pos, %52, %52), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
  %ref_point.6 : Float() = aten::sub(%2184, %2185, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
  %max_dist.6 : Float() = aten::add(%ref_point.6, %19, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:315:0
  %2188 : Scalar = aten::ScalarImplicit(%max_dist.6), scope: __module.funnel/__module.funnel.decoder
  %2189 : Float() = aten::select(%pos, %52, %52), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
  %2190 : Float() = aten::select(%pos, %52, %43), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
  %min_dist.6 : Float() = aten::sub(%2189, %2190, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
  %2192 : Float() = aten::sub(%min_dist.6, %16, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
  %2193 : Scalar = aten::ScalarImplicit(%2192), scope: __module.funnel/__module.funnel.decoder
  %rel_pos.16 : Long(26:1) = aten::arange(%2188, %2193, %43, %50, %52, %49, %48), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
  %2195 : Long(26:1) = aten::slice(%rel_pos.16, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
  %2196 : Long(26:1, 1:1) = aten::unsqueeze(%2195, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
  %rel_pos.17 : Long(26:1, 1:1) = aten::add(%2196, %zero_offset, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
  %2198 : int = aten::size(%rel_pos.17, %52), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:285:0
  %2199 : int[] = prim::ListConstruct(%2198, %46), scope: __module.funnel/__module.funnel.decoder
  %rel_pos.18 : Long(26:1, 768:0) = aten::expand(%rel_pos.17, %2199, %48), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:285:0
  %2201 : Float(26:768, 768:1) = aten::gather(%pos_embed, %52, %rel_pos.18, %48), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:286:0
  %2202 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %2203 : Long(17:13, 13:1) = aten::slice(%2202, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %2204 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%2203, %21), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %2205 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %2206 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%2205, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %token_type_mat.19 : Bool(17:169, 13:13, 13:1) = aten::eq(%2204, %2206), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:22:0
  %cls_ids : Bool(17:13, 13:1) = aten::eq(%token_type_ids, %21), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:22:0
  %2209 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %2210 : Bool(17:13, 13:1) = aten::slice(%2209, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %2211 : Bool(17:13, 13:1, 1:1) = aten::unsqueeze(%2210, %21), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %2212 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %2213 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%2212, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %cls_mat : Bool(17:169, 13:13, 13:1) = aten::__or__(%2211, %2213), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %token_type_mat.20 : Bool(17:169, 13:13, 13:1) = aten::__or__(%cls_mat, %token_type_mat.19), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:211:0
  %2216 : Long() = aten::sub(%seq_len.38, %16, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
  %2217 : int = aten::Int(%2216), scope: __module.funnel/__module.funnel.decoder
  %2218 : Long() = aten::sub(%seq_len.38, %16, %51), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
  %2219 : int = aten::Int(%2218), scope: __module.funnel/__module.funnel.decoder
  %2220 : int[] = prim::ListConstruct(%2217, %2219), scope: __module.funnel/__module.funnel.decoder
  %input.119 : Float(12:12, 12:1) = aten::ones(%2220, %12, %52, %49, %48), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
  %2222 : int[] = prim::ListConstruct(%51, %52, %51, %52), scope: __module.funnel/__module.funnel.decoder
  %cls_mask : Float(13:13, 13:1) = aten::constant_pad_nd(%input.119, %2222, %52), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
  %2224 : __torch__.transformers.modeling_funnel.___torch_mangle_4316.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%2140)
  %2225 : __torch__.transformers.modeling_funnel.___torch_mangle_4310.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%2140)
  %2226 : __torch__.torch.nn.modules.normalization.___torch_mangle_4309.LayerNorm = prim::GetAttr[name="layer_norm"](%2225)
  %2227 : __torch__.torch.nn.modules.linear.___torch_mangle_4308.Linear = prim::GetAttr[name="post_proj"](%2225)
  %2228 : Tensor = prim::GetAttr[name="seg_embed"](%2225)
  %2229 : Tensor = prim::GetAttr[name="r_s_bias"](%2225)
  %2230 : Tensor = prim::GetAttr[name="r_kernel"](%2225)
  %2231 : Tensor = prim::GetAttr[name="r_r_bias"](%2225)
  %2232 : Tensor = prim::GetAttr[name="r_w_bias"](%2225)
  %2233 : __torch__.torch.nn.modules.linear.___torch_mangle_4307.Linear = prim::GetAttr[name="v_head"](%2225)
  %2234 : __torch__.torch.nn.modules.linear.___torch_mangle_4306.Linear = prim::GetAttr[name="k_head"](%2225)
  %2235 : __torch__.torch.nn.modules.linear.___torch_mangle_4305.Linear = prim::GetAttr[name="q_head"](%2225)
  %2236 : int = aten::size(%input_embeds, %52), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:530:0
  %2237 : int = aten::size(%input_embeds, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:530:0
  %2238 : int = aten::size(%input_embeds, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:531:0
  %2239 : Tensor = prim::GetAttr[name="weight"](%2235)
  %2240 : Float(768:1, 768:768) = aten::t(%2239), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.q_head # torch/nn/functional.py:1676:0
  %2241 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %2240), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.q_head # torch/nn/functional.py:1676:0
  %2242 : int[] = prim::ListConstruct(%2236, %2237, %33, %34), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %q_head.25 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2241, %2242), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:535:0
  %2244 : Tensor = prim::GetAttr[name="bias"](%2234)
  %2245 : Tensor = prim::GetAttr[name="weight"](%2234)
  %2246 : Float(768:1, 768:768) = aten::t(%2245), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.63 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %2246), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1676:0
  %2248 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.63, %2244, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1678:0
  %2249 : int[] = prim::ListConstruct(%2236, %2238, %33, %34), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2250 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2248, %2249), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:537:0
  %2251 : Tensor = prim::GetAttr[name="bias"](%2233)
  %2252 : Tensor = prim::GetAttr[name="weight"](%2233)
  %2253 : Float(768:1, 768:768) = aten::t(%2252), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.64 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %2253), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1676:0
  %2255 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.64, %2251, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1678:0
  %2256 : int[] = prim::ListConstruct(%2236, %2238, %33, %34), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2257 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2255, %2256), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.25, %35), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.13 : Float(12:64, 64:1) = aten::mul(%2232, %35), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:542:0
  %2260 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %r_w_bias.13, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:544:0
  %2261 : Tensor[] = prim::ListConstruct(%2260, %2250), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %content_score.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%36, %2261), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %v.13 : Float(12:64, 64:1) = aten::mul(%2231, %35), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:486:0
  %2264 : Tensor[] = prim::ListConstruct(%2201, %2230), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2265 : Float(26:768, 12:64, 64:1) = aten::einsum(%37, %2264), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %2266 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %v.13, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:493:0
  %2267 : Tensor[] = prim::ListConstruct(%2266, %2265), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %positional_attn.73 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%38, %2267), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %2269 : int = aten::size(%positional_attn.73, %52), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %2270 : int = aten::size(%positional_attn.73, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %2271 : int = aten::size(%positional_attn.73, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %2272 : int = aten::size(%positional_attn.73, %39), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.13 : Long() = prim::NumToTensor(%2272), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2274 : int[] = prim::ListConstruct(%2269, %2270, %2272, %2271), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %positional_attn.74 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.73, %2274), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:428:0
  %2276 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.74, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %2277 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%2276, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %2278 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2277, %21, %51, %18, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.75 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2278, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %2280 : Long() = aten::sub(%max_rel_len.13, %16, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:430:0
  %2281 : int = aten::Int(%2280), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2282 : int[] = prim::ListConstruct(%2269, %2270, %2271, %2281), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %positional_attn.76 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.75, %2282), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.77 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.76, %39, %52, %2238, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.78 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.77, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:498:0
  %2286 : int = aten::size(%token_type_mat.20, %52), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
  %2287 : int = aten::size(%token_type_mat.20, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
  %2288 : int = aten::size(%token_type_mat.20, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.13 : Float(12:64, 64:1) = aten::mul(%2229, %35), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:508:0
  %2290 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %r_s_bias.13, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:511:0
  %2291 : Tensor[] = prim::ListConstruct(%2290, %2228), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2292 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%40, %2291), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %2293 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.20, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2294 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%2293, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2295 : int = aten::size(%q_head.26, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2296 : int[] = prim::ListConstruct(%2286, %2295, %2287, %2288), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %token_type_mat.21 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%2294, %2296, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2298 : Tensor[] = aten::split(%2292, %51, %43), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/tensor.py:371:0
  %diff_token_type.13 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.13 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%2298), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2301 : int = aten::size(%token_type_mat.21, %52), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2302 : int = aten::size(%token_type_mat.21, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2303 : int = aten::size(%token_type_mat.21, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2304 : int = aten::size(%token_type_mat.21, %39), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2305 : int[] = prim::ListConstruct(%2301, %2302, %2303, %2304), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2306 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.13, %2305, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2307 : int = aten::size(%token_type_mat.21, %52), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2308 : int = aten::size(%token_type_mat.21, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2309 : int = aten::size(%token_type_mat.21, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2310 : int = aten::size(%token_type_mat.21, %39), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2311 : int[] = prim::ListConstruct(%2307, %2308, %2309, %2310), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2312 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.13, %2311, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.25 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.21, %2306, %2312), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.25, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:522:0
  %2315 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.13, %positional_attn.78, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%2315, %token_type_attn.26, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.38 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.37, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:553:0
  %2318 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2319 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%2318, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2320 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2319, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2321 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%2320, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2322 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%2321, %51, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/tensor.py:396:0
  %2323 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%2322, %41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.39 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.38, %2323, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %input.120 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.39, %43, %12), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:558:0
  %2326 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.120, %47, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %2327 : Tensor[] = prim::ListConstruct(%2326, %2257), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %attn_vec.13 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%42, %2327), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %2329 : int[] = prim::ListConstruct(%2236, %2237, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %input.121 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.13, %2329), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:565:0
  %2331 : Tensor = prim::GetAttr[name="bias"](%2227)
  %2332 : Tensor = prim::GetAttr[name="weight"](%2227)
  %2333 : Float(768:1, 768:768) = aten::t(%2332), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.65 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.121, %2333), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.122 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.65, %2331, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.122, %47, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.123 : Float(17:9984, 13:768, 768:1) = aten::add(%input_embeds, %attn_out.13, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:568:0
  %2338 : Tensor = prim::GetAttr[name="bias"](%2226)
  %2339 : Tensor = prim::GetAttr[name="weight"](%2226)
  %2340 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.layer_norm
  %input.124 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.123, %2340, %2339, %2338, %45, %44), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %2342 : __torch__.torch.nn.modules.normalization.___torch_mangle_4315.LayerNorm = prim::GetAttr[name="layer_norm"](%2224)
  %2343 : __torch__.torch.nn.modules.linear.___torch_mangle_4313.Linear = prim::GetAttr[name="linear_2"](%2224)
  %2344 : __torch__.torch.nn.modules.linear.___torch_mangle_4311.Linear = prim::GetAttr[name="linear_1"](%2224)
  %2345 : Tensor = prim::GetAttr[name="bias"](%2344)
  %2346 : Tensor = prim::GetAttr[name="weight"](%2344)
  %2347 : Float(768:1, 3072:768) = aten::t(%2346), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.66 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.124, %2347), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.15 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.66, %2345, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2350 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.15, %27), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2351 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.15, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2352 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2351, %29), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2353 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.15, %2352, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2354 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2353, %30), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2355 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%2354), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2356 : Float(17:39936, 13:3072, 3072:1) = aten::add(%2355, %31, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %input.125 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2350, %2356), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %input.126 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.125, %32, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2359 : Tensor = prim::GetAttr[name="bias"](%2343)
  %2360 : Tensor = prim::GetAttr[name="weight"](%2343)
  %2361 : Float(3072:1, 768:3072) = aten::t(%2360), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.67 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.126, %2361), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.127 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.67, %2359, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.127, %47, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.128 : Float(17:9984, 13:768, 768:1) = aten::add(%input.124, %h.13, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/modeling_funnel.py:588:0
  %2366 : Tensor = prim::GetAttr[name="bias"](%2342)
  %2367 : Tensor = prim::GetAttr[name="weight"](%2342)
  %2368 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.layer_norm
  %query : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.128, %2368, %2367, %2366, %45, %44), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2370 : __torch__.transformers.modeling_funnel.___torch_mangle_4331.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%2138)
  %2371 : __torch__.transformers.modeling_funnel.___torch_mangle_4325.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%2138)
  %2372 : __torch__.torch.nn.modules.normalization.___torch_mangle_4324.LayerNorm = prim::GetAttr[name="layer_norm"](%2371)
  %2373 : __torch__.torch.nn.modules.linear.___torch_mangle_4323.Linear = prim::GetAttr[name="post_proj"](%2371)
  %2374 : Tensor = prim::GetAttr[name="seg_embed"](%2371)
  %2375 : Tensor = prim::GetAttr[name="r_s_bias"](%2371)
  %2376 : Tensor = prim::GetAttr[name="r_kernel"](%2371)
  %2377 : Tensor = prim::GetAttr[name="r_r_bias"](%2371)
  %2378 : Tensor = prim::GetAttr[name="r_w_bias"](%2371)
  %2379 : __torch__.torch.nn.modules.linear.___torch_mangle_4322.Linear = prim::GetAttr[name="v_head"](%2371)
  %2380 : __torch__.torch.nn.modules.linear.___torch_mangle_4321.Linear = prim::GetAttr[name="k_head"](%2371)
  %2381 : __torch__.torch.nn.modules.linear.___torch_mangle_4320.Linear = prim::GetAttr[name="q_head"](%2371)
  %2382 : int = aten::size(%query, %52), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:530:0
  %2383 : int = aten::size(%query, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:530:0
  %2384 : int = aten::size(%query, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:531:0
  %2385 : Tensor = prim::GetAttr[name="weight"](%2381)
  %2386 : Float(768:1, 768:768) = aten::t(%2385), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.q_head # torch/nn/functional.py:1676:0
  %2387 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query, %2386), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.q_head # torch/nn/functional.py:1676:0
  %2388 : int[] = prim::ListConstruct(%2382, %2383, %33, %34), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %q_head.27 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2387, %2388), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:535:0
  %2390 : Tensor = prim::GetAttr[name="bias"](%2380)
  %2391 : Tensor = prim::GetAttr[name="weight"](%2380)
  %2392 : Float(768:1, 768:768) = aten::t(%2391), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.68 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query, %2392), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1676:0
  %2394 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.68, %2390, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1678:0
  %2395 : int[] = prim::ListConstruct(%2382, %2384, %33, %34), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2396 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2394, %2395), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:537:0
  %2397 : Tensor = prim::GetAttr[name="bias"](%2379)
  %2398 : Tensor = prim::GetAttr[name="weight"](%2379)
  %2399 : Float(768:1, 768:768) = aten::t(%2398), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.69 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query, %2399), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1676:0
  %2401 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.69, %2397, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1678:0
  %2402 : int[] = prim::ListConstruct(%2382, %2384, %33, %34), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2403 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2401, %2402), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:538:0
  %q_head : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.27, %35), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias : Float(12:64, 64:1) = aten::mul(%2378, %35), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:542:0
  %2406 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %r_w_bias, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:544:0
  %2407 : Tensor[] = prim::ListConstruct(%2406, %2396), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %content_score : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%36, %2407), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %v : Float(12:64, 64:1) = aten::mul(%2377, %35), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:486:0
  %2410 : Tensor[] = prim::ListConstruct(%2201, %2376), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2411 : Float(26:768, 12:64, 64:1) = aten::einsum(%37, %2410), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %2412 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %v, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:493:0
  %2413 : Tensor[] = prim::ListConstruct(%2412, %2411), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %positional_attn.79 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%38, %2413), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %2415 : int = aten::size(%positional_attn.79, %52), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %2416 : int = aten::size(%positional_attn.79, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %2417 : int = aten::size(%positional_attn.79, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %2418 : int = aten::size(%positional_attn.79, %39), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len : Long() = prim::NumToTensor(%2418), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2420 : int[] = prim::ListConstruct(%2415, %2416, %2418, %2417), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %positional_attn.80 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.79, %2420), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:428:0
  %2422 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.80, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %2423 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%2422, %51, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %2424 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2423, %21, %51, %18, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.81 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2424, %39, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %2426 : Long() = aten::sub(%max_rel_len, %16, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:430:0
  %2427 : int = aten::Int(%2426), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2428 : int[] = prim::ListConstruct(%2415, %2416, %2417, %2427), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %positional_attn.82 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.81, %2428), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.83 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.82, %39, %52, %2384, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.83, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:498:0
  %2432 : int = aten::size(%token_type_mat.20, %52), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
  %2433 : int = aten::size(%token_type_mat.20, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
  %2434 : int = aten::size(%token_type_mat.20, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias : Float(12:64, 64:1) = aten::mul(%2375, %35), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:508:0
  %2436 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %r_s_bias, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:511:0
  %2437 : Tensor[] = prim::ListConstruct(%2436, %2374), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2438 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%40, %2437), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %2439 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.20, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2440 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%2439, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2441 : int = aten::size(%q_head, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2442 : int[] = prim::ListConstruct(%2432, %2441, %2433, %2434), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %token_type_mat : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%2440, %2442, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2444 : Tensor[] = aten::split(%2438, %51, %43), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/tensor.py:371:0
  %diff_token_type : Float(17:26, 12:442, 13:2, 1:1), %same_token_type : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%2444), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2447 : int = aten::size(%token_type_mat, %52), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2448 : int = aten::size(%token_type_mat, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2449 : int = aten::size(%token_type_mat, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2450 : int = aten::size(%token_type_mat, %39), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2451 : int[] = prim::ListConstruct(%2447, %2448, %2449, %2450), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2452 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type, %2451, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2453 : int = aten::size(%token_type_mat, %52), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2454 : int = aten::size(%token_type_mat, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2455 : int = aten::size(%token_type_mat, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2456 : int = aten::size(%token_type_mat, %39), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2457 : int[] = prim::ListConstruct(%2453, %2454, %2455, %2456), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2458 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type, %2457, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat, %2452, %2458), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.27, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:522:0
  %2461 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score, %positional_attn, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.40 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%2461, %token_type_attn, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.41 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.40, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:553:0
  %2464 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %52, %52, %18, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2465 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%2464, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2466 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2465, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2467 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%2466, %12, %48, %48, %15), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2468 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%2467, %51, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/tensor.py:396:0
  %2469 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%2468, %41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.41, %2469, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %input.129 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score, %43, %12), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:558:0
  %2472 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.129, %47, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %2473 : Tensor[] = prim::ListConstruct(%2472, %2403), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %attn_vec : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%42, %2473), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %2475 : int[] = prim::ListConstruct(%2382, %2383, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %input.130 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec, %2475), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:565:0
  %2477 : Tensor = prim::GetAttr[name="bias"](%2373)
  %2478 : Tensor = prim::GetAttr[name="weight"](%2373)
  %2479 : Float(768:1, 768:768) = aten::t(%2478), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.70 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.130, %2479), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.131 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.70, %2477, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.131, %47, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.132 : Float(17:9984, 13:768, 768:1) = aten::add(%query, %attn_out, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:568:0
  %2484 : Tensor = prim::GetAttr[name="bias"](%2372)
  %2485 : Tensor = prim::GetAttr[name="weight"](%2372)
  %2486 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.layer_norm
  %input.133 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.132, %2486, %2485, %2484, %45, %44), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %2488 : __torch__.torch.nn.modules.normalization.___torch_mangle_4330.LayerNorm = prim::GetAttr[name="layer_norm"](%2370)
  %2489 : __torch__.torch.nn.modules.linear.___torch_mangle_4328.Linear = prim::GetAttr[name="linear_2"](%2370)
  %2490 : __torch__.torch.nn.modules.linear.___torch_mangle_4326.Linear = prim::GetAttr[name="linear_1"](%2370)
  %2491 : Tensor = prim::GetAttr[name="bias"](%2490)
  %2492 : Tensor = prim::GetAttr[name="weight"](%2490)
  %2493 : Float(768:1, 3072:768) = aten::t(%2492), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.71 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.133, %2493), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.71, %2491, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2496 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x, %27), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2497 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2498 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2497, %29), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2499 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x, %2498, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2500 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2499, %30), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2501 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%2500), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2502 : Float(17:39936, 13:3072, 3072:1) = aten::add(%2501, %31, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %input.134 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2496, %2502), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %input.135 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.134, %32, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2505 : Tensor = prim::GetAttr[name="bias"](%2489)
  %2506 : Tensor = prim::GetAttr[name="weight"](%2489)
  %2507 : Float(3072:1, 768:3072) = aten::t(%2506), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.72 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.135, %2507), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.136 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.72, %2505, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.136, %47, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.137 : Float(17:9984, 13:768, 768:1) = aten::add(%input.133, %h, %51), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/modeling_funnel.py:588:0
  %2512 : Tensor = prim::GetAttr[name="bias"](%2488)
  %2513 : Tensor = prim::GetAttr[name="weight"](%2488)
  %2514 : int[] = prim::ListConstruct(%46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.layer_norm
  %input.138 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.137, %2514, %2513, %2512, %45, %44), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2516 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %2517 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %input : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.138, %2517, %2516), scope: __module.dropout # torch/nn/functional.py:973:0
  %2519 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1678:0
  %2520 : Tensor = prim::GetAttr[name="bias"](%3)
  %2521 : Tensor = prim::GetAttr[name="weight"](%3)
  %2522 : Float(768:1, 2:768) = aten::t(%2521), scope: __module.classifier # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %2522), scope: __module.classifier # torch/nn/functional.py:1676:0
  %2524 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %2520, %2519), scope: __module.classifier # torch/nn/functional.py:1678:0
  %9 : (Float(17:26, 13:2, 2:1)) = prim::TupleConstruct(%2524)
  return (%9)
