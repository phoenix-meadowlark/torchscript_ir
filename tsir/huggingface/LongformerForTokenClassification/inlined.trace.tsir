graph(%self.1 : __torch__.transformers.modeling_longformer.LongformerForTokenClassification,
      %input_ids.1 : Long(17:13, 13:1),
      %input.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_7803.Linear = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.torch.nn.modules.dropout.___torch_mangle_7802.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %5 : __torch__.transformers.modeling_longformer.___torch_mangle_7801.LongformerModel = prim::GetAttr[name="longformer"](%self.1)
  %10 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %11 : int = prim::Constant[value=16384](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
  %12 : int = prim::Constant[value=65536](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
  %13 : float = prim::Constant[value=0.](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %14 : int = prim::Constant[value=17](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %15 : float = prim::Constant[value=-10000.](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:275:0
  %16 : int = prim::Constant[value=-256](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %17 : float = prim::Constant[value=-inf](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
  %18 : int = prim::Constant[value=-255](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %19 : int = prim::Constant[value=255](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %20 : int = prim::Constant[value=-257](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %21 : int = prim::Constant[value=204](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %22 : int = prim::Constant[value=257](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %23 : int = prim::Constant[value=513](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
  %24 : int = prim::Constant[value=256](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
  %25 : int = prim::Constant[value=-2](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %26 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %27 : int = prim::Constant[value=13056](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %28 : int = prim::Constant[value=3342336](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %29 : Long() = prim::Constant[value={2}](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %30 : Long() = prim::Constant[value={512}](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %31 : Long() = prim::Constant[value={256}](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %32 : int = prim::Constant[value=64](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
  %33 : int = prim::Constant[value=12](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
  %34 : Double() = prim::Constant[value={8}](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:261:0
  %35 : Long() = prim::Constant[value={1}](), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:114:0
  %36 : int = prim::Constant[value=-1](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %37 : bool = prim::Constant[value=1](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %38 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %39 : int = prim::Constant[value=768](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %40 : float = prim::Constant[value=0.10000000000000001](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.dropout # torch/nn/functional.py:973:0
  %41 : Double() = prim::Constant[value={-10000}](), scope: __module.longformer # transformers/modeling_utils.py:258:0
  %42 : float = prim::Constant[value=1.](), scope: __module.longformer # torch/tensor.py:396:0
  %43 : None = prim::Constant(), scope: __module.longformer
  %44 : int = prim::Constant[value=6](), scope: __module.longformer # transformers/modeling_utils.py:257:0
  %45 : int = prim::Constant[value=3](), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %46 : int = prim::Constant[value=2](), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %47 : int = prim::Constant[value=9223372036854775807](), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %48 : int = prim::Constant[value=512](), scope: __module.longformer # transformers/modeling_longformer.py:1139:0
  %49 : bool = prim::Constant[value=0](), scope: __module.longformer # transformers/modeling_longformer.py:1239:0
  %50 : Device = prim::Constant[value="cpu"](), scope: __module.longformer # transformers/modeling_longformer.py:1239:0
  %51 : int = prim::Constant[value=4](), scope: __module.longformer # transformers/modeling_longformer.py:1239:0
  %52 : int = prim::Constant[value=1](), scope: __module.longformer # transformers/modeling_longformer.py:1228:0
  %53 : int = prim::Constant[value=0](), scope: __module.longformer # transformers/modeling_longformer.py:1228:0
  %54 : __torch__.transformers.modeling_longformer.___torch_mangle_7800.LongformerEncoder = prim::GetAttr[name="encoder"](%5)
  %55 : __torch__.transformers.modeling_longformer.___torch_mangle_7570.LongformerEmbeddings = prim::GetAttr[name="embeddings"](%5)
  %56 : int = aten::size(%input_ids.1, %53), scope: __module.longformer # transformers/modeling_longformer.py:1228:0
  %57 : int = aten::size(%input_ids.1, %52), scope: __module.longformer # transformers/modeling_longformer.py:1228:0
  %58 : int[] = prim::ListConstruct(%56, %57), scope: __module.longformer
  %input.2 : Long(17:13, 13:1) = aten::zeros(%58, %51, %53, %50, %49), scope: __module.longformer # transformers/modeling_longformer.py:1239:0
  %60 : int = aten::size(%input_ids.1, %52), scope: __module.longformer # transformers/modeling_longformer.py:1136:0
  %seq_len.1 : Long() = prim::NumToTensor(%60), scope: __module.longformer
  %62 : Long() = aten::remainder(%seq_len.1, %48), scope: __module.longformer # transformers/modeling_longformer.py:1139:0
  %63 : Long() = aten::rsub(%62, %48, %52), scope: __module.longformer # torch/tensor.py:396:0
  %padding_len : Long() = aten::remainder(%63, %48), scope: __module.longformer # transformers/modeling_longformer.py:1139:0
  %65 : int = aten::Int(%padding_len), scope: __module.longformer
  %66 : int = aten::Int(%padding_len), scope: __module.longformer
  %67 : int = aten::Int(%padding_len), scope: __module.longformer
  %68 : int[] = prim::ListConstruct(%53, %67), scope: __module.longformer
  %input_ids : Long(17:512, 512:1) = aten::constant_pad_nd(%input_ids.1, %68, %52), scope: __module.longformer # torch/nn/functional.py:3552:0
  %70 : int[] = prim::ListConstruct(%53, %66), scope: __module.longformer
  %attention_mask.1 : Long(17:512, 512:1) = aten::constant_pad_nd(%input.1, %70, %53), scope: __module.longformer # torch/nn/functional.py:3552:0
  %72 : int[] = prim::ListConstruct(%53, %65), scope: __module.longformer
  %input.4 : Long(17:512, 512:1) = aten::constant_pad_nd(%input.2, %72, %53), scope: __module.longformer # torch/nn/functional.py:3552:0
  %74 : Long(17:512, 512:1) = aten::slice(%attention_mask.1, %53, %53, %47, %52), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %75 : Long(17:512, 1:512, 512:1) = aten::unsqueeze(%74, %52), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %76 : Long(17:512, 1:512, 1:512, 512:1) = aten::unsqueeze(%75, %46), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:512, 1:512, 1:512, 512:1) = aten::slice(%76, %45, %53, %47, %52), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %78 : Float(17:512, 1:512, 1:512, 512:1) = aten::to(%extended_attention_mask, %44, %49, %49, %43), scope: __module.longformer # transformers/modeling_utils.py:257:0
  %79 : Float(17:512, 1:512, 1:512, 512:1) = aten::rsub(%78, %42, %52), scope: __module.longformer # torch/tensor.py:396:0
  %attention_mask : Float(17:512, 1:512, 1:512, 512:1) = aten::mul(%79, %41), scope: __module.longformer # transformers/modeling_utils.py:258:0
  %81 : __torch__.torch.nn.modules.normalization.___torch_mangle_7568.LayerNorm = prim::GetAttr[name="LayerNorm"](%55)
  %82 : __torch__.torch.nn.modules.sparse.___torch_mangle_7567.Embedding = prim::GetAttr[name="token_type_embeddings"](%55)
  %83 : __torch__.torch.nn.modules.sparse.___torch_mangle_7566.Embedding = prim::GetAttr[name="position_embeddings"](%55)
  %84 : __torch__.torch.nn.modules.sparse.___torch_mangle_7565.Embedding = prim::GetAttr[name="word_embeddings"](%55)
  %85 : Bool(17:512, 512:1) = aten::ne(%input_ids, %52), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:112:0
  %mask : Int(17:512, 512:1) = aten::to(%85, %45, %49, %49, %43), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:112:0
  %87 : Long(17:512, 512:1) = aten::cumsum(%mask, %52, %43), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:113:0
  %88 : Int(17:512, 512:1) = aten::type_as(%87, %mask), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:113:0
  %incremental_indices : Int(17:512, 512:1) = aten::mul(%88, %mask), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:113:0
  %90 : Long(17:512, 512:1) = aten::to(%incremental_indices, %51, %49, %49, %43), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:114:0
  %91 : Long(17:512, 512:1) = aten::add(%90, %35, %52), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:114:0
  %input.3 : Long(17:512, 512:1) = aten::to(%91, %51, %53, %50, %49, %49, %49, %43), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:147:0
  %93 : Tensor = prim::GetAttr[name="weight"](%84)
  %inputs_embeds : Float(17:393216, 512:768, 768:1) = aten::embedding(%93, %input_ids, %52, %49, %49), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %95 : Tensor = prim::GetAttr[name="weight"](%83)
  %position_embeddings : Float(17:393216, 512:768, 768:1) = aten::embedding(%95, %input.3, %52, %49, %49), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %97 : Tensor = prim::GetAttr[name="weight"](%82)
  %token_type_embeddings : Float(17:393216, 512:768, 768:1) = aten::embedding(%97, %input.4, %36, %49, %49), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %99 : Float(17:393216, 512:768, 768:1) = aten::add(%inputs_embeds, %position_embeddings, %52), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:170:0
  %input.5 : Float(17:393216, 512:768, 768:1) = aten::add(%99, %token_type_embeddings, %52), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:170:0
  %101 : Tensor = prim::GetAttr[name="bias"](%81)
  %102 : Tensor = prim::GetAttr[name="weight"](%81)
  %103 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm
  %input.6 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.5, %103, %102, %101, %38, %37), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %hidden_states.1 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.6, %40, %49), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.dropout # torch/nn/functional.py:973:0
  %106 : __torch__.torch.nn.modules.container.___torch_mangle_7799.ModuleList = prim::GetAttr[name="layer"](%54)
  %107 : __torch__.transformers.modeling_longformer.___torch_mangle_7798.LongformerLayer = prim::GetAttr[name="11"](%106)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_7799.ModuleList = prim::GetAttr[name="layer"](%54)
  %109 : __torch__.transformers.modeling_longformer.___torch_mangle_7779.LongformerLayer = prim::GetAttr[name="10"](%108)
  %110 : __torch__.torch.nn.modules.container.___torch_mangle_7799.ModuleList = prim::GetAttr[name="layer"](%54)
  %111 : __torch__.transformers.modeling_longformer.___torch_mangle_7760.LongformerLayer = prim::GetAttr[name="9"](%110)
  %112 : __torch__.torch.nn.modules.container.___torch_mangle_7799.ModuleList = prim::GetAttr[name="layer"](%54)
  %113 : __torch__.transformers.modeling_longformer.___torch_mangle_7741.LongformerLayer = prim::GetAttr[name="8"](%112)
  %114 : __torch__.torch.nn.modules.container.___torch_mangle_7799.ModuleList = prim::GetAttr[name="layer"](%54)
  %115 : __torch__.transformers.modeling_longformer.___torch_mangle_7722.LongformerLayer = prim::GetAttr[name="7"](%114)
  %116 : __torch__.torch.nn.modules.container.___torch_mangle_7799.ModuleList = prim::GetAttr[name="layer"](%54)
  %117 : __torch__.transformers.modeling_longformer.___torch_mangle_7703.LongformerLayer = prim::GetAttr[name="6"](%116)
  %118 : __torch__.torch.nn.modules.container.___torch_mangle_7799.ModuleList = prim::GetAttr[name="layer"](%54)
  %119 : __torch__.transformers.modeling_longformer.___torch_mangle_7684.LongformerLayer = prim::GetAttr[name="5"](%118)
  %120 : __torch__.torch.nn.modules.container.___torch_mangle_7799.ModuleList = prim::GetAttr[name="layer"](%54)
  %121 : __torch__.transformers.modeling_longformer.___torch_mangle_7665.LongformerLayer = prim::GetAttr[name="4"](%120)
  %122 : __torch__.torch.nn.modules.container.___torch_mangle_7799.ModuleList = prim::GetAttr[name="layer"](%54)
  %123 : __torch__.transformers.modeling_longformer.___torch_mangle_7646.LongformerLayer = prim::GetAttr[name="3"](%122)
  %124 : __torch__.torch.nn.modules.container.___torch_mangle_7799.ModuleList = prim::GetAttr[name="layer"](%54)
  %125 : __torch__.transformers.modeling_longformer.___torch_mangle_7627.LongformerLayer = prim::GetAttr[name="2"](%124)
  %126 : __torch__.torch.nn.modules.container.___torch_mangle_7799.ModuleList = prim::GetAttr[name="layer"](%54)
  %127 : __torch__.transformers.modeling_longformer.___torch_mangle_7608.LongformerLayer = prim::GetAttr[name="1"](%126)
  %128 : __torch__.torch.nn.modules.container.___torch_mangle_7799.ModuleList = prim::GetAttr[name="layer"](%54)
  %129 : __torch__.transformers.modeling_longformer.___torch_mangle_7589.LongformerLayer = prim::GetAttr[name="0"](%128)
  %130 : __torch__.transformers.modeling_longformer.___torch_mangle_7588.LongformerOutput = prim::GetAttr[name="output"](%129)
  %131 : __torch__.transformers.modeling_longformer.___torch_mangle_7584.LongformerIntermediate = prim::GetAttr[name="intermediate"](%129)
  %132 : __torch__.transformers.modeling_longformer.___torch_mangle_7582.LongformerAttention = prim::GetAttr[name="attention"](%129)
  %133 : __torch__.transformers.modeling_longformer.___torch_mangle_7581.LongformerSelfOutput = prim::GetAttr[name="output"](%132)
  %134 : __torch__.transformers.modeling_longformer.___torch_mangle_7577.LongformerSelfAttention = prim::GetAttr[name="self"](%132)
  %135 : __torch__.torch.nn.modules.linear.___torch_mangle_7573.Linear = prim::GetAttr[name="value"](%134)
  %136 : __torch__.torch.nn.modules.linear.___torch_mangle_7572.Linear = prim::GetAttr[name="key"](%134)
  %137 : __torch__.torch.nn.modules.linear.___torch_mangle_7571.Linear = prim::GetAttr[name="query"](%134)
  %138 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:241:0
  %139 : Float(17:512, 512:1) = aten::squeeze(%138, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.1 : Bool(17:512, 512:1) = aten::lt(%139, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %input.7 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.1, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:248:0
  %142 : Tensor = prim::GetAttr[name="bias"](%137)
  %143 : Tensor = prim::GetAttr[name="weight"](%137)
  %144 : Float(768:1, 768:768) = aten::t(%143), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.7, %144), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.1 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.1, %142, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %147 : Tensor = prim::GetAttr[name="bias"](%136)
  %148 : Tensor = prim::GetAttr[name="weight"](%136)
  %149 : Float(768:1, 768:768) = aten::t(%148), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.7, %149), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.1 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.2, %147, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %152 : Tensor = prim::GetAttr[name="bias"](%135)
  %153 : Tensor = prim::GetAttr[name="weight"](%135)
  %154 : Float(768:1, 768:768) = aten::t(%153), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.7, %154), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.1 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.3, %152, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %157 : int = aten::size(%input.7, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
  %158 : int = aten::size(%input.7, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
  %159 : int = aten::size(%input.7, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.2 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.1, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:261:0
  %161 : int[] = prim::ListConstruct(%157, %158, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %162 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.2, %161), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
  %query.1 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%162, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
  %164 : int[] = prim::ListConstruct(%157, %158, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %165 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.1, %164), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:264:0
  %key.1 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%165, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:264:0
  %167 : int = aten::size(%query.1, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.2 : Long() = prim::NumToTensor(%167), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %169 : int = aten::size(%query.1, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.3 : Long() = prim::NumToTensor(%169), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %171 : int = aten::size(%query.1, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.1 : Long() = prim::NumToTensor(%171), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %173 : int = aten::size(%query.1, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %174 : Long() = aten::floor_divide(%seq_len.3, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %chunks_count.1 : Long() = aten::sub(%174, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:478:0
  %176 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.1, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %177 : Long() = aten::mul(%batch_size.2, %num_heads.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %178 : int = aten::Int(%177), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %179 : int[] = prim::ListConstruct(%178, %169, %173), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.2 : Float(204:64, 512:13056, 64:1) = aten::reshape(%176, %179), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %181 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.1, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %182 : Long() = aten::mul(%batch_size.2, %num_heads.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %183 : int = aten::Int(%182), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %184 : int[] = prim::ListConstruct(%183, %169, %173), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.4 : Float(204:64, 512:13056, 64:1) = aten::reshape(%181, %184), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %186 : int = aten::size(%hidden_states.2, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
  %187 : int = aten::size(%hidden_states.2, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
  %188 : Long() = prim::NumToTensor(%187), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %189 : Long() = aten::floor_divide(%188, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %190 : int = aten::Int(%189), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %191 : int = aten::size(%hidden_states.2, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
  %192 : int[] = prim::ListConstruct(%186, %190, %48, %191), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.3 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.2, %192), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
  %194 : int = aten::size(%hidden_states.3, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %195 : int = aten::size(%hidden_states.3, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %196 : Long() = prim::NumToTensor(%195), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %197 : int = aten::size(%hidden_states.3, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %198 : int = aten::size(%hidden_states.3, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %199 : Long() = aten::mul(%196, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %200 : Long() = aten::sub(%199, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %201 : int = aten::Int(%200), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %202 : int[] = prim::ListConstruct(%194, %201, %197, %198), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %203 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %204 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.3, %202, %203, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %205 : int = aten::size(%hidden_states.4, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
  %206 : int = aten::size(%hidden_states.4, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
  %207 : Long() = prim::NumToTensor(%206), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %208 : Long() = aten::floor_divide(%207, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %209 : int = aten::Int(%208), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %210 : int = aten::size(%hidden_states.4, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
  %211 : int[] = prim::ListConstruct(%205, %209, %48, %210), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.5 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.4, %211), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
  %213 : int = aten::size(%hidden_states.5, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %214 : int = aten::size(%hidden_states.5, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %215 : Long() = prim::NumToTensor(%214), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %216 : int = aten::size(%hidden_states.5, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %217 : int = aten::size(%hidden_states.5, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %218 : Long() = aten::mul(%215, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %219 : Long() = aten::sub(%218, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %220 : int = aten::Int(%219), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %221 : int[] = prim::ListConstruct(%213, %220, %216, %217), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %222 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %223 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.5, %221, %222, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %224 : Tensor[] = prim::ListConstruct(%204, %223), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %input.8 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %224), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %226 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states_padded.1 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.8, %226, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
  %228 : int = aten::size(%hidden_states_padded.1, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %229 : int = aten::size(%hidden_states_padded.1, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %230 : int = aten::size(%hidden_states_padded.1, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %231 : int = aten::size(%hidden_states_padded.1, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %232 : int[] = prim::ListConstruct(%228, %229, %230, %231), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %diagonal_chunked_attention_scores.1 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.1, %232), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:400:0
  %234 : Long() = aten::mul(%batch_size.2, %num_heads.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
  %235 : int = aten::Int(%234), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %236 : Long() = aten::add(%chunks_count.1, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
  %237 : int = aten::Int(%236), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %238 : int[] = prim::ListConstruct(%235, %237, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %diagonal_attention_scores.1 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.1, %238, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
  %240 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %241 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%240, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %242 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%241, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %243 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%242, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %244 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %245 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%244, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %246 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%245, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %247 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%246, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %248 : int[] = prim::ListConstruct(%21, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %249 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%243, %248), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %250 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%247, %249, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %251 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %252 : Float(204:262656, 512:513, 513:1) = aten::select(%251, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %253 : Float(204:262656, 256:513, 513:1) = aten::slice(%252, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %254 : Float(204:262656, 256:513, 257:1) = aten::slice(%253, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %255 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %256 : Float(204:262656, 256:513, 513:1) = aten::select(%255, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %257 : Float(204:262656, 256:513, 513:1) = aten::slice(%256, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %258 : Float(204:262656, 256:513, 257:1) = aten::slice(%257, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %259 : int[] = prim::ListConstruct(%21, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %260 : Float(204:262656, 256:513, 257:1) = aten::view(%254, %259), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %261 : Float(204:262656, 256:513, 257:1) = aten::copy_(%258, %260, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %262 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %263 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%262, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %264 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%263, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %265 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%264, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %266 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %267 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%266, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %268 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%267, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %269 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%268, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %270 : int[] = prim::ListConstruct(%21, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %271 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%265, %270), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %272 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%269, %271, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %273 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %274 : Float(204:262656, 512:513, 513:1) = aten::select(%273, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %275 : Float(204:262656, 255:513, 513:1) = aten::slice(%274, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %276 : Float(204:262656, 255:513, 255:1) = aten::slice(%275, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %277 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %278 : Float(204:262656, 256:513, 513:1) = aten::select(%277, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %279 : Float(204:262656, 255:513, 513:1) = aten::slice(%278, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %280 : Float(204:262656, 255:513, 255:1) = aten::slice(%279, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %281 : int[] = prim::ListConstruct(%21, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %282 : Float(204:262656, 255:513, 255:1) = aten::view(%276, %281), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %283 : Float(204:262656, 255:513, 255:1) = aten::copy_(%280, %282, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %284 : int[] = prim::ListConstruct(%167, %171, %169, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %285 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.1, %284), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.1 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%285, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
  %287 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %288 : Float(256:257, 257:1) = aten::ones(%287, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %289 : Float(256:257, 257:1) = aten::tril(%288, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %290 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %beginning_mask_2d.1 : Float(256:257, 257:1) = aten::flip(%289, %290), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %292 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.1, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %293 : Float(1:65792, 256:257, 257:1) = aten::slice(%292, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %294 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%293, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.1 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%294, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %296 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %ending_mask.1 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.1, %296), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:460:0
  %298 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.1, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %299 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%298, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %300 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%299, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.1 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%300, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %302 : int = aten::size(%beginning_input.1, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %303 : int = aten::size(%beginning_input.1, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %304 : int = aten::size(%beginning_input.1, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %305 : int = aten::size(%beginning_input.1, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %306 : int[] = prim::ListConstruct(%302, %303, %304, %305), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %307 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.1, %306, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %308 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%307, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %309 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.1, %308, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
  %310 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.1, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %311 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%310, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %312 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%311, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.1 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%312, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %314 : int = aten::size(%ending_input.1, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %315 : int = aten::size(%ending_input.1, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %316 : int = aten::size(%ending_input.1, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %317 : int = aten::size(%ending_input.1, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %318 : int[] = prim::ListConstruct(%314, %315, %316, %317), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %319 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.1, %318, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %320 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%319, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %321 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.1, %320, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:466:0
  %322 : Bool(17:512, 512:1) = aten::ne(%139, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %323 : Bool(17:512, 512:1) = aten::slice(%322, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
  %324 : Bool(17:512, 512:1) = aten::slice(%323, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
  %325 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%324, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.1 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%325, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
  %327 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.1, %query.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.1 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%327, %remove_from_windowed_attention_mask.1, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:275:0
  %329 : int = aten::size(%float_mask.1, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %330 : int = aten::size(%float_mask.1, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %331 : int = aten::size(%float_mask.1, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %332 : int = aten::size(%float_mask.1, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %333 : int[] = prim::ListConstruct(%329, %330, %331, %332), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %query.2 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%333, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %335 : int = aten::size(%query.2, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.3 : Long() = prim::NumToTensor(%335), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %337 : int = aten::size(%query.2, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.4 : Long() = prim::NumToTensor(%337), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %339 : int = aten::size(%query.2, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.2 : Long() = prim::NumToTensor(%339), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %341 : int = aten::size(%query.2, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %342 : Long() = aten::floor_divide(%seq_len.4, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %chunks_count.2 : Long() = aten::sub(%342, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:478:0
  %344 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.2, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %345 : Long() = aten::mul(%batch_size.3, %num_heads.2), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %346 : int = aten::Int(%345), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %347 : int[] = prim::ListConstruct(%346, %337, %341), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.6 : Float(17:512, 512:1, 1:1) = aten::reshape(%344, %347), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %349 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.1, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %350 : Long() = aten::mul(%batch_size.3, %num_heads.2), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %351 : int = aten::Int(%350), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %352 : int[] = prim::ListConstruct(%351, %337, %341), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.8 : Float(17:512, 512:1, 1:1) = aten::reshape(%349, %352), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %354 : int = aten::size(%hidden_states.6, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
  %355 : int = aten::size(%hidden_states.6, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
  %356 : Long() = prim::NumToTensor(%355), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %357 : Long() = aten::floor_divide(%356, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %358 : int = aten::Int(%357), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %359 : int = aten::size(%hidden_states.6, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
  %360 : int[] = prim::ListConstruct(%354, %358, %48, %359), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.7 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.6, %360), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
  %362 : int = aten::size(%hidden_states.7, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %363 : int = aten::size(%hidden_states.7, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %364 : Long() = prim::NumToTensor(%363), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %365 : int = aten::size(%hidden_states.7, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %366 : int = aten::size(%hidden_states.7, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %367 : Long() = aten::mul(%364, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %368 : Long() = aten::sub(%367, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %369 : int = aten::Int(%368), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %370 : int[] = prim::ListConstruct(%362, %369, %365, %366), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %371 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %372 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.7, %370, %371, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %373 : int = aten::size(%hidden_states.8, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
  %374 : int = aten::size(%hidden_states.8, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
  %375 : Long() = prim::NumToTensor(%374), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %376 : Long() = aten::floor_divide(%375, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %377 : int = aten::Int(%376), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %378 : int = aten::size(%hidden_states.8, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
  %379 : int[] = prim::ListConstruct(%373, %377, %48, %378), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.9 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.8, %379), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
  %381 : int = aten::size(%hidden_states.9, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %382 : int = aten::size(%hidden_states.9, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %383 : Long() = prim::NumToTensor(%382), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %384 : int = aten::size(%hidden_states.9, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %385 : int = aten::size(%hidden_states.9, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %386 : Long() = aten::mul(%383, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %387 : Long() = aten::sub(%386, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %388 : int = aten::Int(%387), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %389 : int[] = prim::ListConstruct(%381, %388, %384, %385), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %390 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %391 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.9, %389, %390, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %392 : Tensor[] = prim::ListConstruct(%372, %391), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %input.9 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %392), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %394 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states_padded.2 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.9, %394, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
  %396 : int = aten::size(%hidden_states_padded.2, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %397 : int = aten::size(%hidden_states_padded.2, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %398 : int = aten::size(%hidden_states_padded.2, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %399 : int = aten::size(%hidden_states_padded.2, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %400 : int[] = prim::ListConstruct(%396, %397, %398, %399), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %diagonal_chunked_attention_scores.2 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.2, %400), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:400:0
  %402 : Long() = aten::mul(%batch_size.3, %num_heads.2), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
  %403 : int = aten::Int(%402), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %404 : Long() = aten::add(%chunks_count.2, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
  %405 : int = aten::Int(%404), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %406 : int[] = prim::ListConstruct(%403, %405, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %diagonal_attention_scores.2 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.2, %406, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
  %408 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %409 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%408, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %410 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%409, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %411 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%410, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %412 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %413 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%412, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %414 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%413, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %415 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%414, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %416 : int[] = prim::ListConstruct(%14, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %417 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%411, %416), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %418 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%415, %417, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %419 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %420 : Float(17:262656, 512:513, 513:1) = aten::select(%419, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %421 : Float(17:262656, 256:513, 513:1) = aten::slice(%420, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %422 : Float(17:262656, 256:513, 257:1) = aten::slice(%421, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %423 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %424 : Float(17:262656, 256:513, 513:1) = aten::select(%423, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %425 : Float(17:262656, 256:513, 513:1) = aten::slice(%424, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %426 : Float(17:262656, 256:513, 257:1) = aten::slice(%425, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %427 : int[] = prim::ListConstruct(%14, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %428 : Float(17:262656, 256:513, 257:1) = aten::view(%422, %427), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %429 : Float(17:262656, 256:513, 257:1) = aten::copy_(%426, %428, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %430 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %431 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%430, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %432 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%431, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %433 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%432, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %434 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %435 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%434, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %436 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%435, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %437 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%436, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %438 : int[] = prim::ListConstruct(%14, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %439 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%433, %438), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %440 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%437, %439, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %441 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %442 : Float(17:262656, 512:513, 513:1) = aten::select(%441, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %443 : Float(17:262656, 255:513, 513:1) = aten::slice(%442, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %444 : Float(17:262656, 255:513, 255:1) = aten::slice(%443, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %445 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %446 : Float(17:262656, 256:513, 513:1) = aten::select(%445, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %447 : Float(17:262656, 255:513, 513:1) = aten::slice(%446, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %448 : Float(17:262656, 255:513, 255:1) = aten::slice(%447, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %449 : int[] = prim::ListConstruct(%14, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %450 : Float(17:262656, 255:513, 255:1) = aten::view(%444, %449), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %451 : Float(17:262656, 255:513, 255:1) = aten::copy_(%448, %450, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %452 : int[] = prim::ListConstruct(%335, %339, %337, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %453 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.2, %452), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.2 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%453, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
  %455 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %456 : Float(256:257, 257:1) = aten::ones(%455, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %457 : Float(256:257, 257:1) = aten::tril(%456, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %458 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %beginning_mask_2d.2 : Float(256:257, 257:1) = aten::flip(%457, %458), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %460 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.2, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %461 : Float(1:65792, 256:257, 257:1) = aten::slice(%460, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %462 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%461, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.2 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%462, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %464 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %ending_mask.2 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.2, %464), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:460:0
  %466 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.2, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %467 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%466, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %468 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%467, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.2 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%468, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %470 : int = aten::size(%beginning_input.2, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %471 : int = aten::size(%beginning_input.2, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %472 : int = aten::size(%beginning_input.2, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %473 : int = aten::size(%beginning_input.2, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %474 : int[] = prim::ListConstruct(%470, %471, %472, %473), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %475 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.2, %474, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %476 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%475, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %477 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.2, %476, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
  %478 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.2, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %479 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%478, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %480 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%479, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.2 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%480, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %482 : int = aten::size(%ending_input.2, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %483 : int = aten::size(%ending_input.2, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %484 : int = aten::size(%ending_input.2, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %485 : int = aten::size(%ending_input.2, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %486 : int[] = prim::ListConstruct(%482, %483, %484, %485), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %487 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.2, %486, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %488 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%487, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %489 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.2, %488, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.1 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.1, %input_tensor.2, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.1 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.1, %36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:1500:0
  %492 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.1, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %493 : Bool(17:512, 512:1) = aten::slice(%492, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %494 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%493, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %495 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%494, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %input.10 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.1, %495, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.2 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.10, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:973:0
  %498 : int[] = prim::ListConstruct(%157, %158, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %499 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.1, %498), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:331:0
  %value.1 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%499, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:331:0
  %501 : int = aten::size(%value.1, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.4 : Long() = prim::NumToTensor(%501), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %503 : int = aten::size(%value.1, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.5 : Long() = prim::NumToTensor(%503), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %505 : int = aten::size(%value.1, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.3 : Long() = prim::NumToTensor(%505), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %507 : int = aten::size(%value.1, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
  %508 : Long() = aten::floor_divide(%seq_len.5, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %chunks_count.3 : Long() = aten::sub(%508, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:542:0
  %510 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.2, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:545:0
  %511 : Long() = aten::mul(%batch_size.4, %num_heads.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:546:0
  %512 : int = aten::Int(%511), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %513 : Long() = aten::floor_divide(%seq_len.5, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %514 : int = aten::Int(%513), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %515 : int[] = prim::ListConstruct(%512, %514, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %chunked_hidden_states.1 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%510, %515), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:545:0
  %517 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.1, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
  %518 : Long() = aten::mul(%batch_size.4, %num_heads.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
  %519 : int = aten::Int(%518), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %520 : int[] = prim::ListConstruct(%519, %503, %507), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %input.11 : Float(204:64, 512:13056, 64:1) = aten::reshape(%517, %520), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
  %522 : int[] = prim::ListConstruct(%53, %53, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %padded_value.1 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.11, %522, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
  %524 : Long() = aten::mul(%batch_size.4, %num_heads.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:556:0
  %525 : int = aten::Int(%524), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %526 : Long() = aten::add(%chunks_count.3, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:556:0
  %527 : int = aten::Int(%526), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %528 : int[] = prim::ListConstruct(%525, %527, %39, %507), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %529 : int[] = prim::ListConstruct(%12, %11, %32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %530 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.1, %528, %529, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
  %531 : int = aten::size(%chunked_hidden_states.1, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
  %532 : int = aten::size(%chunked_hidden_states.1, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
  %533 : int = aten::size(%chunked_hidden_states.1, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.1 : Long() = prim::NumToTensor(%533), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %535 : int = aten::size(%chunked_hidden_states.1, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.1 : Long() = prim::NumToTensor(%535), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %537 : Long() = aten::add(%window_overlap.1, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:422:0
  %538 : int = aten::Int(%537), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %539 : int[] = prim::ListConstruct(%53, %538), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %chunked_hidden_states.2 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.1, %539, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
  %541 : int[] = prim::ListConstruct(%531, %532, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %chunked_hidden_states.3 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.2, %541), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:424:0
  %543 : Long() = aten::neg(%window_overlap.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:428:0
  %544 : int = aten::Int(%543), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %545 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.3, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
  %546 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%545, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.4 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%546, %46, %53, %544, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
  %548 : Long() = aten::add(%window_overlap.1, %hidden_dim.1, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:431:0
  %549 : int = aten::Int(%548), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %550 : int[] = prim::ListConstruct(%531, %532, %533, %549), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %chunked_hidden_states.5 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.4, %550), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:430:0
  %552 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.5, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
  %553 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%552, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
  %554 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%553, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
  %555 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%554, %45, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
  %556 : Tensor[] = prim::ListConstruct(%555, %530), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %context.1 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%10, %556), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %558 : int[] = prim::ListConstruct(%501, %505, %503, %507), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %559 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.1, %558), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.1 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%559, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:569:0
  %561 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.1, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
  %562 : int[] = prim::ListConstruct(%157, %158, %159), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %563 : Float(512:13056, 17:768, 768:1) = aten::reshape(%561, %562), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.2 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%563, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
  %input.12 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.2, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:374:0
  %566 : __torch__.torch.nn.modules.normalization.___torch_mangle_7579.LayerNorm = prim::GetAttr[name="LayerNorm"](%133)
  %567 : __torch__.torch.nn.modules.linear.___torch_mangle_7578.Linear = prim::GetAttr[name="dense"](%133)
  %568 : Tensor = prim::GetAttr[name="bias"](%567)
  %569 : Tensor = prim::GetAttr[name="weight"](%567)
  %570 : Float(768:1, 768:768) = aten::t(%569), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.12, %570), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.13 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.4, %568, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.13, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.10, %hidden_states.1, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output # transformers/modeling_longformer.py:758:0
  %575 : Tensor = prim::GetAttr[name="bias"](%566)
  %576 : Tensor = prim::GetAttr[name="weight"](%566)
  %577 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.3 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.14, %577, %576, %575, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %579 : __torch__.torch.nn.modules.linear.___torch_mangle_7583.Linear = prim::GetAttr[name="dense"](%131)
  %580 : Tensor = prim::GetAttr[name="bias"](%579)
  %581 : Tensor = prim::GetAttr[name="weight"](%579)
  %582 : Float(768:1, 3072:768) = aten::t(%581), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.intermediate/__module.longformer.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.3, %582), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.intermediate/__module.longformer.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.15 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.5, %580, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.intermediate/__module.longformer.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.16 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %586 : __torch__.torch.nn.modules.normalization.___torch_mangle_7586.LayerNorm = prim::GetAttr[name="LayerNorm"](%130)
  %587 : __torch__.torch.nn.modules.linear.___torch_mangle_7585.Linear = prim::GetAttr[name="dense"](%130)
  %588 : Tensor = prim::GetAttr[name="bias"](%587)
  %589 : Tensor = prim::GetAttr[name="weight"](%587)
  %590 : Float(3072:1, 768:3072) = aten::t(%589), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.16, %590), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.17 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.6, %588, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.17, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.18 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.11, %input_tensor.3, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output # transformers/modeling_longformer.py:830:0
  %595 : Tensor = prim::GetAttr[name="bias"](%586)
  %596 : Tensor = prim::GetAttr[name="weight"](%586)
  %597 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.LayerNorm
  %hidden_states.12 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.18, %597, %596, %595, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %599 : __torch__.transformers.modeling_longformer.___torch_mangle_7607.LongformerOutput = prim::GetAttr[name="output"](%127)
  %600 : __torch__.transformers.modeling_longformer.___torch_mangle_7603.LongformerIntermediate = prim::GetAttr[name="intermediate"](%127)
  %601 : __torch__.transformers.modeling_longformer.___torch_mangle_7601.LongformerAttention = prim::GetAttr[name="attention"](%127)
  %602 : __torch__.transformers.modeling_longformer.___torch_mangle_7600.LongformerSelfOutput = prim::GetAttr[name="output"](%601)
  %603 : __torch__.transformers.modeling_longformer.___torch_mangle_7596.LongformerSelfAttention = prim::GetAttr[name="self"](%601)
  %604 : __torch__.torch.nn.modules.linear.___torch_mangle_7592.Linear = prim::GetAttr[name="value"](%603)
  %605 : __torch__.torch.nn.modules.linear.___torch_mangle_7591.Linear = prim::GetAttr[name="key"](%603)
  %606 : __torch__.torch.nn.modules.linear.___torch_mangle_7590.Linear = prim::GetAttr[name="query"](%603)
  %607 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:241:0
  %608 : Float(17:512, 512:1) = aten::squeeze(%607, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.2 : Bool(17:512, 512:1) = aten::lt(%608, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %input.19 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.12, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:248:0
  %611 : Tensor = prim::GetAttr[name="bias"](%606)
  %612 : Tensor = prim::GetAttr[name="weight"](%606)
  %613 : Float(768:1, 768:768) = aten::t(%612), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.19, %613), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.3 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.7, %611, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %616 : Tensor = prim::GetAttr[name="bias"](%605)
  %617 : Tensor = prim::GetAttr[name="weight"](%605)
  %618 : Float(768:1, 768:768) = aten::t(%617), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.19, %618), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.2 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.8, %616, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %621 : Tensor = prim::GetAttr[name="bias"](%604)
  %622 : Tensor = prim::GetAttr[name="weight"](%604)
  %623 : Float(768:1, 768:768) = aten::t(%622), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.19, %623), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.2 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.9, %621, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %626 : int = aten::size(%input.19, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
  %627 : int = aten::size(%input.19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
  %628 : int = aten::size(%input.19, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.4 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.3, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:261:0
  %630 : int[] = prim::ListConstruct(%626, %627, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %631 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.4, %630), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:263:0
  %query.3 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%631, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:263:0
  %633 : int[] = prim::ListConstruct(%626, %627, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %634 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.2, %633), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:264:0
  %key.2 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%634, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:264:0
  %636 : int = aten::size(%query.3, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.6 : Long() = prim::NumToTensor(%636), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %638 : int = aten::size(%query.3, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.7 : Long() = prim::NumToTensor(%638), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %640 : int = aten::size(%query.3, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.4 : Long() = prim::NumToTensor(%640), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %642 : int = aten::size(%query.3, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %643 : Long() = aten::floor_divide(%seq_len.7, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %chunks_count.4 : Long() = aten::sub(%643, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:478:0
  %645 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.3, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %646 : Long() = aten::mul(%batch_size.6, %num_heads.4), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %647 : int = aten::Int(%646), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %648 : int[] = prim::ListConstruct(%647, %638, %642), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.13 : Float(204:64, 512:13056, 64:1) = aten::reshape(%645, %648), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %650 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.2, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %651 : Long() = aten::mul(%batch_size.6, %num_heads.4), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %652 : int = aten::Int(%651), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %653 : int[] = prim::ListConstruct(%652, %638, %642), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.15 : Float(204:64, 512:13056, 64:1) = aten::reshape(%650, %653), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %655 : int = aten::size(%hidden_states.13, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
  %656 : int = aten::size(%hidden_states.13, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
  %657 : Long() = prim::NumToTensor(%656), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %658 : Long() = aten::floor_divide(%657, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %659 : int = aten::Int(%658), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %660 : int = aten::size(%hidden_states.13, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
  %661 : int[] = prim::ListConstruct(%655, %659, %48, %660), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.14 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.13, %661), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
  %663 : int = aten::size(%hidden_states.14, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %664 : int = aten::size(%hidden_states.14, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %665 : Long() = prim::NumToTensor(%664), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %666 : int = aten::size(%hidden_states.14, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %667 : int = aten::size(%hidden_states.14, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %668 : Long() = aten::mul(%665, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %669 : Long() = aten::sub(%668, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %670 : int = aten::Int(%669), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %671 : int[] = prim::ListConstruct(%663, %670, %666, %667), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %672 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %673 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.14, %671, %672, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
  %674 : int = aten::size(%hidden_states.15, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
  %675 : int = aten::size(%hidden_states.15, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
  %676 : Long() = prim::NumToTensor(%675), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %677 : Long() = aten::floor_divide(%676, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %678 : int = aten::Int(%677), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %679 : int = aten::size(%hidden_states.15, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
  %680 : int[] = prim::ListConstruct(%674, %678, %48, %679), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.16 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.15, %680), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
  %682 : int = aten::size(%hidden_states.16, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %683 : int = aten::size(%hidden_states.16, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %684 : Long() = prim::NumToTensor(%683), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %685 : int = aten::size(%hidden_states.16, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %686 : int = aten::size(%hidden_states.16, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %687 : Long() = aten::mul(%684, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %688 : Long() = aten::sub(%687, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %689 : int = aten::Int(%688), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %690 : int[] = prim::ListConstruct(%682, %689, %685, %686), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %691 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %692 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.16, %690, %691, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
  %693 : Tensor[] = prim::ListConstruct(%673, %692), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %input.20 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %693), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/functional.py:327:0
  %695 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states_padded.3 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.20, %695, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
  %697 : int = aten::size(%hidden_states_padded.3, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %698 : int = aten::size(%hidden_states_padded.3, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %699 : int = aten::size(%hidden_states_padded.3, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %700 : int = aten::size(%hidden_states_padded.3, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %701 : int[] = prim::ListConstruct(%697, %698, %699, %700), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %diagonal_chunked_attention_scores.3 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.3, %701), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:400:0
  %703 : Long() = aten::mul(%batch_size.6, %num_heads.4), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
  %704 : int = aten::Int(%703), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %705 : Long() = aten::add(%chunks_count.4, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
  %706 : int = aten::Int(%705), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %707 : int[] = prim::ListConstruct(%704, %706, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %diagonal_attention_scores.3 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.3, %707, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
  %709 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %710 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%709, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %711 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%710, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %712 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%711, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %713 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %714 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%713, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %715 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%714, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %716 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%715, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %717 : int[] = prim::ListConstruct(%21, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %718 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%712, %717), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %719 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%716, %718, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %720 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %721 : Float(204:262656, 512:513, 513:1) = aten::select(%720, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %722 : Float(204:262656, 256:513, 513:1) = aten::slice(%721, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %723 : Float(204:262656, 256:513, 257:1) = aten::slice(%722, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %724 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %725 : Float(204:262656, 256:513, 513:1) = aten::select(%724, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %726 : Float(204:262656, 256:513, 513:1) = aten::slice(%725, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %727 : Float(204:262656, 256:513, 257:1) = aten::slice(%726, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %728 : int[] = prim::ListConstruct(%21, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %729 : Float(204:262656, 256:513, 257:1) = aten::view(%723, %728), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %730 : Float(204:262656, 256:513, 257:1) = aten::copy_(%727, %729, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %731 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %732 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%731, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %733 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%732, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %734 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%733, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %735 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %736 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%735, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %737 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%736, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %738 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%737, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %739 : int[] = prim::ListConstruct(%21, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %740 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%734, %739), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %741 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%738, %740, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %742 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %743 : Float(204:262656, 512:513, 513:1) = aten::select(%742, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %744 : Float(204:262656, 255:513, 513:1) = aten::slice(%743, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %745 : Float(204:262656, 255:513, 255:1) = aten::slice(%744, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %746 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %747 : Float(204:262656, 256:513, 513:1) = aten::select(%746, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %748 : Float(204:262656, 255:513, 513:1) = aten::slice(%747, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %749 : Float(204:262656, 255:513, 255:1) = aten::slice(%748, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %750 : int[] = prim::ListConstruct(%21, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %751 : Float(204:262656, 255:513, 255:1) = aten::view(%745, %750), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %752 : Float(204:262656, 255:513, 255:1) = aten::copy_(%749, %751, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %753 : int[] = prim::ListConstruct(%636, %640, %638, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %754 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.3, %753), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.4 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%754, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
  %756 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %757 : Float(256:257, 257:1) = aten::ones(%756, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %758 : Float(256:257, 257:1) = aten::tril(%757, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %759 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %beginning_mask_2d.3 : Float(256:257, 257:1) = aten::flip(%758, %759), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %761 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.3, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %762 : Float(1:65792, 256:257, 257:1) = aten::slice(%761, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %763 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%762, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.3 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%763, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %765 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %ending_mask.3 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.3, %765), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:460:0
  %767 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.4, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %768 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%767, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %769 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%768, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.3 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%769, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %771 : int = aten::size(%beginning_input.3, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %772 : int = aten::size(%beginning_input.3, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %773 : int = aten::size(%beginning_input.3, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %774 : int = aten::size(%beginning_input.3, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %775 : int[] = prim::ListConstruct(%771, %772, %773, %774), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %776 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.3, %775, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %777 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%776, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %778 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.3, %777, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:463:0
  %779 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.4, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %780 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%779, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %781 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%780, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.3 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%781, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %783 : int = aten::size(%ending_input.3, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %784 : int = aten::size(%ending_input.3, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %785 : int = aten::size(%ending_input.3, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %786 : int = aten::size(%ending_input.3, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %787 : int[] = prim::ListConstruct(%783, %784, %785, %786), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %788 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.3, %787, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %789 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%788, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %790 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.3, %789, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:466:0
  %791 : Bool(17:512, 512:1) = aten::ne(%608, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %792 : Bool(17:512, 512:1) = aten::slice(%791, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
  %793 : Bool(17:512, 512:1) = aten::slice(%792, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
  %794 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%793, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.2 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%794, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
  %796 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.2, %query.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.2 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%796, %remove_from_windowed_attention_mask.2, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:275:0
  %798 : int = aten::size(%float_mask.2, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %799 : int = aten::size(%float_mask.2, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %800 : int = aten::size(%float_mask.2, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %801 : int = aten::size(%float_mask.2, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %802 : int[] = prim::ListConstruct(%798, %799, %800, %801), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %query.4 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%802, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %804 : int = aten::size(%query.4, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.7 : Long() = prim::NumToTensor(%804), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %806 : int = aten::size(%query.4, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.8 : Long() = prim::NumToTensor(%806), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %808 : int = aten::size(%query.4, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.5 : Long() = prim::NumToTensor(%808), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %810 : int = aten::size(%query.4, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %811 : Long() = aten::floor_divide(%seq_len.8, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %chunks_count.5 : Long() = aten::sub(%811, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:478:0
  %813 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.4, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %814 : Long() = aten::mul(%batch_size.7, %num_heads.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %815 : int = aten::Int(%814), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %816 : int[] = prim::ListConstruct(%815, %806, %810), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.17 : Float(17:512, 512:1, 1:1) = aten::reshape(%813, %816), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %818 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.2, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %819 : Long() = aten::mul(%batch_size.7, %num_heads.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %820 : int = aten::Int(%819), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %821 : int[] = prim::ListConstruct(%820, %806, %810), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.19 : Float(17:512, 512:1, 1:1) = aten::reshape(%818, %821), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %823 : int = aten::size(%hidden_states.17, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
  %824 : int = aten::size(%hidden_states.17, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
  %825 : Long() = prim::NumToTensor(%824), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %826 : Long() = aten::floor_divide(%825, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %827 : int = aten::Int(%826), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %828 : int = aten::size(%hidden_states.17, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
  %829 : int[] = prim::ListConstruct(%823, %827, %48, %828), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.18 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.17, %829), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
  %831 : int = aten::size(%hidden_states.18, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %832 : int = aten::size(%hidden_states.18, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %833 : Long() = prim::NumToTensor(%832), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %834 : int = aten::size(%hidden_states.18, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %835 : int = aten::size(%hidden_states.18, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %836 : Long() = aten::mul(%833, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %837 : Long() = aten::sub(%836, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %838 : int = aten::Int(%837), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %839 : int[] = prim::ListConstruct(%831, %838, %834, %835), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %840 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %841 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.18, %839, %840, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
  %842 : int = aten::size(%hidden_states.19, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
  %843 : int = aten::size(%hidden_states.19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
  %844 : Long() = prim::NumToTensor(%843), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %845 : Long() = aten::floor_divide(%844, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %846 : int = aten::Int(%845), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %847 : int = aten::size(%hidden_states.19, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
  %848 : int[] = prim::ListConstruct(%842, %846, %48, %847), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.20 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.19, %848), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
  %850 : int = aten::size(%hidden_states.20, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %851 : int = aten::size(%hidden_states.20, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %852 : Long() = prim::NumToTensor(%851), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %853 : int = aten::size(%hidden_states.20, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %854 : int = aten::size(%hidden_states.20, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %855 : Long() = aten::mul(%852, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %856 : Long() = aten::sub(%855, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %857 : int = aten::Int(%856), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %858 : int[] = prim::ListConstruct(%850, %857, %853, %854), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %859 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %860 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.20, %858, %859, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
  %861 : Tensor[] = prim::ListConstruct(%841, %860), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %input.21 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %861), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/functional.py:327:0
  %863 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states_padded.4 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.21, %863, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
  %865 : int = aten::size(%hidden_states_padded.4, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %866 : int = aten::size(%hidden_states_padded.4, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %867 : int = aten::size(%hidden_states_padded.4, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %868 : int = aten::size(%hidden_states_padded.4, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %869 : int[] = prim::ListConstruct(%865, %866, %867, %868), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %diagonal_chunked_attention_scores.4 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.4, %869), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:400:0
  %871 : Long() = aten::mul(%batch_size.7, %num_heads.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
  %872 : int = aten::Int(%871), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %873 : Long() = aten::add(%chunks_count.5, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
  %874 : int = aten::Int(%873), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %875 : int[] = prim::ListConstruct(%872, %874, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %diagonal_attention_scores.4 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.4, %875, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
  %877 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %878 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%877, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %879 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%878, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %880 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%879, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %881 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %882 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%881, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %883 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%882, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %884 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%883, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %885 : int[] = prim::ListConstruct(%14, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %886 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%880, %885), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %887 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%884, %886, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %888 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %889 : Float(17:262656, 512:513, 513:1) = aten::select(%888, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %890 : Float(17:262656, 256:513, 513:1) = aten::slice(%889, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %891 : Float(17:262656, 256:513, 257:1) = aten::slice(%890, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %892 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %893 : Float(17:262656, 256:513, 513:1) = aten::select(%892, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %894 : Float(17:262656, 256:513, 513:1) = aten::slice(%893, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %895 : Float(17:262656, 256:513, 257:1) = aten::slice(%894, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %896 : int[] = prim::ListConstruct(%14, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %897 : Float(17:262656, 256:513, 257:1) = aten::view(%891, %896), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %898 : Float(17:262656, 256:513, 257:1) = aten::copy_(%895, %897, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %899 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %900 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%899, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %901 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%900, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %902 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%901, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %903 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %904 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%903, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %905 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%904, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %906 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%905, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %907 : int[] = prim::ListConstruct(%14, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %908 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%902, %907), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %909 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%906, %908, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %910 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %911 : Float(17:262656, 512:513, 513:1) = aten::select(%910, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %912 : Float(17:262656, 255:513, 513:1) = aten::slice(%911, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %913 : Float(17:262656, 255:513, 255:1) = aten::slice(%912, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %914 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %915 : Float(17:262656, 256:513, 513:1) = aten::select(%914, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %916 : Float(17:262656, 255:513, 513:1) = aten::slice(%915, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %917 : Float(17:262656, 255:513, 255:1) = aten::slice(%916, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %918 : int[] = prim::ListConstruct(%14, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %919 : Float(17:262656, 255:513, 255:1) = aten::view(%913, %918), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %920 : Float(17:262656, 255:513, 255:1) = aten::copy_(%917, %919, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %921 : int[] = prim::ListConstruct(%804, %808, %806, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %922 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.4, %921), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.5 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%922, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
  %924 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %925 : Float(256:257, 257:1) = aten::ones(%924, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %926 : Float(256:257, 257:1) = aten::tril(%925, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %927 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %beginning_mask_2d.4 : Float(256:257, 257:1) = aten::flip(%926, %927), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %929 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.4, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %930 : Float(1:65792, 256:257, 257:1) = aten::slice(%929, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %931 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%930, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.4 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%931, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %933 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %ending_mask.4 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.4, %933), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:460:0
  %935 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.5, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %936 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%935, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %937 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%936, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.4 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%937, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %939 : int = aten::size(%beginning_input.4, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %940 : int = aten::size(%beginning_input.4, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %941 : int = aten::size(%beginning_input.4, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %942 : int = aten::size(%beginning_input.4, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %943 : int[] = prim::ListConstruct(%939, %940, %941, %942), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %944 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.4, %943, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %945 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%944, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %946 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.4, %945, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:463:0
  %947 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.5, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %948 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%947, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %949 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%948, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.4 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%949, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %951 : int = aten::size(%ending_input.4, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %952 : int = aten::size(%ending_input.4, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %953 : int = aten::size(%ending_input.4, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %954 : int = aten::size(%ending_input.4, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %955 : int[] = prim::ListConstruct(%951, %952, %953, %954), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %956 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.4, %955, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %957 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%956, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %958 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.4, %957, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.2 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.4, %input_tensor.5, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.2 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.2, %36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:1500:0
  %961 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.2, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %962 : Bool(17:512, 512:1) = aten::slice(%961, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %963 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%962, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %964 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%963, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %input.22 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.2, %964, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.4 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.22, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:973:0
  %967 : int[] = prim::ListConstruct(%626, %627, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %968 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.2, %967), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:331:0
  %value.2 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%968, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:331:0
  %970 : int = aten::size(%value.2, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.8 : Long() = prim::NumToTensor(%970), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %972 : int = aten::size(%value.2, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.9 : Long() = prim::NumToTensor(%972), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %974 : int = aten::size(%value.2, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.6 : Long() = prim::NumToTensor(%974), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %976 : int = aten::size(%value.2, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
  %977 : Long() = aten::floor_divide(%seq_len.9, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %chunks_count.6 : Long() = aten::sub(%977, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:542:0
  %979 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.4, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:545:0
  %980 : Long() = aten::mul(%batch_size.8, %num_heads.6), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:546:0
  %981 : int = aten::Int(%980), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %982 : Long() = aten::floor_divide(%seq_len.9, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %983 : int = aten::Int(%982), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %984 : int[] = prim::ListConstruct(%981, %983, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %chunked_hidden_states.6 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%979, %984), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:545:0
  %986 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.2, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
  %987 : Long() = aten::mul(%batch_size.8, %num_heads.6), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
  %988 : int = aten::Int(%987), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %989 : int[] = prim::ListConstruct(%988, %972, %976), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %input.23 : Float(204:64, 512:13056, 64:1) = aten::reshape(%986, %989), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
  %991 : int[] = prim::ListConstruct(%53, %53, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %padded_value.2 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.23, %991, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
  %993 : Long() = aten::mul(%batch_size.8, %num_heads.6), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:556:0
  %994 : int = aten::Int(%993), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %995 : Long() = aten::add(%chunks_count.6, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:556:0
  %996 : int = aten::Int(%995), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %997 : int[] = prim::ListConstruct(%994, %996, %39, %976), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %998 : int[] = prim::ListConstruct(%12, %11, %32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %999 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.2, %997, %998, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:564:0
  %1000 : int = aten::size(%chunked_hidden_states.6, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
  %1001 : int = aten::size(%chunked_hidden_states.6, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
  %1002 : int = aten::size(%chunked_hidden_states.6, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.2 : Long() = prim::NumToTensor(%1002), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1004 : int = aten::size(%chunked_hidden_states.6, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.2 : Long() = prim::NumToTensor(%1004), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1006 : Long() = aten::add(%window_overlap.2, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:422:0
  %1007 : int = aten::Int(%1006), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1008 : int[] = prim::ListConstruct(%53, %1007), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %chunked_hidden_states.7 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.6, %1008, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
  %1010 : int[] = prim::ListConstruct(%1000, %1001, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %chunked_hidden_states.8 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.7, %1010), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:424:0
  %1012 : Long() = aten::neg(%window_overlap.2), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:428:0
  %1013 : int = aten::Int(%1012), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1014 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.8, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
  %1015 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1014, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.9 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1015, %46, %53, %1013, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
  %1017 : Long() = aten::add(%window_overlap.2, %hidden_dim.2, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:431:0
  %1018 : int = aten::Int(%1017), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1019 : int[] = prim::ListConstruct(%1000, %1001, %1002, %1018), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %chunked_hidden_states.10 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.9, %1019), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:430:0
  %1021 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.10, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
  %1022 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1021, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
  %1023 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1022, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
  %1024 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1023, %45, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
  %1025 : Tensor[] = prim::ListConstruct(%1024, %999), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %context.2 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%10, %1025), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/functional.py:327:0
  %1027 : int[] = prim::ListConstruct(%970, %974, %972, %976), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1028 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.2, %1027), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.3 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1028, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:569:0
  %1030 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.3, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
  %1031 : int[] = prim::ListConstruct(%626, %627, %628), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1032 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1030, %1031), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.4 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1032, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
  %input.24 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.4, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:374:0
  %1035 : __torch__.torch.nn.modules.normalization.___torch_mangle_7598.LayerNorm = prim::GetAttr[name="LayerNorm"](%602)
  %1036 : __torch__.torch.nn.modules.linear.___torch_mangle_7597.Linear = prim::GetAttr[name="dense"](%602)
  %1037 : Tensor = prim::GetAttr[name="bias"](%1036)
  %1038 : Tensor = prim::GetAttr[name="weight"](%1036)
  %1039 : Float(768:1, 768:768) = aten::t(%1038), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.24, %1039), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.25 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.10, %1037, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.25, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.26 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.21, %hidden_states.12, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output # transformers/modeling_longformer.py:758:0
  %1044 : Tensor = prim::GetAttr[name="bias"](%1035)
  %1045 : Tensor = prim::GetAttr[name="weight"](%1035)
  %1046 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.6 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.26, %1046, %1045, %1044, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1048 : __torch__.torch.nn.modules.linear.___torch_mangle_7602.Linear = prim::GetAttr[name="dense"](%600)
  %1049 : Tensor = prim::GetAttr[name="bias"](%1048)
  %1050 : Tensor = prim::GetAttr[name="weight"](%1048)
  %1051 : Float(768:1, 3072:768) = aten::t(%1050), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.intermediate/__module.longformer.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.6, %1051), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.intermediate/__module.longformer.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.27 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.11, %1049, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.intermediate/__module.longformer.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.28 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %1055 : __torch__.torch.nn.modules.normalization.___torch_mangle_7605.LayerNorm = prim::GetAttr[name="LayerNorm"](%599)
  %1056 : __torch__.torch.nn.modules.linear.___torch_mangle_7604.Linear = prim::GetAttr[name="dense"](%599)
  %1057 : Tensor = prim::GetAttr[name="bias"](%1056)
  %1058 : Tensor = prim::GetAttr[name="weight"](%1056)
  %1059 : Float(3072:1, 768:3072) = aten::t(%1058), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.28, %1059), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.29 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.12, %1057, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.29, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.30 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.22, %input_tensor.6, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output # transformers/modeling_longformer.py:830:0
  %1064 : Tensor = prim::GetAttr[name="bias"](%1055)
  %1065 : Tensor = prim::GetAttr[name="weight"](%1055)
  %1066 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.LayerNorm
  %hidden_states.23 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.30, %1066, %1065, %1064, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %1068 : __torch__.transformers.modeling_longformer.___torch_mangle_7626.LongformerOutput = prim::GetAttr[name="output"](%125)
  %1069 : __torch__.transformers.modeling_longformer.___torch_mangle_7622.LongformerIntermediate = prim::GetAttr[name="intermediate"](%125)
  %1070 : __torch__.transformers.modeling_longformer.___torch_mangle_7620.LongformerAttention = prim::GetAttr[name="attention"](%125)
  %1071 : __torch__.transformers.modeling_longformer.___torch_mangle_7619.LongformerSelfOutput = prim::GetAttr[name="output"](%1070)
  %1072 : __torch__.transformers.modeling_longformer.___torch_mangle_7615.LongformerSelfAttention = prim::GetAttr[name="self"](%1070)
  %1073 : __torch__.torch.nn.modules.linear.___torch_mangle_7611.Linear = prim::GetAttr[name="value"](%1072)
  %1074 : __torch__.torch.nn.modules.linear.___torch_mangle_7610.Linear = prim::GetAttr[name="key"](%1072)
  %1075 : __torch__.torch.nn.modules.linear.___torch_mangle_7609.Linear = prim::GetAttr[name="query"](%1072)
  %1076 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:241:0
  %1077 : Float(17:512, 512:1) = aten::squeeze(%1076, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.3 : Bool(17:512, 512:1) = aten::lt(%1077, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %input.31 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.23, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:248:0
  %1080 : Tensor = prim::GetAttr[name="bias"](%1075)
  %1081 : Tensor = prim::GetAttr[name="weight"](%1075)
  %1082 : Float(768:1, 768:768) = aten::t(%1081), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.31, %1082), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.5 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.13, %1080, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %1085 : Tensor = prim::GetAttr[name="bias"](%1074)
  %1086 : Tensor = prim::GetAttr[name="weight"](%1074)
  %1087 : Float(768:1, 768:768) = aten::t(%1086), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.31, %1087), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.3 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.14, %1085, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %1090 : Tensor = prim::GetAttr[name="bias"](%1073)
  %1091 : Tensor = prim::GetAttr[name="weight"](%1073)
  %1092 : Float(768:1, 768:768) = aten::t(%1091), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.31, %1092), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.3 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.15, %1090, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %1095 : int = aten::size(%input.31, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
  %1096 : int = aten::size(%input.31, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
  %1097 : int = aten::size(%input.31, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.6 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.5, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:261:0
  %1099 : int[] = prim::ListConstruct(%1095, %1096, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1100 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.6, %1099), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:263:0
  %query.5 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1100, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:263:0
  %1102 : int[] = prim::ListConstruct(%1095, %1096, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1103 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.3, %1102), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:264:0
  %key.3 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1103, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:264:0
  %1105 : int = aten::size(%query.5, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.10 : Long() = prim::NumToTensor(%1105), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1107 : int = aten::size(%query.5, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.11 : Long() = prim::NumToTensor(%1107), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1109 : int = aten::size(%query.5, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.7 : Long() = prim::NumToTensor(%1109), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1111 : int = aten::size(%query.5, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %1112 : Long() = aten::floor_divide(%seq_len.11, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %chunks_count.7 : Long() = aten::sub(%1112, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:478:0
  %1114 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.5, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1115 : Long() = aten::mul(%batch_size.10, %num_heads.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1116 : int = aten::Int(%1115), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1117 : int[] = prim::ListConstruct(%1116, %1107, %1111), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.24 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1114, %1117), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1119 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.3, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1120 : Long() = aten::mul(%batch_size.10, %num_heads.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1121 : int = aten::Int(%1120), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1122 : int[] = prim::ListConstruct(%1121, %1107, %1111), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.26 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1119, %1122), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1124 : int = aten::size(%hidden_states.24, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
  %1125 : int = aten::size(%hidden_states.24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
  %1126 : Long() = prim::NumToTensor(%1125), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1127 : Long() = aten::floor_divide(%1126, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1128 : int = aten::Int(%1127), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1129 : int = aten::size(%hidden_states.24, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
  %1130 : int[] = prim::ListConstruct(%1124, %1128, %48, %1129), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.25 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.24, %1130), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
  %1132 : int = aten::size(%hidden_states.25, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1133 : int = aten::size(%hidden_states.25, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1134 : Long() = prim::NumToTensor(%1133), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1135 : int = aten::size(%hidden_states.25, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1136 : int = aten::size(%hidden_states.25, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1137 : Long() = aten::mul(%1134, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1138 : Long() = aten::sub(%1137, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1139 : int = aten::Int(%1138), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1140 : int[] = prim::ListConstruct(%1132, %1139, %1135, %1136), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1141 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1142 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.25, %1140, %1141, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
  %1143 : int = aten::size(%hidden_states.26, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
  %1144 : int = aten::size(%hidden_states.26, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
  %1145 : Long() = prim::NumToTensor(%1144), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1146 : Long() = aten::floor_divide(%1145, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1147 : int = aten::Int(%1146), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1148 : int = aten::size(%hidden_states.26, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
  %1149 : int[] = prim::ListConstruct(%1143, %1147, %48, %1148), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.27 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.26, %1149), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
  %1151 : int = aten::size(%hidden_states.27, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1152 : int = aten::size(%hidden_states.27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1153 : Long() = prim::NumToTensor(%1152), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1154 : int = aten::size(%hidden_states.27, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1155 : int = aten::size(%hidden_states.27, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1156 : Long() = aten::mul(%1153, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1157 : Long() = aten::sub(%1156, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1158 : int = aten::Int(%1157), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1159 : int[] = prim::ListConstruct(%1151, %1158, %1154, %1155), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1160 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1161 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.27, %1159, %1160, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
  %1162 : Tensor[] = prim::ListConstruct(%1142, %1161), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %input.32 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %1162), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/functional.py:327:0
  %1164 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states_padded.5 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.32, %1164, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
  %1166 : int = aten::size(%hidden_states_padded.5, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1167 : int = aten::size(%hidden_states_padded.5, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1168 : int = aten::size(%hidden_states_padded.5, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1169 : int = aten::size(%hidden_states_padded.5, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1170 : int[] = prim::ListConstruct(%1166, %1167, %1168, %1169), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %diagonal_chunked_attention_scores.5 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.5, %1170), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:400:0
  %1172 : Long() = aten::mul(%batch_size.10, %num_heads.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
  %1173 : int = aten::Int(%1172), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1174 : Long() = aten::add(%chunks_count.7, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
  %1175 : int = aten::Int(%1174), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1176 : int[] = prim::ListConstruct(%1173, %1175, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %diagonal_attention_scores.5 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.5, %1176, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
  %1178 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1179 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%1178, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1180 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%1179, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1181 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%1180, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1182 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1183 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1182, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1184 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1183, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1185 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%1184, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1186 : int[] = prim::ListConstruct(%21, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1187 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%1181, %1186), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1188 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%1185, %1187, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1189 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1190 : Float(204:262656, 512:513, 513:1) = aten::select(%1189, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1191 : Float(204:262656, 256:513, 513:1) = aten::slice(%1190, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1192 : Float(204:262656, 256:513, 257:1) = aten::slice(%1191, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1193 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1194 : Float(204:262656, 256:513, 513:1) = aten::select(%1193, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1195 : Float(204:262656, 256:513, 513:1) = aten::slice(%1194, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1196 : Float(204:262656, 256:513, 257:1) = aten::slice(%1195, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1197 : int[] = prim::ListConstruct(%21, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1198 : Float(204:262656, 256:513, 257:1) = aten::view(%1192, %1197), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1199 : Float(204:262656, 256:513, 257:1) = aten::copy_(%1196, %1198, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1200 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1201 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%1200, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1202 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%1201, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1203 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%1202, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1204 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1205 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1204, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1206 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1205, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1207 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%1206, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1208 : int[] = prim::ListConstruct(%21, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1209 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%1203, %1208), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1210 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%1207, %1209, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1211 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1212 : Float(204:262656, 512:513, 513:1) = aten::select(%1211, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1213 : Float(204:262656, 255:513, 513:1) = aten::slice(%1212, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1214 : Float(204:262656, 255:513, 255:1) = aten::slice(%1213, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1215 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1216 : Float(204:262656, 256:513, 513:1) = aten::select(%1215, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1217 : Float(204:262656, 255:513, 513:1) = aten::slice(%1216, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1218 : Float(204:262656, 255:513, 255:1) = aten::slice(%1217, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1219 : int[] = prim::ListConstruct(%21, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1220 : Float(204:262656, 255:513, 255:1) = aten::view(%1214, %1219), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1221 : Float(204:262656, 255:513, 255:1) = aten::copy_(%1218, %1220, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1222 : int[] = prim::ListConstruct(%1105, %1109, %1107, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1223 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.5, %1222), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.7 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%1223, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
  %1225 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1226 : Float(256:257, 257:1) = aten::ones(%1225, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1227 : Float(256:257, 257:1) = aten::tril(%1226, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1228 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %beginning_mask_2d.5 : Float(256:257, 257:1) = aten::flip(%1227, %1228), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1230 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.5, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1231 : Float(1:65792, 256:257, 257:1) = aten::slice(%1230, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1232 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1231, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.5 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1232, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1234 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %ending_mask.5 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.5, %1234), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:460:0
  %1236 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.7, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1237 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1236, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1238 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1237, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.5 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%1238, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1240 : int = aten::size(%beginning_input.5, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1241 : int = aten::size(%beginning_input.5, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1242 : int = aten::size(%beginning_input.5, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1243 : int = aten::size(%beginning_input.5, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1244 : int[] = prim::ListConstruct(%1240, %1241, %1242, %1243), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1245 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.5, %1244, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1246 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%1245, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1247 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.5, %1246, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:463:0
  %1248 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.7, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1249 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1248, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1250 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1249, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.5 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%1250, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1252 : int = aten::size(%ending_input.5, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1253 : int = aten::size(%ending_input.5, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1254 : int = aten::size(%ending_input.5, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1255 : int = aten::size(%ending_input.5, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1256 : int[] = prim::ListConstruct(%1252, %1253, %1254, %1255), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1257 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.5, %1256, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1258 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%1257, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1259 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.5, %1258, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:466:0
  %1260 : Bool(17:512, 512:1) = aten::ne(%1077, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1261 : Bool(17:512, 512:1) = aten::slice(%1260, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
  %1262 : Bool(17:512, 512:1) = aten::slice(%1261, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
  %1263 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1262, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.3 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1263, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
  %1265 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.3, %query.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.3 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%1265, %remove_from_windowed_attention_mask.3, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:275:0
  %1267 : int = aten::size(%float_mask.3, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1268 : int = aten::size(%float_mask.3, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1269 : int = aten::size(%float_mask.3, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1270 : int = aten::size(%float_mask.3, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1271 : int[] = prim::ListConstruct(%1267, %1268, %1269, %1270), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %query.6 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%1271, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1273 : int = aten::size(%query.6, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.11 : Long() = prim::NumToTensor(%1273), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1275 : int = aten::size(%query.6, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.12 : Long() = prim::NumToTensor(%1275), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1277 : int = aten::size(%query.6, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.8 : Long() = prim::NumToTensor(%1277), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1279 : int = aten::size(%query.6, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %1280 : Long() = aten::floor_divide(%seq_len.12, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %chunks_count.8 : Long() = aten::sub(%1280, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:478:0
  %1282 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.6, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1283 : Long() = aten::mul(%batch_size.11, %num_heads.8), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1284 : int = aten::Int(%1283), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1285 : int[] = prim::ListConstruct(%1284, %1275, %1279), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.28 : Float(17:512, 512:1, 1:1) = aten::reshape(%1282, %1285), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1287 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.3, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1288 : Long() = aten::mul(%batch_size.11, %num_heads.8), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1289 : int = aten::Int(%1288), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1290 : int[] = prim::ListConstruct(%1289, %1275, %1279), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.30 : Float(17:512, 512:1, 1:1) = aten::reshape(%1287, %1290), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1292 : int = aten::size(%hidden_states.28, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
  %1293 : int = aten::size(%hidden_states.28, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
  %1294 : Long() = prim::NumToTensor(%1293), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1295 : Long() = aten::floor_divide(%1294, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1296 : int = aten::Int(%1295), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1297 : int = aten::size(%hidden_states.28, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
  %1298 : int[] = prim::ListConstruct(%1292, %1296, %48, %1297), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.29 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.28, %1298), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
  %1300 : int = aten::size(%hidden_states.29, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1301 : int = aten::size(%hidden_states.29, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1302 : Long() = prim::NumToTensor(%1301), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1303 : int = aten::size(%hidden_states.29, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1304 : int = aten::size(%hidden_states.29, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1305 : Long() = aten::mul(%1302, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1306 : Long() = aten::sub(%1305, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1307 : int = aten::Int(%1306), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1308 : int[] = prim::ListConstruct(%1300, %1307, %1303, %1304), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1309 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1310 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.29, %1308, %1309, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
  %1311 : int = aten::size(%hidden_states.30, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
  %1312 : int = aten::size(%hidden_states.30, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
  %1313 : Long() = prim::NumToTensor(%1312), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1314 : Long() = aten::floor_divide(%1313, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1315 : int = aten::Int(%1314), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1316 : int = aten::size(%hidden_states.30, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
  %1317 : int[] = prim::ListConstruct(%1311, %1315, %48, %1316), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.31 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.30, %1317), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
  %1319 : int = aten::size(%hidden_states.31, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1320 : int = aten::size(%hidden_states.31, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1321 : Long() = prim::NumToTensor(%1320), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1322 : int = aten::size(%hidden_states.31, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1323 : int = aten::size(%hidden_states.31, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1324 : Long() = aten::mul(%1321, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1325 : Long() = aten::sub(%1324, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1326 : int = aten::Int(%1325), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1327 : int[] = prim::ListConstruct(%1319, %1326, %1322, %1323), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1328 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1329 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.31, %1327, %1328, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
  %1330 : Tensor[] = prim::ListConstruct(%1310, %1329), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %input.33 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %1330), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/functional.py:327:0
  %1332 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states_padded.6 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.33, %1332, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
  %1334 : int = aten::size(%hidden_states_padded.6, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1335 : int = aten::size(%hidden_states_padded.6, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1336 : int = aten::size(%hidden_states_padded.6, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1337 : int = aten::size(%hidden_states_padded.6, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1338 : int[] = prim::ListConstruct(%1334, %1335, %1336, %1337), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %diagonal_chunked_attention_scores.6 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.6, %1338), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:400:0
  %1340 : Long() = aten::mul(%batch_size.11, %num_heads.8), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
  %1341 : int = aten::Int(%1340), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1342 : Long() = aten::add(%chunks_count.8, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
  %1343 : int = aten::Int(%1342), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1344 : int[] = prim::ListConstruct(%1341, %1343, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %diagonal_attention_scores.6 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.6, %1344, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
  %1346 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1347 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%1346, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1348 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%1347, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1349 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%1348, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1350 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1351 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1350, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1352 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1351, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1353 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%1352, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1354 : int[] = prim::ListConstruct(%14, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1355 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%1349, %1354), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1356 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%1353, %1355, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1357 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1358 : Float(17:262656, 512:513, 513:1) = aten::select(%1357, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1359 : Float(17:262656, 256:513, 513:1) = aten::slice(%1358, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1360 : Float(17:262656, 256:513, 257:1) = aten::slice(%1359, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1361 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1362 : Float(17:262656, 256:513, 513:1) = aten::select(%1361, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1363 : Float(17:262656, 256:513, 513:1) = aten::slice(%1362, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1364 : Float(17:262656, 256:513, 257:1) = aten::slice(%1363, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1365 : int[] = prim::ListConstruct(%14, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1366 : Float(17:262656, 256:513, 257:1) = aten::view(%1360, %1365), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1367 : Float(17:262656, 256:513, 257:1) = aten::copy_(%1364, %1366, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1368 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1369 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%1368, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1370 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%1369, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1371 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%1370, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1372 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1373 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1372, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1374 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1373, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1375 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%1374, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1376 : int[] = prim::ListConstruct(%14, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1377 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%1371, %1376), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1378 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%1375, %1377, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1379 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1380 : Float(17:262656, 512:513, 513:1) = aten::select(%1379, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1381 : Float(17:262656, 255:513, 513:1) = aten::slice(%1380, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1382 : Float(17:262656, 255:513, 255:1) = aten::slice(%1381, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1383 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1384 : Float(17:262656, 256:513, 513:1) = aten::select(%1383, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1385 : Float(17:262656, 255:513, 513:1) = aten::slice(%1384, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1386 : Float(17:262656, 255:513, 255:1) = aten::slice(%1385, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1387 : int[] = prim::ListConstruct(%14, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1388 : Float(17:262656, 255:513, 255:1) = aten::view(%1382, %1387), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1389 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1386, %1388, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1390 : int[] = prim::ListConstruct(%1273, %1277, %1275, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1391 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.6, %1390), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.8 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1391, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
  %1393 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1394 : Float(256:257, 257:1) = aten::ones(%1393, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1395 : Float(256:257, 257:1) = aten::tril(%1394, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1396 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %beginning_mask_2d.6 : Float(256:257, 257:1) = aten::flip(%1395, %1396), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1398 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.6, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1399 : Float(1:65792, 256:257, 257:1) = aten::slice(%1398, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1400 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1399, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.6 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1400, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1402 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %ending_mask.6 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.6, %1402), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:460:0
  %1404 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.8, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1405 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1404, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1406 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1405, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.6 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1406, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1408 : int = aten::size(%beginning_input.6, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1409 : int = aten::size(%beginning_input.6, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1410 : int = aten::size(%beginning_input.6, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1411 : int = aten::size(%beginning_input.6, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1412 : int[] = prim::ListConstruct(%1408, %1409, %1410, %1411), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1413 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.6, %1412, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1414 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1413, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1415 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.6, %1414, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:463:0
  %1416 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.8, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1417 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1416, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1418 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1417, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.6 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1418, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1420 : int = aten::size(%ending_input.6, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1421 : int = aten::size(%ending_input.6, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1422 : int = aten::size(%ending_input.6, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1423 : int = aten::size(%ending_input.6, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1424 : int[] = prim::ListConstruct(%1420, %1421, %1422, %1423), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1425 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.6, %1424, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1426 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1425, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1427 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.6, %1426, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.3 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.7, %input_tensor.8, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.3 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.3, %36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:1500:0
  %1430 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.3, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %1431 : Bool(17:512, 512:1) = aten::slice(%1430, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %1432 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1431, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %1433 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1432, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %input.34 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.3, %1433, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.6 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.34, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:973:0
  %1436 : int[] = prim::ListConstruct(%1095, %1096, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1437 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.3, %1436), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:331:0
  %value.3 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1437, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:331:0
  %1439 : int = aten::size(%value.3, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.12 : Long() = prim::NumToTensor(%1439), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1441 : int = aten::size(%value.3, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.13 : Long() = prim::NumToTensor(%1441), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1443 : int = aten::size(%value.3, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.9 : Long() = prim::NumToTensor(%1443), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1445 : int = aten::size(%value.3, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
  %1446 : Long() = aten::floor_divide(%seq_len.13, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %chunks_count.9 : Long() = aten::sub(%1446, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:542:0
  %1448 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.6, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:545:0
  %1449 : Long() = aten::mul(%batch_size.12, %num_heads.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:546:0
  %1450 : int = aten::Int(%1449), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1451 : Long() = aten::floor_divide(%seq_len.13, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1452 : int = aten::Int(%1451), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1453 : int[] = prim::ListConstruct(%1450, %1452, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %chunked_hidden_states.11 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1448, %1453), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:545:0
  %1455 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.3, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
  %1456 : Long() = aten::mul(%batch_size.12, %num_heads.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
  %1457 : int = aten::Int(%1456), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1458 : int[] = prim::ListConstruct(%1457, %1441, %1445), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %input.35 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1455, %1458), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
  %1460 : int[] = prim::ListConstruct(%53, %53, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %padded_value.3 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.35, %1460, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
  %1462 : Long() = aten::mul(%batch_size.12, %num_heads.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:556:0
  %1463 : int = aten::Int(%1462), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1464 : Long() = aten::add(%chunks_count.9, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:556:0
  %1465 : int = aten::Int(%1464), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1466 : int[] = prim::ListConstruct(%1463, %1465, %39, %1445), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1467 : int[] = prim::ListConstruct(%12, %11, %32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1468 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.3, %1466, %1467, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:564:0
  %1469 : int = aten::size(%chunked_hidden_states.11, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
  %1470 : int = aten::size(%chunked_hidden_states.11, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
  %1471 : int = aten::size(%chunked_hidden_states.11, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.3 : Long() = prim::NumToTensor(%1471), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1473 : int = aten::size(%chunked_hidden_states.11, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.3 : Long() = prim::NumToTensor(%1473), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1475 : Long() = aten::add(%window_overlap.3, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:422:0
  %1476 : int = aten::Int(%1475), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1477 : int[] = prim::ListConstruct(%53, %1476), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %chunked_hidden_states.12 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.11, %1477, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
  %1479 : int[] = prim::ListConstruct(%1469, %1470, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %chunked_hidden_states.13 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.12, %1479), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:424:0
  %1481 : Long() = aten::neg(%window_overlap.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:428:0
  %1482 : int = aten::Int(%1481), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1483 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.13, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
  %1484 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1483, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.14 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1484, %46, %53, %1482, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
  %1486 : Long() = aten::add(%window_overlap.3, %hidden_dim.3, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:431:0
  %1487 : int = aten::Int(%1486), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1488 : int[] = prim::ListConstruct(%1469, %1470, %1471, %1487), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %chunked_hidden_states.15 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.14, %1488), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:430:0
  %1490 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.15, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
  %1491 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1490, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
  %1492 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1491, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
  %1493 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1492, %45, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
  %1494 : Tensor[] = prim::ListConstruct(%1493, %1468), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %context.3 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%10, %1494), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/functional.py:327:0
  %1496 : int[] = prim::ListConstruct(%1439, %1443, %1441, %1445), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1497 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.3, %1496), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.5 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1497, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:569:0
  %1499 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.5, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
  %1500 : int[] = prim::ListConstruct(%1095, %1096, %1097), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1501 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1499, %1500), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.6 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1501, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
  %input.36 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.6, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:374:0
  %1504 : __torch__.torch.nn.modules.normalization.___torch_mangle_7617.LayerNorm = prim::GetAttr[name="LayerNorm"](%1071)
  %1505 : __torch__.torch.nn.modules.linear.___torch_mangle_7616.Linear = prim::GetAttr[name="dense"](%1071)
  %1506 : Tensor = prim::GetAttr[name="bias"](%1505)
  %1507 : Tensor = prim::GetAttr[name="weight"](%1505)
  %1508 : Float(768:1, 768:768) = aten::t(%1507), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.36, %1508), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.37 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.16, %1506, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.32 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.37, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.38 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.32, %hidden_states.23, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output # transformers/modeling_longformer.py:758:0
  %1513 : Tensor = prim::GetAttr[name="bias"](%1504)
  %1514 : Tensor = prim::GetAttr[name="weight"](%1504)
  %1515 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.9 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.38, %1515, %1514, %1513, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1517 : __torch__.torch.nn.modules.linear.___torch_mangle_7621.Linear = prim::GetAttr[name="dense"](%1069)
  %1518 : Tensor = prim::GetAttr[name="bias"](%1517)
  %1519 : Tensor = prim::GetAttr[name="weight"](%1517)
  %1520 : Float(768:1, 3072:768) = aten::t(%1519), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.intermediate/__module.longformer.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.9, %1520), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.intermediate/__module.longformer.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.39 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.17, %1518, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.intermediate/__module.longformer.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.40 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %1524 : __torch__.torch.nn.modules.normalization.___torch_mangle_7624.LayerNorm = prim::GetAttr[name="LayerNorm"](%1068)
  %1525 : __torch__.torch.nn.modules.linear.___torch_mangle_7623.Linear = prim::GetAttr[name="dense"](%1068)
  %1526 : Tensor = prim::GetAttr[name="bias"](%1525)
  %1527 : Tensor = prim::GetAttr[name="weight"](%1525)
  %1528 : Float(3072:1, 768:3072) = aten::t(%1527), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.40, %1528), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.41 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.18, %1526, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.33 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.41, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.42 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.33, %input_tensor.9, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output # transformers/modeling_longformer.py:830:0
  %1533 : Tensor = prim::GetAttr[name="bias"](%1524)
  %1534 : Tensor = prim::GetAttr[name="weight"](%1524)
  %1535 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.LayerNorm
  %hidden_states.34 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.42, %1535, %1534, %1533, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %1537 : __torch__.transformers.modeling_longformer.___torch_mangle_7645.LongformerOutput = prim::GetAttr[name="output"](%123)
  %1538 : __torch__.transformers.modeling_longformer.___torch_mangle_7641.LongformerIntermediate = prim::GetAttr[name="intermediate"](%123)
  %1539 : __torch__.transformers.modeling_longformer.___torch_mangle_7639.LongformerAttention = prim::GetAttr[name="attention"](%123)
  %1540 : __torch__.transformers.modeling_longformer.___torch_mangle_7638.LongformerSelfOutput = prim::GetAttr[name="output"](%1539)
  %1541 : __torch__.transformers.modeling_longformer.___torch_mangle_7634.LongformerSelfAttention = prim::GetAttr[name="self"](%1539)
  %1542 : __torch__.torch.nn.modules.linear.___torch_mangle_7630.Linear = prim::GetAttr[name="value"](%1541)
  %1543 : __torch__.torch.nn.modules.linear.___torch_mangle_7629.Linear = prim::GetAttr[name="key"](%1541)
  %1544 : __torch__.torch.nn.modules.linear.___torch_mangle_7628.Linear = prim::GetAttr[name="query"](%1541)
  %1545 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:241:0
  %1546 : Float(17:512, 512:1) = aten::squeeze(%1545, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.4 : Bool(17:512, 512:1) = aten::lt(%1546, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %input.43 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.34, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:248:0
  %1549 : Tensor = prim::GetAttr[name="bias"](%1544)
  %1550 : Tensor = prim::GetAttr[name="weight"](%1544)
  %1551 : Float(768:1, 768:768) = aten::t(%1550), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.43, %1551), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.7 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.19, %1549, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %1554 : Tensor = prim::GetAttr[name="bias"](%1543)
  %1555 : Tensor = prim::GetAttr[name="weight"](%1543)
  %1556 : Float(768:1, 768:768) = aten::t(%1555), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.43, %1556), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.4 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.20, %1554, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %1559 : Tensor = prim::GetAttr[name="bias"](%1542)
  %1560 : Tensor = prim::GetAttr[name="weight"](%1542)
  %1561 : Float(768:1, 768:768) = aten::t(%1560), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.43, %1561), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.4 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.21, %1559, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %1564 : int = aten::size(%input.43, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
  %1565 : int = aten::size(%input.43, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
  %1566 : int = aten::size(%input.43, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.8 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.7, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:261:0
  %1568 : int[] = prim::ListConstruct(%1564, %1565, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1569 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.8, %1568), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:263:0
  %query.7 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1569, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:263:0
  %1571 : int[] = prim::ListConstruct(%1564, %1565, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1572 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.4, %1571), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:264:0
  %key.4 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1572, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:264:0
  %1574 : int = aten::size(%query.7, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.14 : Long() = prim::NumToTensor(%1574), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1576 : int = aten::size(%query.7, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.15 : Long() = prim::NumToTensor(%1576), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1578 : int = aten::size(%query.7, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.10 : Long() = prim::NumToTensor(%1578), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1580 : int = aten::size(%query.7, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %1581 : Long() = aten::floor_divide(%seq_len.15, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %chunks_count.10 : Long() = aten::sub(%1581, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:478:0
  %1583 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.7, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1584 : Long() = aten::mul(%batch_size.14, %num_heads.10), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1585 : int = aten::Int(%1584), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1586 : int[] = prim::ListConstruct(%1585, %1576, %1580), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.35 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1583, %1586), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1588 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.4, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1589 : Long() = aten::mul(%batch_size.14, %num_heads.10), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1590 : int = aten::Int(%1589), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1591 : int[] = prim::ListConstruct(%1590, %1576, %1580), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.37 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1588, %1591), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1593 : int = aten::size(%hidden_states.35, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
  %1594 : int = aten::size(%hidden_states.35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
  %1595 : Long() = prim::NumToTensor(%1594), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1596 : Long() = aten::floor_divide(%1595, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1597 : int = aten::Int(%1596), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1598 : int = aten::size(%hidden_states.35, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
  %1599 : int[] = prim::ListConstruct(%1593, %1597, %48, %1598), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.36 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.35, %1599), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
  %1601 : int = aten::size(%hidden_states.36, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1602 : int = aten::size(%hidden_states.36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1603 : Long() = prim::NumToTensor(%1602), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1604 : int = aten::size(%hidden_states.36, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1605 : int = aten::size(%hidden_states.36, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1606 : Long() = aten::mul(%1603, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1607 : Long() = aten::sub(%1606, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1608 : int = aten::Int(%1607), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1609 : int[] = prim::ListConstruct(%1601, %1608, %1604, %1605), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1610 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1611 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.36, %1609, %1610, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
  %1612 : int = aten::size(%hidden_states.37, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
  %1613 : int = aten::size(%hidden_states.37, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
  %1614 : Long() = prim::NumToTensor(%1613), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1615 : Long() = aten::floor_divide(%1614, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1616 : int = aten::Int(%1615), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1617 : int = aten::size(%hidden_states.37, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
  %1618 : int[] = prim::ListConstruct(%1612, %1616, %48, %1617), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.38 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.37, %1618), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
  %1620 : int = aten::size(%hidden_states.38, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1621 : int = aten::size(%hidden_states.38, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1622 : Long() = prim::NumToTensor(%1621), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1623 : int = aten::size(%hidden_states.38, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1624 : int = aten::size(%hidden_states.38, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1625 : Long() = aten::mul(%1622, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1626 : Long() = aten::sub(%1625, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1627 : int = aten::Int(%1626), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1628 : int[] = prim::ListConstruct(%1620, %1627, %1623, %1624), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1629 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1630 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.38, %1628, %1629, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
  %1631 : Tensor[] = prim::ListConstruct(%1611, %1630), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %input.44 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %1631), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/functional.py:327:0
  %1633 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states_padded.7 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.44, %1633, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
  %1635 : int = aten::size(%hidden_states_padded.7, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1636 : int = aten::size(%hidden_states_padded.7, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1637 : int = aten::size(%hidden_states_padded.7, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1638 : int = aten::size(%hidden_states_padded.7, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1639 : int[] = prim::ListConstruct(%1635, %1636, %1637, %1638), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %diagonal_chunked_attention_scores.7 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.7, %1639), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:400:0
  %1641 : Long() = aten::mul(%batch_size.14, %num_heads.10), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
  %1642 : int = aten::Int(%1641), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1643 : Long() = aten::add(%chunks_count.10, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
  %1644 : int = aten::Int(%1643), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1645 : int[] = prim::ListConstruct(%1642, %1644, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %diagonal_attention_scores.7 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.7, %1645, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
  %1647 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1648 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%1647, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1649 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%1648, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1650 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%1649, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1651 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1652 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1651, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1653 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1652, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1654 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%1653, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1655 : int[] = prim::ListConstruct(%21, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1656 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%1650, %1655), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1657 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%1654, %1656, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1658 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1659 : Float(204:262656, 512:513, 513:1) = aten::select(%1658, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1660 : Float(204:262656, 256:513, 513:1) = aten::slice(%1659, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1661 : Float(204:262656, 256:513, 257:1) = aten::slice(%1660, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1662 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1663 : Float(204:262656, 256:513, 513:1) = aten::select(%1662, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1664 : Float(204:262656, 256:513, 513:1) = aten::slice(%1663, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1665 : Float(204:262656, 256:513, 257:1) = aten::slice(%1664, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1666 : int[] = prim::ListConstruct(%21, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1667 : Float(204:262656, 256:513, 257:1) = aten::view(%1661, %1666), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1668 : Float(204:262656, 256:513, 257:1) = aten::copy_(%1665, %1667, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1669 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1670 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%1669, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1671 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%1670, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1672 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%1671, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1673 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1674 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1673, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1675 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1674, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1676 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%1675, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1677 : int[] = prim::ListConstruct(%21, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1678 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%1672, %1677), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1679 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%1676, %1678, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1680 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1681 : Float(204:262656, 512:513, 513:1) = aten::select(%1680, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1682 : Float(204:262656, 255:513, 513:1) = aten::slice(%1681, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1683 : Float(204:262656, 255:513, 255:1) = aten::slice(%1682, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1684 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1685 : Float(204:262656, 256:513, 513:1) = aten::select(%1684, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1686 : Float(204:262656, 255:513, 513:1) = aten::slice(%1685, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1687 : Float(204:262656, 255:513, 255:1) = aten::slice(%1686, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1688 : int[] = prim::ListConstruct(%21, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1689 : Float(204:262656, 255:513, 255:1) = aten::view(%1683, %1688), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1690 : Float(204:262656, 255:513, 255:1) = aten::copy_(%1687, %1689, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1691 : int[] = prim::ListConstruct(%1574, %1578, %1576, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1692 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.7, %1691), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.10 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%1692, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
  %1694 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1695 : Float(256:257, 257:1) = aten::ones(%1694, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1696 : Float(256:257, 257:1) = aten::tril(%1695, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1697 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %beginning_mask_2d.7 : Float(256:257, 257:1) = aten::flip(%1696, %1697), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1699 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.7, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1700 : Float(1:65792, 256:257, 257:1) = aten::slice(%1699, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1701 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1700, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.7 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1701, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1703 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %ending_mask.7 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.7, %1703), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:460:0
  %1705 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.10, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1706 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1705, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1707 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1706, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.7 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%1707, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1709 : int = aten::size(%beginning_input.7, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1710 : int = aten::size(%beginning_input.7, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1711 : int = aten::size(%beginning_input.7, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1712 : int = aten::size(%beginning_input.7, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1713 : int[] = prim::ListConstruct(%1709, %1710, %1711, %1712), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1714 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.7, %1713, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1715 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%1714, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1716 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.7, %1715, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:463:0
  %1717 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.10, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1718 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1717, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1719 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1718, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.7 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%1719, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1721 : int = aten::size(%ending_input.7, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1722 : int = aten::size(%ending_input.7, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1723 : int = aten::size(%ending_input.7, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1724 : int = aten::size(%ending_input.7, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1725 : int[] = prim::ListConstruct(%1721, %1722, %1723, %1724), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1726 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.7, %1725, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1727 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%1726, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1728 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.7, %1727, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:466:0
  %1729 : Bool(17:512, 512:1) = aten::ne(%1546, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1730 : Bool(17:512, 512:1) = aten::slice(%1729, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
  %1731 : Bool(17:512, 512:1) = aten::slice(%1730, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
  %1732 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1731, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.4 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1732, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
  %1734 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.4, %query.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.4 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%1734, %remove_from_windowed_attention_mask.4, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:275:0
  %1736 : int = aten::size(%float_mask.4, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1737 : int = aten::size(%float_mask.4, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1738 : int = aten::size(%float_mask.4, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1739 : int = aten::size(%float_mask.4, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1740 : int[] = prim::ListConstruct(%1736, %1737, %1738, %1739), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %query.8 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%1740, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1742 : int = aten::size(%query.8, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.15 : Long() = prim::NumToTensor(%1742), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1744 : int = aten::size(%query.8, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.16 : Long() = prim::NumToTensor(%1744), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1746 : int = aten::size(%query.8, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.11 : Long() = prim::NumToTensor(%1746), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1748 : int = aten::size(%query.8, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %1749 : Long() = aten::floor_divide(%seq_len.16, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %chunks_count.11 : Long() = aten::sub(%1749, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:478:0
  %1751 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.8, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1752 : Long() = aten::mul(%batch_size.15, %num_heads.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1753 : int = aten::Int(%1752), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1754 : int[] = prim::ListConstruct(%1753, %1744, %1748), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.39 : Float(17:512, 512:1, 1:1) = aten::reshape(%1751, %1754), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1756 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.4, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1757 : Long() = aten::mul(%batch_size.15, %num_heads.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1758 : int = aten::Int(%1757), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1759 : int[] = prim::ListConstruct(%1758, %1744, %1748), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.41 : Float(17:512, 512:1, 1:1) = aten::reshape(%1756, %1759), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1761 : int = aten::size(%hidden_states.39, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
  %1762 : int = aten::size(%hidden_states.39, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
  %1763 : Long() = prim::NumToTensor(%1762), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1764 : Long() = aten::floor_divide(%1763, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1765 : int = aten::Int(%1764), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1766 : int = aten::size(%hidden_states.39, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
  %1767 : int[] = prim::ListConstruct(%1761, %1765, %48, %1766), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.40 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.39, %1767), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
  %1769 : int = aten::size(%hidden_states.40, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1770 : int = aten::size(%hidden_states.40, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1771 : Long() = prim::NumToTensor(%1770), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1772 : int = aten::size(%hidden_states.40, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1773 : int = aten::size(%hidden_states.40, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1774 : Long() = aten::mul(%1771, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1775 : Long() = aten::sub(%1774, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1776 : int = aten::Int(%1775), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1777 : int[] = prim::ListConstruct(%1769, %1776, %1772, %1773), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1778 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1779 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.40, %1777, %1778, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
  %1780 : int = aten::size(%hidden_states.41, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
  %1781 : int = aten::size(%hidden_states.41, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
  %1782 : Long() = prim::NumToTensor(%1781), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1783 : Long() = aten::floor_divide(%1782, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1784 : int = aten::Int(%1783), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1785 : int = aten::size(%hidden_states.41, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
  %1786 : int[] = prim::ListConstruct(%1780, %1784, %48, %1785), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.42 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.41, %1786), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
  %1788 : int = aten::size(%hidden_states.42, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1789 : int = aten::size(%hidden_states.42, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1790 : Long() = prim::NumToTensor(%1789), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1791 : int = aten::size(%hidden_states.42, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1792 : int = aten::size(%hidden_states.42, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1793 : Long() = aten::mul(%1790, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1794 : Long() = aten::sub(%1793, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1795 : int = aten::Int(%1794), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1796 : int[] = prim::ListConstruct(%1788, %1795, %1791, %1792), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1797 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1798 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.42, %1796, %1797, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
  %1799 : Tensor[] = prim::ListConstruct(%1779, %1798), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %input.45 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %1799), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/functional.py:327:0
  %1801 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states_padded.8 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.45, %1801, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
  %1803 : int = aten::size(%hidden_states_padded.8, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1804 : int = aten::size(%hidden_states_padded.8, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1805 : int = aten::size(%hidden_states_padded.8, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1806 : int = aten::size(%hidden_states_padded.8, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1807 : int[] = prim::ListConstruct(%1803, %1804, %1805, %1806), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %diagonal_chunked_attention_scores.8 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.8, %1807), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:400:0
  %1809 : Long() = aten::mul(%batch_size.15, %num_heads.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
  %1810 : int = aten::Int(%1809), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1811 : Long() = aten::add(%chunks_count.11, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
  %1812 : int = aten::Int(%1811), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1813 : int[] = prim::ListConstruct(%1810, %1812, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %diagonal_attention_scores.8 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.8, %1813, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
  %1815 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1816 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%1815, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1817 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%1816, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1818 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%1817, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1819 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1820 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1819, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1821 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1820, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1822 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%1821, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1823 : int[] = prim::ListConstruct(%14, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1824 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%1818, %1823), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1825 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%1822, %1824, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1826 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1827 : Float(17:262656, 512:513, 513:1) = aten::select(%1826, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1828 : Float(17:262656, 256:513, 513:1) = aten::slice(%1827, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1829 : Float(17:262656, 256:513, 257:1) = aten::slice(%1828, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1830 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1831 : Float(17:262656, 256:513, 513:1) = aten::select(%1830, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1832 : Float(17:262656, 256:513, 513:1) = aten::slice(%1831, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1833 : Float(17:262656, 256:513, 257:1) = aten::slice(%1832, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1834 : int[] = prim::ListConstruct(%14, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1835 : Float(17:262656, 256:513, 257:1) = aten::view(%1829, %1834), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1836 : Float(17:262656, 256:513, 257:1) = aten::copy_(%1833, %1835, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1837 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1838 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%1837, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1839 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%1838, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1840 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%1839, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1841 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1842 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1841, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1843 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1842, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1844 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%1843, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1845 : int[] = prim::ListConstruct(%14, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1846 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%1840, %1845), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1847 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%1844, %1846, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1848 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1849 : Float(17:262656, 512:513, 513:1) = aten::select(%1848, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1850 : Float(17:262656, 255:513, 513:1) = aten::slice(%1849, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1851 : Float(17:262656, 255:513, 255:1) = aten::slice(%1850, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1852 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1853 : Float(17:262656, 256:513, 513:1) = aten::select(%1852, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1854 : Float(17:262656, 255:513, 513:1) = aten::slice(%1853, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1855 : Float(17:262656, 255:513, 255:1) = aten::slice(%1854, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1856 : int[] = prim::ListConstruct(%14, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1857 : Float(17:262656, 255:513, 255:1) = aten::view(%1851, %1856), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1858 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1855, %1857, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1859 : int[] = prim::ListConstruct(%1742, %1746, %1744, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1860 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.8, %1859), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.11 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1860, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
  %1862 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1863 : Float(256:257, 257:1) = aten::ones(%1862, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1864 : Float(256:257, 257:1) = aten::tril(%1863, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1865 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %beginning_mask_2d.8 : Float(256:257, 257:1) = aten::flip(%1864, %1865), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1867 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.8, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1868 : Float(1:65792, 256:257, 257:1) = aten::slice(%1867, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1869 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1868, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.8 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1869, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1871 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %ending_mask.8 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.8, %1871), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:460:0
  %1873 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.11, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1874 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1873, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1875 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1874, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.8 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1875, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1877 : int = aten::size(%beginning_input.8, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1878 : int = aten::size(%beginning_input.8, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1879 : int = aten::size(%beginning_input.8, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1880 : int = aten::size(%beginning_input.8, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1881 : int[] = prim::ListConstruct(%1877, %1878, %1879, %1880), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1882 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.8, %1881, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1883 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1882, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1884 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.8, %1883, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:463:0
  %1885 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.11, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1886 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1885, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1887 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1886, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.8 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1887, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1889 : int = aten::size(%ending_input.8, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1890 : int = aten::size(%ending_input.8, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1891 : int = aten::size(%ending_input.8, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1892 : int = aten::size(%ending_input.8, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1893 : int[] = prim::ListConstruct(%1889, %1890, %1891, %1892), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1894 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.8, %1893, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1895 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1894, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1896 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.8, %1895, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.4 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.10, %input_tensor.11, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.4 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.4, %36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:1500:0
  %1899 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.4, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %1900 : Bool(17:512, 512:1) = aten::slice(%1899, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %1901 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1900, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %1902 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1901, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %input.46 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.4, %1902, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.8 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.46, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:973:0
  %1905 : int[] = prim::ListConstruct(%1564, %1565, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1906 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.4, %1905), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:331:0
  %value.4 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1906, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:331:0
  %1908 : int = aten::size(%value.4, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.16 : Long() = prim::NumToTensor(%1908), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1910 : int = aten::size(%value.4, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.17 : Long() = prim::NumToTensor(%1910), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1912 : int = aten::size(%value.4, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.12 : Long() = prim::NumToTensor(%1912), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1914 : int = aten::size(%value.4, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
  %1915 : Long() = aten::floor_divide(%seq_len.17, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %chunks_count.12 : Long() = aten::sub(%1915, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:542:0
  %1917 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.8, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:545:0
  %1918 : Long() = aten::mul(%batch_size.16, %num_heads.12), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:546:0
  %1919 : int = aten::Int(%1918), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1920 : Long() = aten::floor_divide(%seq_len.17, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1921 : int = aten::Int(%1920), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1922 : int[] = prim::ListConstruct(%1919, %1921, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %chunked_hidden_states.16 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1917, %1922), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:545:0
  %1924 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.4, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
  %1925 : Long() = aten::mul(%batch_size.16, %num_heads.12), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
  %1926 : int = aten::Int(%1925), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1927 : int[] = prim::ListConstruct(%1926, %1910, %1914), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %input.47 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1924, %1927), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
  %1929 : int[] = prim::ListConstruct(%53, %53, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %padded_value.4 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.47, %1929, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
  %1931 : Long() = aten::mul(%batch_size.16, %num_heads.12), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:556:0
  %1932 : int = aten::Int(%1931), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1933 : Long() = aten::add(%chunks_count.12, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:556:0
  %1934 : int = aten::Int(%1933), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1935 : int[] = prim::ListConstruct(%1932, %1934, %39, %1914), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1936 : int[] = prim::ListConstruct(%12, %11, %32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1937 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.4, %1935, %1936, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:564:0
  %1938 : int = aten::size(%chunked_hidden_states.16, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
  %1939 : int = aten::size(%chunked_hidden_states.16, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
  %1940 : int = aten::size(%chunked_hidden_states.16, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.4 : Long() = prim::NumToTensor(%1940), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1942 : int = aten::size(%chunked_hidden_states.16, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.4 : Long() = prim::NumToTensor(%1942), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1944 : Long() = aten::add(%window_overlap.4, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:422:0
  %1945 : int = aten::Int(%1944), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1946 : int[] = prim::ListConstruct(%53, %1945), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %chunked_hidden_states.17 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.16, %1946, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
  %1948 : int[] = prim::ListConstruct(%1938, %1939, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %chunked_hidden_states.18 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.17, %1948), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:424:0
  %1950 : Long() = aten::neg(%window_overlap.4), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:428:0
  %1951 : int = aten::Int(%1950), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1952 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.18, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
  %1953 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1952, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.19 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1953, %46, %53, %1951, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
  %1955 : Long() = aten::add(%window_overlap.4, %hidden_dim.4, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:431:0
  %1956 : int = aten::Int(%1955), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1957 : int[] = prim::ListConstruct(%1938, %1939, %1940, %1956), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %chunked_hidden_states.20 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.19, %1957), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:430:0
  %1959 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.20, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
  %1960 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1959, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
  %1961 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1960, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
  %1962 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1961, %45, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
  %1963 : Tensor[] = prim::ListConstruct(%1962, %1937), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %context.4 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%10, %1963), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/functional.py:327:0
  %1965 : int[] = prim::ListConstruct(%1908, %1912, %1910, %1914), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1966 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.4, %1965), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.7 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1966, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:569:0
  %1968 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.7, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
  %1969 : int[] = prim::ListConstruct(%1564, %1565, %1566), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1970 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1968, %1969), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.8 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1970, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
  %input.48 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.8, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:374:0
  %1973 : __torch__.torch.nn.modules.normalization.___torch_mangle_7636.LayerNorm = prim::GetAttr[name="LayerNorm"](%1540)
  %1974 : __torch__.torch.nn.modules.linear.___torch_mangle_7635.Linear = prim::GetAttr[name="dense"](%1540)
  %1975 : Tensor = prim::GetAttr[name="bias"](%1974)
  %1976 : Tensor = prim::GetAttr[name="weight"](%1974)
  %1977 : Float(768:1, 768:768) = aten::t(%1976), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.48, %1977), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.49 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.22, %1975, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.43 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.49, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.43, %hidden_states.34, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output # transformers/modeling_longformer.py:758:0
  %1982 : Tensor = prim::GetAttr[name="bias"](%1973)
  %1983 : Tensor = prim::GetAttr[name="weight"](%1973)
  %1984 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.12 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.50, %1984, %1983, %1982, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1986 : __torch__.torch.nn.modules.linear.___torch_mangle_7640.Linear = prim::GetAttr[name="dense"](%1538)
  %1987 : Tensor = prim::GetAttr[name="bias"](%1986)
  %1988 : Tensor = prim::GetAttr[name="weight"](%1986)
  %1989 : Float(768:1, 3072:768) = aten::t(%1988), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.intermediate/__module.longformer.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.12, %1989), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.intermediate/__module.longformer.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.51 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.23, %1987, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.intermediate/__module.longformer.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.52 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %1993 : __torch__.torch.nn.modules.normalization.___torch_mangle_7643.LayerNorm = prim::GetAttr[name="LayerNorm"](%1537)
  %1994 : __torch__.torch.nn.modules.linear.___torch_mangle_7642.Linear = prim::GetAttr[name="dense"](%1537)
  %1995 : Tensor = prim::GetAttr[name="bias"](%1994)
  %1996 : Tensor = prim::GetAttr[name="weight"](%1994)
  %1997 : Float(3072:1, 768:3072) = aten::t(%1996), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.52, %1997), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.53 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.24, %1995, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.44 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.53, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.54 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.44, %input_tensor.12, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output # transformers/modeling_longformer.py:830:0
  %2002 : Tensor = prim::GetAttr[name="bias"](%1993)
  %2003 : Tensor = prim::GetAttr[name="weight"](%1993)
  %2004 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.LayerNorm
  %hidden_states.45 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.54, %2004, %2003, %2002, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %2006 : __torch__.transformers.modeling_longformer.___torch_mangle_7664.LongformerOutput = prim::GetAttr[name="output"](%121)
  %2007 : __torch__.transformers.modeling_longformer.___torch_mangle_7660.LongformerIntermediate = prim::GetAttr[name="intermediate"](%121)
  %2008 : __torch__.transformers.modeling_longformer.___torch_mangle_7658.LongformerAttention = prim::GetAttr[name="attention"](%121)
  %2009 : __torch__.transformers.modeling_longformer.___torch_mangle_7657.LongformerSelfOutput = prim::GetAttr[name="output"](%2008)
  %2010 : __torch__.transformers.modeling_longformer.___torch_mangle_7653.LongformerSelfAttention = prim::GetAttr[name="self"](%2008)
  %2011 : __torch__.torch.nn.modules.linear.___torch_mangle_7649.Linear = prim::GetAttr[name="value"](%2010)
  %2012 : __torch__.torch.nn.modules.linear.___torch_mangle_7648.Linear = prim::GetAttr[name="key"](%2010)
  %2013 : __torch__.torch.nn.modules.linear.___torch_mangle_7647.Linear = prim::GetAttr[name="query"](%2010)
  %2014 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:241:0
  %2015 : Float(17:512, 512:1) = aten::squeeze(%2014, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.5 : Bool(17:512, 512:1) = aten::lt(%2015, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %input.55 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.45, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:248:0
  %2018 : Tensor = prim::GetAttr[name="bias"](%2013)
  %2019 : Tensor = prim::GetAttr[name="weight"](%2013)
  %2020 : Float(768:1, 768:768) = aten::t(%2019), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.55, %2020), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.9 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.25, %2018, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %2023 : Tensor = prim::GetAttr[name="bias"](%2012)
  %2024 : Tensor = prim::GetAttr[name="weight"](%2012)
  %2025 : Float(768:1, 768:768) = aten::t(%2024), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.55, %2025), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.5 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.26, %2023, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %2028 : Tensor = prim::GetAttr[name="bias"](%2011)
  %2029 : Tensor = prim::GetAttr[name="weight"](%2011)
  %2030 : Float(768:1, 768:768) = aten::t(%2029), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.55, %2030), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.5 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.27, %2028, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %2033 : int = aten::size(%input.55, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
  %2034 : int = aten::size(%input.55, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
  %2035 : int = aten::size(%input.55, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.10 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.9, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:261:0
  %2037 : int[] = prim::ListConstruct(%2033, %2034, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2038 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.10, %2037), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:263:0
  %query.9 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2038, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:263:0
  %2040 : int[] = prim::ListConstruct(%2033, %2034, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2041 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.5, %2040), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:264:0
  %key.5 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2041, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:264:0
  %2043 : int = aten::size(%query.9, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.18 : Long() = prim::NumToTensor(%2043), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2045 : int = aten::size(%query.9, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.19 : Long() = prim::NumToTensor(%2045), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2047 : int = aten::size(%query.9, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.13 : Long() = prim::NumToTensor(%2047), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2049 : int = aten::size(%query.9, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %2050 : Long() = aten::floor_divide(%seq_len.19, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %chunks_count.13 : Long() = aten::sub(%2050, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:478:0
  %2052 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.9, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2053 : Long() = aten::mul(%batch_size.18, %num_heads.13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2054 : int = aten::Int(%2053), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2055 : int[] = prim::ListConstruct(%2054, %2045, %2049), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.46 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2052, %2055), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2057 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.5, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2058 : Long() = aten::mul(%batch_size.18, %num_heads.13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2059 : int = aten::Int(%2058), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2060 : int[] = prim::ListConstruct(%2059, %2045, %2049), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.48 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2057, %2060), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2062 : int = aten::size(%hidden_states.46, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
  %2063 : int = aten::size(%hidden_states.46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
  %2064 : Long() = prim::NumToTensor(%2063), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2065 : Long() = aten::floor_divide(%2064, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2066 : int = aten::Int(%2065), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2067 : int = aten::size(%hidden_states.46, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
  %2068 : int[] = prim::ListConstruct(%2062, %2066, %48, %2067), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.47 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.46, %2068), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
  %2070 : int = aten::size(%hidden_states.47, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2071 : int = aten::size(%hidden_states.47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2072 : Long() = prim::NumToTensor(%2071), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2073 : int = aten::size(%hidden_states.47, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2074 : int = aten::size(%hidden_states.47, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2075 : Long() = aten::mul(%2072, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2076 : Long() = aten::sub(%2075, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2077 : int = aten::Int(%2076), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2078 : int[] = prim::ListConstruct(%2070, %2077, %2073, %2074), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2079 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2080 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.47, %2078, %2079, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
  %2081 : int = aten::size(%hidden_states.48, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
  %2082 : int = aten::size(%hidden_states.48, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
  %2083 : Long() = prim::NumToTensor(%2082), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2084 : Long() = aten::floor_divide(%2083, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2085 : int = aten::Int(%2084), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2086 : int = aten::size(%hidden_states.48, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
  %2087 : int[] = prim::ListConstruct(%2081, %2085, %48, %2086), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.49 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.48, %2087), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
  %2089 : int = aten::size(%hidden_states.49, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2090 : int = aten::size(%hidden_states.49, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2091 : Long() = prim::NumToTensor(%2090), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2092 : int = aten::size(%hidden_states.49, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2093 : int = aten::size(%hidden_states.49, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2094 : Long() = aten::mul(%2091, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2095 : Long() = aten::sub(%2094, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2096 : int = aten::Int(%2095), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2097 : int[] = prim::ListConstruct(%2089, %2096, %2092, %2093), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2098 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2099 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.49, %2097, %2098, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
  %2100 : Tensor[] = prim::ListConstruct(%2080, %2099), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %input.56 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %2100), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/functional.py:327:0
  %2102 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states_padded.9 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.56, %2102, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
  %2104 : int = aten::size(%hidden_states_padded.9, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2105 : int = aten::size(%hidden_states_padded.9, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2106 : int = aten::size(%hidden_states_padded.9, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2107 : int = aten::size(%hidden_states_padded.9, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2108 : int[] = prim::ListConstruct(%2104, %2105, %2106, %2107), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %diagonal_chunked_attention_scores.9 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.9, %2108), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:400:0
  %2110 : Long() = aten::mul(%batch_size.18, %num_heads.13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
  %2111 : int = aten::Int(%2110), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2112 : Long() = aten::add(%chunks_count.13, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
  %2113 : int = aten::Int(%2112), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2114 : int[] = prim::ListConstruct(%2111, %2113, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %diagonal_attention_scores.9 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.9, %2114, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
  %2116 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2117 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%2116, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2118 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%2117, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2119 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%2118, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2120 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2121 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2120, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2122 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2121, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2123 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%2122, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2124 : int[] = prim::ListConstruct(%21, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2125 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%2119, %2124), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2126 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%2123, %2125, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2127 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2128 : Float(204:262656, 512:513, 513:1) = aten::select(%2127, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2129 : Float(204:262656, 256:513, 513:1) = aten::slice(%2128, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2130 : Float(204:262656, 256:513, 257:1) = aten::slice(%2129, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2131 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2132 : Float(204:262656, 256:513, 513:1) = aten::select(%2131, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2133 : Float(204:262656, 256:513, 513:1) = aten::slice(%2132, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2134 : Float(204:262656, 256:513, 257:1) = aten::slice(%2133, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2135 : int[] = prim::ListConstruct(%21, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2136 : Float(204:262656, 256:513, 257:1) = aten::view(%2130, %2135), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2137 : Float(204:262656, 256:513, 257:1) = aten::copy_(%2134, %2136, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2138 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2139 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%2138, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2140 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%2139, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2141 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%2140, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2142 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2143 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2142, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2144 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2143, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2145 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%2144, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2146 : int[] = prim::ListConstruct(%21, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2147 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%2141, %2146), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2148 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%2145, %2147, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2149 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2150 : Float(204:262656, 512:513, 513:1) = aten::select(%2149, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2151 : Float(204:262656, 255:513, 513:1) = aten::slice(%2150, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2152 : Float(204:262656, 255:513, 255:1) = aten::slice(%2151, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2153 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2154 : Float(204:262656, 256:513, 513:1) = aten::select(%2153, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2155 : Float(204:262656, 255:513, 513:1) = aten::slice(%2154, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2156 : Float(204:262656, 255:513, 255:1) = aten::slice(%2155, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2157 : int[] = prim::ListConstruct(%21, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2158 : Float(204:262656, 255:513, 255:1) = aten::view(%2152, %2157), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2159 : Float(204:262656, 255:513, 255:1) = aten::copy_(%2156, %2158, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2160 : int[] = prim::ListConstruct(%2043, %2047, %2045, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2161 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.9, %2160), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.13 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%2161, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
  %2163 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2164 : Float(256:257, 257:1) = aten::ones(%2163, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2165 : Float(256:257, 257:1) = aten::tril(%2164, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2166 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %beginning_mask_2d.9 : Float(256:257, 257:1) = aten::flip(%2165, %2166), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2168 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.9, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2169 : Float(1:65792, 256:257, 257:1) = aten::slice(%2168, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2170 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%2169, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.9 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%2170, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2172 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %ending_mask.9 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.9, %2172), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:460:0
  %2174 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.13, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2175 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2174, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2176 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2175, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.9 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%2176, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2178 : int = aten::size(%beginning_input.9, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2179 : int = aten::size(%beginning_input.9, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2180 : int = aten::size(%beginning_input.9, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2181 : int = aten::size(%beginning_input.9, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2182 : int[] = prim::ListConstruct(%2178, %2179, %2180, %2181), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2183 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.9, %2182, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2184 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%2183, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2185 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.9, %2184, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:463:0
  %2186 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.13, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2187 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2186, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2188 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2187, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.9 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%2188, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2190 : int = aten::size(%ending_input.9, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2191 : int = aten::size(%ending_input.9, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2192 : int = aten::size(%ending_input.9, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2193 : int = aten::size(%ending_input.9, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2194 : int[] = prim::ListConstruct(%2190, %2191, %2192, %2193), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2195 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.9, %2194, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2196 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%2195, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2197 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.9, %2196, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:466:0
  %2198 : Bool(17:512, 512:1) = aten::ne(%2015, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2199 : Bool(17:512, 512:1) = aten::slice(%2198, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
  %2200 : Bool(17:512, 512:1) = aten::slice(%2199, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
  %2201 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%2200, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.5 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%2201, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
  %2203 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.5, %query.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.5 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%2203, %remove_from_windowed_attention_mask.5, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:275:0
  %2205 : int = aten::size(%float_mask.5, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2206 : int = aten::size(%float_mask.5, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2207 : int = aten::size(%float_mask.5, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2208 : int = aten::size(%float_mask.5, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2209 : int[] = prim::ListConstruct(%2205, %2206, %2207, %2208), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %query.10 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%2209, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2211 : int = aten::size(%query.10, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.19 : Long() = prim::NumToTensor(%2211), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2213 : int = aten::size(%query.10, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.20 : Long() = prim::NumToTensor(%2213), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2215 : int = aten::size(%query.10, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.14 : Long() = prim::NumToTensor(%2215), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2217 : int = aten::size(%query.10, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %2218 : Long() = aten::floor_divide(%seq_len.20, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %chunks_count.14 : Long() = aten::sub(%2218, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:478:0
  %2220 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.10, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2221 : Long() = aten::mul(%batch_size.19, %num_heads.14), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2222 : int = aten::Int(%2221), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2223 : int[] = prim::ListConstruct(%2222, %2213, %2217), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.50 : Float(17:512, 512:1, 1:1) = aten::reshape(%2220, %2223), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2225 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.5, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2226 : Long() = aten::mul(%batch_size.19, %num_heads.14), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2227 : int = aten::Int(%2226), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2228 : int[] = prim::ListConstruct(%2227, %2213, %2217), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.52 : Float(17:512, 512:1, 1:1) = aten::reshape(%2225, %2228), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2230 : int = aten::size(%hidden_states.50, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
  %2231 : int = aten::size(%hidden_states.50, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
  %2232 : Long() = prim::NumToTensor(%2231), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2233 : Long() = aten::floor_divide(%2232, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2234 : int = aten::Int(%2233), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2235 : int = aten::size(%hidden_states.50, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
  %2236 : int[] = prim::ListConstruct(%2230, %2234, %48, %2235), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.51 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.50, %2236), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
  %2238 : int = aten::size(%hidden_states.51, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2239 : int = aten::size(%hidden_states.51, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2240 : Long() = prim::NumToTensor(%2239), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2241 : int = aten::size(%hidden_states.51, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2242 : int = aten::size(%hidden_states.51, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2243 : Long() = aten::mul(%2240, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2244 : Long() = aten::sub(%2243, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2245 : int = aten::Int(%2244), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2246 : int[] = prim::ListConstruct(%2238, %2245, %2241, %2242), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2247 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2248 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.51, %2246, %2247, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
  %2249 : int = aten::size(%hidden_states.52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
  %2250 : int = aten::size(%hidden_states.52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
  %2251 : Long() = prim::NumToTensor(%2250), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2252 : Long() = aten::floor_divide(%2251, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2253 : int = aten::Int(%2252), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2254 : int = aten::size(%hidden_states.52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
  %2255 : int[] = prim::ListConstruct(%2249, %2253, %48, %2254), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.53 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.52, %2255), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
  %2257 : int = aten::size(%hidden_states.53, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2258 : int = aten::size(%hidden_states.53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2259 : Long() = prim::NumToTensor(%2258), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2260 : int = aten::size(%hidden_states.53, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2261 : int = aten::size(%hidden_states.53, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2262 : Long() = aten::mul(%2259, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2263 : Long() = aten::sub(%2262, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2264 : int = aten::Int(%2263), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2265 : int[] = prim::ListConstruct(%2257, %2264, %2260, %2261), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2266 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2267 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.53, %2265, %2266, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
  %2268 : Tensor[] = prim::ListConstruct(%2248, %2267), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %input.57 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %2268), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/functional.py:327:0
  %2270 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states_padded.10 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.57, %2270, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
  %2272 : int = aten::size(%hidden_states_padded.10, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2273 : int = aten::size(%hidden_states_padded.10, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2274 : int = aten::size(%hidden_states_padded.10, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2275 : int = aten::size(%hidden_states_padded.10, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2276 : int[] = prim::ListConstruct(%2272, %2273, %2274, %2275), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %diagonal_chunked_attention_scores.10 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.10, %2276), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:400:0
  %2278 : Long() = aten::mul(%batch_size.19, %num_heads.14), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
  %2279 : int = aten::Int(%2278), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2280 : Long() = aten::add(%chunks_count.14, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
  %2281 : int = aten::Int(%2280), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2282 : int[] = prim::ListConstruct(%2279, %2281, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %diagonal_attention_scores.10 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.10, %2282, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
  %2284 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2285 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%2284, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2286 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%2285, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2287 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%2286, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2288 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2289 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2288, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2290 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2289, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2291 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%2290, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2292 : int[] = prim::ListConstruct(%14, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2293 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%2287, %2292), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2294 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%2291, %2293, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2295 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2296 : Float(17:262656, 512:513, 513:1) = aten::select(%2295, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2297 : Float(17:262656, 256:513, 513:1) = aten::slice(%2296, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2298 : Float(17:262656, 256:513, 257:1) = aten::slice(%2297, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2299 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2300 : Float(17:262656, 256:513, 513:1) = aten::select(%2299, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2301 : Float(17:262656, 256:513, 513:1) = aten::slice(%2300, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2302 : Float(17:262656, 256:513, 257:1) = aten::slice(%2301, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2303 : int[] = prim::ListConstruct(%14, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2304 : Float(17:262656, 256:513, 257:1) = aten::view(%2298, %2303), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2305 : Float(17:262656, 256:513, 257:1) = aten::copy_(%2302, %2304, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2306 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2307 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%2306, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2308 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%2307, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2309 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%2308, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2310 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2311 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2310, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2312 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2311, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2313 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%2312, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2314 : int[] = prim::ListConstruct(%14, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2315 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%2309, %2314), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2316 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%2313, %2315, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2317 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2318 : Float(17:262656, 512:513, 513:1) = aten::select(%2317, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2319 : Float(17:262656, 255:513, 513:1) = aten::slice(%2318, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2320 : Float(17:262656, 255:513, 255:1) = aten::slice(%2319, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2321 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2322 : Float(17:262656, 256:513, 513:1) = aten::select(%2321, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2323 : Float(17:262656, 255:513, 513:1) = aten::slice(%2322, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2324 : Float(17:262656, 255:513, 255:1) = aten::slice(%2323, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2325 : int[] = prim::ListConstruct(%14, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2326 : Float(17:262656, 255:513, 255:1) = aten::view(%2320, %2325), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2327 : Float(17:262656, 255:513, 255:1) = aten::copy_(%2324, %2326, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2328 : int[] = prim::ListConstruct(%2211, %2215, %2213, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2329 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.10, %2328), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.14 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%2329, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
  %2331 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2332 : Float(256:257, 257:1) = aten::ones(%2331, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2333 : Float(256:257, 257:1) = aten::tril(%2332, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2334 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %beginning_mask_2d.10 : Float(256:257, 257:1) = aten::flip(%2333, %2334), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2336 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.10, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2337 : Float(1:65792, 256:257, 257:1) = aten::slice(%2336, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2338 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%2337, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.10 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%2338, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2340 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %ending_mask.10 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.10, %2340), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:460:0
  %2342 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.14, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2343 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2342, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2344 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2343, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.10 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%2344, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2346 : int = aten::size(%beginning_input.10, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2347 : int = aten::size(%beginning_input.10, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2348 : int = aten::size(%beginning_input.10, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2349 : int = aten::size(%beginning_input.10, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2350 : int[] = prim::ListConstruct(%2346, %2347, %2348, %2349), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2351 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.10, %2350, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2352 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%2351, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2353 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.10, %2352, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:463:0
  %2354 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.14, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2355 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2354, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2356 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2355, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.10 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%2356, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2358 : int = aten::size(%ending_input.10, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2359 : int = aten::size(%ending_input.10, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2360 : int = aten::size(%ending_input.10, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2361 : int = aten::size(%ending_input.10, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2362 : int[] = prim::ListConstruct(%2358, %2359, %2360, %2361), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2363 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.10, %2362, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2364 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%2363, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2365 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.10, %2364, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.5 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.13, %input_tensor.14, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.5 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.5, %36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:1500:0
  %2368 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.5, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %2369 : Bool(17:512, 512:1) = aten::slice(%2368, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %2370 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%2369, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %2371 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%2370, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %input.58 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.5, %2371, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.10 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.58, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:973:0
  %2374 : int[] = prim::ListConstruct(%2033, %2034, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2375 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.5, %2374), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:331:0
  %value.5 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2375, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:331:0
  %2377 : int = aten::size(%value.5, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.20 : Long() = prim::NumToTensor(%2377), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2379 : int = aten::size(%value.5, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.21 : Long() = prim::NumToTensor(%2379), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2381 : int = aten::size(%value.5, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.15 : Long() = prim::NumToTensor(%2381), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2383 : int = aten::size(%value.5, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
  %2384 : Long() = aten::floor_divide(%seq_len.21, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %chunks_count.15 : Long() = aten::sub(%2384, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:542:0
  %2386 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.10, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:545:0
  %2387 : Long() = aten::mul(%batch_size.20, %num_heads.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:546:0
  %2388 : int = aten::Int(%2387), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2389 : Long() = aten::floor_divide(%seq_len.21, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2390 : int = aten::Int(%2389), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2391 : int[] = prim::ListConstruct(%2388, %2390, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %chunked_hidden_states.21 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%2386, %2391), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:545:0
  %2393 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.5, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
  %2394 : Long() = aten::mul(%batch_size.20, %num_heads.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
  %2395 : int = aten::Int(%2394), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2396 : int[] = prim::ListConstruct(%2395, %2379, %2383), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %input.59 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2393, %2396), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
  %2398 : int[] = prim::ListConstruct(%53, %53, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %padded_value.5 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.59, %2398, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
  %2400 : Long() = aten::mul(%batch_size.20, %num_heads.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:556:0
  %2401 : int = aten::Int(%2400), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2402 : Long() = aten::add(%chunks_count.15, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:556:0
  %2403 : int = aten::Int(%2402), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2404 : int[] = prim::ListConstruct(%2401, %2403, %39, %2383), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2405 : int[] = prim::ListConstruct(%12, %11, %32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2406 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.5, %2404, %2405, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:564:0
  %2407 : int = aten::size(%chunked_hidden_states.21, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
  %2408 : int = aten::size(%chunked_hidden_states.21, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
  %2409 : int = aten::size(%chunked_hidden_states.21, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.5 : Long() = prim::NumToTensor(%2409), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2411 : int = aten::size(%chunked_hidden_states.21, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.5 : Long() = prim::NumToTensor(%2411), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2413 : Long() = aten::add(%window_overlap.5, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:422:0
  %2414 : int = aten::Int(%2413), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2415 : int[] = prim::ListConstruct(%53, %2414), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %chunked_hidden_states.22 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.21, %2415, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
  %2417 : int[] = prim::ListConstruct(%2407, %2408, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %chunked_hidden_states.23 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.22, %2417), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:424:0
  %2419 : Long() = aten::neg(%window_overlap.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:428:0
  %2420 : int = aten::Int(%2419), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2421 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.23, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
  %2422 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%2421, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.24 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%2422, %46, %53, %2420, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
  %2424 : Long() = aten::add(%window_overlap.5, %hidden_dim.5, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:431:0
  %2425 : int = aten::Int(%2424), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2426 : int[] = prim::ListConstruct(%2407, %2408, %2409, %2425), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %chunked_hidden_states.25 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.24, %2426), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:430:0
  %2428 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.25, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
  %2429 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%2428, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
  %2430 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%2429, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
  %2431 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%2430, %45, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
  %2432 : Tensor[] = prim::ListConstruct(%2431, %2406), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %context.5 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%10, %2432), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/functional.py:327:0
  %2434 : int[] = prim::ListConstruct(%2377, %2381, %2379, %2383), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2435 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.5, %2434), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.9 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%2435, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:569:0
  %2437 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.9, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
  %2438 : int[] = prim::ListConstruct(%2033, %2034, %2035), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2439 : Float(512:13056, 17:768, 768:1) = aten::reshape(%2437, %2438), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.10 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%2439, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
  %input.60 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.10, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:374:0
  %2442 : __torch__.torch.nn.modules.normalization.___torch_mangle_7655.LayerNorm = prim::GetAttr[name="LayerNorm"](%2009)
  %2443 : __torch__.torch.nn.modules.linear.___torch_mangle_7654.Linear = prim::GetAttr[name="dense"](%2009)
  %2444 : Tensor = prim::GetAttr[name="bias"](%2443)
  %2445 : Tensor = prim::GetAttr[name="weight"](%2443)
  %2446 : Float(768:1, 768:768) = aten::t(%2445), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.60, %2446), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.61 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.28, %2444, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.54 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.61, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.62 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.54, %hidden_states.45, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output # transformers/modeling_longformer.py:758:0
  %2451 : Tensor = prim::GetAttr[name="bias"](%2442)
  %2452 : Tensor = prim::GetAttr[name="weight"](%2442)
  %2453 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.15 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.62, %2453, %2452, %2451, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2455 : __torch__.torch.nn.modules.linear.___torch_mangle_7659.Linear = prim::GetAttr[name="dense"](%2007)
  %2456 : Tensor = prim::GetAttr[name="bias"](%2455)
  %2457 : Tensor = prim::GetAttr[name="weight"](%2455)
  %2458 : Float(768:1, 3072:768) = aten::t(%2457), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.intermediate/__module.longformer.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.15, %2458), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.intermediate/__module.longformer.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.63 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.29, %2456, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.intermediate/__module.longformer.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.64 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %2462 : __torch__.torch.nn.modules.normalization.___torch_mangle_7662.LayerNorm = prim::GetAttr[name="LayerNorm"](%2006)
  %2463 : __torch__.torch.nn.modules.linear.___torch_mangle_7661.Linear = prim::GetAttr[name="dense"](%2006)
  %2464 : Tensor = prim::GetAttr[name="bias"](%2463)
  %2465 : Tensor = prim::GetAttr[name="weight"](%2463)
  %2466 : Float(3072:1, 768:3072) = aten::t(%2465), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.64, %2466), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.65 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.30, %2464, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.55 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.65, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.66 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.55, %input_tensor.15, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output # transformers/modeling_longformer.py:830:0
  %2471 : Tensor = prim::GetAttr[name="bias"](%2462)
  %2472 : Tensor = prim::GetAttr[name="weight"](%2462)
  %2473 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.LayerNorm
  %hidden_states.56 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.66, %2473, %2472, %2471, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %2475 : __torch__.transformers.modeling_longformer.___torch_mangle_7683.LongformerOutput = prim::GetAttr[name="output"](%119)
  %2476 : __torch__.transformers.modeling_longformer.___torch_mangle_7679.LongformerIntermediate = prim::GetAttr[name="intermediate"](%119)
  %2477 : __torch__.transformers.modeling_longformer.___torch_mangle_7677.LongformerAttention = prim::GetAttr[name="attention"](%119)
  %2478 : __torch__.transformers.modeling_longformer.___torch_mangle_7676.LongformerSelfOutput = prim::GetAttr[name="output"](%2477)
  %2479 : __torch__.transformers.modeling_longformer.___torch_mangle_7672.LongformerSelfAttention = prim::GetAttr[name="self"](%2477)
  %2480 : __torch__.torch.nn.modules.linear.___torch_mangle_7668.Linear = prim::GetAttr[name="value"](%2479)
  %2481 : __torch__.torch.nn.modules.linear.___torch_mangle_7667.Linear = prim::GetAttr[name="key"](%2479)
  %2482 : __torch__.torch.nn.modules.linear.___torch_mangle_7666.Linear = prim::GetAttr[name="query"](%2479)
  %2483 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:241:0
  %2484 : Float(17:512, 512:1) = aten::squeeze(%2483, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.6 : Bool(17:512, 512:1) = aten::lt(%2484, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %input.67 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.56, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:248:0
  %2487 : Tensor = prim::GetAttr[name="bias"](%2482)
  %2488 : Tensor = prim::GetAttr[name="weight"](%2482)
  %2489 : Float(768:1, 768:768) = aten::t(%2488), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.67, %2489), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.11 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.31, %2487, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %2492 : Tensor = prim::GetAttr[name="bias"](%2481)
  %2493 : Tensor = prim::GetAttr[name="weight"](%2481)
  %2494 : Float(768:1, 768:768) = aten::t(%2493), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.67, %2494), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.6 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.32, %2492, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %2497 : Tensor = prim::GetAttr[name="bias"](%2480)
  %2498 : Tensor = prim::GetAttr[name="weight"](%2480)
  %2499 : Float(768:1, 768:768) = aten::t(%2498), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.67, %2499), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.6 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.33, %2497, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %2502 : int = aten::size(%input.67, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
  %2503 : int = aten::size(%input.67, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
  %2504 : int = aten::size(%input.67, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.12 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.11, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:261:0
  %2506 : int[] = prim::ListConstruct(%2502, %2503, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2507 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.12, %2506), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:263:0
  %query.11 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2507, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:263:0
  %2509 : int[] = prim::ListConstruct(%2502, %2503, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2510 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.6, %2509), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:264:0
  %key.6 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2510, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:264:0
  %2512 : int = aten::size(%query.11, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.22 : Long() = prim::NumToTensor(%2512), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2514 : int = aten::size(%query.11, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.23 : Long() = prim::NumToTensor(%2514), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2516 : int = aten::size(%query.11, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.16 : Long() = prim::NumToTensor(%2516), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2518 : int = aten::size(%query.11, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %2519 : Long() = aten::floor_divide(%seq_len.23, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %chunks_count.16 : Long() = aten::sub(%2519, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:478:0
  %2521 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.11, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2522 : Long() = aten::mul(%batch_size.22, %num_heads.16), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2523 : int = aten::Int(%2522), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2524 : int[] = prim::ListConstruct(%2523, %2514, %2518), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.57 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2521, %2524), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2526 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.6, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2527 : Long() = aten::mul(%batch_size.22, %num_heads.16), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2528 : int = aten::Int(%2527), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2529 : int[] = prim::ListConstruct(%2528, %2514, %2518), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.59 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2526, %2529), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2531 : int = aten::size(%hidden_states.57, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
  %2532 : int = aten::size(%hidden_states.57, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
  %2533 : Long() = prim::NumToTensor(%2532), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2534 : Long() = aten::floor_divide(%2533, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2535 : int = aten::Int(%2534), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2536 : int = aten::size(%hidden_states.57, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
  %2537 : int[] = prim::ListConstruct(%2531, %2535, %48, %2536), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.58 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.57, %2537), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
  %2539 : int = aten::size(%hidden_states.58, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2540 : int = aten::size(%hidden_states.58, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2541 : Long() = prim::NumToTensor(%2540), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2542 : int = aten::size(%hidden_states.58, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2543 : int = aten::size(%hidden_states.58, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2544 : Long() = aten::mul(%2541, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2545 : Long() = aten::sub(%2544, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2546 : int = aten::Int(%2545), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2547 : int[] = prim::ListConstruct(%2539, %2546, %2542, %2543), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2548 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2549 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.58, %2547, %2548, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
  %2550 : int = aten::size(%hidden_states.59, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
  %2551 : int = aten::size(%hidden_states.59, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
  %2552 : Long() = prim::NumToTensor(%2551), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2553 : Long() = aten::floor_divide(%2552, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2554 : int = aten::Int(%2553), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2555 : int = aten::size(%hidden_states.59, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
  %2556 : int[] = prim::ListConstruct(%2550, %2554, %48, %2555), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.60 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.59, %2556), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
  %2558 : int = aten::size(%hidden_states.60, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2559 : int = aten::size(%hidden_states.60, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2560 : Long() = prim::NumToTensor(%2559), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2561 : int = aten::size(%hidden_states.60, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2562 : int = aten::size(%hidden_states.60, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2563 : Long() = aten::mul(%2560, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2564 : Long() = aten::sub(%2563, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2565 : int = aten::Int(%2564), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2566 : int[] = prim::ListConstruct(%2558, %2565, %2561, %2562), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2567 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2568 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.60, %2566, %2567, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
  %2569 : Tensor[] = prim::ListConstruct(%2549, %2568), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %input.68 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %2569), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/functional.py:327:0
  %2571 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states_padded.11 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.68, %2571, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
  %2573 : int = aten::size(%hidden_states_padded.11, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2574 : int = aten::size(%hidden_states_padded.11, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2575 : int = aten::size(%hidden_states_padded.11, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2576 : int = aten::size(%hidden_states_padded.11, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2577 : int[] = prim::ListConstruct(%2573, %2574, %2575, %2576), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %diagonal_chunked_attention_scores.11 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.11, %2577), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:400:0
  %2579 : Long() = aten::mul(%batch_size.22, %num_heads.16), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
  %2580 : int = aten::Int(%2579), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2581 : Long() = aten::add(%chunks_count.16, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
  %2582 : int = aten::Int(%2581), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2583 : int[] = prim::ListConstruct(%2580, %2582, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %diagonal_attention_scores.11 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.11, %2583, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
  %2585 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2586 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%2585, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2587 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%2586, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2588 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%2587, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2589 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2590 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2589, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2591 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2590, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2592 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%2591, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2593 : int[] = prim::ListConstruct(%21, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2594 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%2588, %2593), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2595 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%2592, %2594, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2596 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2597 : Float(204:262656, 512:513, 513:1) = aten::select(%2596, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2598 : Float(204:262656, 256:513, 513:1) = aten::slice(%2597, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2599 : Float(204:262656, 256:513, 257:1) = aten::slice(%2598, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2600 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2601 : Float(204:262656, 256:513, 513:1) = aten::select(%2600, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2602 : Float(204:262656, 256:513, 513:1) = aten::slice(%2601, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2603 : Float(204:262656, 256:513, 257:1) = aten::slice(%2602, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2604 : int[] = prim::ListConstruct(%21, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2605 : Float(204:262656, 256:513, 257:1) = aten::view(%2599, %2604), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2606 : Float(204:262656, 256:513, 257:1) = aten::copy_(%2603, %2605, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2607 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2608 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%2607, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2609 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%2608, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2610 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%2609, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2611 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2612 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2611, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2613 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2612, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2614 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%2613, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2615 : int[] = prim::ListConstruct(%21, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2616 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%2610, %2615), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2617 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%2614, %2616, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2618 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2619 : Float(204:262656, 512:513, 513:1) = aten::select(%2618, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2620 : Float(204:262656, 255:513, 513:1) = aten::slice(%2619, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2621 : Float(204:262656, 255:513, 255:1) = aten::slice(%2620, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2622 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2623 : Float(204:262656, 256:513, 513:1) = aten::select(%2622, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2624 : Float(204:262656, 255:513, 513:1) = aten::slice(%2623, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2625 : Float(204:262656, 255:513, 255:1) = aten::slice(%2624, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2626 : int[] = prim::ListConstruct(%21, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2627 : Float(204:262656, 255:513, 255:1) = aten::view(%2621, %2626), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2628 : Float(204:262656, 255:513, 255:1) = aten::copy_(%2625, %2627, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2629 : int[] = prim::ListConstruct(%2512, %2516, %2514, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2630 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.11, %2629), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.16 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%2630, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
  %2632 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2633 : Float(256:257, 257:1) = aten::ones(%2632, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2634 : Float(256:257, 257:1) = aten::tril(%2633, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2635 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %beginning_mask_2d.11 : Float(256:257, 257:1) = aten::flip(%2634, %2635), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2637 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.11, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2638 : Float(1:65792, 256:257, 257:1) = aten::slice(%2637, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2639 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%2638, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.11 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%2639, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2641 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %ending_mask.11 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.11, %2641), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:460:0
  %2643 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.16, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2644 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2643, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2645 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2644, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.11 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%2645, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2647 : int = aten::size(%beginning_input.11, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2648 : int = aten::size(%beginning_input.11, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2649 : int = aten::size(%beginning_input.11, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2650 : int = aten::size(%beginning_input.11, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2651 : int[] = prim::ListConstruct(%2647, %2648, %2649, %2650), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2652 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.11, %2651, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2653 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%2652, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2654 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.11, %2653, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:463:0
  %2655 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.16, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2656 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2655, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2657 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2656, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.11 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%2657, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2659 : int = aten::size(%ending_input.11, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2660 : int = aten::size(%ending_input.11, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2661 : int = aten::size(%ending_input.11, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2662 : int = aten::size(%ending_input.11, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2663 : int[] = prim::ListConstruct(%2659, %2660, %2661, %2662), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2664 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.11, %2663, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2665 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%2664, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2666 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.11, %2665, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:466:0
  %2667 : Bool(17:512, 512:1) = aten::ne(%2484, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2668 : Bool(17:512, 512:1) = aten::slice(%2667, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
  %2669 : Bool(17:512, 512:1) = aten::slice(%2668, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
  %2670 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%2669, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.6 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%2670, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
  %2672 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.6, %query.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.6 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%2672, %remove_from_windowed_attention_mask.6, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:275:0
  %2674 : int = aten::size(%float_mask.6, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2675 : int = aten::size(%float_mask.6, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2676 : int = aten::size(%float_mask.6, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2677 : int = aten::size(%float_mask.6, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2678 : int[] = prim::ListConstruct(%2674, %2675, %2676, %2677), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %query.12 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%2678, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2680 : int = aten::size(%query.12, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.23 : Long() = prim::NumToTensor(%2680), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2682 : int = aten::size(%query.12, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.24 : Long() = prim::NumToTensor(%2682), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2684 : int = aten::size(%query.12, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.17 : Long() = prim::NumToTensor(%2684), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2686 : int = aten::size(%query.12, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %2687 : Long() = aten::floor_divide(%seq_len.24, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %chunks_count.17 : Long() = aten::sub(%2687, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:478:0
  %2689 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.12, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2690 : Long() = aten::mul(%batch_size.23, %num_heads.17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2691 : int = aten::Int(%2690), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2692 : int[] = prim::ListConstruct(%2691, %2682, %2686), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.61 : Float(17:512, 512:1, 1:1) = aten::reshape(%2689, %2692), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2694 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.6, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2695 : Long() = aten::mul(%batch_size.23, %num_heads.17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2696 : int = aten::Int(%2695), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2697 : int[] = prim::ListConstruct(%2696, %2682, %2686), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.63 : Float(17:512, 512:1, 1:1) = aten::reshape(%2694, %2697), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2699 : int = aten::size(%hidden_states.61, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
  %2700 : int = aten::size(%hidden_states.61, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
  %2701 : Long() = prim::NumToTensor(%2700), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2702 : Long() = aten::floor_divide(%2701, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2703 : int = aten::Int(%2702), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2704 : int = aten::size(%hidden_states.61, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
  %2705 : int[] = prim::ListConstruct(%2699, %2703, %48, %2704), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.62 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.61, %2705), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
  %2707 : int = aten::size(%hidden_states.62, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2708 : int = aten::size(%hidden_states.62, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2709 : Long() = prim::NumToTensor(%2708), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2710 : int = aten::size(%hidden_states.62, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2711 : int = aten::size(%hidden_states.62, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2712 : Long() = aten::mul(%2709, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2713 : Long() = aten::sub(%2712, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2714 : int = aten::Int(%2713), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2715 : int[] = prim::ListConstruct(%2707, %2714, %2710, %2711), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2716 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2717 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.62, %2715, %2716, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
  %2718 : int = aten::size(%hidden_states.63, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
  %2719 : int = aten::size(%hidden_states.63, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
  %2720 : Long() = prim::NumToTensor(%2719), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2721 : Long() = aten::floor_divide(%2720, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2722 : int = aten::Int(%2721), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2723 : int = aten::size(%hidden_states.63, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
  %2724 : int[] = prim::ListConstruct(%2718, %2722, %48, %2723), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.64 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.63, %2724), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
  %2726 : int = aten::size(%hidden_states.64, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2727 : int = aten::size(%hidden_states.64, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2728 : Long() = prim::NumToTensor(%2727), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2729 : int = aten::size(%hidden_states.64, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2730 : int = aten::size(%hidden_states.64, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2731 : Long() = aten::mul(%2728, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2732 : Long() = aten::sub(%2731, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2733 : int = aten::Int(%2732), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2734 : int[] = prim::ListConstruct(%2726, %2733, %2729, %2730), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2735 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2736 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.64, %2734, %2735, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
  %2737 : Tensor[] = prim::ListConstruct(%2717, %2736), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %input.69 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %2737), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/functional.py:327:0
  %2739 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states_padded.12 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.69, %2739, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
  %2741 : int = aten::size(%hidden_states_padded.12, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2742 : int = aten::size(%hidden_states_padded.12, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2743 : int = aten::size(%hidden_states_padded.12, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2744 : int = aten::size(%hidden_states_padded.12, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2745 : int[] = prim::ListConstruct(%2741, %2742, %2743, %2744), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %diagonal_chunked_attention_scores.12 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.12, %2745), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:400:0
  %2747 : Long() = aten::mul(%batch_size.23, %num_heads.17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
  %2748 : int = aten::Int(%2747), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2749 : Long() = aten::add(%chunks_count.17, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
  %2750 : int = aten::Int(%2749), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2751 : int[] = prim::ListConstruct(%2748, %2750, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %diagonal_attention_scores.12 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.12, %2751, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
  %2753 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2754 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%2753, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2755 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%2754, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2756 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%2755, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2757 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2758 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2757, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2759 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2758, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2760 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%2759, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2761 : int[] = prim::ListConstruct(%14, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2762 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%2756, %2761), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2763 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%2760, %2762, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2764 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2765 : Float(17:262656, 512:513, 513:1) = aten::select(%2764, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2766 : Float(17:262656, 256:513, 513:1) = aten::slice(%2765, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2767 : Float(17:262656, 256:513, 257:1) = aten::slice(%2766, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2768 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2769 : Float(17:262656, 256:513, 513:1) = aten::select(%2768, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2770 : Float(17:262656, 256:513, 513:1) = aten::slice(%2769, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2771 : Float(17:262656, 256:513, 257:1) = aten::slice(%2770, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2772 : int[] = prim::ListConstruct(%14, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2773 : Float(17:262656, 256:513, 257:1) = aten::view(%2767, %2772), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2774 : Float(17:262656, 256:513, 257:1) = aten::copy_(%2771, %2773, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2775 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2776 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%2775, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2777 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%2776, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2778 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%2777, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2779 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2780 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2779, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2781 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2780, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2782 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%2781, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2783 : int[] = prim::ListConstruct(%14, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2784 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%2778, %2783), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2785 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%2782, %2784, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2786 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2787 : Float(17:262656, 512:513, 513:1) = aten::select(%2786, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2788 : Float(17:262656, 255:513, 513:1) = aten::slice(%2787, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2789 : Float(17:262656, 255:513, 255:1) = aten::slice(%2788, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2790 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2791 : Float(17:262656, 256:513, 513:1) = aten::select(%2790, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2792 : Float(17:262656, 255:513, 513:1) = aten::slice(%2791, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2793 : Float(17:262656, 255:513, 255:1) = aten::slice(%2792, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2794 : int[] = prim::ListConstruct(%14, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2795 : Float(17:262656, 255:513, 255:1) = aten::view(%2789, %2794), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2796 : Float(17:262656, 255:513, 255:1) = aten::copy_(%2793, %2795, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2797 : int[] = prim::ListConstruct(%2680, %2684, %2682, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2798 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.12, %2797), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.17 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%2798, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
  %2800 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2801 : Float(256:257, 257:1) = aten::ones(%2800, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2802 : Float(256:257, 257:1) = aten::tril(%2801, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2803 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %beginning_mask_2d.12 : Float(256:257, 257:1) = aten::flip(%2802, %2803), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2805 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.12, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2806 : Float(1:65792, 256:257, 257:1) = aten::slice(%2805, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2807 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%2806, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.12 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%2807, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2809 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %ending_mask.12 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.12, %2809), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:460:0
  %2811 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.17, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2812 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2811, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2813 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2812, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.12 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%2813, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2815 : int = aten::size(%beginning_input.12, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2816 : int = aten::size(%beginning_input.12, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2817 : int = aten::size(%beginning_input.12, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2818 : int = aten::size(%beginning_input.12, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2819 : int[] = prim::ListConstruct(%2815, %2816, %2817, %2818), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2820 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.12, %2819, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2821 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%2820, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2822 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.12, %2821, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:463:0
  %2823 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.17, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2824 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2823, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2825 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2824, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.12 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%2825, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2827 : int = aten::size(%ending_input.12, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2828 : int = aten::size(%ending_input.12, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2829 : int = aten::size(%ending_input.12, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2830 : int = aten::size(%ending_input.12, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2831 : int[] = prim::ListConstruct(%2827, %2828, %2829, %2830), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2832 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.12, %2831, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2833 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%2832, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2834 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.12, %2833, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.6 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.16, %input_tensor.17, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.6 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.6, %36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:1500:0
  %2837 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.6, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %2838 : Bool(17:512, 512:1) = aten::slice(%2837, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %2839 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%2838, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %2840 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%2839, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %input.70 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.6, %2840, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.12 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.70, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:973:0
  %2843 : int[] = prim::ListConstruct(%2502, %2503, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2844 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.6, %2843), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:331:0
  %value.6 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2844, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:331:0
  %2846 : int = aten::size(%value.6, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.24 : Long() = prim::NumToTensor(%2846), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2848 : int = aten::size(%value.6, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.25 : Long() = prim::NumToTensor(%2848), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2850 : int = aten::size(%value.6, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.18 : Long() = prim::NumToTensor(%2850), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2852 : int = aten::size(%value.6, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
  %2853 : Long() = aten::floor_divide(%seq_len.25, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %chunks_count.18 : Long() = aten::sub(%2853, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:542:0
  %2855 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.12, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:545:0
  %2856 : Long() = aten::mul(%batch_size.24, %num_heads.18), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:546:0
  %2857 : int = aten::Int(%2856), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2858 : Long() = aten::floor_divide(%seq_len.25, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2859 : int = aten::Int(%2858), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2860 : int[] = prim::ListConstruct(%2857, %2859, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %chunked_hidden_states.26 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%2855, %2860), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:545:0
  %2862 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.6, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
  %2863 : Long() = aten::mul(%batch_size.24, %num_heads.18), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
  %2864 : int = aten::Int(%2863), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2865 : int[] = prim::ListConstruct(%2864, %2848, %2852), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %input.71 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2862, %2865), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
  %2867 : int[] = prim::ListConstruct(%53, %53, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %padded_value.6 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.71, %2867, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
  %2869 : Long() = aten::mul(%batch_size.24, %num_heads.18), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:556:0
  %2870 : int = aten::Int(%2869), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2871 : Long() = aten::add(%chunks_count.18, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:556:0
  %2872 : int = aten::Int(%2871), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2873 : int[] = prim::ListConstruct(%2870, %2872, %39, %2852), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2874 : int[] = prim::ListConstruct(%12, %11, %32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2875 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.6, %2873, %2874, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:564:0
  %2876 : int = aten::size(%chunked_hidden_states.26, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
  %2877 : int = aten::size(%chunked_hidden_states.26, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
  %2878 : int = aten::size(%chunked_hidden_states.26, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.6 : Long() = prim::NumToTensor(%2878), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2880 : int = aten::size(%chunked_hidden_states.26, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.6 : Long() = prim::NumToTensor(%2880), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2882 : Long() = aten::add(%window_overlap.6, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:422:0
  %2883 : int = aten::Int(%2882), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2884 : int[] = prim::ListConstruct(%53, %2883), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %chunked_hidden_states.27 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.26, %2884, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
  %2886 : int[] = prim::ListConstruct(%2876, %2877, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %chunked_hidden_states.28 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.27, %2886), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:424:0
  %2888 : Long() = aten::neg(%window_overlap.6), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:428:0
  %2889 : int = aten::Int(%2888), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2890 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.28, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
  %2891 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%2890, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.29 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%2891, %46, %53, %2889, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
  %2893 : Long() = aten::add(%window_overlap.6, %hidden_dim.6, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:431:0
  %2894 : int = aten::Int(%2893), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2895 : int[] = prim::ListConstruct(%2876, %2877, %2878, %2894), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %chunked_hidden_states.30 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.29, %2895), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:430:0
  %2897 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.30, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
  %2898 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%2897, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
  %2899 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%2898, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
  %2900 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%2899, %45, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
  %2901 : Tensor[] = prim::ListConstruct(%2900, %2875), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %context.6 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%10, %2901), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/functional.py:327:0
  %2903 : int[] = prim::ListConstruct(%2846, %2850, %2848, %2852), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2904 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.6, %2903), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.11 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%2904, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:569:0
  %2906 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.11, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
  %2907 : int[] = prim::ListConstruct(%2502, %2503, %2504), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2908 : Float(512:13056, 17:768, 768:1) = aten::reshape(%2906, %2907), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.12 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%2908, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
  %input.72 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.12, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:374:0
  %2911 : __torch__.torch.nn.modules.normalization.___torch_mangle_7674.LayerNorm = prim::GetAttr[name="LayerNorm"](%2478)
  %2912 : __torch__.torch.nn.modules.linear.___torch_mangle_7673.Linear = prim::GetAttr[name="dense"](%2478)
  %2913 : Tensor = prim::GetAttr[name="bias"](%2912)
  %2914 : Tensor = prim::GetAttr[name="weight"](%2912)
  %2915 : Float(768:1, 768:768) = aten::t(%2914), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.72, %2915), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.73 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.34, %2913, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.65 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.73, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.65, %hidden_states.56, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output # transformers/modeling_longformer.py:758:0
  %2920 : Tensor = prim::GetAttr[name="bias"](%2911)
  %2921 : Tensor = prim::GetAttr[name="weight"](%2911)
  %2922 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.18 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.74, %2922, %2921, %2920, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2924 : __torch__.torch.nn.modules.linear.___torch_mangle_7678.Linear = prim::GetAttr[name="dense"](%2476)
  %2925 : Tensor = prim::GetAttr[name="bias"](%2924)
  %2926 : Tensor = prim::GetAttr[name="weight"](%2924)
  %2927 : Float(768:1, 3072:768) = aten::t(%2926), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.intermediate/__module.longformer.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.18, %2927), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.intermediate/__module.longformer.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.75 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.35, %2925, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.intermediate/__module.longformer.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.76 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.75), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %2931 : __torch__.torch.nn.modules.normalization.___torch_mangle_7681.LayerNorm = prim::GetAttr[name="LayerNorm"](%2475)
  %2932 : __torch__.torch.nn.modules.linear.___torch_mangle_7680.Linear = prim::GetAttr[name="dense"](%2475)
  %2933 : Tensor = prim::GetAttr[name="bias"](%2932)
  %2934 : Tensor = prim::GetAttr[name="weight"](%2932)
  %2935 : Float(3072:1, 768:3072) = aten::t(%2934), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.76, %2935), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.77 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.36, %2933, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.66 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.77, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.66, %input_tensor.18, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output # transformers/modeling_longformer.py:830:0
  %2940 : Tensor = prim::GetAttr[name="bias"](%2931)
  %2941 : Tensor = prim::GetAttr[name="weight"](%2931)
  %2942 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.LayerNorm
  %hidden_states.67 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.78, %2942, %2941, %2940, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %2944 : __torch__.transformers.modeling_longformer.___torch_mangle_7702.LongformerOutput = prim::GetAttr[name="output"](%117)
  %2945 : __torch__.transformers.modeling_longformer.___torch_mangle_7698.LongformerIntermediate = prim::GetAttr[name="intermediate"](%117)
  %2946 : __torch__.transformers.modeling_longformer.___torch_mangle_7696.LongformerAttention = prim::GetAttr[name="attention"](%117)
  %2947 : __torch__.transformers.modeling_longformer.___torch_mangle_7695.LongformerSelfOutput = prim::GetAttr[name="output"](%2946)
  %2948 : __torch__.transformers.modeling_longformer.___torch_mangle_7691.LongformerSelfAttention = prim::GetAttr[name="self"](%2946)
  %2949 : __torch__.torch.nn.modules.linear.___torch_mangle_7687.Linear = prim::GetAttr[name="value"](%2948)
  %2950 : __torch__.torch.nn.modules.linear.___torch_mangle_7686.Linear = prim::GetAttr[name="key"](%2948)
  %2951 : __torch__.torch.nn.modules.linear.___torch_mangle_7685.Linear = prim::GetAttr[name="query"](%2948)
  %2952 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:241:0
  %2953 : Float(17:512, 512:1) = aten::squeeze(%2952, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.7 : Bool(17:512, 512:1) = aten::lt(%2953, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %input.79 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.67, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:248:0
  %2956 : Tensor = prim::GetAttr[name="bias"](%2951)
  %2957 : Tensor = prim::GetAttr[name="weight"](%2951)
  %2958 : Float(768:1, 768:768) = aten::t(%2957), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.79, %2958), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.13 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.37, %2956, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %2961 : Tensor = prim::GetAttr[name="bias"](%2950)
  %2962 : Tensor = prim::GetAttr[name="weight"](%2950)
  %2963 : Float(768:1, 768:768) = aten::t(%2962), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.79, %2963), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.7 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.38, %2961, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %2966 : Tensor = prim::GetAttr[name="bias"](%2949)
  %2967 : Tensor = prim::GetAttr[name="weight"](%2949)
  %2968 : Float(768:1, 768:768) = aten::t(%2967), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.79, %2968), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.7 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.39, %2966, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %2971 : int = aten::size(%input.79, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
  %2972 : int = aten::size(%input.79, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
  %2973 : int = aten::size(%input.79, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.14 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.13, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:261:0
  %2975 : int[] = prim::ListConstruct(%2971, %2972, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2976 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.14, %2975), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:263:0
  %query.13 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2976, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:263:0
  %2978 : int[] = prim::ListConstruct(%2971, %2972, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2979 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.7, %2978), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:264:0
  %key.7 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2979, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:264:0
  %2981 : int = aten::size(%query.13, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.26 : Long() = prim::NumToTensor(%2981), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2983 : int = aten::size(%query.13, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.27 : Long() = prim::NumToTensor(%2983), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2985 : int = aten::size(%query.13, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.19 : Long() = prim::NumToTensor(%2985), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2987 : int = aten::size(%query.13, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %2988 : Long() = aten::floor_divide(%seq_len.27, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %chunks_count.19 : Long() = aten::sub(%2988, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:478:0
  %2990 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.13, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %2991 : Long() = aten::mul(%batch_size.26, %num_heads.19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %2992 : int = aten::Int(%2991), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2993 : int[] = prim::ListConstruct(%2992, %2983, %2987), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.68 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2990, %2993), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %2995 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.7, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %2996 : Long() = aten::mul(%batch_size.26, %num_heads.19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %2997 : int = aten::Int(%2996), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2998 : int[] = prim::ListConstruct(%2997, %2983, %2987), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.70 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2995, %2998), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3000 : int = aten::size(%hidden_states.68, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
  %3001 : int = aten::size(%hidden_states.68, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
  %3002 : Long() = prim::NumToTensor(%3001), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3003 : Long() = aten::floor_divide(%3002, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3004 : int = aten::Int(%3003), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3005 : int = aten::size(%hidden_states.68, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
  %3006 : int[] = prim::ListConstruct(%3000, %3004, %48, %3005), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.69 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.68, %3006), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
  %3008 : int = aten::size(%hidden_states.69, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3009 : int = aten::size(%hidden_states.69, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3010 : Long() = prim::NumToTensor(%3009), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3011 : int = aten::size(%hidden_states.69, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3012 : int = aten::size(%hidden_states.69, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3013 : Long() = aten::mul(%3010, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3014 : Long() = aten::sub(%3013, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3015 : int = aten::Int(%3014), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3016 : int[] = prim::ListConstruct(%3008, %3015, %3011, %3012), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3017 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3018 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.69, %3016, %3017, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
  %3019 : int = aten::size(%hidden_states.70, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
  %3020 : int = aten::size(%hidden_states.70, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
  %3021 : Long() = prim::NumToTensor(%3020), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3022 : Long() = aten::floor_divide(%3021, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3023 : int = aten::Int(%3022), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3024 : int = aten::size(%hidden_states.70, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
  %3025 : int[] = prim::ListConstruct(%3019, %3023, %48, %3024), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.71 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.70, %3025), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
  %3027 : int = aten::size(%hidden_states.71, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3028 : int = aten::size(%hidden_states.71, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3029 : Long() = prim::NumToTensor(%3028), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3030 : int = aten::size(%hidden_states.71, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3031 : int = aten::size(%hidden_states.71, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3032 : Long() = aten::mul(%3029, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3033 : Long() = aten::sub(%3032, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3034 : int = aten::Int(%3033), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3035 : int[] = prim::ListConstruct(%3027, %3034, %3030, %3031), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3036 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3037 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.71, %3035, %3036, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
  %3038 : Tensor[] = prim::ListConstruct(%3018, %3037), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %input.80 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %3038), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/functional.py:327:0
  %3040 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states_padded.13 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.80, %3040, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
  %3042 : int = aten::size(%hidden_states_padded.13, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3043 : int = aten::size(%hidden_states_padded.13, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3044 : int = aten::size(%hidden_states_padded.13, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3045 : int = aten::size(%hidden_states_padded.13, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3046 : int[] = prim::ListConstruct(%3042, %3043, %3044, %3045), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %diagonal_chunked_attention_scores.13 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.13, %3046), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:400:0
  %3048 : Long() = aten::mul(%batch_size.26, %num_heads.19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
  %3049 : int = aten::Int(%3048), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3050 : Long() = aten::add(%chunks_count.19, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
  %3051 : int = aten::Int(%3050), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3052 : int[] = prim::ListConstruct(%3049, %3051, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %diagonal_attention_scores.13 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.13, %3052, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
  %3054 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3055 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3054, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3056 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3055, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3057 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%3056, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3058 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3059 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3058, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3060 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3059, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3061 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%3060, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3062 : int[] = prim::ListConstruct(%21, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3063 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%3057, %3062), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3064 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3061, %3063, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3065 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3066 : Float(204:262656, 512:513, 513:1) = aten::select(%3065, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3067 : Float(204:262656, 256:513, 513:1) = aten::slice(%3066, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3068 : Float(204:262656, 256:513, 257:1) = aten::slice(%3067, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3069 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3070 : Float(204:262656, 256:513, 513:1) = aten::select(%3069, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3071 : Float(204:262656, 256:513, 513:1) = aten::slice(%3070, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3072 : Float(204:262656, 256:513, 257:1) = aten::slice(%3071, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3073 : int[] = prim::ListConstruct(%21, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3074 : Float(204:262656, 256:513, 257:1) = aten::view(%3068, %3073), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3075 : Float(204:262656, 256:513, 257:1) = aten::copy_(%3072, %3074, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3076 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3077 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3076, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3078 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3077, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3079 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%3078, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3080 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3081 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3080, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3082 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3081, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3083 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%3082, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3084 : int[] = prim::ListConstruct(%21, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3085 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%3079, %3084), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3086 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%3083, %3085, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3087 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3088 : Float(204:262656, 512:513, 513:1) = aten::select(%3087, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3089 : Float(204:262656, 255:513, 513:1) = aten::slice(%3088, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3090 : Float(204:262656, 255:513, 255:1) = aten::slice(%3089, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3091 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3092 : Float(204:262656, 256:513, 513:1) = aten::select(%3091, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3093 : Float(204:262656, 255:513, 513:1) = aten::slice(%3092, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3094 : Float(204:262656, 255:513, 255:1) = aten::slice(%3093, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3095 : int[] = prim::ListConstruct(%21, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3096 : Float(204:262656, 255:513, 255:1) = aten::view(%3090, %3095), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3097 : Float(204:262656, 255:513, 255:1) = aten::copy_(%3094, %3096, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3098 : int[] = prim::ListConstruct(%2981, %2985, %2983, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3099 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.13, %3098), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.19 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%3099, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
  %3101 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3102 : Float(256:257, 257:1) = aten::ones(%3101, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3103 : Float(256:257, 257:1) = aten::tril(%3102, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3104 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %beginning_mask_2d.13 : Float(256:257, 257:1) = aten::flip(%3103, %3104), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3106 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.13, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3107 : Float(1:65792, 256:257, 257:1) = aten::slice(%3106, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3108 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%3107, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.13 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%3108, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3110 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %ending_mask.13 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.13, %3110), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:460:0
  %3112 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.19, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3113 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3112, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3114 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3113, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.13 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%3114, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3116 : int = aten::size(%beginning_input.13, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3117 : int = aten::size(%beginning_input.13, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3118 : int = aten::size(%beginning_input.13, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3119 : int = aten::size(%beginning_input.13, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3120 : int[] = prim::ListConstruct(%3116, %3117, %3118, %3119), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3121 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.13, %3120, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3122 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%3121, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3123 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.13, %3122, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:463:0
  %3124 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.19, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3125 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3124, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3126 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3125, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.13 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%3126, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3128 : int = aten::size(%ending_input.13, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3129 : int = aten::size(%ending_input.13, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3130 : int = aten::size(%ending_input.13, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3131 : int = aten::size(%ending_input.13, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3132 : int[] = prim::ListConstruct(%3128, %3129, %3130, %3131), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3133 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.13, %3132, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3134 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%3133, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3135 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.13, %3134, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:466:0
  %3136 : Bool(17:512, 512:1) = aten::ne(%2953, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3137 : Bool(17:512, 512:1) = aten::slice(%3136, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
  %3138 : Bool(17:512, 512:1) = aten::slice(%3137, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
  %3139 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%3138, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.7 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%3139, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
  %3141 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.7, %query.13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.7 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%3141, %remove_from_windowed_attention_mask.7, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:275:0
  %3143 : int = aten::size(%float_mask.7, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3144 : int = aten::size(%float_mask.7, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3145 : int = aten::size(%float_mask.7, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3146 : int = aten::size(%float_mask.7, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3147 : int[] = prim::ListConstruct(%3143, %3144, %3145, %3146), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %query.14 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%3147, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3149 : int = aten::size(%query.14, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.27 : Long() = prim::NumToTensor(%3149), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3151 : int = aten::size(%query.14, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.28 : Long() = prim::NumToTensor(%3151), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3153 : int = aten::size(%query.14, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.20 : Long() = prim::NumToTensor(%3153), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3155 : int = aten::size(%query.14, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %3156 : Long() = aten::floor_divide(%seq_len.28, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %chunks_count.20 : Long() = aten::sub(%3156, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:478:0
  %3158 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.14, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %3159 : Long() = aten::mul(%batch_size.27, %num_heads.20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %3160 : int = aten::Int(%3159), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3161 : int[] = prim::ListConstruct(%3160, %3151, %3155), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.72 : Float(17:512, 512:1, 1:1) = aten::reshape(%3158, %3161), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %3163 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.7, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3164 : Long() = aten::mul(%batch_size.27, %num_heads.20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3165 : int = aten::Int(%3164), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3166 : int[] = prim::ListConstruct(%3165, %3151, %3155), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.74 : Float(17:512, 512:1, 1:1) = aten::reshape(%3163, %3166), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3168 : int = aten::size(%hidden_states.72, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
  %3169 : int = aten::size(%hidden_states.72, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
  %3170 : Long() = prim::NumToTensor(%3169), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3171 : Long() = aten::floor_divide(%3170, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3172 : int = aten::Int(%3171), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3173 : int = aten::size(%hidden_states.72, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
  %3174 : int[] = prim::ListConstruct(%3168, %3172, %48, %3173), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.73 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.72, %3174), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
  %3176 : int = aten::size(%hidden_states.73, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3177 : int = aten::size(%hidden_states.73, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3178 : Long() = prim::NumToTensor(%3177), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3179 : int = aten::size(%hidden_states.73, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3180 : int = aten::size(%hidden_states.73, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3181 : Long() = aten::mul(%3178, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3182 : Long() = aten::sub(%3181, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3183 : int = aten::Int(%3182), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3184 : int[] = prim::ListConstruct(%3176, %3183, %3179, %3180), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3185 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3186 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.73, %3184, %3185, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
  %3187 : int = aten::size(%hidden_states.74, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
  %3188 : int = aten::size(%hidden_states.74, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
  %3189 : Long() = prim::NumToTensor(%3188), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3190 : Long() = aten::floor_divide(%3189, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3191 : int = aten::Int(%3190), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3192 : int = aten::size(%hidden_states.74, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
  %3193 : int[] = prim::ListConstruct(%3187, %3191, %48, %3192), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.75 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.74, %3193), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
  %3195 : int = aten::size(%hidden_states.75, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3196 : int = aten::size(%hidden_states.75, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3197 : Long() = prim::NumToTensor(%3196), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3198 : int = aten::size(%hidden_states.75, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3199 : int = aten::size(%hidden_states.75, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3200 : Long() = aten::mul(%3197, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3201 : Long() = aten::sub(%3200, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3202 : int = aten::Int(%3201), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3203 : int[] = prim::ListConstruct(%3195, %3202, %3198, %3199), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3204 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3205 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.75, %3203, %3204, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
  %3206 : Tensor[] = prim::ListConstruct(%3186, %3205), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %input.81 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %3206), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/functional.py:327:0
  %3208 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states_padded.14 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.81, %3208, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
  %3210 : int = aten::size(%hidden_states_padded.14, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3211 : int = aten::size(%hidden_states_padded.14, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3212 : int = aten::size(%hidden_states_padded.14, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3213 : int = aten::size(%hidden_states_padded.14, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3214 : int[] = prim::ListConstruct(%3210, %3211, %3212, %3213), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %diagonal_chunked_attention_scores.14 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.14, %3214), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:400:0
  %3216 : Long() = aten::mul(%batch_size.27, %num_heads.20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
  %3217 : int = aten::Int(%3216), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3218 : Long() = aten::add(%chunks_count.20, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
  %3219 : int = aten::Int(%3218), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3220 : int[] = prim::ListConstruct(%3217, %3219, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %diagonal_attention_scores.14 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.14, %3220, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
  %3222 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3223 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%3222, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3224 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%3223, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3225 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%3224, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3226 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3227 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3226, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3228 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3227, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3229 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%3228, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3230 : int[] = prim::ListConstruct(%14, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3231 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%3225, %3230), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3232 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3229, %3231, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3233 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3234 : Float(17:262656, 512:513, 513:1) = aten::select(%3233, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3235 : Float(17:262656, 256:513, 513:1) = aten::slice(%3234, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3236 : Float(17:262656, 256:513, 257:1) = aten::slice(%3235, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3237 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3238 : Float(17:262656, 256:513, 513:1) = aten::select(%3237, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3239 : Float(17:262656, 256:513, 513:1) = aten::slice(%3238, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3240 : Float(17:262656, 256:513, 257:1) = aten::slice(%3239, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3241 : int[] = prim::ListConstruct(%14, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3242 : Float(17:262656, 256:513, 257:1) = aten::view(%3236, %3241), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3243 : Float(17:262656, 256:513, 257:1) = aten::copy_(%3240, %3242, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3244 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3245 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%3244, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3246 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%3245, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3247 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%3246, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3248 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3249 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3248, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3250 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3249, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3251 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%3250, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3252 : int[] = prim::ListConstruct(%14, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3253 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%3247, %3252), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3254 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%3251, %3253, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3255 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3256 : Float(17:262656, 512:513, 513:1) = aten::select(%3255, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3257 : Float(17:262656, 255:513, 513:1) = aten::slice(%3256, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3258 : Float(17:262656, 255:513, 255:1) = aten::slice(%3257, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3259 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3260 : Float(17:262656, 256:513, 513:1) = aten::select(%3259, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3261 : Float(17:262656, 255:513, 513:1) = aten::slice(%3260, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3262 : Float(17:262656, 255:513, 255:1) = aten::slice(%3261, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3263 : int[] = prim::ListConstruct(%14, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3264 : Float(17:262656, 255:513, 255:1) = aten::view(%3258, %3263), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3265 : Float(17:262656, 255:513, 255:1) = aten::copy_(%3262, %3264, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3266 : int[] = prim::ListConstruct(%3149, %3153, %3151, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3267 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.14, %3266), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.20 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%3267, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
  %3269 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3270 : Float(256:257, 257:1) = aten::ones(%3269, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3271 : Float(256:257, 257:1) = aten::tril(%3270, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3272 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %beginning_mask_2d.14 : Float(256:257, 257:1) = aten::flip(%3271, %3272), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3274 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.14, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3275 : Float(1:65792, 256:257, 257:1) = aten::slice(%3274, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3276 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%3275, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.14 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%3276, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3278 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %ending_mask.14 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.14, %3278), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:460:0
  %3280 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.20, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3281 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3280, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3282 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3281, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.14 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%3282, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3284 : int = aten::size(%beginning_input.14, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3285 : int = aten::size(%beginning_input.14, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3286 : int = aten::size(%beginning_input.14, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3287 : int = aten::size(%beginning_input.14, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3288 : int[] = prim::ListConstruct(%3284, %3285, %3286, %3287), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3289 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.14, %3288, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3290 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%3289, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3291 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.14, %3290, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:463:0
  %3292 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.20, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3293 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3292, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3294 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3293, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.14 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%3294, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3296 : int = aten::size(%ending_input.14, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3297 : int = aten::size(%ending_input.14, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3298 : int = aten::size(%ending_input.14, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3299 : int = aten::size(%ending_input.14, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3300 : int[] = prim::ListConstruct(%3296, %3297, %3298, %3299), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3301 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.14, %3300, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3302 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%3301, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3303 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.14, %3302, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.7 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.19, %input_tensor.20, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.7 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.7, %36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:1500:0
  %3306 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.7, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %3307 : Bool(17:512, 512:1) = aten::slice(%3306, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %3308 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%3307, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %3309 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%3308, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %input.82 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.7, %3309, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.14 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.82, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:973:0
  %3312 : int[] = prim::ListConstruct(%2971, %2972, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3313 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.7, %3312), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:331:0
  %value.7 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3313, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:331:0
  %3315 : int = aten::size(%value.7, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.28 : Long() = prim::NumToTensor(%3315), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3317 : int = aten::size(%value.7, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.29 : Long() = prim::NumToTensor(%3317), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3319 : int = aten::size(%value.7, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.21 : Long() = prim::NumToTensor(%3319), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3321 : int = aten::size(%value.7, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
  %3322 : Long() = aten::floor_divide(%seq_len.29, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %chunks_count.21 : Long() = aten::sub(%3322, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:542:0
  %3324 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.14, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:545:0
  %3325 : Long() = aten::mul(%batch_size.28, %num_heads.21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:546:0
  %3326 : int = aten::Int(%3325), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3327 : Long() = aten::floor_divide(%seq_len.29, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3328 : int = aten::Int(%3327), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3329 : int[] = prim::ListConstruct(%3326, %3328, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %chunked_hidden_states.31 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%3324, %3329), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:545:0
  %3331 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.7, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
  %3332 : Long() = aten::mul(%batch_size.28, %num_heads.21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
  %3333 : int = aten::Int(%3332), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3334 : int[] = prim::ListConstruct(%3333, %3317, %3321), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %input.83 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3331, %3334), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
  %3336 : int[] = prim::ListConstruct(%53, %53, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %padded_value.7 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.83, %3336, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
  %3338 : Long() = aten::mul(%batch_size.28, %num_heads.21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:556:0
  %3339 : int = aten::Int(%3338), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3340 : Long() = aten::add(%chunks_count.21, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:556:0
  %3341 : int = aten::Int(%3340), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3342 : int[] = prim::ListConstruct(%3339, %3341, %39, %3321), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3343 : int[] = prim::ListConstruct(%12, %11, %32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3344 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.7, %3342, %3343, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:564:0
  %3345 : int = aten::size(%chunked_hidden_states.31, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
  %3346 : int = aten::size(%chunked_hidden_states.31, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
  %3347 : int = aten::size(%chunked_hidden_states.31, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.7 : Long() = prim::NumToTensor(%3347), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3349 : int = aten::size(%chunked_hidden_states.31, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.7 : Long() = prim::NumToTensor(%3349), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3351 : Long() = aten::add(%window_overlap.7, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:422:0
  %3352 : int = aten::Int(%3351), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3353 : int[] = prim::ListConstruct(%53, %3352), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %chunked_hidden_states.32 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.31, %3353, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
  %3355 : int[] = prim::ListConstruct(%3345, %3346, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %chunked_hidden_states.33 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.32, %3355), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:424:0
  %3357 : Long() = aten::neg(%window_overlap.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:428:0
  %3358 : int = aten::Int(%3357), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3359 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.33, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
  %3360 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%3359, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.34 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%3360, %46, %53, %3358, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
  %3362 : Long() = aten::add(%window_overlap.7, %hidden_dim.7, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:431:0
  %3363 : int = aten::Int(%3362), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3364 : int[] = prim::ListConstruct(%3345, %3346, %3347, %3363), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %chunked_hidden_states.35 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.34, %3364), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:430:0
  %3366 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.35, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
  %3367 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%3366, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
  %3368 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%3367, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
  %3369 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%3368, %45, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
  %3370 : Tensor[] = prim::ListConstruct(%3369, %3344), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %context.7 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%10, %3370), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/functional.py:327:0
  %3372 : int[] = prim::ListConstruct(%3315, %3319, %3317, %3321), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3373 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.7, %3372), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.13 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%3373, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:569:0
  %3375 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.13, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
  %3376 : int[] = prim::ListConstruct(%2971, %2972, %2973), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3377 : Float(512:13056, 17:768, 768:1) = aten::reshape(%3375, %3376), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.14 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%3377, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
  %input.84 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.14, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:374:0
  %3380 : __torch__.torch.nn.modules.normalization.___torch_mangle_7693.LayerNorm = prim::GetAttr[name="LayerNorm"](%2947)
  %3381 : __torch__.torch.nn.modules.linear.___torch_mangle_7692.Linear = prim::GetAttr[name="dense"](%2947)
  %3382 : Tensor = prim::GetAttr[name="bias"](%3381)
  %3383 : Tensor = prim::GetAttr[name="weight"](%3381)
  %3384 : Float(768:1, 768:768) = aten::t(%3383), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.84, %3384), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.85 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.40, %3382, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.76 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.85, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.86 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.76, %hidden_states.67, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output # transformers/modeling_longformer.py:758:0
  %3389 : Tensor = prim::GetAttr[name="bias"](%3380)
  %3390 : Tensor = prim::GetAttr[name="weight"](%3380)
  %3391 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.21 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.86, %3391, %3390, %3389, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %3393 : __torch__.torch.nn.modules.linear.___torch_mangle_7697.Linear = prim::GetAttr[name="dense"](%2945)
  %3394 : Tensor = prim::GetAttr[name="bias"](%3393)
  %3395 : Tensor = prim::GetAttr[name="weight"](%3393)
  %3396 : Float(768:1, 3072:768) = aten::t(%3395), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.intermediate/__module.longformer.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.21, %3396), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.intermediate/__module.longformer.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.87 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.41, %3394, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.intermediate/__module.longformer.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.88 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.87), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %3400 : __torch__.torch.nn.modules.normalization.___torch_mangle_7700.LayerNorm = prim::GetAttr[name="LayerNorm"](%2944)
  %3401 : __torch__.torch.nn.modules.linear.___torch_mangle_7699.Linear = prim::GetAttr[name="dense"](%2944)
  %3402 : Tensor = prim::GetAttr[name="bias"](%3401)
  %3403 : Tensor = prim::GetAttr[name="weight"](%3401)
  %3404 : Float(3072:1, 768:3072) = aten::t(%3403), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.88, %3404), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.89 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.42, %3402, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.77 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.89, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.90 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.77, %input_tensor.21, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output # transformers/modeling_longformer.py:830:0
  %3409 : Tensor = prim::GetAttr[name="bias"](%3400)
  %3410 : Tensor = prim::GetAttr[name="weight"](%3400)
  %3411 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.LayerNorm
  %hidden_states.78 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.90, %3411, %3410, %3409, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %3413 : __torch__.transformers.modeling_longformer.___torch_mangle_7721.LongformerOutput = prim::GetAttr[name="output"](%115)
  %3414 : __torch__.transformers.modeling_longformer.___torch_mangle_7717.LongformerIntermediate = prim::GetAttr[name="intermediate"](%115)
  %3415 : __torch__.transformers.modeling_longformer.___torch_mangle_7715.LongformerAttention = prim::GetAttr[name="attention"](%115)
  %3416 : __torch__.transformers.modeling_longformer.___torch_mangle_7714.LongformerSelfOutput = prim::GetAttr[name="output"](%3415)
  %3417 : __torch__.transformers.modeling_longformer.___torch_mangle_7710.LongformerSelfAttention = prim::GetAttr[name="self"](%3415)
  %3418 : __torch__.torch.nn.modules.linear.___torch_mangle_7706.Linear = prim::GetAttr[name="value"](%3417)
  %3419 : __torch__.torch.nn.modules.linear.___torch_mangle_7705.Linear = prim::GetAttr[name="key"](%3417)
  %3420 : __torch__.torch.nn.modules.linear.___torch_mangle_7704.Linear = prim::GetAttr[name="query"](%3417)
  %3421 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:241:0
  %3422 : Float(17:512, 512:1) = aten::squeeze(%3421, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.8 : Bool(17:512, 512:1) = aten::lt(%3422, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %input.91 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.78, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:248:0
  %3425 : Tensor = prim::GetAttr[name="bias"](%3420)
  %3426 : Tensor = prim::GetAttr[name="weight"](%3420)
  %3427 : Float(768:1, 768:768) = aten::t(%3426), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.91, %3427), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.15 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.43, %3425, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %3430 : Tensor = prim::GetAttr[name="bias"](%3419)
  %3431 : Tensor = prim::GetAttr[name="weight"](%3419)
  %3432 : Float(768:1, 768:768) = aten::t(%3431), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.91, %3432), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.8 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.44, %3430, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %3435 : Tensor = prim::GetAttr[name="bias"](%3418)
  %3436 : Tensor = prim::GetAttr[name="weight"](%3418)
  %3437 : Float(768:1, 768:768) = aten::t(%3436), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.91, %3437), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.8 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.45, %3435, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %3440 : int = aten::size(%input.91, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
  %3441 : int = aten::size(%input.91, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
  %3442 : int = aten::size(%input.91, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.16 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.15, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:261:0
  %3444 : int[] = prim::ListConstruct(%3440, %3441, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3445 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.16, %3444), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:263:0
  %query.15 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3445, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:263:0
  %3447 : int[] = prim::ListConstruct(%3440, %3441, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3448 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.8, %3447), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:264:0
  %key.8 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3448, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:264:0
  %3450 : int = aten::size(%query.15, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.30 : Long() = prim::NumToTensor(%3450), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3452 : int = aten::size(%query.15, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.31 : Long() = prim::NumToTensor(%3452), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3454 : int = aten::size(%query.15, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.22 : Long() = prim::NumToTensor(%3454), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3456 : int = aten::size(%query.15, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %3457 : Long() = aten::floor_divide(%seq_len.31, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %chunks_count.22 : Long() = aten::sub(%3457, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:478:0
  %3459 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.15, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3460 : Long() = aten::mul(%batch_size.30, %num_heads.22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3461 : int = aten::Int(%3460), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3462 : int[] = prim::ListConstruct(%3461, %3452, %3456), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.79 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3459, %3462), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3464 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.8, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3465 : Long() = aten::mul(%batch_size.30, %num_heads.22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3466 : int = aten::Int(%3465), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3467 : int[] = prim::ListConstruct(%3466, %3452, %3456), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.81 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3464, %3467), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3469 : int = aten::size(%hidden_states.79, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
  %3470 : int = aten::size(%hidden_states.79, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
  %3471 : Long() = prim::NumToTensor(%3470), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3472 : Long() = aten::floor_divide(%3471, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3473 : int = aten::Int(%3472), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3474 : int = aten::size(%hidden_states.79, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
  %3475 : int[] = prim::ListConstruct(%3469, %3473, %48, %3474), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.80 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.79, %3475), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
  %3477 : int = aten::size(%hidden_states.80, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3478 : int = aten::size(%hidden_states.80, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3479 : Long() = prim::NumToTensor(%3478), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3480 : int = aten::size(%hidden_states.80, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3481 : int = aten::size(%hidden_states.80, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3482 : Long() = aten::mul(%3479, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3483 : Long() = aten::sub(%3482, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3484 : int = aten::Int(%3483), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3485 : int[] = prim::ListConstruct(%3477, %3484, %3480, %3481), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3486 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3487 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.80, %3485, %3486, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
  %3488 : int = aten::size(%hidden_states.81, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
  %3489 : int = aten::size(%hidden_states.81, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
  %3490 : Long() = prim::NumToTensor(%3489), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3491 : Long() = aten::floor_divide(%3490, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3492 : int = aten::Int(%3491), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3493 : int = aten::size(%hidden_states.81, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
  %3494 : int[] = prim::ListConstruct(%3488, %3492, %48, %3493), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.82 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.81, %3494), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
  %3496 : int = aten::size(%hidden_states.82, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3497 : int = aten::size(%hidden_states.82, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3498 : Long() = prim::NumToTensor(%3497), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3499 : int = aten::size(%hidden_states.82, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3500 : int = aten::size(%hidden_states.82, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3501 : Long() = aten::mul(%3498, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3502 : Long() = aten::sub(%3501, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3503 : int = aten::Int(%3502), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3504 : int[] = prim::ListConstruct(%3496, %3503, %3499, %3500), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3505 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3506 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.82, %3504, %3505, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
  %3507 : Tensor[] = prim::ListConstruct(%3487, %3506), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %input.92 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %3507), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/functional.py:327:0
  %3509 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states_padded.15 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.92, %3509, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
  %3511 : int = aten::size(%hidden_states_padded.15, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3512 : int = aten::size(%hidden_states_padded.15, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3513 : int = aten::size(%hidden_states_padded.15, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3514 : int = aten::size(%hidden_states_padded.15, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3515 : int[] = prim::ListConstruct(%3511, %3512, %3513, %3514), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %diagonal_chunked_attention_scores.15 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.15, %3515), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:400:0
  %3517 : Long() = aten::mul(%batch_size.30, %num_heads.22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
  %3518 : int = aten::Int(%3517), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3519 : Long() = aten::add(%chunks_count.22, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
  %3520 : int = aten::Int(%3519), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3521 : int[] = prim::ListConstruct(%3518, %3520, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %diagonal_attention_scores.15 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.15, %3521, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
  %3523 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3524 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3523, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3525 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3524, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3526 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%3525, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3527 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3528 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3527, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3529 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3528, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3530 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%3529, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3531 : int[] = prim::ListConstruct(%21, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3532 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%3526, %3531), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3533 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3530, %3532, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3534 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3535 : Float(204:262656, 512:513, 513:1) = aten::select(%3534, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3536 : Float(204:262656, 256:513, 513:1) = aten::slice(%3535, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3537 : Float(204:262656, 256:513, 257:1) = aten::slice(%3536, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3538 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3539 : Float(204:262656, 256:513, 513:1) = aten::select(%3538, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3540 : Float(204:262656, 256:513, 513:1) = aten::slice(%3539, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3541 : Float(204:262656, 256:513, 257:1) = aten::slice(%3540, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3542 : int[] = prim::ListConstruct(%21, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3543 : Float(204:262656, 256:513, 257:1) = aten::view(%3537, %3542), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3544 : Float(204:262656, 256:513, 257:1) = aten::copy_(%3541, %3543, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3545 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3546 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3545, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3547 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3546, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3548 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%3547, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3549 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3550 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3549, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3551 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3550, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3552 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%3551, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3553 : int[] = prim::ListConstruct(%21, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3554 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%3548, %3553), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3555 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%3552, %3554, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3556 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3557 : Float(204:262656, 512:513, 513:1) = aten::select(%3556, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3558 : Float(204:262656, 255:513, 513:1) = aten::slice(%3557, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3559 : Float(204:262656, 255:513, 255:1) = aten::slice(%3558, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3560 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3561 : Float(204:262656, 256:513, 513:1) = aten::select(%3560, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3562 : Float(204:262656, 255:513, 513:1) = aten::slice(%3561, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3563 : Float(204:262656, 255:513, 255:1) = aten::slice(%3562, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3564 : int[] = prim::ListConstruct(%21, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3565 : Float(204:262656, 255:513, 255:1) = aten::view(%3559, %3564), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3566 : Float(204:262656, 255:513, 255:1) = aten::copy_(%3563, %3565, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3567 : int[] = prim::ListConstruct(%3450, %3454, %3452, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3568 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.15, %3567), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.22 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%3568, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
  %3570 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3571 : Float(256:257, 257:1) = aten::ones(%3570, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3572 : Float(256:257, 257:1) = aten::tril(%3571, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3573 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %beginning_mask_2d.15 : Float(256:257, 257:1) = aten::flip(%3572, %3573), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3575 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.15, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3576 : Float(1:65792, 256:257, 257:1) = aten::slice(%3575, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3577 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%3576, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.15 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%3577, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3579 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %ending_mask.15 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.15, %3579), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:460:0
  %3581 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.22, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3582 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3581, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3583 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3582, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.15 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%3583, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3585 : int = aten::size(%beginning_input.15, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3586 : int = aten::size(%beginning_input.15, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3587 : int = aten::size(%beginning_input.15, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3588 : int = aten::size(%beginning_input.15, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3589 : int[] = prim::ListConstruct(%3585, %3586, %3587, %3588), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3590 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.15, %3589, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3591 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%3590, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3592 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.15, %3591, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:463:0
  %3593 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.22, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3594 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3593, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3595 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3594, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.15 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%3595, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3597 : int = aten::size(%ending_input.15, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3598 : int = aten::size(%ending_input.15, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3599 : int = aten::size(%ending_input.15, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3600 : int = aten::size(%ending_input.15, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3601 : int[] = prim::ListConstruct(%3597, %3598, %3599, %3600), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3602 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.15, %3601, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3603 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%3602, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3604 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.15, %3603, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:466:0
  %3605 : Bool(17:512, 512:1) = aten::ne(%3422, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3606 : Bool(17:512, 512:1) = aten::slice(%3605, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
  %3607 : Bool(17:512, 512:1) = aten::slice(%3606, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
  %3608 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%3607, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.8 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%3608, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
  %3610 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.8, %query.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.8 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%3610, %remove_from_windowed_attention_mask.8, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:275:0
  %3612 : int = aten::size(%float_mask.8, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3613 : int = aten::size(%float_mask.8, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3614 : int = aten::size(%float_mask.8, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3615 : int = aten::size(%float_mask.8, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3616 : int[] = prim::ListConstruct(%3612, %3613, %3614, %3615), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %query.16 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%3616, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3618 : int = aten::size(%query.16, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.31 : Long() = prim::NumToTensor(%3618), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3620 : int = aten::size(%query.16, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.32 : Long() = prim::NumToTensor(%3620), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3622 : int = aten::size(%query.16, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.23 : Long() = prim::NumToTensor(%3622), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3624 : int = aten::size(%query.16, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %3625 : Long() = aten::floor_divide(%seq_len.32, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %chunks_count.23 : Long() = aten::sub(%3625, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:478:0
  %3627 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.16, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3628 : Long() = aten::mul(%batch_size.31, %num_heads.23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3629 : int = aten::Int(%3628), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3630 : int[] = prim::ListConstruct(%3629, %3620, %3624), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.83 : Float(17:512, 512:1, 1:1) = aten::reshape(%3627, %3630), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3632 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.8, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3633 : Long() = aten::mul(%batch_size.31, %num_heads.23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3634 : int = aten::Int(%3633), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3635 : int[] = prim::ListConstruct(%3634, %3620, %3624), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.85 : Float(17:512, 512:1, 1:1) = aten::reshape(%3632, %3635), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3637 : int = aten::size(%hidden_states.83, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
  %3638 : int = aten::size(%hidden_states.83, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
  %3639 : Long() = prim::NumToTensor(%3638), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3640 : Long() = aten::floor_divide(%3639, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3641 : int = aten::Int(%3640), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3642 : int = aten::size(%hidden_states.83, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
  %3643 : int[] = prim::ListConstruct(%3637, %3641, %48, %3642), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.84 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.83, %3643), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
  %3645 : int = aten::size(%hidden_states.84, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3646 : int = aten::size(%hidden_states.84, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3647 : Long() = prim::NumToTensor(%3646), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3648 : int = aten::size(%hidden_states.84, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3649 : int = aten::size(%hidden_states.84, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3650 : Long() = aten::mul(%3647, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3651 : Long() = aten::sub(%3650, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3652 : int = aten::Int(%3651), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3653 : int[] = prim::ListConstruct(%3645, %3652, %3648, %3649), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3654 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3655 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.84, %3653, %3654, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
  %3656 : int = aten::size(%hidden_states.85, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
  %3657 : int = aten::size(%hidden_states.85, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
  %3658 : Long() = prim::NumToTensor(%3657), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3659 : Long() = aten::floor_divide(%3658, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3660 : int = aten::Int(%3659), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3661 : int = aten::size(%hidden_states.85, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
  %3662 : int[] = prim::ListConstruct(%3656, %3660, %48, %3661), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.86 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.85, %3662), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
  %3664 : int = aten::size(%hidden_states.86, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3665 : int = aten::size(%hidden_states.86, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3666 : Long() = prim::NumToTensor(%3665), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3667 : int = aten::size(%hidden_states.86, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3668 : int = aten::size(%hidden_states.86, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3669 : Long() = aten::mul(%3666, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3670 : Long() = aten::sub(%3669, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3671 : int = aten::Int(%3670), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3672 : int[] = prim::ListConstruct(%3664, %3671, %3667, %3668), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3673 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3674 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.86, %3672, %3673, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
  %3675 : Tensor[] = prim::ListConstruct(%3655, %3674), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %input.93 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %3675), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/functional.py:327:0
  %3677 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states_padded.16 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.93, %3677, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
  %3679 : int = aten::size(%hidden_states_padded.16, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3680 : int = aten::size(%hidden_states_padded.16, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3681 : int = aten::size(%hidden_states_padded.16, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3682 : int = aten::size(%hidden_states_padded.16, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3683 : int[] = prim::ListConstruct(%3679, %3680, %3681, %3682), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %diagonal_chunked_attention_scores.16 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.16, %3683), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:400:0
  %3685 : Long() = aten::mul(%batch_size.31, %num_heads.23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
  %3686 : int = aten::Int(%3685), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3687 : Long() = aten::add(%chunks_count.23, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
  %3688 : int = aten::Int(%3687), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3689 : int[] = prim::ListConstruct(%3686, %3688, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %diagonal_attention_scores.16 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.16, %3689, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
  %3691 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3692 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%3691, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3693 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%3692, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3694 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%3693, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3695 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3696 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3695, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3697 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3696, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3698 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%3697, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3699 : int[] = prim::ListConstruct(%14, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3700 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%3694, %3699), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3701 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3698, %3700, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3702 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3703 : Float(17:262656, 512:513, 513:1) = aten::select(%3702, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3704 : Float(17:262656, 256:513, 513:1) = aten::slice(%3703, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3705 : Float(17:262656, 256:513, 257:1) = aten::slice(%3704, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3706 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3707 : Float(17:262656, 256:513, 513:1) = aten::select(%3706, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3708 : Float(17:262656, 256:513, 513:1) = aten::slice(%3707, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3709 : Float(17:262656, 256:513, 257:1) = aten::slice(%3708, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3710 : int[] = prim::ListConstruct(%14, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3711 : Float(17:262656, 256:513, 257:1) = aten::view(%3705, %3710), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3712 : Float(17:262656, 256:513, 257:1) = aten::copy_(%3709, %3711, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3713 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3714 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%3713, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3715 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%3714, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3716 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%3715, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3717 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3718 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3717, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3719 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3718, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3720 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%3719, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3721 : int[] = prim::ListConstruct(%14, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3722 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%3716, %3721), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3723 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%3720, %3722, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3724 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3725 : Float(17:262656, 512:513, 513:1) = aten::select(%3724, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3726 : Float(17:262656, 255:513, 513:1) = aten::slice(%3725, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3727 : Float(17:262656, 255:513, 255:1) = aten::slice(%3726, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3728 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3729 : Float(17:262656, 256:513, 513:1) = aten::select(%3728, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3730 : Float(17:262656, 255:513, 513:1) = aten::slice(%3729, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3731 : Float(17:262656, 255:513, 255:1) = aten::slice(%3730, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3732 : int[] = prim::ListConstruct(%14, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3733 : Float(17:262656, 255:513, 255:1) = aten::view(%3727, %3732), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3734 : Float(17:262656, 255:513, 255:1) = aten::copy_(%3731, %3733, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3735 : int[] = prim::ListConstruct(%3618, %3622, %3620, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3736 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.16, %3735), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.23 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%3736, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
  %3738 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3739 : Float(256:257, 257:1) = aten::ones(%3738, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3740 : Float(256:257, 257:1) = aten::tril(%3739, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3741 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %beginning_mask_2d.16 : Float(256:257, 257:1) = aten::flip(%3740, %3741), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3743 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.16, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3744 : Float(1:65792, 256:257, 257:1) = aten::slice(%3743, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3745 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%3744, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.16 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%3745, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3747 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %ending_mask.16 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.16, %3747), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:460:0
  %3749 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.23, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3750 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3749, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3751 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3750, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.16 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%3751, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3753 : int = aten::size(%beginning_input.16, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3754 : int = aten::size(%beginning_input.16, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3755 : int = aten::size(%beginning_input.16, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3756 : int = aten::size(%beginning_input.16, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3757 : int[] = prim::ListConstruct(%3753, %3754, %3755, %3756), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3758 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.16, %3757, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3759 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%3758, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3760 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.16, %3759, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:463:0
  %3761 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.23, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3762 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3761, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3763 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3762, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.16 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%3763, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3765 : int = aten::size(%ending_input.16, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3766 : int = aten::size(%ending_input.16, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3767 : int = aten::size(%ending_input.16, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3768 : int = aten::size(%ending_input.16, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3769 : int[] = prim::ListConstruct(%3765, %3766, %3767, %3768), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3770 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.16, %3769, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3771 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%3770, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3772 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.16, %3771, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.8 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.22, %input_tensor.23, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.8 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.8, %36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:1500:0
  %3775 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.8, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %3776 : Bool(17:512, 512:1) = aten::slice(%3775, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %3777 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%3776, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %3778 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%3777, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %input.94 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.8, %3778, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.16 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.94, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:973:0
  %3781 : int[] = prim::ListConstruct(%3440, %3441, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3782 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.8, %3781), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:331:0
  %value.8 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3782, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:331:0
  %3784 : int = aten::size(%value.8, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.32 : Long() = prim::NumToTensor(%3784), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3786 : int = aten::size(%value.8, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.33 : Long() = prim::NumToTensor(%3786), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3788 : int = aten::size(%value.8, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.24 : Long() = prim::NumToTensor(%3788), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3790 : int = aten::size(%value.8, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
  %3791 : Long() = aten::floor_divide(%seq_len.33, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %chunks_count.24 : Long() = aten::sub(%3791, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:542:0
  %3793 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.16, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:545:0
  %3794 : Long() = aten::mul(%batch_size.32, %num_heads.24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:546:0
  %3795 : int = aten::Int(%3794), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3796 : Long() = aten::floor_divide(%seq_len.33, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3797 : int = aten::Int(%3796), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3798 : int[] = prim::ListConstruct(%3795, %3797, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %chunked_hidden_states.36 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%3793, %3798), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:545:0
  %3800 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.8, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
  %3801 : Long() = aten::mul(%batch_size.32, %num_heads.24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
  %3802 : int = aten::Int(%3801), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3803 : int[] = prim::ListConstruct(%3802, %3786, %3790), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %input.95 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3800, %3803), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
  %3805 : int[] = prim::ListConstruct(%53, %53, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %padded_value.8 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.95, %3805, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
  %3807 : Long() = aten::mul(%batch_size.32, %num_heads.24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:556:0
  %3808 : int = aten::Int(%3807), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3809 : Long() = aten::add(%chunks_count.24, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:556:0
  %3810 : int = aten::Int(%3809), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3811 : int[] = prim::ListConstruct(%3808, %3810, %39, %3790), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3812 : int[] = prim::ListConstruct(%12, %11, %32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3813 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.8, %3811, %3812, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:564:0
  %3814 : int = aten::size(%chunked_hidden_states.36, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
  %3815 : int = aten::size(%chunked_hidden_states.36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
  %3816 : int = aten::size(%chunked_hidden_states.36, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.8 : Long() = prim::NumToTensor(%3816), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3818 : int = aten::size(%chunked_hidden_states.36, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.8 : Long() = prim::NumToTensor(%3818), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3820 : Long() = aten::add(%window_overlap.8, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:422:0
  %3821 : int = aten::Int(%3820), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3822 : int[] = prim::ListConstruct(%53, %3821), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %chunked_hidden_states.37 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.36, %3822, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
  %3824 : int[] = prim::ListConstruct(%3814, %3815, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %chunked_hidden_states.38 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.37, %3824), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:424:0
  %3826 : Long() = aten::neg(%window_overlap.8), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:428:0
  %3827 : int = aten::Int(%3826), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3828 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.38, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
  %3829 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%3828, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.39 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%3829, %46, %53, %3827, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
  %3831 : Long() = aten::add(%window_overlap.8, %hidden_dim.8, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:431:0
  %3832 : int = aten::Int(%3831), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3833 : int[] = prim::ListConstruct(%3814, %3815, %3816, %3832), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %chunked_hidden_states.40 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.39, %3833), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:430:0
  %3835 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.40, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
  %3836 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%3835, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
  %3837 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%3836, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
  %3838 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%3837, %45, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
  %3839 : Tensor[] = prim::ListConstruct(%3838, %3813), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %context.8 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%10, %3839), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/functional.py:327:0
  %3841 : int[] = prim::ListConstruct(%3784, %3788, %3786, %3790), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3842 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.8, %3841), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.15 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%3842, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:569:0
  %3844 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.15, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
  %3845 : int[] = prim::ListConstruct(%3440, %3441, %3442), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3846 : Float(512:13056, 17:768, 768:1) = aten::reshape(%3844, %3845), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.16 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%3846, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
  %input.96 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.16, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:374:0
  %3849 : __torch__.torch.nn.modules.normalization.___torch_mangle_7712.LayerNorm = prim::GetAttr[name="LayerNorm"](%3416)
  %3850 : __torch__.torch.nn.modules.linear.___torch_mangle_7711.Linear = prim::GetAttr[name="dense"](%3416)
  %3851 : Tensor = prim::GetAttr[name="bias"](%3850)
  %3852 : Tensor = prim::GetAttr[name="weight"](%3850)
  %3853 : Float(768:1, 768:768) = aten::t(%3852), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.96, %3853), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.97 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.46, %3851, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.87 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.97, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.98 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.87, %hidden_states.78, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output # transformers/modeling_longformer.py:758:0
  %3858 : Tensor = prim::GetAttr[name="bias"](%3849)
  %3859 : Tensor = prim::GetAttr[name="weight"](%3849)
  %3860 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.24 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.98, %3860, %3859, %3858, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %3862 : __torch__.torch.nn.modules.linear.___torch_mangle_7716.Linear = prim::GetAttr[name="dense"](%3414)
  %3863 : Tensor = prim::GetAttr[name="bias"](%3862)
  %3864 : Tensor = prim::GetAttr[name="weight"](%3862)
  %3865 : Float(768:1, 3072:768) = aten::t(%3864), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.intermediate/__module.longformer.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.24, %3865), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.intermediate/__module.longformer.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.99 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.47, %3863, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.intermediate/__module.longformer.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.100 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.99), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %3869 : __torch__.torch.nn.modules.normalization.___torch_mangle_7719.LayerNorm = prim::GetAttr[name="LayerNorm"](%3413)
  %3870 : __torch__.torch.nn.modules.linear.___torch_mangle_7718.Linear = prim::GetAttr[name="dense"](%3413)
  %3871 : Tensor = prim::GetAttr[name="bias"](%3870)
  %3872 : Tensor = prim::GetAttr[name="weight"](%3870)
  %3873 : Float(3072:1, 768:3072) = aten::t(%3872), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.100, %3873), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.101 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.48, %3871, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.88 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.101, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.102 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.88, %input_tensor.24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output # transformers/modeling_longformer.py:830:0
  %3878 : Tensor = prim::GetAttr[name="bias"](%3869)
  %3879 : Tensor = prim::GetAttr[name="weight"](%3869)
  %3880 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.LayerNorm
  %hidden_states.89 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.102, %3880, %3879, %3878, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %3882 : __torch__.transformers.modeling_longformer.___torch_mangle_7740.LongformerOutput = prim::GetAttr[name="output"](%113)
  %3883 : __torch__.transformers.modeling_longformer.___torch_mangle_7736.LongformerIntermediate = prim::GetAttr[name="intermediate"](%113)
  %3884 : __torch__.transformers.modeling_longformer.___torch_mangle_7734.LongformerAttention = prim::GetAttr[name="attention"](%113)
  %3885 : __torch__.transformers.modeling_longformer.___torch_mangle_7733.LongformerSelfOutput = prim::GetAttr[name="output"](%3884)
  %3886 : __torch__.transformers.modeling_longformer.___torch_mangle_7729.LongformerSelfAttention = prim::GetAttr[name="self"](%3884)
  %3887 : __torch__.torch.nn.modules.linear.___torch_mangle_7725.Linear = prim::GetAttr[name="value"](%3886)
  %3888 : __torch__.torch.nn.modules.linear.___torch_mangle_7724.Linear = prim::GetAttr[name="key"](%3886)
  %3889 : __torch__.torch.nn.modules.linear.___torch_mangle_7723.Linear = prim::GetAttr[name="query"](%3886)
  %3890 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:241:0
  %3891 : Float(17:512, 512:1) = aten::squeeze(%3890, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.9 : Bool(17:512, 512:1) = aten::lt(%3891, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %input.103 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.89, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:248:0
  %3894 : Tensor = prim::GetAttr[name="bias"](%3889)
  %3895 : Tensor = prim::GetAttr[name="weight"](%3889)
  %3896 : Float(768:1, 768:768) = aten::t(%3895), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.103, %3896), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.17 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.49, %3894, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %3899 : Tensor = prim::GetAttr[name="bias"](%3888)
  %3900 : Tensor = prim::GetAttr[name="weight"](%3888)
  %3901 : Float(768:1, 768:768) = aten::t(%3900), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.103, %3901), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.9 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.50, %3899, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %3904 : Tensor = prim::GetAttr[name="bias"](%3887)
  %3905 : Tensor = prim::GetAttr[name="weight"](%3887)
  %3906 : Float(768:1, 768:768) = aten::t(%3905), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.103, %3906), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.9 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.51, %3904, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %3909 : int = aten::size(%input.103, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
  %3910 : int = aten::size(%input.103, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
  %3911 : int = aten::size(%input.103, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.18 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.17, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:261:0
  %3913 : int[] = prim::ListConstruct(%3909, %3910, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3914 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.18, %3913), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:263:0
  %query.17 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3914, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:263:0
  %3916 : int[] = prim::ListConstruct(%3909, %3910, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3917 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.9, %3916), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:264:0
  %key.9 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3917, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:264:0
  %3919 : int = aten::size(%query.17, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.34 : Long() = prim::NumToTensor(%3919), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3921 : int = aten::size(%query.17, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.35 : Long() = prim::NumToTensor(%3921), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3923 : int = aten::size(%query.17, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.25 : Long() = prim::NumToTensor(%3923), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3925 : int = aten::size(%query.17, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %3926 : Long() = aten::floor_divide(%seq_len.35, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %chunks_count.25 : Long() = aten::sub(%3926, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:478:0
  %3928 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.17, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %3929 : Long() = aten::mul(%batch_size.34, %num_heads.25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %3930 : int = aten::Int(%3929), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3931 : int[] = prim::ListConstruct(%3930, %3921, %3925), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.90 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3928, %3931), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %3933 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.9, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %3934 : Long() = aten::mul(%batch_size.34, %num_heads.25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %3935 : int = aten::Int(%3934), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3936 : int[] = prim::ListConstruct(%3935, %3921, %3925), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.92 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3933, %3936), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %3938 : int = aten::size(%hidden_states.90, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
  %3939 : int = aten::size(%hidden_states.90, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
  %3940 : Long() = prim::NumToTensor(%3939), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3941 : Long() = aten::floor_divide(%3940, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %3942 : int = aten::Int(%3941), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3943 : int = aten::size(%hidden_states.90, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
  %3944 : int[] = prim::ListConstruct(%3938, %3942, %48, %3943), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.91 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.90, %3944), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
  %3946 : int = aten::size(%hidden_states.91, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3947 : int = aten::size(%hidden_states.91, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3948 : Long() = prim::NumToTensor(%3947), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3949 : int = aten::size(%hidden_states.91, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3950 : int = aten::size(%hidden_states.91, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3951 : Long() = aten::mul(%3948, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %3952 : Long() = aten::sub(%3951, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %3953 : int = aten::Int(%3952), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3954 : int[] = prim::ListConstruct(%3946, %3953, %3949, %3950), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3955 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3956 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.91, %3954, %3955, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
  %3957 : int = aten::size(%hidden_states.92, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
  %3958 : int = aten::size(%hidden_states.92, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
  %3959 : Long() = prim::NumToTensor(%3958), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3960 : Long() = aten::floor_divide(%3959, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %3961 : int = aten::Int(%3960), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3962 : int = aten::size(%hidden_states.92, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
  %3963 : int[] = prim::ListConstruct(%3957, %3961, %48, %3962), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.93 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.92, %3963), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
  %3965 : int = aten::size(%hidden_states.93, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3966 : int = aten::size(%hidden_states.93, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3967 : Long() = prim::NumToTensor(%3966), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3968 : int = aten::size(%hidden_states.93, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3969 : int = aten::size(%hidden_states.93, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3970 : Long() = aten::mul(%3967, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %3971 : Long() = aten::sub(%3970, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %3972 : int = aten::Int(%3971), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3973 : int[] = prim::ListConstruct(%3965, %3972, %3968, %3969), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3974 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3975 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.93, %3973, %3974, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
  %3976 : Tensor[] = prim::ListConstruct(%3956, %3975), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %input.104 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %3976), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/functional.py:327:0
  %3978 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states_padded.17 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.104, %3978, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
  %3980 : int = aten::size(%hidden_states_padded.17, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %3981 : int = aten::size(%hidden_states_padded.17, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %3982 : int = aten::size(%hidden_states_padded.17, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %3983 : int = aten::size(%hidden_states_padded.17, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %3984 : int[] = prim::ListConstruct(%3980, %3981, %3982, %3983), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %diagonal_chunked_attention_scores.17 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.17, %3984), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:400:0
  %3986 : Long() = aten::mul(%batch_size.34, %num_heads.25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
  %3987 : int = aten::Int(%3986), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3988 : Long() = aten::add(%chunks_count.25, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
  %3989 : int = aten::Int(%3988), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3990 : int[] = prim::ListConstruct(%3987, %3989, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %diagonal_attention_scores.17 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.17, %3990, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
  %3992 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3993 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3992, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3994 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3993, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3995 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%3994, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3996 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3997 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3996, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3998 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3997, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %3999 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%3998, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4000 : int[] = prim::ListConstruct(%21, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4001 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%3995, %4000), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4002 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3999, %4001, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4003 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4004 : Float(204:262656, 512:513, 513:1) = aten::select(%4003, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4005 : Float(204:262656, 256:513, 513:1) = aten::slice(%4004, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4006 : Float(204:262656, 256:513, 257:1) = aten::slice(%4005, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4007 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4008 : Float(204:262656, 256:513, 513:1) = aten::select(%4007, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4009 : Float(204:262656, 256:513, 513:1) = aten::slice(%4008, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4010 : Float(204:262656, 256:513, 257:1) = aten::slice(%4009, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4011 : int[] = prim::ListConstruct(%21, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4012 : Float(204:262656, 256:513, 257:1) = aten::view(%4006, %4011), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4013 : Float(204:262656, 256:513, 257:1) = aten::copy_(%4010, %4012, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4014 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4015 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4014, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4016 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4015, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4017 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%4016, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4018 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4019 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4018, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4020 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4019, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4021 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%4020, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4022 : int[] = prim::ListConstruct(%21, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4023 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%4017, %4022), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4024 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4021, %4023, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4025 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4026 : Float(204:262656, 512:513, 513:1) = aten::select(%4025, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4027 : Float(204:262656, 255:513, 513:1) = aten::slice(%4026, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4028 : Float(204:262656, 255:513, 255:1) = aten::slice(%4027, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4029 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4030 : Float(204:262656, 256:513, 513:1) = aten::select(%4029, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4031 : Float(204:262656, 255:513, 513:1) = aten::slice(%4030, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4032 : Float(204:262656, 255:513, 255:1) = aten::slice(%4031, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4033 : int[] = prim::ListConstruct(%21, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4034 : Float(204:262656, 255:513, 255:1) = aten::view(%4028, %4033), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4035 : Float(204:262656, 255:513, 255:1) = aten::copy_(%4032, %4034, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4036 : int[] = prim::ListConstruct(%3919, %3923, %3921, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4037 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.17, %4036), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.25 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%4037, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
  %4039 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4040 : Float(256:257, 257:1) = aten::ones(%4039, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4041 : Float(256:257, 257:1) = aten::tril(%4040, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4042 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %beginning_mask_2d.17 : Float(256:257, 257:1) = aten::flip(%4041, %4042), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4044 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.17, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4045 : Float(1:65792, 256:257, 257:1) = aten::slice(%4044, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4046 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4045, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.17 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4046, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4048 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %ending_mask.17 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.17, %4048), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:460:0
  %4050 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.25, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4051 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4050, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4052 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4051, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.17 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4052, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4054 : int = aten::size(%beginning_input.17, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4055 : int = aten::size(%beginning_input.17, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4056 : int = aten::size(%beginning_input.17, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4057 : int = aten::size(%beginning_input.17, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4058 : int[] = prim::ListConstruct(%4054, %4055, %4056, %4057), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4059 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.17, %4058, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4060 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4059, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4061 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.17, %4060, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:463:0
  %4062 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.25, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4063 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4062, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4064 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4063, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.17 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4064, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4066 : int = aten::size(%ending_input.17, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4067 : int = aten::size(%ending_input.17, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4068 : int = aten::size(%ending_input.17, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4069 : int = aten::size(%ending_input.17, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4070 : int[] = prim::ListConstruct(%4066, %4067, %4068, %4069), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4071 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.17, %4070, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4072 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4071, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4073 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.17, %4072, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:466:0
  %4074 : Bool(17:512, 512:1) = aten::ne(%3891, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4075 : Bool(17:512, 512:1) = aten::slice(%4074, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
  %4076 : Bool(17:512, 512:1) = aten::slice(%4075, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
  %4077 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%4076, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.9 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%4077, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
  %4079 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.9, %query.17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.9 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%4079, %remove_from_windowed_attention_mask.9, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:275:0
  %4081 : int = aten::size(%float_mask.9, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4082 : int = aten::size(%float_mask.9, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4083 : int = aten::size(%float_mask.9, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4084 : int = aten::size(%float_mask.9, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4085 : int[] = prim::ListConstruct(%4081, %4082, %4083, %4084), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %query.18 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%4085, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4087 : int = aten::size(%query.18, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.35 : Long() = prim::NumToTensor(%4087), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4089 : int = aten::size(%query.18, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.36 : Long() = prim::NumToTensor(%4089), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4091 : int = aten::size(%query.18, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.26 : Long() = prim::NumToTensor(%4091), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4093 : int = aten::size(%query.18, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %4094 : Long() = aten::floor_divide(%seq_len.36, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %chunks_count.26 : Long() = aten::sub(%4094, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:478:0
  %4096 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.18, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %4097 : Long() = aten::mul(%batch_size.35, %num_heads.26), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %4098 : int = aten::Int(%4097), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4099 : int[] = prim::ListConstruct(%4098, %4089, %4093), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.94 : Float(17:512, 512:1, 1:1) = aten::reshape(%4096, %4099), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %4101 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.9, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %4102 : Long() = aten::mul(%batch_size.35, %num_heads.26), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %4103 : int = aten::Int(%4102), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4104 : int[] = prim::ListConstruct(%4103, %4089, %4093), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.96 : Float(17:512, 512:1, 1:1) = aten::reshape(%4101, %4104), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %4106 : int = aten::size(%hidden_states.94, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
  %4107 : int = aten::size(%hidden_states.94, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
  %4108 : Long() = prim::NumToTensor(%4107), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4109 : Long() = aten::floor_divide(%4108, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %4110 : int = aten::Int(%4109), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4111 : int = aten::size(%hidden_states.94, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
  %4112 : int[] = prim::ListConstruct(%4106, %4110, %48, %4111), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.95 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.94, %4112), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
  %4114 : int = aten::size(%hidden_states.95, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4115 : int = aten::size(%hidden_states.95, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4116 : Long() = prim::NumToTensor(%4115), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4117 : int = aten::size(%hidden_states.95, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4118 : int = aten::size(%hidden_states.95, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4119 : Long() = aten::mul(%4116, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %4120 : Long() = aten::sub(%4119, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %4121 : int = aten::Int(%4120), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4122 : int[] = prim::ListConstruct(%4114, %4121, %4117, %4118), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4123 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4124 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.95, %4122, %4123, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
  %4125 : int = aten::size(%hidden_states.96, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
  %4126 : int = aten::size(%hidden_states.96, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
  %4127 : Long() = prim::NumToTensor(%4126), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4128 : Long() = aten::floor_divide(%4127, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %4129 : int = aten::Int(%4128), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4130 : int = aten::size(%hidden_states.96, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
  %4131 : int[] = prim::ListConstruct(%4125, %4129, %48, %4130), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.97 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.96, %4131), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
  %4133 : int = aten::size(%hidden_states.97, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4134 : int = aten::size(%hidden_states.97, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4135 : Long() = prim::NumToTensor(%4134), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4136 : int = aten::size(%hidden_states.97, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4137 : int = aten::size(%hidden_states.97, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4138 : Long() = aten::mul(%4135, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %4139 : Long() = aten::sub(%4138, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %4140 : int = aten::Int(%4139), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4141 : int[] = prim::ListConstruct(%4133, %4140, %4136, %4137), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4142 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4143 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.97, %4141, %4142, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
  %4144 : Tensor[] = prim::ListConstruct(%4124, %4143), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %input.105 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %4144), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/functional.py:327:0
  %4146 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states_padded.18 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.105, %4146, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
  %4148 : int = aten::size(%hidden_states_padded.18, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4149 : int = aten::size(%hidden_states_padded.18, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4150 : int = aten::size(%hidden_states_padded.18, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4151 : int = aten::size(%hidden_states_padded.18, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4152 : int[] = prim::ListConstruct(%4148, %4149, %4150, %4151), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %diagonal_chunked_attention_scores.18 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.18, %4152), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:400:0
  %4154 : Long() = aten::mul(%batch_size.35, %num_heads.26), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
  %4155 : int = aten::Int(%4154), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4156 : Long() = aten::add(%chunks_count.26, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
  %4157 : int = aten::Int(%4156), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4158 : int[] = prim::ListConstruct(%4155, %4157, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %diagonal_attention_scores.18 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.18, %4158, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
  %4160 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4161 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%4160, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4162 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%4161, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4163 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%4162, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4164 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4165 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4164, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4166 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4165, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4167 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%4166, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4168 : int[] = prim::ListConstruct(%14, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4169 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%4163, %4168), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4170 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4167, %4169, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4171 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4172 : Float(17:262656, 512:513, 513:1) = aten::select(%4171, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4173 : Float(17:262656, 256:513, 513:1) = aten::slice(%4172, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4174 : Float(17:262656, 256:513, 257:1) = aten::slice(%4173, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4175 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4176 : Float(17:262656, 256:513, 513:1) = aten::select(%4175, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4177 : Float(17:262656, 256:513, 513:1) = aten::slice(%4176, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4178 : Float(17:262656, 256:513, 257:1) = aten::slice(%4177, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4179 : int[] = prim::ListConstruct(%14, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4180 : Float(17:262656, 256:513, 257:1) = aten::view(%4174, %4179), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4181 : Float(17:262656, 256:513, 257:1) = aten::copy_(%4178, %4180, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4182 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4183 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%4182, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4184 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%4183, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4185 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%4184, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4186 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4187 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4186, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4188 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4187, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4189 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%4188, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4190 : int[] = prim::ListConstruct(%14, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4191 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%4185, %4190), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4192 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4189, %4191, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4193 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4194 : Float(17:262656, 512:513, 513:1) = aten::select(%4193, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4195 : Float(17:262656, 255:513, 513:1) = aten::slice(%4194, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4196 : Float(17:262656, 255:513, 255:1) = aten::slice(%4195, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4197 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4198 : Float(17:262656, 256:513, 513:1) = aten::select(%4197, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4199 : Float(17:262656, 255:513, 513:1) = aten::slice(%4198, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4200 : Float(17:262656, 255:513, 255:1) = aten::slice(%4199, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4201 : int[] = prim::ListConstruct(%14, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4202 : Float(17:262656, 255:513, 255:1) = aten::view(%4196, %4201), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4203 : Float(17:262656, 255:513, 255:1) = aten::copy_(%4200, %4202, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4204 : int[] = prim::ListConstruct(%4087, %4091, %4089, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4205 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.18, %4204), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.26 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%4205, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
  %4207 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4208 : Float(256:257, 257:1) = aten::ones(%4207, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4209 : Float(256:257, 257:1) = aten::tril(%4208, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4210 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %beginning_mask_2d.18 : Float(256:257, 257:1) = aten::flip(%4209, %4210), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4212 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.18, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4213 : Float(1:65792, 256:257, 257:1) = aten::slice(%4212, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4214 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4213, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.18 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4214, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4216 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %ending_mask.18 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.18, %4216), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:460:0
  %4218 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.26, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4219 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4218, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4220 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4219, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.18 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%4220, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4222 : int = aten::size(%beginning_input.18, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4223 : int = aten::size(%beginning_input.18, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4224 : int = aten::size(%beginning_input.18, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4225 : int = aten::size(%beginning_input.18, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4226 : int[] = prim::ListConstruct(%4222, %4223, %4224, %4225), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4227 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.18, %4226, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4228 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%4227, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4229 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.18, %4228, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:463:0
  %4230 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.26, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4231 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4230, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4232 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4231, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.18 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%4232, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4234 : int = aten::size(%ending_input.18, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4235 : int = aten::size(%ending_input.18, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4236 : int = aten::size(%ending_input.18, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4237 : int = aten::size(%ending_input.18, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4238 : int[] = prim::ListConstruct(%4234, %4235, %4236, %4237), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4239 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.18, %4238, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4240 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%4239, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4241 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.18, %4240, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.9 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.25, %input_tensor.26, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.9 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.9, %36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:1500:0
  %4244 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.9, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %4245 : Bool(17:512, 512:1) = aten::slice(%4244, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %4246 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%4245, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %4247 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%4246, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %input.106 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.9, %4247, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.18 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.106, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:973:0
  %4250 : int[] = prim::ListConstruct(%3909, %3910, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4251 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.9, %4250), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:331:0
  %value.9 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4251, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:331:0
  %4253 : int = aten::size(%value.9, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.36 : Long() = prim::NumToTensor(%4253), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4255 : int = aten::size(%value.9, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.37 : Long() = prim::NumToTensor(%4255), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4257 : int = aten::size(%value.9, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.27 : Long() = prim::NumToTensor(%4257), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4259 : int = aten::size(%value.9, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
  %4260 : Long() = aten::floor_divide(%seq_len.37, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %chunks_count.27 : Long() = aten::sub(%4260, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:542:0
  %4262 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.18, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:545:0
  %4263 : Long() = aten::mul(%batch_size.36, %num_heads.27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:546:0
  %4264 : int = aten::Int(%4263), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4265 : Long() = aten::floor_divide(%seq_len.37, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %4266 : int = aten::Int(%4265), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4267 : int[] = prim::ListConstruct(%4264, %4266, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %chunked_hidden_states.41 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%4262, %4267), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:545:0
  %4269 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.9, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
  %4270 : Long() = aten::mul(%batch_size.36, %num_heads.27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
  %4271 : int = aten::Int(%4270), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4272 : int[] = prim::ListConstruct(%4271, %4255, %4259), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %input.107 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4269, %4272), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
  %4274 : int[] = prim::ListConstruct(%53, %53, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %padded_value.9 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.107, %4274, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
  %4276 : Long() = aten::mul(%batch_size.36, %num_heads.27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:556:0
  %4277 : int = aten::Int(%4276), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4278 : Long() = aten::add(%chunks_count.27, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:556:0
  %4279 : int = aten::Int(%4278), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4280 : int[] = prim::ListConstruct(%4277, %4279, %39, %4259), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4281 : int[] = prim::ListConstruct(%12, %11, %32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4282 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.9, %4280, %4281, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:564:0
  %4283 : int = aten::size(%chunked_hidden_states.41, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
  %4284 : int = aten::size(%chunked_hidden_states.41, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
  %4285 : int = aten::size(%chunked_hidden_states.41, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.9 : Long() = prim::NumToTensor(%4285), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4287 : int = aten::size(%chunked_hidden_states.41, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.9 : Long() = prim::NumToTensor(%4287), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4289 : Long() = aten::add(%window_overlap.9, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:422:0
  %4290 : int = aten::Int(%4289), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4291 : int[] = prim::ListConstruct(%53, %4290), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %chunked_hidden_states.42 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.41, %4291, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
  %4293 : int[] = prim::ListConstruct(%4283, %4284, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %chunked_hidden_states.43 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.42, %4293), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:424:0
  %4295 : Long() = aten::neg(%window_overlap.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:428:0
  %4296 : int = aten::Int(%4295), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4297 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.43, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
  %4298 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%4297, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.44 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%4298, %46, %53, %4296, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
  %4300 : Long() = aten::add(%window_overlap.9, %hidden_dim.9, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:431:0
  %4301 : int = aten::Int(%4300), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4302 : int[] = prim::ListConstruct(%4283, %4284, %4285, %4301), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %chunked_hidden_states.45 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.44, %4302), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:430:0
  %4304 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.45, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
  %4305 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%4304, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
  %4306 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%4305, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
  %4307 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%4306, %45, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
  %4308 : Tensor[] = prim::ListConstruct(%4307, %4282), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %context.9 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%10, %4308), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/functional.py:327:0
  %4310 : int[] = prim::ListConstruct(%4253, %4257, %4255, %4259), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4311 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.9, %4310), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.17 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%4311, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:569:0
  %4313 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.17, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
  %4314 : int[] = prim::ListConstruct(%3909, %3910, %3911), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4315 : Float(512:13056, 17:768, 768:1) = aten::reshape(%4313, %4314), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.18 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%4315, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
  %input.108 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.18, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:374:0
  %4318 : __torch__.torch.nn.modules.normalization.___torch_mangle_7731.LayerNorm = prim::GetAttr[name="LayerNorm"](%3885)
  %4319 : __torch__.torch.nn.modules.linear.___torch_mangle_7730.Linear = prim::GetAttr[name="dense"](%3885)
  %4320 : Tensor = prim::GetAttr[name="bias"](%4319)
  %4321 : Tensor = prim::GetAttr[name="weight"](%4319)
  %4322 : Float(768:1, 768:768) = aten::t(%4321), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.108, %4322), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.109 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.52, %4320, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.98 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.109, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.98, %hidden_states.89, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output # transformers/modeling_longformer.py:758:0
  %4327 : Tensor = prim::GetAttr[name="bias"](%4318)
  %4328 : Tensor = prim::GetAttr[name="weight"](%4318)
  %4329 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.27 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.110, %4329, %4328, %4327, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %4331 : __torch__.torch.nn.modules.linear.___torch_mangle_7735.Linear = prim::GetAttr[name="dense"](%3883)
  %4332 : Tensor = prim::GetAttr[name="bias"](%4331)
  %4333 : Tensor = prim::GetAttr[name="weight"](%4331)
  %4334 : Float(768:1, 3072:768) = aten::t(%4333), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.intermediate/__module.longformer.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.27, %4334), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.intermediate/__module.longformer.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.111 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.53, %4332, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.intermediate/__module.longformer.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.112 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.111), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %4338 : __torch__.torch.nn.modules.normalization.___torch_mangle_7738.LayerNorm = prim::GetAttr[name="LayerNorm"](%3882)
  %4339 : __torch__.torch.nn.modules.linear.___torch_mangle_7737.Linear = prim::GetAttr[name="dense"](%3882)
  %4340 : Tensor = prim::GetAttr[name="bias"](%4339)
  %4341 : Tensor = prim::GetAttr[name="weight"](%4339)
  %4342 : Float(3072:1, 768:3072) = aten::t(%4341), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.112, %4342), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.113 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.54, %4340, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.99 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.113, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.99, %input_tensor.27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output # transformers/modeling_longformer.py:830:0
  %4347 : Tensor = prim::GetAttr[name="bias"](%4338)
  %4348 : Tensor = prim::GetAttr[name="weight"](%4338)
  %4349 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.LayerNorm
  %hidden_states.100 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.114, %4349, %4348, %4347, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %4351 : __torch__.transformers.modeling_longformer.___torch_mangle_7759.LongformerOutput = prim::GetAttr[name="output"](%111)
  %4352 : __torch__.transformers.modeling_longformer.___torch_mangle_7755.LongformerIntermediate = prim::GetAttr[name="intermediate"](%111)
  %4353 : __torch__.transformers.modeling_longformer.___torch_mangle_7753.LongformerAttention = prim::GetAttr[name="attention"](%111)
  %4354 : __torch__.transformers.modeling_longformer.___torch_mangle_7752.LongformerSelfOutput = prim::GetAttr[name="output"](%4353)
  %4355 : __torch__.transformers.modeling_longformer.___torch_mangle_7748.LongformerSelfAttention = prim::GetAttr[name="self"](%4353)
  %4356 : __torch__.torch.nn.modules.linear.___torch_mangle_7744.Linear = prim::GetAttr[name="value"](%4355)
  %4357 : __torch__.torch.nn.modules.linear.___torch_mangle_7743.Linear = prim::GetAttr[name="key"](%4355)
  %4358 : __torch__.torch.nn.modules.linear.___torch_mangle_7742.Linear = prim::GetAttr[name="query"](%4355)
  %4359 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:241:0
  %4360 : Float(17:512, 512:1) = aten::squeeze(%4359, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.10 : Bool(17:512, 512:1) = aten::lt(%4360, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %input.115 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.100, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:248:0
  %4363 : Tensor = prim::GetAttr[name="bias"](%4358)
  %4364 : Tensor = prim::GetAttr[name="weight"](%4358)
  %4365 : Float(768:1, 768:768) = aten::t(%4364), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.115, %4365), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.19 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.55, %4363, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %4368 : Tensor = prim::GetAttr[name="bias"](%4357)
  %4369 : Tensor = prim::GetAttr[name="weight"](%4357)
  %4370 : Float(768:1, 768:768) = aten::t(%4369), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.115, %4370), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.10 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.56, %4368, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %4373 : Tensor = prim::GetAttr[name="bias"](%4356)
  %4374 : Tensor = prim::GetAttr[name="weight"](%4356)
  %4375 : Float(768:1, 768:768) = aten::t(%4374), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.115, %4375), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.10 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.57, %4373, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %4378 : int = aten::size(%input.115, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
  %4379 : int = aten::size(%input.115, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
  %4380 : int = aten::size(%input.115, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.20 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.19, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:261:0
  %4382 : int[] = prim::ListConstruct(%4378, %4379, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4383 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.20, %4382), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:263:0
  %query.19 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4383, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:263:0
  %4385 : int[] = prim::ListConstruct(%4378, %4379, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4386 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.10, %4385), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:264:0
  %key.10 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4386, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:264:0
  %4388 : int = aten::size(%query.19, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.38 : Long() = prim::NumToTensor(%4388), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4390 : int = aten::size(%query.19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.39 : Long() = prim::NumToTensor(%4390), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4392 : int = aten::size(%query.19, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.28 : Long() = prim::NumToTensor(%4392), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4394 : int = aten::size(%query.19, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %4395 : Long() = aten::floor_divide(%seq_len.39, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %chunks_count.28 : Long() = aten::sub(%4395, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:478:0
  %4397 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.19, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4398 : Long() = aten::mul(%batch_size.38, %num_heads.28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4399 : int = aten::Int(%4398), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4400 : int[] = prim::ListConstruct(%4399, %4390, %4394), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.101 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4397, %4400), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4402 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.10, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4403 : Long() = aten::mul(%batch_size.38, %num_heads.28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4404 : int = aten::Int(%4403), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4405 : int[] = prim::ListConstruct(%4404, %4390, %4394), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.103 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4402, %4405), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4407 : int = aten::size(%hidden_states.101, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
  %4408 : int = aten::size(%hidden_states.101, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
  %4409 : Long() = prim::NumToTensor(%4408), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4410 : Long() = aten::floor_divide(%4409, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4411 : int = aten::Int(%4410), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4412 : int = aten::size(%hidden_states.101, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
  %4413 : int[] = prim::ListConstruct(%4407, %4411, %48, %4412), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.102 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.101, %4413), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
  %4415 : int = aten::size(%hidden_states.102, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4416 : int = aten::size(%hidden_states.102, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4417 : Long() = prim::NumToTensor(%4416), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4418 : int = aten::size(%hidden_states.102, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4419 : int = aten::size(%hidden_states.102, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4420 : Long() = aten::mul(%4417, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4421 : Long() = aten::sub(%4420, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4422 : int = aten::Int(%4421), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4423 : int[] = prim::ListConstruct(%4415, %4422, %4418, %4419), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4424 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4425 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.102, %4423, %4424, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
  %4426 : int = aten::size(%hidden_states.103, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
  %4427 : int = aten::size(%hidden_states.103, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
  %4428 : Long() = prim::NumToTensor(%4427), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4429 : Long() = aten::floor_divide(%4428, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4430 : int = aten::Int(%4429), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4431 : int = aten::size(%hidden_states.103, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
  %4432 : int[] = prim::ListConstruct(%4426, %4430, %48, %4431), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.104 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.103, %4432), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
  %4434 : int = aten::size(%hidden_states.104, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4435 : int = aten::size(%hidden_states.104, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4436 : Long() = prim::NumToTensor(%4435), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4437 : int = aten::size(%hidden_states.104, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4438 : int = aten::size(%hidden_states.104, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4439 : Long() = aten::mul(%4436, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4440 : Long() = aten::sub(%4439, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4441 : int = aten::Int(%4440), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4442 : int[] = prim::ListConstruct(%4434, %4441, %4437, %4438), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4443 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4444 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.104, %4442, %4443, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
  %4445 : Tensor[] = prim::ListConstruct(%4425, %4444), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %input.116 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %4445), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/functional.py:327:0
  %4447 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states_padded.19 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.116, %4447, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
  %4449 : int = aten::size(%hidden_states_padded.19, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4450 : int = aten::size(%hidden_states_padded.19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4451 : int = aten::size(%hidden_states_padded.19, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4452 : int = aten::size(%hidden_states_padded.19, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4453 : int[] = prim::ListConstruct(%4449, %4450, %4451, %4452), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %diagonal_chunked_attention_scores.19 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.19, %4453), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:400:0
  %4455 : Long() = aten::mul(%batch_size.38, %num_heads.28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
  %4456 : int = aten::Int(%4455), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4457 : Long() = aten::add(%chunks_count.28, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
  %4458 : int = aten::Int(%4457), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4459 : int[] = prim::ListConstruct(%4456, %4458, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %diagonal_attention_scores.19 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.19, %4459, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
  %4461 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4462 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4461, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4463 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4462, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4464 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%4463, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4465 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4466 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4465, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4467 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4466, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4468 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%4467, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4469 : int[] = prim::ListConstruct(%21, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4470 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%4464, %4469), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4471 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4468, %4470, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4472 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4473 : Float(204:262656, 512:513, 513:1) = aten::select(%4472, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4474 : Float(204:262656, 256:513, 513:1) = aten::slice(%4473, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4475 : Float(204:262656, 256:513, 257:1) = aten::slice(%4474, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4476 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4477 : Float(204:262656, 256:513, 513:1) = aten::select(%4476, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4478 : Float(204:262656, 256:513, 513:1) = aten::slice(%4477, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4479 : Float(204:262656, 256:513, 257:1) = aten::slice(%4478, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4480 : int[] = prim::ListConstruct(%21, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4481 : Float(204:262656, 256:513, 257:1) = aten::view(%4475, %4480), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4482 : Float(204:262656, 256:513, 257:1) = aten::copy_(%4479, %4481, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4483 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4484 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4483, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4485 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4484, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4486 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%4485, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4487 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4488 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4487, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4489 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4488, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4490 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%4489, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4491 : int[] = prim::ListConstruct(%21, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4492 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%4486, %4491), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4493 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4490, %4492, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4494 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4495 : Float(204:262656, 512:513, 513:1) = aten::select(%4494, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4496 : Float(204:262656, 255:513, 513:1) = aten::slice(%4495, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4497 : Float(204:262656, 255:513, 255:1) = aten::slice(%4496, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4498 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4499 : Float(204:262656, 256:513, 513:1) = aten::select(%4498, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4500 : Float(204:262656, 255:513, 513:1) = aten::slice(%4499, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4501 : Float(204:262656, 255:513, 255:1) = aten::slice(%4500, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4502 : int[] = prim::ListConstruct(%21, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4503 : Float(204:262656, 255:513, 255:1) = aten::view(%4497, %4502), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4504 : Float(204:262656, 255:513, 255:1) = aten::copy_(%4501, %4503, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4505 : int[] = prim::ListConstruct(%4388, %4392, %4390, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4506 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.19, %4505), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.28 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%4506, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
  %4508 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4509 : Float(256:257, 257:1) = aten::ones(%4508, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4510 : Float(256:257, 257:1) = aten::tril(%4509, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4511 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %beginning_mask_2d.19 : Float(256:257, 257:1) = aten::flip(%4510, %4511), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4513 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.19, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4514 : Float(1:65792, 256:257, 257:1) = aten::slice(%4513, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4515 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4514, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.19 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4515, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4517 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %ending_mask.19 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.19, %4517), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:460:0
  %4519 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.28, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4520 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4519, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4521 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4520, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.19 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4521, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4523 : int = aten::size(%beginning_input.19, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4524 : int = aten::size(%beginning_input.19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4525 : int = aten::size(%beginning_input.19, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4526 : int = aten::size(%beginning_input.19, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4527 : int[] = prim::ListConstruct(%4523, %4524, %4525, %4526), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4528 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.19, %4527, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4529 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4528, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4530 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.19, %4529, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:463:0
  %4531 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.28, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4532 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4531, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4533 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4532, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.19 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4533, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4535 : int = aten::size(%ending_input.19, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4536 : int = aten::size(%ending_input.19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4537 : int = aten::size(%ending_input.19, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4538 : int = aten::size(%ending_input.19, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4539 : int[] = prim::ListConstruct(%4535, %4536, %4537, %4538), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4540 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.19, %4539, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4541 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4540, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4542 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.19, %4541, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:466:0
  %4543 : Bool(17:512, 512:1) = aten::ne(%4360, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4544 : Bool(17:512, 512:1) = aten::slice(%4543, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
  %4545 : Bool(17:512, 512:1) = aten::slice(%4544, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
  %4546 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%4545, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.10 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%4546, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
  %4548 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.10, %query.19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.10 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%4548, %remove_from_windowed_attention_mask.10, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:275:0
  %4550 : int = aten::size(%float_mask.10, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4551 : int = aten::size(%float_mask.10, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4552 : int = aten::size(%float_mask.10, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4553 : int = aten::size(%float_mask.10, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4554 : int[] = prim::ListConstruct(%4550, %4551, %4552, %4553), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %query.20 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%4554, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4556 : int = aten::size(%query.20, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.39 : Long() = prim::NumToTensor(%4556), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4558 : int = aten::size(%query.20, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.40 : Long() = prim::NumToTensor(%4558), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4560 : int = aten::size(%query.20, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.29 : Long() = prim::NumToTensor(%4560), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4562 : int = aten::size(%query.20, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %4563 : Long() = aten::floor_divide(%seq_len.40, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %chunks_count.29 : Long() = aten::sub(%4563, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:478:0
  %4565 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.20, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4566 : Long() = aten::mul(%batch_size.39, %num_heads.29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4567 : int = aten::Int(%4566), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4568 : int[] = prim::ListConstruct(%4567, %4558, %4562), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.105 : Float(17:512, 512:1, 1:1) = aten::reshape(%4565, %4568), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4570 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.10, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4571 : Long() = aten::mul(%batch_size.39, %num_heads.29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4572 : int = aten::Int(%4571), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4573 : int[] = prim::ListConstruct(%4572, %4558, %4562), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.107 : Float(17:512, 512:1, 1:1) = aten::reshape(%4570, %4573), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4575 : int = aten::size(%hidden_states.105, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
  %4576 : int = aten::size(%hidden_states.105, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
  %4577 : Long() = prim::NumToTensor(%4576), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4578 : Long() = aten::floor_divide(%4577, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4579 : int = aten::Int(%4578), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4580 : int = aten::size(%hidden_states.105, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
  %4581 : int[] = prim::ListConstruct(%4575, %4579, %48, %4580), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.106 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.105, %4581), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
  %4583 : int = aten::size(%hidden_states.106, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4584 : int = aten::size(%hidden_states.106, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4585 : Long() = prim::NumToTensor(%4584), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4586 : int = aten::size(%hidden_states.106, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4587 : int = aten::size(%hidden_states.106, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4588 : Long() = aten::mul(%4585, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4589 : Long() = aten::sub(%4588, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4590 : int = aten::Int(%4589), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4591 : int[] = prim::ListConstruct(%4583, %4590, %4586, %4587), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4592 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4593 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.106, %4591, %4592, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
  %4594 : int = aten::size(%hidden_states.107, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
  %4595 : int = aten::size(%hidden_states.107, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
  %4596 : Long() = prim::NumToTensor(%4595), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4597 : Long() = aten::floor_divide(%4596, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4598 : int = aten::Int(%4597), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4599 : int = aten::size(%hidden_states.107, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
  %4600 : int[] = prim::ListConstruct(%4594, %4598, %48, %4599), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.108 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.107, %4600), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
  %4602 : int = aten::size(%hidden_states.108, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4603 : int = aten::size(%hidden_states.108, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4604 : Long() = prim::NumToTensor(%4603), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4605 : int = aten::size(%hidden_states.108, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4606 : int = aten::size(%hidden_states.108, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4607 : Long() = aten::mul(%4604, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4608 : Long() = aten::sub(%4607, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4609 : int = aten::Int(%4608), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4610 : int[] = prim::ListConstruct(%4602, %4609, %4605, %4606), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4611 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4612 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.108, %4610, %4611, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
  %4613 : Tensor[] = prim::ListConstruct(%4593, %4612), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %input.117 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %4613), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/functional.py:327:0
  %4615 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states_padded.20 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.117, %4615, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
  %4617 : int = aten::size(%hidden_states_padded.20, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4618 : int = aten::size(%hidden_states_padded.20, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4619 : int = aten::size(%hidden_states_padded.20, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4620 : int = aten::size(%hidden_states_padded.20, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4621 : int[] = prim::ListConstruct(%4617, %4618, %4619, %4620), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %diagonal_chunked_attention_scores.20 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.20, %4621), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:400:0
  %4623 : Long() = aten::mul(%batch_size.39, %num_heads.29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
  %4624 : int = aten::Int(%4623), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4625 : Long() = aten::add(%chunks_count.29, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
  %4626 : int = aten::Int(%4625), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4627 : int[] = prim::ListConstruct(%4624, %4626, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %diagonal_attention_scores.20 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.20, %4627, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
  %4629 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4630 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%4629, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4631 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%4630, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4632 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%4631, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4633 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4634 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4633, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4635 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4634, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4636 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%4635, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4637 : int[] = prim::ListConstruct(%14, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4638 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%4632, %4637), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4639 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4636, %4638, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4640 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4641 : Float(17:262656, 512:513, 513:1) = aten::select(%4640, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4642 : Float(17:262656, 256:513, 513:1) = aten::slice(%4641, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4643 : Float(17:262656, 256:513, 257:1) = aten::slice(%4642, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4644 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4645 : Float(17:262656, 256:513, 513:1) = aten::select(%4644, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4646 : Float(17:262656, 256:513, 513:1) = aten::slice(%4645, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4647 : Float(17:262656, 256:513, 257:1) = aten::slice(%4646, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4648 : int[] = prim::ListConstruct(%14, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4649 : Float(17:262656, 256:513, 257:1) = aten::view(%4643, %4648), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4650 : Float(17:262656, 256:513, 257:1) = aten::copy_(%4647, %4649, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4651 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4652 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%4651, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4653 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%4652, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4654 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%4653, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4655 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4656 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4655, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4657 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4656, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4658 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%4657, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4659 : int[] = prim::ListConstruct(%14, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4660 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%4654, %4659), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4661 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4658, %4660, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4662 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4663 : Float(17:262656, 512:513, 513:1) = aten::select(%4662, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4664 : Float(17:262656, 255:513, 513:1) = aten::slice(%4663, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4665 : Float(17:262656, 255:513, 255:1) = aten::slice(%4664, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4666 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4667 : Float(17:262656, 256:513, 513:1) = aten::select(%4666, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4668 : Float(17:262656, 255:513, 513:1) = aten::slice(%4667, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4669 : Float(17:262656, 255:513, 255:1) = aten::slice(%4668, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4670 : int[] = prim::ListConstruct(%14, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4671 : Float(17:262656, 255:513, 255:1) = aten::view(%4665, %4670), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4672 : Float(17:262656, 255:513, 255:1) = aten::copy_(%4669, %4671, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4673 : int[] = prim::ListConstruct(%4556, %4560, %4558, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4674 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.20, %4673), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.29 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%4674, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
  %4676 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4677 : Float(256:257, 257:1) = aten::ones(%4676, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4678 : Float(256:257, 257:1) = aten::tril(%4677, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4679 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %beginning_mask_2d.20 : Float(256:257, 257:1) = aten::flip(%4678, %4679), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4681 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.20, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4682 : Float(1:65792, 256:257, 257:1) = aten::slice(%4681, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4683 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4682, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.20 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4683, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4685 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %ending_mask.20 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.20, %4685), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:460:0
  %4687 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.29, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4688 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4687, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4689 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4688, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.20 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%4689, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4691 : int = aten::size(%beginning_input.20, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4692 : int = aten::size(%beginning_input.20, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4693 : int = aten::size(%beginning_input.20, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4694 : int = aten::size(%beginning_input.20, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4695 : int[] = prim::ListConstruct(%4691, %4692, %4693, %4694), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4696 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.20, %4695, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4697 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%4696, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4698 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.20, %4697, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:463:0
  %4699 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.29, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4700 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4699, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4701 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4700, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.20 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%4701, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4703 : int = aten::size(%ending_input.20, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4704 : int = aten::size(%ending_input.20, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4705 : int = aten::size(%ending_input.20, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4706 : int = aten::size(%ending_input.20, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4707 : int[] = prim::ListConstruct(%4703, %4704, %4705, %4706), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4708 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.20, %4707, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4709 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%4708, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4710 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.20, %4709, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.10 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.28, %input_tensor.29, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.10 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.10, %36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:1500:0
  %4713 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.10, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %4714 : Bool(17:512, 512:1) = aten::slice(%4713, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %4715 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%4714, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %4716 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%4715, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %input.118 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.10, %4716, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.20 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.118, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:973:0
  %4719 : int[] = prim::ListConstruct(%4378, %4379, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4720 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.10, %4719), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:331:0
  %value.10 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4720, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:331:0
  %4722 : int = aten::size(%value.10, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.40 : Long() = prim::NumToTensor(%4722), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4724 : int = aten::size(%value.10, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.41 : Long() = prim::NumToTensor(%4724), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4726 : int = aten::size(%value.10, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.30 : Long() = prim::NumToTensor(%4726), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4728 : int = aten::size(%value.10, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
  %4729 : Long() = aten::floor_divide(%seq_len.41, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %chunks_count.30 : Long() = aten::sub(%4729, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:542:0
  %4731 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.20, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:545:0
  %4732 : Long() = aten::mul(%batch_size.40, %num_heads.30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:546:0
  %4733 : int = aten::Int(%4732), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4734 : Long() = aten::floor_divide(%seq_len.41, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4735 : int = aten::Int(%4734), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4736 : int[] = prim::ListConstruct(%4733, %4735, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %chunked_hidden_states.46 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%4731, %4736), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:545:0
  %4738 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.10, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
  %4739 : Long() = aten::mul(%batch_size.40, %num_heads.30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
  %4740 : int = aten::Int(%4739), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4741 : int[] = prim::ListConstruct(%4740, %4724, %4728), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %input.119 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4738, %4741), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
  %4743 : int[] = prim::ListConstruct(%53, %53, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %padded_value.10 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.119, %4743, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
  %4745 : Long() = aten::mul(%batch_size.40, %num_heads.30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:556:0
  %4746 : int = aten::Int(%4745), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4747 : Long() = aten::add(%chunks_count.30, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:556:0
  %4748 : int = aten::Int(%4747), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4749 : int[] = prim::ListConstruct(%4746, %4748, %39, %4728), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4750 : int[] = prim::ListConstruct(%12, %11, %32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4751 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.10, %4749, %4750, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:564:0
  %4752 : int = aten::size(%chunked_hidden_states.46, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
  %4753 : int = aten::size(%chunked_hidden_states.46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
  %4754 : int = aten::size(%chunked_hidden_states.46, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.10 : Long() = prim::NumToTensor(%4754), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4756 : int = aten::size(%chunked_hidden_states.46, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.10 : Long() = prim::NumToTensor(%4756), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4758 : Long() = aten::add(%window_overlap.10, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:422:0
  %4759 : int = aten::Int(%4758), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4760 : int[] = prim::ListConstruct(%53, %4759), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %chunked_hidden_states.47 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.46, %4760, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
  %4762 : int[] = prim::ListConstruct(%4752, %4753, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %chunked_hidden_states.48 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.47, %4762), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:424:0
  %4764 : Long() = aten::neg(%window_overlap.10), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:428:0
  %4765 : int = aten::Int(%4764), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4766 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.48, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
  %4767 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%4766, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.49 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%4767, %46, %53, %4765, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
  %4769 : Long() = aten::add(%window_overlap.10, %hidden_dim.10, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:431:0
  %4770 : int = aten::Int(%4769), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4771 : int[] = prim::ListConstruct(%4752, %4753, %4754, %4770), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %chunked_hidden_states.50 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.49, %4771), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:430:0
  %4773 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.50, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
  %4774 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%4773, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
  %4775 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%4774, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
  %4776 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%4775, %45, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
  %4777 : Tensor[] = prim::ListConstruct(%4776, %4751), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %context.10 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%10, %4777), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/functional.py:327:0
  %4779 : int[] = prim::ListConstruct(%4722, %4726, %4724, %4728), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4780 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.10, %4779), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.19 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%4780, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:569:0
  %4782 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.19, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
  %4783 : int[] = prim::ListConstruct(%4378, %4379, %4380), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4784 : Float(512:13056, 17:768, 768:1) = aten::reshape(%4782, %4783), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.20 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%4784, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
  %input.120 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.20, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:374:0
  %4787 : __torch__.torch.nn.modules.normalization.___torch_mangle_7750.LayerNorm = prim::GetAttr[name="LayerNorm"](%4354)
  %4788 : __torch__.torch.nn.modules.linear.___torch_mangle_7749.Linear = prim::GetAttr[name="dense"](%4354)
  %4789 : Tensor = prim::GetAttr[name="bias"](%4788)
  %4790 : Tensor = prim::GetAttr[name="weight"](%4788)
  %4791 : Float(768:1, 768:768) = aten::t(%4790), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.120, %4791), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.121 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.58, %4789, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.109 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.121, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.122 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.109, %hidden_states.100, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output # transformers/modeling_longformer.py:758:0
  %4796 : Tensor = prim::GetAttr[name="bias"](%4787)
  %4797 : Tensor = prim::GetAttr[name="weight"](%4787)
  %4798 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.30 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.122, %4798, %4797, %4796, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %4800 : __torch__.torch.nn.modules.linear.___torch_mangle_7754.Linear = prim::GetAttr[name="dense"](%4352)
  %4801 : Tensor = prim::GetAttr[name="bias"](%4800)
  %4802 : Tensor = prim::GetAttr[name="weight"](%4800)
  %4803 : Float(768:1, 3072:768) = aten::t(%4802), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.intermediate/__module.longformer.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.30, %4803), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.intermediate/__module.longformer.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.123 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.59, %4801, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.intermediate/__module.longformer.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.124 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.123), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %4807 : __torch__.torch.nn.modules.normalization.___torch_mangle_7757.LayerNorm = prim::GetAttr[name="LayerNorm"](%4351)
  %4808 : __torch__.torch.nn.modules.linear.___torch_mangle_7756.Linear = prim::GetAttr[name="dense"](%4351)
  %4809 : Tensor = prim::GetAttr[name="bias"](%4808)
  %4810 : Tensor = prim::GetAttr[name="weight"](%4808)
  %4811 : Float(3072:1, 768:3072) = aten::t(%4810), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.124, %4811), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.125 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.60, %4809, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.110 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.125, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.126 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.110, %input_tensor.30, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output # transformers/modeling_longformer.py:830:0
  %4816 : Tensor = prim::GetAttr[name="bias"](%4807)
  %4817 : Tensor = prim::GetAttr[name="weight"](%4807)
  %4818 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.LayerNorm
  %hidden_states.111 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.126, %4818, %4817, %4816, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %4820 : __torch__.transformers.modeling_longformer.___torch_mangle_7778.LongformerOutput = prim::GetAttr[name="output"](%109)
  %4821 : __torch__.transformers.modeling_longformer.___torch_mangle_7774.LongformerIntermediate = prim::GetAttr[name="intermediate"](%109)
  %4822 : __torch__.transformers.modeling_longformer.___torch_mangle_7772.LongformerAttention = prim::GetAttr[name="attention"](%109)
  %4823 : __torch__.transformers.modeling_longformer.___torch_mangle_7771.LongformerSelfOutput = prim::GetAttr[name="output"](%4822)
  %4824 : __torch__.transformers.modeling_longformer.___torch_mangle_7767.LongformerSelfAttention = prim::GetAttr[name="self"](%4822)
  %4825 : __torch__.torch.nn.modules.linear.___torch_mangle_7763.Linear = prim::GetAttr[name="value"](%4824)
  %4826 : __torch__.torch.nn.modules.linear.___torch_mangle_7762.Linear = prim::GetAttr[name="key"](%4824)
  %4827 : __torch__.torch.nn.modules.linear.___torch_mangle_7761.Linear = prim::GetAttr[name="query"](%4824)
  %4828 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:241:0
  %4829 : Float(17:512, 512:1) = aten::squeeze(%4828, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.11 : Bool(17:512, 512:1) = aten::lt(%4829, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %input.127 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.111, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:248:0
  %4832 : Tensor = prim::GetAttr[name="bias"](%4827)
  %4833 : Tensor = prim::GetAttr[name="weight"](%4827)
  %4834 : Float(768:1, 768:768) = aten::t(%4833), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.127, %4834), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.21 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.61, %4832, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %4837 : Tensor = prim::GetAttr[name="bias"](%4826)
  %4838 : Tensor = prim::GetAttr[name="weight"](%4826)
  %4839 : Float(768:1, 768:768) = aten::t(%4838), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.127, %4839), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.11 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.62, %4837, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %4842 : Tensor = prim::GetAttr[name="bias"](%4825)
  %4843 : Tensor = prim::GetAttr[name="weight"](%4825)
  %4844 : Float(768:1, 768:768) = aten::t(%4843), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.127, %4844), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.11 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.63, %4842, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %4847 : int = aten::size(%input.127, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
  %4848 : int = aten::size(%input.127, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
  %4849 : int = aten::size(%input.127, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.22 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.21, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:261:0
  %4851 : int[] = prim::ListConstruct(%4847, %4848, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4852 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.22, %4851), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:263:0
  %query.21 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4852, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:263:0
  %4854 : int[] = prim::ListConstruct(%4847, %4848, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4855 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.11, %4854), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:264:0
  %key.11 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4855, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:264:0
  %4857 : int = aten::size(%query.21, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.42 : Long() = prim::NumToTensor(%4857), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4859 : int = aten::size(%query.21, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.43 : Long() = prim::NumToTensor(%4859), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4861 : int = aten::size(%query.21, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.31 : Long() = prim::NumToTensor(%4861), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4863 : int = aten::size(%query.21, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %4864 : Long() = aten::floor_divide(%seq_len.43, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %chunks_count.31 : Long() = aten::sub(%4864, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:478:0
  %4866 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.21, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %4867 : Long() = aten::mul(%batch_size.42, %num_heads.31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %4868 : int = aten::Int(%4867), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4869 : int[] = prim::ListConstruct(%4868, %4859, %4863), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.112 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4866, %4869), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %4871 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.11, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %4872 : Long() = aten::mul(%batch_size.42, %num_heads.31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %4873 : int = aten::Int(%4872), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4874 : int[] = prim::ListConstruct(%4873, %4859, %4863), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.114 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4871, %4874), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %4876 : int = aten::size(%hidden_states.112, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
  %4877 : int = aten::size(%hidden_states.112, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
  %4878 : Long() = prim::NumToTensor(%4877), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4879 : Long() = aten::floor_divide(%4878, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %4880 : int = aten::Int(%4879), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4881 : int = aten::size(%hidden_states.112, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
  %4882 : int[] = prim::ListConstruct(%4876, %4880, %48, %4881), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.113 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.112, %4882), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
  %4884 : int = aten::size(%hidden_states.113, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4885 : int = aten::size(%hidden_states.113, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4886 : Long() = prim::NumToTensor(%4885), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4887 : int = aten::size(%hidden_states.113, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4888 : int = aten::size(%hidden_states.113, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4889 : Long() = aten::mul(%4886, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %4890 : Long() = aten::sub(%4889, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %4891 : int = aten::Int(%4890), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4892 : int[] = prim::ListConstruct(%4884, %4891, %4887, %4888), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4893 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4894 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.113, %4892, %4893, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
  %4895 : int = aten::size(%hidden_states.114, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
  %4896 : int = aten::size(%hidden_states.114, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
  %4897 : Long() = prim::NumToTensor(%4896), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4898 : Long() = aten::floor_divide(%4897, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %4899 : int = aten::Int(%4898), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4900 : int = aten::size(%hidden_states.114, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
  %4901 : int[] = prim::ListConstruct(%4895, %4899, %48, %4900), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.115 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.114, %4901), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
  %4903 : int = aten::size(%hidden_states.115, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4904 : int = aten::size(%hidden_states.115, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4905 : Long() = prim::NumToTensor(%4904), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4906 : int = aten::size(%hidden_states.115, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4907 : int = aten::size(%hidden_states.115, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4908 : Long() = aten::mul(%4905, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %4909 : Long() = aten::sub(%4908, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %4910 : int = aten::Int(%4909), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4911 : int[] = prim::ListConstruct(%4903, %4910, %4906, %4907), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4912 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4913 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.115, %4911, %4912, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
  %4914 : Tensor[] = prim::ListConstruct(%4894, %4913), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %input.128 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %4914), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/functional.py:327:0
  %4916 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states_padded.21 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.128, %4916, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
  %4918 : int = aten::size(%hidden_states_padded.21, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %4919 : int = aten::size(%hidden_states_padded.21, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %4920 : int = aten::size(%hidden_states_padded.21, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %4921 : int = aten::size(%hidden_states_padded.21, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %4922 : int[] = prim::ListConstruct(%4918, %4919, %4920, %4921), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %diagonal_chunked_attention_scores.21 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.21, %4922), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:400:0
  %4924 : Long() = aten::mul(%batch_size.42, %num_heads.31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
  %4925 : int = aten::Int(%4924), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4926 : Long() = aten::add(%chunks_count.31, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
  %4927 : int = aten::Int(%4926), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4928 : int[] = prim::ListConstruct(%4925, %4927, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %diagonal_attention_scores.21 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.21, %4928, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
  %4930 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4931 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4930, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4932 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4931, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4933 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%4932, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4934 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4935 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4934, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4936 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4935, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4937 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%4936, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4938 : int[] = prim::ListConstruct(%21, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4939 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%4933, %4938), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4940 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4937, %4939, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4941 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4942 : Float(204:262656, 512:513, 513:1) = aten::select(%4941, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4943 : Float(204:262656, 256:513, 513:1) = aten::slice(%4942, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4944 : Float(204:262656, 256:513, 257:1) = aten::slice(%4943, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4945 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4946 : Float(204:262656, 256:513, 513:1) = aten::select(%4945, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4947 : Float(204:262656, 256:513, 513:1) = aten::slice(%4946, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4948 : Float(204:262656, 256:513, 257:1) = aten::slice(%4947, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4949 : int[] = prim::ListConstruct(%21, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4950 : Float(204:262656, 256:513, 257:1) = aten::view(%4944, %4949), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4951 : Float(204:262656, 256:513, 257:1) = aten::copy_(%4948, %4950, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4952 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4953 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4952, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4954 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4953, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4955 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%4954, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4956 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4957 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4956, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4958 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4957, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4959 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%4958, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4960 : int[] = prim::ListConstruct(%21, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4961 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%4955, %4960), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4962 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4959, %4961, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4963 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4964 : Float(204:262656, 512:513, 513:1) = aten::select(%4963, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4965 : Float(204:262656, 255:513, 513:1) = aten::slice(%4964, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4966 : Float(204:262656, 255:513, 255:1) = aten::slice(%4965, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4967 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4968 : Float(204:262656, 256:513, 513:1) = aten::select(%4967, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4969 : Float(204:262656, 255:513, 513:1) = aten::slice(%4968, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4970 : Float(204:262656, 255:513, 255:1) = aten::slice(%4969, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4971 : int[] = prim::ListConstruct(%21, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4972 : Float(204:262656, 255:513, 255:1) = aten::view(%4966, %4971), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4973 : Float(204:262656, 255:513, 255:1) = aten::copy_(%4970, %4972, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4974 : int[] = prim::ListConstruct(%4857, %4861, %4859, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4975 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.21, %4974), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.31 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%4975, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
  %4977 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4978 : Float(256:257, 257:1) = aten::ones(%4977, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %4979 : Float(256:257, 257:1) = aten::tril(%4978, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %4980 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %beginning_mask_2d.21 : Float(256:257, 257:1) = aten::flip(%4979, %4980), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %4982 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.21, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %4983 : Float(1:65792, 256:257, 257:1) = aten::slice(%4982, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %4984 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4983, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.21 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4984, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %4986 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %ending_mask.21 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.21, %4986), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:460:0
  %4988 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.31, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %4989 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4988, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %4990 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4989, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.21 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4990, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %4992 : int = aten::size(%beginning_input.21, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %4993 : int = aten::size(%beginning_input.21, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %4994 : int = aten::size(%beginning_input.21, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %4995 : int = aten::size(%beginning_input.21, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %4996 : int[] = prim::ListConstruct(%4992, %4993, %4994, %4995), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4997 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.21, %4996, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %4998 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4997, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %4999 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.21, %4998, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:463:0
  %5000 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.31, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5001 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5000, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5002 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5001, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.21 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%5002, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5004 : int = aten::size(%ending_input.21, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5005 : int = aten::size(%ending_input.21, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5006 : int = aten::size(%ending_input.21, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5007 : int = aten::size(%ending_input.21, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5008 : int[] = prim::ListConstruct(%5004, %5005, %5006, %5007), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5009 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.21, %5008, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5010 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%5009, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5011 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.21, %5010, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:466:0
  %5012 : Bool(17:512, 512:1) = aten::ne(%4829, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5013 : Bool(17:512, 512:1) = aten::slice(%5012, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
  %5014 : Bool(17:512, 512:1) = aten::slice(%5013, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
  %5015 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%5014, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.11 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%5015, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
  %5017 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.11, %query.21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.11 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%5017, %remove_from_windowed_attention_mask.11, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:275:0
  %5019 : int = aten::size(%float_mask.11, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5020 : int = aten::size(%float_mask.11, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5021 : int = aten::size(%float_mask.11, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5022 : int = aten::size(%float_mask.11, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5023 : int[] = prim::ListConstruct(%5019, %5020, %5021, %5022), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %query.22 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%5023, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5025 : int = aten::size(%query.22, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.43 : Long() = prim::NumToTensor(%5025), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5027 : int = aten::size(%query.22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.44 : Long() = prim::NumToTensor(%5027), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5029 : int = aten::size(%query.22, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.32 : Long() = prim::NumToTensor(%5029), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5031 : int = aten::size(%query.22, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %5032 : Long() = aten::floor_divide(%seq_len.44, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %chunks_count.32 : Long() = aten::sub(%5032, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:478:0
  %5034 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.22, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %5035 : Long() = aten::mul(%batch_size.43, %num_heads.32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %5036 : int = aten::Int(%5035), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5037 : int[] = prim::ListConstruct(%5036, %5027, %5031), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.116 : Float(17:512, 512:1, 1:1) = aten::reshape(%5034, %5037), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %5039 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.11, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %5040 : Long() = aten::mul(%batch_size.43, %num_heads.32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %5041 : int = aten::Int(%5040), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5042 : int[] = prim::ListConstruct(%5041, %5027, %5031), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.118 : Float(17:512, 512:1, 1:1) = aten::reshape(%5039, %5042), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %5044 : int = aten::size(%hidden_states.116, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
  %5045 : int = aten::size(%hidden_states.116, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
  %5046 : Long() = prim::NumToTensor(%5045), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5047 : Long() = aten::floor_divide(%5046, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %5048 : int = aten::Int(%5047), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5049 : int = aten::size(%hidden_states.116, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
  %5050 : int[] = prim::ListConstruct(%5044, %5048, %48, %5049), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.117 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.116, %5050), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
  %5052 : int = aten::size(%hidden_states.117, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5053 : int = aten::size(%hidden_states.117, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5054 : Long() = prim::NumToTensor(%5053), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5055 : int = aten::size(%hidden_states.117, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5056 : int = aten::size(%hidden_states.117, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5057 : Long() = aten::mul(%5054, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %5058 : Long() = aten::sub(%5057, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %5059 : int = aten::Int(%5058), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5060 : int[] = prim::ListConstruct(%5052, %5059, %5055, %5056), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5061 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5062 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.117, %5060, %5061, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
  %5063 : int = aten::size(%hidden_states.118, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
  %5064 : int = aten::size(%hidden_states.118, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
  %5065 : Long() = prim::NumToTensor(%5064), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5066 : Long() = aten::floor_divide(%5065, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %5067 : int = aten::Int(%5066), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5068 : int = aten::size(%hidden_states.118, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
  %5069 : int[] = prim::ListConstruct(%5063, %5067, %48, %5068), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.119 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.118, %5069), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
  %5071 : int = aten::size(%hidden_states.119, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5072 : int = aten::size(%hidden_states.119, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5073 : Long() = prim::NumToTensor(%5072), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5074 : int = aten::size(%hidden_states.119, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5075 : int = aten::size(%hidden_states.119, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5076 : Long() = aten::mul(%5073, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %5077 : Long() = aten::sub(%5076, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %5078 : int = aten::Int(%5077), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5079 : int[] = prim::ListConstruct(%5071, %5078, %5074, %5075), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5080 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5081 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.119, %5079, %5080, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
  %5082 : Tensor[] = prim::ListConstruct(%5062, %5081), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %input.129 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %5082), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/functional.py:327:0
  %5084 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states_padded.22 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.129, %5084, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
  %5086 : int = aten::size(%hidden_states_padded.22, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %5087 : int = aten::size(%hidden_states_padded.22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %5088 : int = aten::size(%hidden_states_padded.22, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %5089 : int = aten::size(%hidden_states_padded.22, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %5090 : int[] = prim::ListConstruct(%5086, %5087, %5088, %5089), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %diagonal_chunked_attention_scores.22 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.22, %5090), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:400:0
  %5092 : Long() = aten::mul(%batch_size.43, %num_heads.32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
  %5093 : int = aten::Int(%5092), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5094 : Long() = aten::add(%chunks_count.32, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
  %5095 : int = aten::Int(%5094), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5096 : int[] = prim::ListConstruct(%5093, %5095, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %diagonal_attention_scores.22 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.22, %5096, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
  %5098 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5099 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%5098, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5100 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%5099, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5101 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%5100, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5102 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5103 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5102, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5104 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5103, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5105 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%5104, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5106 : int[] = prim::ListConstruct(%14, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5107 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%5101, %5106), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5108 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%5105, %5107, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5109 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5110 : Float(17:262656, 512:513, 513:1) = aten::select(%5109, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5111 : Float(17:262656, 256:513, 513:1) = aten::slice(%5110, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5112 : Float(17:262656, 256:513, 257:1) = aten::slice(%5111, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5113 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5114 : Float(17:262656, 256:513, 513:1) = aten::select(%5113, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5115 : Float(17:262656, 256:513, 513:1) = aten::slice(%5114, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5116 : Float(17:262656, 256:513, 257:1) = aten::slice(%5115, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5117 : int[] = prim::ListConstruct(%14, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5118 : Float(17:262656, 256:513, 257:1) = aten::view(%5112, %5117), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5119 : Float(17:262656, 256:513, 257:1) = aten::copy_(%5116, %5118, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5120 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5121 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%5120, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5122 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%5121, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5123 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%5122, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5124 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5125 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5124, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5126 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5125, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5127 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%5126, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5128 : int[] = prim::ListConstruct(%14, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5129 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%5123, %5128), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5130 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%5127, %5129, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5131 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5132 : Float(17:262656, 512:513, 513:1) = aten::select(%5131, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5133 : Float(17:262656, 255:513, 513:1) = aten::slice(%5132, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5134 : Float(17:262656, 255:513, 255:1) = aten::slice(%5133, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5135 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5136 : Float(17:262656, 256:513, 513:1) = aten::select(%5135, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5137 : Float(17:262656, 255:513, 513:1) = aten::slice(%5136, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5138 : Float(17:262656, 255:513, 255:1) = aten::slice(%5137, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5139 : int[] = prim::ListConstruct(%14, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5140 : Float(17:262656, 255:513, 255:1) = aten::view(%5134, %5139), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5141 : Float(17:262656, 255:513, 255:1) = aten::copy_(%5138, %5140, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5142 : int[] = prim::ListConstruct(%5025, %5029, %5027, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5143 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.22, %5142), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.32 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%5143, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
  %5145 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5146 : Float(256:257, 257:1) = aten::ones(%5145, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %5147 : Float(256:257, 257:1) = aten::tril(%5146, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %5148 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %beginning_mask_2d.22 : Float(256:257, 257:1) = aten::flip(%5147, %5148), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %5150 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.22, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %5151 : Float(1:65792, 256:257, 257:1) = aten::slice(%5150, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %5152 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%5151, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.22 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%5152, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %5154 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %ending_mask.22 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.22, %5154), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:460:0
  %5156 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.32, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %5157 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5156, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %5158 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5157, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.22 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%5158, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %5160 : int = aten::size(%beginning_input.22, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5161 : int = aten::size(%beginning_input.22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5162 : int = aten::size(%beginning_input.22, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5163 : int = aten::size(%beginning_input.22, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5164 : int[] = prim::ListConstruct(%5160, %5161, %5162, %5163), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5165 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.22, %5164, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5166 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%5165, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5167 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.22, %5166, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:463:0
  %5168 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.32, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5169 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5168, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5170 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5169, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.22 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%5170, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5172 : int = aten::size(%ending_input.22, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5173 : int = aten::size(%ending_input.22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5174 : int = aten::size(%ending_input.22, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5175 : int = aten::size(%ending_input.22, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5176 : int[] = prim::ListConstruct(%5172, %5173, %5174, %5175), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5177 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.22, %5176, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5178 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%5177, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5179 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.22, %5178, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.11 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.31, %input_tensor.32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.11 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.11, %36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:1500:0
  %5182 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.11, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %5183 : Bool(17:512, 512:1) = aten::slice(%5182, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %5184 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%5183, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %5185 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%5184, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %input.130 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.11, %5185, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.22 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.130, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:973:0
  %5188 : int[] = prim::ListConstruct(%4847, %4848, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5189 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.11, %5188), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:331:0
  %value.11 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%5189, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:331:0
  %5191 : int = aten::size(%value.11, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.44 : Long() = prim::NumToTensor(%5191), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5193 : int = aten::size(%value.11, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.45 : Long() = prim::NumToTensor(%5193), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5195 : int = aten::size(%value.11, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.33 : Long() = prim::NumToTensor(%5195), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5197 : int = aten::size(%value.11, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
  %5198 : Long() = aten::floor_divide(%seq_len.45, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %chunks_count.33 : Long() = aten::sub(%5198, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:542:0
  %5200 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.22, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:545:0
  %5201 : Long() = aten::mul(%batch_size.44, %num_heads.33), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:546:0
  %5202 : int = aten::Int(%5201), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5203 : Long() = aten::floor_divide(%seq_len.45, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %5204 : int = aten::Int(%5203), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5205 : int[] = prim::ListConstruct(%5202, %5204, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %chunked_hidden_states.51 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%5200, %5205), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:545:0
  %5207 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.11, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
  %5208 : Long() = aten::mul(%batch_size.44, %num_heads.33), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
  %5209 : int = aten::Int(%5208), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5210 : int[] = prim::ListConstruct(%5209, %5193, %5197), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %input.131 : Float(204:64, 512:13056, 64:1) = aten::reshape(%5207, %5210), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
  %5212 : int[] = prim::ListConstruct(%53, %53, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %padded_value.11 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.131, %5212, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
  %5214 : Long() = aten::mul(%batch_size.44, %num_heads.33), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:556:0
  %5215 : int = aten::Int(%5214), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5216 : Long() = aten::add(%chunks_count.33, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:556:0
  %5217 : int = aten::Int(%5216), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5218 : int[] = prim::ListConstruct(%5215, %5217, %39, %5197), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5219 : int[] = prim::ListConstruct(%12, %11, %32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5220 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.11, %5218, %5219, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:564:0
  %5221 : int = aten::size(%chunked_hidden_states.51, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
  %5222 : int = aten::size(%chunked_hidden_states.51, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
  %5223 : int = aten::size(%chunked_hidden_states.51, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.11 : Long() = prim::NumToTensor(%5223), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5225 : int = aten::size(%chunked_hidden_states.51, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.11 : Long() = prim::NumToTensor(%5225), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5227 : Long() = aten::add(%window_overlap.11, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:422:0
  %5228 : int = aten::Int(%5227), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5229 : int[] = prim::ListConstruct(%53, %5228), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %chunked_hidden_states.52 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.51, %5229, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
  %5231 : int[] = prim::ListConstruct(%5221, %5222, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %chunked_hidden_states.53 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.52, %5231), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:424:0
  %5233 : Long() = aten::neg(%window_overlap.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:428:0
  %5234 : int = aten::Int(%5233), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5235 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.53, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
  %5236 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%5235, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.54 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%5236, %46, %53, %5234, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
  %5238 : Long() = aten::add(%window_overlap.11, %hidden_dim.11, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:431:0
  %5239 : int = aten::Int(%5238), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5240 : int[] = prim::ListConstruct(%5221, %5222, %5223, %5239), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %chunked_hidden_states.55 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.54, %5240), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:430:0
  %5242 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.55, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
  %5243 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%5242, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
  %5244 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%5243, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
  %5245 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%5244, %45, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
  %5246 : Tensor[] = prim::ListConstruct(%5245, %5220), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %context.11 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%10, %5246), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/functional.py:327:0
  %5248 : int[] = prim::ListConstruct(%5191, %5195, %5193, %5197), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5249 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.11, %5248), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.21 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%5249, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:569:0
  %5251 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.21, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
  %5252 : int[] = prim::ListConstruct(%4847, %4848, %4849), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5253 : Float(512:13056, 17:768, 768:1) = aten::reshape(%5251, %5252), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.22 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%5253, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
  %input.132 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.22, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:374:0
  %5256 : __torch__.torch.nn.modules.normalization.___torch_mangle_7769.LayerNorm = prim::GetAttr[name="LayerNorm"](%4823)
  %5257 : __torch__.torch.nn.modules.linear.___torch_mangle_7768.Linear = prim::GetAttr[name="dense"](%4823)
  %5258 : Tensor = prim::GetAttr[name="bias"](%5257)
  %5259 : Tensor = prim::GetAttr[name="weight"](%5257)
  %5260 : Float(768:1, 768:768) = aten::t(%5259), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.132, %5260), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.133 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.64, %5258, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.120 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.133, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.134 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.120, %hidden_states.111, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output # transformers/modeling_longformer.py:758:0
  %5265 : Tensor = prim::GetAttr[name="bias"](%5256)
  %5266 : Tensor = prim::GetAttr[name="weight"](%5256)
  %5267 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.33 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.134, %5267, %5266, %5265, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %5269 : __torch__.torch.nn.modules.linear.___torch_mangle_7773.Linear = prim::GetAttr[name="dense"](%4821)
  %5270 : Tensor = prim::GetAttr[name="bias"](%5269)
  %5271 : Tensor = prim::GetAttr[name="weight"](%5269)
  %5272 : Float(768:1, 3072:768) = aten::t(%5271), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.intermediate/__module.longformer.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.33, %5272), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.intermediate/__module.longformer.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.135 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.65, %5270, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.intermediate/__module.longformer.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.136 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.135), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %5276 : __torch__.torch.nn.modules.normalization.___torch_mangle_7776.LayerNorm = prim::GetAttr[name="LayerNorm"](%4820)
  %5277 : __torch__.torch.nn.modules.linear.___torch_mangle_7775.Linear = prim::GetAttr[name="dense"](%4820)
  %5278 : Tensor = prim::GetAttr[name="bias"](%5277)
  %5279 : Tensor = prim::GetAttr[name="weight"](%5277)
  %5280 : Float(3072:1, 768:3072) = aten::t(%5279), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.136, %5280), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.137 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.66, %5278, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.121 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.137, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.138 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.121, %input_tensor.33, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output # transformers/modeling_longformer.py:830:0
  %5285 : Tensor = prim::GetAttr[name="bias"](%5276)
  %5286 : Tensor = prim::GetAttr[name="weight"](%5276)
  %5287 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.LayerNorm
  %hidden_states.122 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.138, %5287, %5286, %5285, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %5289 : __torch__.transformers.modeling_longformer.___torch_mangle_7797.LongformerOutput = prim::GetAttr[name="output"](%107)
  %5290 : __torch__.transformers.modeling_longformer.___torch_mangle_7793.LongformerIntermediate = prim::GetAttr[name="intermediate"](%107)
  %5291 : __torch__.transformers.modeling_longformer.___torch_mangle_7791.LongformerAttention = prim::GetAttr[name="attention"](%107)
  %5292 : __torch__.transformers.modeling_longformer.___torch_mangle_7790.LongformerSelfOutput = prim::GetAttr[name="output"](%5291)
  %5293 : __torch__.transformers.modeling_longformer.___torch_mangle_7786.LongformerSelfAttention = prim::GetAttr[name="self"](%5291)
  %5294 : __torch__.torch.nn.modules.linear.___torch_mangle_7782.Linear = prim::GetAttr[name="value"](%5293)
  %5295 : __torch__.torch.nn.modules.linear.___torch_mangle_7781.Linear = prim::GetAttr[name="key"](%5293)
  %5296 : __torch__.torch.nn.modules.linear.___torch_mangle_7780.Linear = prim::GetAttr[name="query"](%5293)
  %5297 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:241:0
  %5298 : Float(17:512, 512:1) = aten::squeeze(%5297, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked : Bool(17:512, 512:1) = aten::lt(%5298, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %input.139 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.122, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:248:0
  %5301 : Tensor = prim::GetAttr[name="bias"](%5296)
  %5302 : Tensor = prim::GetAttr[name="weight"](%5296)
  %5303 : Float(768:1, 768:768) = aten::t(%5302), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.139, %5303), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.23 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.67, %5301, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %5306 : Tensor = prim::GetAttr[name="bias"](%5295)
  %5307 : Tensor = prim::GetAttr[name="weight"](%5295)
  %5308 : Float(768:1, 768:768) = aten::t(%5307), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.139, %5308), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors : Float(512:13056, 17:768, 768:1) = aten::add_(%output.68, %5306, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %5311 : Tensor = prim::GetAttr[name="bias"](%5294)
  %5312 : Tensor = prim::GetAttr[name="weight"](%5294)
  %5313 : Float(768:1, 768:768) = aten::t(%5312), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.139, %5313), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors : Float(512:13056, 17:768, 768:1) = aten::add_(%output.69, %5311, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %5316 : int = aten::size(%input.139, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
  %5317 : int = aten::size(%input.139, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
  %5318 : int = aten::size(%input.139, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.23, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:261:0
  %5320 : int[] = prim::ListConstruct(%5316, %5317, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5321 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors, %5320), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:263:0
  %query.23 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%5321, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:263:0
  %5323 : int[] = prim::ListConstruct(%5316, %5317, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5324 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors, %5323), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:264:0
  %key : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%5324, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:264:0
  %5326 : int = aten::size(%query.23, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.46 : Long() = prim::NumToTensor(%5326), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5328 : int = aten::size(%query.23, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.47 : Long() = prim::NumToTensor(%5328), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5330 : int = aten::size(%query.23, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.34 : Long() = prim::NumToTensor(%5330), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5332 : int = aten::size(%query.23, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %5333 : Long() = aten::floor_divide(%seq_len.47, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %chunks_count.34 : Long() = aten::sub(%5333, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:478:0
  %5335 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.23, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5336 : Long() = aten::mul(%batch_size.46, %num_heads.34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5337 : int = aten::Int(%5336), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5338 : int[] = prim::ListConstruct(%5337, %5328, %5332), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.123 : Float(204:64, 512:13056, 64:1) = aten::reshape(%5335, %5338), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5340 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5341 : Long() = aten::mul(%batch_size.46, %num_heads.34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5342 : int = aten::Int(%5341), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5343 : int[] = prim::ListConstruct(%5342, %5328, %5332), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.125 : Float(204:64, 512:13056, 64:1) = aten::reshape(%5340, %5343), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5345 : int = aten::size(%hidden_states.123, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
  %5346 : int = aten::size(%hidden_states.123, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
  %5347 : Long() = prim::NumToTensor(%5346), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5348 : Long() = aten::floor_divide(%5347, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5349 : int = aten::Int(%5348), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5350 : int = aten::size(%hidden_states.123, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
  %5351 : int[] = prim::ListConstruct(%5345, %5349, %48, %5350), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.124 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.123, %5351), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
  %5353 : int = aten::size(%hidden_states.124, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5354 : int = aten::size(%hidden_states.124, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5355 : Long() = prim::NumToTensor(%5354), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5356 : int = aten::size(%hidden_states.124, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5357 : int = aten::size(%hidden_states.124, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5358 : Long() = aten::mul(%5355, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5359 : Long() = aten::sub(%5358, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5360 : int = aten::Int(%5359), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5361 : int[] = prim::ListConstruct(%5353, %5360, %5356, %5357), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5362 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5363 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.124, %5361, %5362, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
  %5364 : int = aten::size(%hidden_states.125, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
  %5365 : int = aten::size(%hidden_states.125, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
  %5366 : Long() = prim::NumToTensor(%5365), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5367 : Long() = aten::floor_divide(%5366, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5368 : int = aten::Int(%5367), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5369 : int = aten::size(%hidden_states.125, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
  %5370 : int[] = prim::ListConstruct(%5364, %5368, %48, %5369), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.126 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.125, %5370), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
  %5372 : int = aten::size(%hidden_states.126, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5373 : int = aten::size(%hidden_states.126, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5374 : Long() = prim::NumToTensor(%5373), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5375 : int = aten::size(%hidden_states.126, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5376 : int = aten::size(%hidden_states.126, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5377 : Long() = aten::mul(%5374, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5378 : Long() = aten::sub(%5377, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5379 : int = aten::Int(%5378), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5380 : int[] = prim::ListConstruct(%5372, %5379, %5375, %5376), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5381 : int[] = prim::ListConstruct(%32, %28, %27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5382 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.126, %5380, %5381, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
  %5383 : Tensor[] = prim::ListConstruct(%5363, %5382), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %input.140 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %5383), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/functional.py:327:0
  %5385 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states_padded.23 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.140, %5385, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
  %5387 : int = aten::size(%hidden_states_padded.23, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5388 : int = aten::size(%hidden_states_padded.23, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5389 : int = aten::size(%hidden_states_padded.23, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5390 : int = aten::size(%hidden_states_padded.23, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5391 : int[] = prim::ListConstruct(%5387, %5388, %5389, %5390), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %diagonal_chunked_attention_scores.23 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.23, %5391), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:400:0
  %5393 : Long() = aten::mul(%batch_size.46, %num_heads.34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
  %5394 : int = aten::Int(%5393), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5395 : Long() = aten::add(%chunks_count.34, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
  %5396 : int = aten::Int(%5395), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5397 : int[] = prim::ListConstruct(%5394, %5396, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %diagonal_attention_scores.23 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.23, %5397, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
  %5399 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5400 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%5399, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5401 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%5400, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5402 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%5401, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5403 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5404 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%5403, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5405 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%5404, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5406 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%5405, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5407 : int[] = prim::ListConstruct(%21, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5408 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%5402, %5407), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5409 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%5406, %5408, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5410 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5411 : Float(204:262656, 512:513, 513:1) = aten::select(%5410, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5412 : Float(204:262656, 256:513, 513:1) = aten::slice(%5411, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5413 : Float(204:262656, 256:513, 257:1) = aten::slice(%5412, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5414 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5415 : Float(204:262656, 256:513, 513:1) = aten::select(%5414, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5416 : Float(204:262656, 256:513, 513:1) = aten::slice(%5415, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5417 : Float(204:262656, 256:513, 257:1) = aten::slice(%5416, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5418 : int[] = prim::ListConstruct(%21, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5419 : Float(204:262656, 256:513, 257:1) = aten::view(%5413, %5418), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5420 : Float(204:262656, 256:513, 257:1) = aten::copy_(%5417, %5419, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5421 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5422 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%5421, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5423 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%5422, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5424 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%5423, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5425 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5426 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%5425, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5427 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%5426, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5428 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%5427, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5429 : int[] = prim::ListConstruct(%21, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5430 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%5424, %5429), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5431 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%5428, %5430, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5432 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5433 : Float(204:262656, 512:513, 513:1) = aten::select(%5432, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5434 : Float(204:262656, 255:513, 513:1) = aten::slice(%5433, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5435 : Float(204:262656, 255:513, 255:1) = aten::slice(%5434, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5436 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5437 : Float(204:262656, 256:513, 513:1) = aten::select(%5436, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5438 : Float(204:262656, 255:513, 513:1) = aten::slice(%5437, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5439 : Float(204:262656, 255:513, 255:1) = aten::slice(%5438, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5440 : int[] = prim::ListConstruct(%21, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5441 : Float(204:262656, 255:513, 255:1) = aten::view(%5435, %5440), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5442 : Float(204:262656, 255:513, 255:1) = aten::copy_(%5439, %5441, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5443 : int[] = prim::ListConstruct(%5326, %5330, %5328, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5444 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.23, %5443), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.34 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%5444, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
  %5446 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5447 : Float(256:257, 257:1) = aten::ones(%5446, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5448 : Float(256:257, 257:1) = aten::tril(%5447, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5449 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %beginning_mask_2d.23 : Float(256:257, 257:1) = aten::flip(%5448, %5449), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5451 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.23, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5452 : Float(1:65792, 256:257, 257:1) = aten::slice(%5451, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5453 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%5452, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.23 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%5453, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5455 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %ending_mask.23 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.23, %5455), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:460:0
  %5457 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.34, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5458 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5457, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5459 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5458, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.23 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%5459, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5461 : int = aten::size(%beginning_input.23, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5462 : int = aten::size(%beginning_input.23, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5463 : int = aten::size(%beginning_input.23, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5464 : int = aten::size(%beginning_input.23, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5465 : int[] = prim::ListConstruct(%5461, %5462, %5463, %5464), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5466 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.23, %5465, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5467 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%5466, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5468 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.23, %5467, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:463:0
  %5469 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.34, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5470 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5469, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5471 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5470, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.23 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%5471, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5473 : int = aten::size(%ending_input.23, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5474 : int = aten::size(%ending_input.23, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5475 : int = aten::size(%ending_input.23, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5476 : int = aten::size(%ending_input.23, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5477 : int[] = prim::ListConstruct(%5473, %5474, %5475, %5476), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5478 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.23, %5477, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5479 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%5478, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5480 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.23, %5479, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:466:0
  %5481 : Bool(17:512, 512:1) = aten::ne(%5298, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5482 : Bool(17:512, 512:1) = aten::slice(%5481, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
  %5483 : Bool(17:512, 512:1) = aten::slice(%5482, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
  %5484 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%5483, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%5484, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
  %5486 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask, %query.23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%5486, %remove_from_windowed_attention_mask, %15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:275:0
  %5488 : int = aten::size(%float_mask, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5489 : int = aten::size(%float_mask, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5490 : int = aten::size(%float_mask, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5491 : int = aten::size(%float_mask, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5492 : int[] = prim::ListConstruct(%5488, %5489, %5490, %5491), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %query : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%5492, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5494 : int = aten::size(%query, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.47 : Long() = prim::NumToTensor(%5494), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5496 : int = aten::size(%query, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.48 : Long() = prim::NumToTensor(%5496), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5498 : int = aten::size(%query, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.35 : Long() = prim::NumToTensor(%5498), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5500 : int = aten::size(%query, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %5501 : Long() = aten::floor_divide(%seq_len.48, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %chunks_count.35 : Long() = aten::sub(%5501, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:478:0
  %5503 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5504 : Long() = aten::mul(%batch_size.47, %num_heads.35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5505 : int = aten::Int(%5504), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5506 : int[] = prim::ListConstruct(%5505, %5496, %5500), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.127 : Float(17:512, 512:1, 1:1) = aten::reshape(%5503, %5506), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5508 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5509 : Long() = aten::mul(%batch_size.47, %num_heads.35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5510 : int = aten::Int(%5509), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5511 : int[] = prim::ListConstruct(%5510, %5496, %5500), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.129 : Float(17:512, 512:1, 1:1) = aten::reshape(%5508, %5511), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5513 : int = aten::size(%hidden_states.127, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
  %5514 : int = aten::size(%hidden_states.127, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
  %5515 : Long() = prim::NumToTensor(%5514), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5516 : Long() = aten::floor_divide(%5515, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5517 : int = aten::Int(%5516), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5518 : int = aten::size(%hidden_states.127, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
  %5519 : int[] = prim::ListConstruct(%5513, %5517, %48, %5518), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.128 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.127, %5519), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
  %5521 : int = aten::size(%hidden_states.128, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5522 : int = aten::size(%hidden_states.128, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5523 : Long() = prim::NumToTensor(%5522), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5524 : int = aten::size(%hidden_states.128, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5525 : int = aten::size(%hidden_states.128, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5526 : Long() = aten::mul(%5523, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5527 : Long() = aten::sub(%5526, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5528 : int = aten::Int(%5527), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5529 : int[] = prim::ListConstruct(%5521, %5528, %5524, %5525), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5530 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5531 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.128, %5529, %5530, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
  %5532 : int = aten::size(%hidden_states.129, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
  %5533 : int = aten::size(%hidden_states.129, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
  %5534 : Long() = prim::NumToTensor(%5533), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5535 : Long() = aten::floor_divide(%5534, %30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5536 : int = aten::Int(%5535), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5537 : int = aten::size(%hidden_states.129, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
  %5538 : int[] = prim::ListConstruct(%5532, %5536, %48, %5537), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.130 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.129, %5538), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
  %5540 : int = aten::size(%hidden_states.130, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5541 : int = aten::size(%hidden_states.130, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5542 : Long() = prim::NumToTensor(%5541), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5543 : int = aten::size(%hidden_states.130, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5544 : int = aten::size(%hidden_states.130, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5545 : Long() = aten::mul(%5542, %29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5546 : Long() = aten::sub(%5545, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5547 : int = aten::Int(%5546), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5548 : int[] = prim::ListConstruct(%5540, %5547, %5543, %5544), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5549 : int[] = prim::ListConstruct(%48, %24, %52, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5550 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.130, %5548, %5549, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
  %5551 : Tensor[] = prim::ListConstruct(%5531, %5550), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %input.141 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%26, %5551), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/functional.py:327:0
  %5553 : int[] = prim::ListConstruct(%53, %53, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states_padded : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.141, %5553, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
  %5555 : int = aten::size(%hidden_states_padded, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5556 : int = aten::size(%hidden_states_padded, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5557 : int = aten::size(%hidden_states_padded, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5558 : int = aten::size(%hidden_states_padded, %25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5559 : int[] = prim::ListConstruct(%5555, %5556, %5557, %5558), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %diagonal_chunked_attention_scores : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded, %5559), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:400:0
  %5561 : Long() = aten::mul(%batch_size.47, %num_heads.35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
  %5562 : int = aten::Int(%5561), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5563 : Long() = aten::add(%chunks_count.35, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
  %5564 : int = aten::Int(%5563), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5565 : int[] = prim::ListConstruct(%5562, %5564, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %diagonal_attention_scores : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores, %5565, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
  %5567 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5568 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%5567, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5569 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%5568, %46, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5570 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%5569, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5571 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5572 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5571, %52, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5573 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5572, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5574 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%5573, %45, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5575 : int[] = prim::ListConstruct(%14, %52, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5576 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%5570, %5575), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5577 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%5574, %5576, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5578 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5579 : Float(17:262656, 512:513, 513:1) = aten::select(%5578, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5580 : Float(17:262656, 256:513, 513:1) = aten::slice(%5579, %52, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5581 : Float(17:262656, 256:513, 257:1) = aten::slice(%5580, %46, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5582 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5583 : Float(17:262656, 256:513, 513:1) = aten::select(%5582, %52, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5584 : Float(17:262656, 256:513, 513:1) = aten::slice(%5583, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5585 : Float(17:262656, 256:513, 257:1) = aten::slice(%5584, %46, %24, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5586 : int[] = prim::ListConstruct(%14, %24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5587 : Float(17:262656, 256:513, 257:1) = aten::view(%5581, %5586), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5588 : Float(17:262656, 256:513, 257:1) = aten::copy_(%5585, %5587, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5589 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5590 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%5589, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5591 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%5590, %46, %20, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5592 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%5591, %45, %22, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5593 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5594 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5593, %52, %52, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5595 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5594, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5596 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%5595, %45, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5597 : int[] = prim::ListConstruct(%14, %52, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5598 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%5592, %5597), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5599 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%5596, %5598, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5600 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5601 : Float(17:262656, 512:513, 513:1) = aten::select(%5600, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5602 : Float(17:262656, 255:513, 513:1) = aten::slice(%5601, %52, %53, %19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5603 : Float(17:262656, 255:513, 255:1) = aten::slice(%5602, %46, %18, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5604 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5605 : Float(17:262656, 256:513, 513:1) = aten::select(%5604, %52, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5606 : Float(17:262656, 255:513, 513:1) = aten::slice(%5605, %52, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5607 : Float(17:262656, 255:513, 255:1) = aten::slice(%5606, %46, %52, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5608 : int[] = prim::ListConstruct(%14, %19, %19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5609 : Float(17:262656, 255:513, 255:1) = aten::view(%5603, %5608), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5610 : Float(17:262656, 255:513, 255:1) = aten::copy_(%5607, %5609, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5611 : int[] = prim::ListConstruct(%5494, %5498, %5496, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5612 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores, %5611), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.35 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%5612, %46, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
  %5614 : int[] = prim::ListConstruct(%24, %22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5615 : Float(256:257, 257:1) = aten::ones(%5614, %44, %53, %50, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5616 : Float(256:257, 257:1) = aten::tril(%5615, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5617 : int[] = prim::ListConstruct(%53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %beginning_mask_2d : Float(256:257, 257:1) = aten::flip(%5616, %5617), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5619 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5620 : Float(1:65792, 256:257, 257:1) = aten::slice(%5619, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5621 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%5620, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%5621, %45, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5623 : int[] = prim::ListConstruct(%52, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %ending_mask : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask, %5623), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:460:0
  %5625 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.35, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5626 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5625, %52, %53, %24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5627 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5626, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%5627, %45, %53, %22, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5629 : int = aten::size(%beginning_input, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5630 : int = aten::size(%beginning_input, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5631 : int = aten::size(%beginning_input, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5632 : int = aten::size(%beginning_input, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5633 : int[] = prim::ListConstruct(%5629, %5630, %5631, %5632), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5634 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask, %5633, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5635 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%5634, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5636 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input, %5635, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:463:0
  %5637 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.35, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5638 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5637, %52, %16, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5639 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5638, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%5639, %45, %20, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5641 : int = aten::size(%ending_input, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5642 : int = aten::size(%ending_input, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5643 : int = aten::size(%ending_input, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5644 : int = aten::size(%ending_input, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5645 : int[] = prim::ListConstruct(%5641, %5642, %5643, %5644), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5646 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask, %5645, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5647 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%5646, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5648 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input, %5647, %17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.34, %input_tensor.35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores, %36, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:1500:0
  %5651 : Bool(17:512, 512:1) = aten::slice(%is_index_masked, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %5652 : Bool(17:512, 512:1) = aten::slice(%5651, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %5653 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%5652, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %5654 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%5653, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %input.142 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32, %5654, %13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.142, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:973:0
  %5657 : int[] = prim::ListConstruct(%5316, %5317, %33, %32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5658 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors, %5657), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:331:0
  %value : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%5658, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:331:0
  %5660 : int = aten::size(%value, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size : Long() = prim::NumToTensor(%5660), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5662 : int = aten::size(%value, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len : Long() = prim::NumToTensor(%5662), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5664 : int = aten::size(%value, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads : Long() = prim::NumToTensor(%5664), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5666 : int = aten::size(%value, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
  %5667 : Long() = aten::floor_divide(%seq_len, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %chunks_count : Long() = aten::sub(%5667, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:542:0
  %5669 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:545:0
  %5670 : Long() = aten::mul(%batch_size, %num_heads), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:546:0
  %5671 : int = aten::Int(%5670), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5672 : Long() = aten::floor_divide(%seq_len, %31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5673 : int = aten::Int(%5672), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5674 : int[] = prim::ListConstruct(%5671, %5673, %24, %23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %chunked_hidden_states.56 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%5669, %5674), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:545:0
  %5676 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
  %5677 : Long() = aten::mul(%batch_size, %num_heads), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
  %5678 : int = aten::Int(%5677), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5679 : int[] = prim::ListConstruct(%5678, %5662, %5666), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %input.143 : Float(204:64, 512:13056, 64:1) = aten::reshape(%5676, %5679), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
  %5681 : int[] = prim::ListConstruct(%53, %53, %24, %24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %padded_value : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.143, %5681, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
  %5683 : Long() = aten::mul(%batch_size, %num_heads), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:556:0
  %5684 : int = aten::Int(%5683), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5685 : Long() = aten::add(%chunks_count, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:556:0
  %5686 : int = aten::Int(%5685), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5687 : int[] = prim::ListConstruct(%5684, %5686, %39, %5666), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5688 : int[] = prim::ListConstruct(%12, %11, %32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5689 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value, %5687, %5688, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:564:0
  %5690 : int = aten::size(%chunked_hidden_states.56, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
  %5691 : int = aten::size(%chunked_hidden_states.56, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
  %5692 : int = aten::size(%chunked_hidden_states.56, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap : Long() = prim::NumToTensor(%5692), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5694 : int = aten::size(%chunked_hidden_states.56, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim : Long() = prim::NumToTensor(%5694), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5696 : Long() = aten::add(%window_overlap, %35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:422:0
  %5697 : int = aten::Int(%5696), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5698 : int[] = prim::ListConstruct(%53, %5697), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %chunked_hidden_states.57 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.56, %5698, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
  %5700 : int[] = prim::ListConstruct(%5690, %5691, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %chunked_hidden_states.58 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.57, %5700), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:424:0
  %5702 : Long() = aten::neg(%window_overlap), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:428:0
  %5703 : int = aten::Int(%5702), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5704 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.58, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
  %5705 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%5704, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.59 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%5705, %46, %53, %5703, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
  %5707 : Long() = aten::add(%window_overlap, %hidden_dim, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:431:0
  %5708 : int = aten::Int(%5707), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5709 : int[] = prim::ListConstruct(%5690, %5691, %5692, %5708), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %chunked_hidden_states : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.59, %5709), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:430:0
  %5711 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states, %53, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
  %5712 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%5711, %52, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
  %5713 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%5712, %46, %53, %47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
  %5714 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%5713, %45, %53, %36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
  %5715 : Tensor[] = prim::ListConstruct(%5714, %5689), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %context : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%10, %5715), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/functional.py:327:0
  %5717 : int[] = prim::ListConstruct(%5660, %5664, %5662, %5666), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5718 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context, %5717), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.23 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%5718, %52, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:569:0
  %5720 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.23, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
  %5721 : int[] = prim::ListConstruct(%5316, %5317, %5318), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5722 : Float(512:13056, 17:768, 768:1) = aten::reshape(%5720, %5721), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output : Float(512:13056, 17:768, 768:1) = aten::contiguous(%5722, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
  %input.144 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output, %53, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:374:0
  %5725 : __torch__.torch.nn.modules.normalization.___torch_mangle_7788.LayerNorm = prim::GetAttr[name="LayerNorm"](%5292)
  %5726 : __torch__.torch.nn.modules.linear.___torch_mangle_7787.Linear = prim::GetAttr[name="dense"](%5292)
  %5727 : Tensor = prim::GetAttr[name="bias"](%5726)
  %5728 : Tensor = prim::GetAttr[name="weight"](%5726)
  %5729 : Float(768:1, 768:768) = aten::t(%5728), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.144, %5729), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.145 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.70, %5727, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.131 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.145, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.146 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.131, %hidden_states.122, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output # transformers/modeling_longformer.py:758:0
  %5734 : Tensor = prim::GetAttr[name="bias"](%5725)
  %5735 : Tensor = prim::GetAttr[name="weight"](%5725)
  %5736 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.146, %5736, %5735, %5734, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %5738 : __torch__.torch.nn.modules.linear.___torch_mangle_7792.Linear = prim::GetAttr[name="dense"](%5290)
  %5739 : Tensor = prim::GetAttr[name="bias"](%5738)
  %5740 : Tensor = prim::GetAttr[name="weight"](%5738)
  %5741 : Float(768:1, 3072:768) = aten::t(%5740), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.intermediate/__module.longformer.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor, %5741), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.intermediate/__module.longformer.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.147 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.71, %5739, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.intermediate/__module.longformer.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.148 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.147), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %5745 : __torch__.torch.nn.modules.normalization.___torch_mangle_7795.LayerNorm = prim::GetAttr[name="LayerNorm"](%5289)
  %5746 : __torch__.torch.nn.modules.linear.___torch_mangle_7794.Linear = prim::GetAttr[name="dense"](%5289)
  %5747 : Tensor = prim::GetAttr[name="bias"](%5746)
  %5748 : Tensor = prim::GetAttr[name="weight"](%5746)
  %5749 : Float(3072:1, 768:3072) = aten::t(%5748), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.148, %5749), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.149 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.72, %5747, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.149, %40, %49), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.150 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states, %input_tensor, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output # transformers/modeling_longformer.py:830:0
  %5754 : Tensor = prim::GetAttr[name="bias"](%5745)
  %5755 : Tensor = prim::GetAttr[name="weight"](%5745)
  %5756 : int[] = prim::ListConstruct(%39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.LayerNorm
  %sequence_output : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.150, %5756, %5755, %5754, %38, %37), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %5758 : Long() = aten::neg(%padding_len), scope: __module.longformer # transformers/modeling_longformer.py:1275:0
  %5759 : int = aten::Int(%5758), scope: __module.longformer
  %5760 : Float(17:393216, 512:768, 768:1) = aten::slice(%sequence_output, %53, %53, %47, %52), scope: __module.longformer # transformers/modeling_longformer.py:1275:0
  %input.151 : Float(17:393216, 13:768, 768:1) = aten::slice(%5760, %52, %53, %5759, %52), scope: __module.longformer # transformers/modeling_longformer.py:1275:0
  %5762 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %5763 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %input : Float(17:393216, 13:768, 768:1) = aten::dropout(%input.151, %5763, %5762), scope: __module.dropout # torch/nn/functional.py:973:0
  %5765 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1678:0
  %5766 : Tensor = prim::GetAttr[name="bias"](%3)
  %5767 : Tensor = prim::GetAttr[name="weight"](%3)
  %5768 : Float(768:1, 2:768) = aten::t(%5767), scope: __module.classifier # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %5768), scope: __module.classifier # torch/nn/functional.py:1676:0
  %5770 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %5766, %5765), scope: __module.classifier # torch/nn/functional.py:1678:0
  %9 : (Float(17:26, 13:2, 2:1)) = prim::TupleConstruct(%5770)
  return (%9)
