graph(%self.1 : __torch__.transformers.modeling_funnel.FunnelForMaskedLM,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_2167.Linear = prim::GetAttr[name="lm_head"](%self.1)
  %4 : __torch__.transformers.modeling_funnel.FunnelModel = prim::GetAttr[name="funnel"](%self.1)
  %8 : int = prim::Constant[value=384](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %9 : float = prim::Constant[value=1.](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %10 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %11 : Long() = prim::Constant[value={384}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:248:0
  %12 : Float() = prim::Constant[value={10000}](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %13 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder
  %14 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %15 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %16 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %17 : Long() = prim::Constant[value={13}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %18 : Float(1:1) = prim::Constant[value={-1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %19 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %20 : Long() = prim::Constant[value={14}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %21 : int = prim::Constant[value=-2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %22 : Float(1:1) = prim::Constant[value={-3}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %23 : Long() = prim::Constant[value={16}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %24 : int = prim::Constant[value=-4](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %25 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %26 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %27 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %28 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %29 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %30 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %31 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %32 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %33 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %34 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %35 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %36 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %37 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %38 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %39 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %40 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %41 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %42 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %43 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %44 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %45 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
  %46 : bool = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
  %47 : Device = prim::Constant[value="cpu"](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
  %48 : int = prim::Constant[value=4](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
  %49 : int = prim::Constant[value=1](), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
  %50 : int = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
  %51 : __torch__.transformers.modeling_funnel.FunnelDecoder = prim::GetAttr[name="decoder"](%4)
  %52 : __torch__.transformers.modeling_funnel.___torch_mangle_2132.FunnelEncoder = prim::GetAttr[name="encoder"](%4)
  %53 : __torch__.transformers.modeling_funnel.___torch_mangle_1944.FunnelEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %54 : int = aten::size(%input_ids, %50), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
  %55 : int = aten::size(%input_ids, %49), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
  %56 : int[] = prim::ListConstruct(%54, %55), scope: __module.funnel
  %token_type_ids : Long(17:13, 13:1) = aten::zeros(%56, %48, %50, %47, %46), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
  %58 : __torch__.torch.nn.modules.normalization.___torch_mangle_1942.LayerNorm = prim::GetAttr[name="layer_norm"](%53)
  %59 : __torch__.torch.nn.modules.sparse.___torch_mangle_1941.Embedding = prim::GetAttr[name="word_embeddings"](%53)
  %60 : Tensor = prim::GetAttr[name="weight"](%59)
  %input.1 : Float(17:9984, 13:768, 768:1) = aten::embedding(%60, %input_ids, %41, %46, %46), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %62 : Tensor = prim::GetAttr[name="bias"](%58)
  %63 : Tensor = prim::GetAttr[name="weight"](%58)
  %64 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm
  %input.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.1, %64, %63, %62, %43, %42), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.2, %45, %46), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
  %67 : __torch__.torch.nn.modules.container.___torch_mangle_2131.ModuleList = prim::GetAttr[name="blocks"](%52)
  %68 : __torch__.torch.nn.modules.container.___torch_mangle_2130.ModuleList = prim::GetAttr[name="2"](%67)
  %69 : __torch__.transformers.modeling_funnel.___torch_mangle_2129.FunnelLayer = prim::GetAttr[name="3"](%68)
  %70 : __torch__.torch.nn.modules.container.___torch_mangle_2131.ModuleList = prim::GetAttr[name="blocks"](%52)
  %71 : __torch__.torch.nn.modules.container.___torch_mangle_2130.ModuleList = prim::GetAttr[name="2"](%70)
  %72 : __torch__.transformers.modeling_funnel.___torch_mangle_2114.FunnelLayer = prim::GetAttr[name="2"](%71)
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_2131.ModuleList = prim::GetAttr[name="blocks"](%52)
  %74 : __torch__.torch.nn.modules.container.___torch_mangle_2130.ModuleList = prim::GetAttr[name="2"](%73)
  %75 : __torch__.transformers.modeling_funnel.___torch_mangle_2099.FunnelLayer = prim::GetAttr[name="1"](%74)
  %76 : __torch__.torch.nn.modules.container.___torch_mangle_2131.ModuleList = prim::GetAttr[name="blocks"](%52)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_2130.ModuleList = prim::GetAttr[name="2"](%76)
  %78 : __torch__.transformers.modeling_funnel.___torch_mangle_2084.FunnelLayer = prim::GetAttr[name="0"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_2131.ModuleList = prim::GetAttr[name="blocks"](%52)
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_2069.ModuleList = prim::GetAttr[name="1"](%79)
  %81 : __torch__.transformers.modeling_funnel.___torch_mangle_2068.FunnelLayer = prim::GetAttr[name="3"](%80)
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_2131.ModuleList = prim::GetAttr[name="blocks"](%52)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_2069.ModuleList = prim::GetAttr[name="1"](%82)
  %84 : __torch__.transformers.modeling_funnel.___torch_mangle_2053.FunnelLayer = prim::GetAttr[name="2"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_2131.ModuleList = prim::GetAttr[name="blocks"](%52)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_2069.ModuleList = prim::GetAttr[name="1"](%85)
  %87 : __torch__.transformers.modeling_funnel.___torch_mangle_2038.FunnelLayer = prim::GetAttr[name="1"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_2131.ModuleList = prim::GetAttr[name="blocks"](%52)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_2069.ModuleList = prim::GetAttr[name="1"](%88)
  %90 : __torch__.transformers.modeling_funnel.___torch_mangle_2023.FunnelLayer = prim::GetAttr[name="0"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_2131.ModuleList = prim::GetAttr[name="blocks"](%52)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_2008.ModuleList = prim::GetAttr[name="0"](%91)
  %93 : __torch__.transformers.modeling_funnel.___torch_mangle_2007.FunnelLayer = prim::GetAttr[name="3"](%92)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_2131.ModuleList = prim::GetAttr[name="blocks"](%52)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_2008.ModuleList = prim::GetAttr[name="0"](%94)
  %96 : __torch__.transformers.modeling_funnel.___torch_mangle_1992.FunnelLayer = prim::GetAttr[name="2"](%95)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_2131.ModuleList = prim::GetAttr[name="blocks"](%52)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_2008.ModuleList = prim::GetAttr[name="0"](%97)
  %99 : __torch__.transformers.modeling_funnel.___torch_mangle_1977.FunnelLayer = prim::GetAttr[name="1"](%98)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_2131.ModuleList = prim::GetAttr[name="blocks"](%52)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_2008.ModuleList = prim::GetAttr[name="0"](%100)
  %102 : __torch__.transformers.modeling_funnel.___torch_mangle_1962.FunnelLayer = prim::GetAttr[name="0"](%101)
  %attention_mask.2 : Float(17:13, 13:1) = aten::type_as(%attention_mask.1, %inputs_embeds), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:625:0
  %104 : int = aten::size(%inputs_embeds, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:195:0
  %seq_len.1 : Long() = prim::NumToTensor(%104), scope: __module.funnel/__module.funnel.encoder
  %freq_seq.1 : Float(384:1) = aten::arange(%50, %8, %9, %10, %50, %47, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %107 : Float(384:1) = aten::div(%freq_seq.1, %11), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:248:0
  %108 : Float() = aten::to(%12, %47, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %109 : Float() = aten::detach(%108), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %110 : Float(384:1) = aten::pow(%109, %107), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %111 : Float(384:1) = aten::reciprocal(%110), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %inv_freq.1 : Float(384:1) = aten::mul(%111, %14), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %113 : Long() = aten::neg(%seq_len.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %114 : Long() = aten::mul(%113, %15), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %115 : Scalar = aten::ScalarImplicit(%114), scope: __module.funnel/__module.funnel.encoder
  %116 : Long() = aten::mul(%seq_len.1, %15), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %117 : Scalar = aten::ScalarImplicit(%116), scope: __module.funnel/__module.funnel.encoder
  %rel_pos_id.1 : Float(52:1) = aten::arange(%115, %117, %9, %10, %50, %47, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %zero_offset.1 : Long() = aten::mul(%seq_len.1, %15), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:251:0
  %120 : Float(52:1) = aten::slice(%rel_pos_id.1, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %121 : Float(52:1, 1:1) = aten::unsqueeze(%120, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %122 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq.1, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %sinusoid.1 : Float(52:384, 384:1) = aten::mul(%121, %122), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %input.3 : Float(52:384, 384:1) = aten::sin(%sinusoid.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:253:0
  %sin_embed.1 : Float(52:384, 384:1) = aten::dropout(%input.3, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %input.4 : Float(52:384, 384:1) = aten::cos(%sinusoid.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:254:0
  %cos_embed.1 : Float(52:384, 384:1) = aten::dropout(%input.4, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
  %128 : Tensor[] = prim::ListConstruct(%sin_embed.1, %cos_embed.1), scope: __module.funnel/__module.funnel.encoder
  %pos_embed.1 : Float(52:768, 768:1) = aten::cat(%128, %41), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:255:0
  %pos.1 : Float(13:1) = aten::arange(%50, %104, %49, %10, %50, %47, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:257:0
  %131 : Float() = aten::select(%pos.1, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %132 : Float() = aten::select(%pos.1, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.1 : Float() = aten::sub(%131, %132, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.1 : Float() = aten::add(%ref_point.1, %17, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %135 : Scalar = aten::ScalarImplicit(%max_dist.1), scope: __module.funnel/__module.funnel.encoder
  %136 : Float() = aten::select(%pos.1, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %137 : Float() = aten::select(%pos.1, %50, %41), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.1 : Float() = aten::sub(%136, %137, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %139 : Float() = aten::sub(%min_dist.1, %14, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %140 : Scalar = aten::ScalarImplicit(%139), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.1 : Long(26:1) = aten::arange(%135, %140, %41, %48, %50, %47, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %142 : Long(26:1) = aten::slice(%rel_pos.1, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %143 : Long(26:1, 1:1) = aten::unsqueeze(%142, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.2 : Long(26:1, 1:1) = aten::add(%143, %zero_offset.1, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %145 : int = aten::size(%rel_pos.2, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %146 : int[] = prim::ListConstruct(%145, %44), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.3 : Long(26:1, 768:0) = aten::expand(%rel_pos.2, %146, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %148 : Float(26:768, 768:1) = aten::gather(%pos_embed.1, %50, %rel_pos.3, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %149 : Float(1:1) = aten::to(%18, %47, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos.1 : Float(1:1) = aten::detach(%149), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id.1 : Float(11:1) = aten::slice(%pos.1, %50, %49, %41, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
  %152 : Float(6:2) = aten::slice(%pooled_pos_id.1, %50, %50, %16, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %153 : Tensor[] = prim::ListConstruct(%cls_pos.1, %152), scope: __module.funnel/__module.funnel.encoder
  %pooled_pos.1 : Float(7:1) = aten::cat(%153, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %155 : Float() = aten::select(%pooled_pos.1, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %156 : Float() = aten::select(%pos.1, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.2 : Float() = aten::sub(%155, %156, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.2 : Float() = aten::add(%ref_point.2, %20, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %159 : Scalar = aten::ScalarImplicit(%max_dist.2), scope: __module.funnel/__module.funnel.encoder
  %160 : Float() = aten::select(%pooled_pos.1, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %161 : Float() = aten::select(%pos.1, %50, %41), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.2 : Float() = aten::sub(%160, %161, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %163 : Float() = aten::sub(%min_dist.2, %14, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %164 : Scalar = aten::ScalarImplicit(%163), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.4 : Long(27:1) = aten::arange(%159, %164, %41, %48, %50, %47, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %166 : Long(27:1) = aten::slice(%rel_pos.4, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %167 : Long(27:1, 1:1) = aten::unsqueeze(%166, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.5 : Long(27:1, 1:1) = aten::add(%167, %zero_offset.1, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %169 : int = aten::size(%rel_pos.5, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %170 : int[] = prim::ListConstruct(%169, %44), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.6 : Long(27:1, 768:0) = aten::expand(%rel_pos.5, %170, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %172 : Float(27:768, 768:1) = aten::gather(%pos_embed.1, %50, %rel_pos.6, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
  %173 : Float() = aten::select(%pooled_pos.1, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %174 : Float() = aten::select(%pooled_pos.1, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.3 : Float() = aten::sub(%173, %174, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.3 : Float() = aten::add(%ref_point.3, %20, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %177 : Scalar = aten::ScalarImplicit(%max_dist.3), scope: __module.funnel/__module.funnel.encoder
  %178 : Float() = aten::select(%pooled_pos.1, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %179 : Float() = aten::select(%pooled_pos.1, %50, %41), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.3 : Float() = aten::sub(%178, %179, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %181 : Float() = aten::sub(%min_dist.3, %14, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %182 : Scalar = aten::ScalarImplicit(%181), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.7 : Long(14:1) = aten::arange(%177, %182, %21, %48, %50, %47, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %184 : Long(14:1) = aten::slice(%rel_pos.7, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %185 : Long(14:1, 1:1) = aten::unsqueeze(%184, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.8 : Long(14:1, 1:1) = aten::add(%185, %zero_offset.1, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %187 : int = aten::size(%rel_pos.8, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %188 : int[] = prim::ListConstruct(%187, %44), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.9 : Long(14:1, 768:0) = aten::expand(%rel_pos.8, %188, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %190 : Float(14:768, 768:1) = aten::gather(%pos_embed.1, %50, %rel_pos.9, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %191 : Float(1:1) = aten::to(%22, %47, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos.2 : Float(1:1) = aten::detach(%191), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id.2 : Float(5:1) = aten::slice(%pooled_pos.1, %50, %49, %41, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
  %194 : Float(3:2) = aten::slice(%pooled_pos_id.2, %50, %50, %16, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %195 : Tensor[] = prim::ListConstruct(%cls_pos.2, %194), scope: __module.funnel/__module.funnel.encoder
  %pooled_pos.2 : Float(4:1) = aten::cat(%195, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %197 : Float() = aten::select(%pooled_pos.2, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %198 : Float() = aten::select(%pooled_pos.1, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.4 : Float() = aten::sub(%197, %198, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.4 : Float() = aten::add(%ref_point.4, %23, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %201 : Scalar = aten::ScalarImplicit(%max_dist.4), scope: __module.funnel/__module.funnel.encoder
  %202 : Float() = aten::select(%pooled_pos.2, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %203 : Float() = aten::select(%pooled_pos.1, %50, %41), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.4 : Float() = aten::sub(%202, %203, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %205 : Float() = aten::sub(%min_dist.4, %14, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %206 : Scalar = aten::ScalarImplicit(%205), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.10 : Long(15:1) = aten::arange(%201, %206, %21, %48, %50, %47, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %208 : Long(15:1) = aten::slice(%rel_pos.10, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %209 : Long(15:1, 1:1) = aten::unsqueeze(%208, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.11 : Long(15:1, 1:1) = aten::add(%209, %zero_offset.1, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %211 : int = aten::size(%rel_pos.11, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %212 : int[] = prim::ListConstruct(%211, %44), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.12 : Long(15:1, 768:0) = aten::expand(%rel_pos.11, %212, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %214 : Float(15:768, 768:1) = aten::gather(%pos_embed.1, %50, %rel_pos.12, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
  %215 : Float() = aten::select(%pooled_pos.2, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %216 : Float() = aten::select(%pooled_pos.2, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.5 : Float() = aten::sub(%215, %216, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.5 : Float() = aten::add(%ref_point.5, %23, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %219 : Scalar = aten::ScalarImplicit(%max_dist.5), scope: __module.funnel/__module.funnel.encoder
  %220 : Float() = aten::select(%pooled_pos.2, %50, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %221 : Float() = aten::select(%pooled_pos.2, %50, %41), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.5 : Float() = aten::sub(%220, %221, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %223 : Float() = aten::sub(%min_dist.5, %14, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %224 : Scalar = aten::ScalarImplicit(%223), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.13 : Long(8:1) = aten::arange(%219, %224, %24, %48, %50, %47, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %226 : Long(8:1) = aten::slice(%rel_pos.13, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %227 : Long(8:1, 1:1) = aten::unsqueeze(%226, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.14 : Long(8:1, 1:1) = aten::add(%227, %zero_offset.1, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %229 : int = aten::size(%rel_pos.14, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %230 : int[] = prim::ListConstruct(%229, %44), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.15 : Long(8:1, 768:0) = aten::expand(%rel_pos.14, %230, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %232 : Float(8:768, 768:1) = aten::gather(%pos_embed.1, %50, %rel_pos.15, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %233 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %234 : Long(17:13, 13:1) = aten::slice(%233, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %235 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%234, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %236 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %237 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%236, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %token_type_mat.1 : Bool(17:169, 13:13, 13:1) = aten::eq(%235, %237), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
  %cls_ids.1 : Bool(17:13, 13:1) = aten::eq(%token_type_ids, %19), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
  %240 : Bool(17:13, 13:1) = aten::slice(%cls_ids.1, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %241 : Bool(17:13, 13:1) = aten::slice(%240, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %242 : Bool(17:13, 13:1, 1:1) = aten::unsqueeze(%241, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %243 : Bool(17:13, 13:1) = aten::slice(%cls_ids.1, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %244 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%243, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %cls_mat.1 : Bool(17:169, 13:13, 13:1) = aten::__or__(%242, %244), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %token_type_mat.2 : Bool(17:169, 13:13, 13:1) = aten::__or__(%cls_mat.1, %token_type_mat.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:211:0
  %247 : Long() = aten::sub(%seq_len.1, %14, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %248 : int = aten::Int(%247), scope: __module.funnel/__module.funnel.encoder
  %249 : Long() = aten::sub(%seq_len.1, %14, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %250 : int = aten::Int(%249), scope: __module.funnel/__module.funnel.encoder
  %251 : int[] = prim::ListConstruct(%248, %250), scope: __module.funnel/__module.funnel.encoder
  %input.5 : Float(12:12, 12:1) = aten::ones(%251, %10, %50, %47, %46), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %253 : int[] = prim::ListConstruct(%49, %50, %49, %50), scope: __module.funnel/__module.funnel.encoder
  %cls_mask.1 : Float(13:13, 13:1) = aten::constant_pad_nd(%input.5, %253, %50), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:3552:0
  %255 : __torch__.transformers.modeling_funnel.___torch_mangle_1961.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%102)
  %256 : __torch__.transformers.modeling_funnel.___torch_mangle_1955.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%102)
  %257 : __torch__.torch.nn.modules.normalization.___torch_mangle_1954.LayerNorm = prim::GetAttr[name="layer_norm"](%256)
  %258 : __torch__.torch.nn.modules.linear.___torch_mangle_1953.Linear = prim::GetAttr[name="post_proj"](%256)
  %259 : Tensor = prim::GetAttr[name="seg_embed"](%256)
  %260 : Tensor = prim::GetAttr[name="r_s_bias"](%256)
  %261 : Tensor = prim::GetAttr[name="r_kernel"](%256)
  %262 : Tensor = prim::GetAttr[name="r_r_bias"](%256)
  %263 : Tensor = prim::GetAttr[name="r_w_bias"](%256)
  %264 : __torch__.torch.nn.modules.linear.___torch_mangle_1952.Linear = prim::GetAttr[name="v_head"](%256)
  %265 : __torch__.torch.nn.modules.linear.___torch_mangle_1951.Linear = prim::GetAttr[name="k_head"](%256)
  %266 : __torch__.torch.nn.modules.linear.___torch_mangle_1950.Linear = prim::GetAttr[name="q_head"](%256)
  %267 : int = aten::size(%inputs_embeds, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %268 : int = aten::size(%inputs_embeds, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %269 : int = aten::size(%inputs_embeds, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:531:0
  %270 : Tensor = prim::GetAttr[name="weight"](%266)
  %271 : Float(768:1, 768:768) = aten::t(%270), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %272 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %271), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %273 : int[] = prim::ListConstruct(%267, %268, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %q_head.1 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%272, %273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %275 : Tensor = prim::GetAttr[name="bias"](%265)
  %276 : Tensor = prim::GetAttr[name="weight"](%265)
  %277 : Float(768:1, 768:768) = aten::t(%276), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %277), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %279 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %275, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1678:0
  %280 : int[] = prim::ListConstruct(%267, %269, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %281 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%279, %280), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:537:0
  %282 : Tensor = prim::GetAttr[name="bias"](%264)
  %283 : Tensor = prim::GetAttr[name="weight"](%264)
  %284 : Float(768:1, 768:768) = aten::t(%283), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %284), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %286 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %282, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1678:0
  %287 : int[] = prim::ListConstruct(%267, %269, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %288 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%286, %287), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.1, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.1 : Float(12:64, 64:1) = aten::mul(%263, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:542:0
  %291 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_w_bias.1, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:544:0
  %292 : Tensor[] = prim::ListConstruct(%291, %281), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %content_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%34, %292), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %v.1 : Float(12:64, 64:1) = aten::mul(%262, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:486:0
  %295 : Tensor[] = prim::ListConstruct(%148, %261), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %296 : Float(26:768, 12:64, 64:1) = aten::einsum(%35, %295), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %297 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %v.1, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:493:0
  %298 : Tensor[] = prim::ListConstruct(%297, %296), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.1 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%36, %298), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %300 : int = aten::size(%positional_attn.1, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %301 : int = aten::size(%positional_attn.1, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %302 : int = aten::size(%positional_attn.1, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %303 : int = aten::size(%positional_attn.1, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.1 : Long() = prim::NumToTensor(%303), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %305 : int[] = prim::ListConstruct(%300, %301, %303, %302), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.2 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.1, %305), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:428:0
  %307 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %308 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%307, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %309 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%308, %19, %49, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.3 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%309, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %311 : Long() = aten::sub(%max_rel_len.1, %14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %312 : int = aten::Int(%311), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %313 : int[] = prim::ListConstruct(%300, %301, %302, %312), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.4 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.3, %313), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.5 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.4, %37, %50, %269, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.6 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.5, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:498:0
  %317 : int = aten::size(%token_type_mat.2, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %318 : int = aten::size(%token_type_mat.2, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %319 : int = aten::size(%token_type_mat.2, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.1 : Float(12:64, 64:1) = aten::mul(%260, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:508:0
  %321 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_s_bias.1, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:511:0
  %322 : Tensor[] = prim::ListConstruct(%321, %259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %323 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%38, %322), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %324 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %325 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%324, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %326 : int = aten::size(%q_head.2, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %327 : int[] = prim::ListConstruct(%317, %326, %318, %319), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %token_type_mat.3 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%325, %327, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %329 : Tensor[] = aten::split(%323, %49, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:371:0
  %diff_token_type.1 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.1 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%329), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %332 : int = aten::size(%token_type_mat.3, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %333 : int = aten::size(%token_type_mat.3, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %334 : int = aten::size(%token_type_mat.3, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %335 : int = aten::size(%token_type_mat.3, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %336 : int[] = prim::ListConstruct(%332, %333, %334, %335), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %337 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.1, %336, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %338 : int = aten::size(%token_type_mat.3, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %339 : int = aten::size(%token_type_mat.3, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %340 : int = aten::size(%token_type_mat.3, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %341 : int = aten::size(%token_type_mat.3, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %342 : int[] = prim::ListConstruct(%338, %339, %340, %341), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %343 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.1, %342, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.3, %337, %343), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.1, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:522:0
  %346 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.1, %positional_attn.6, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%346, %token_type_attn.2, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.1, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:553:0
  %349 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %350 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%349, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %351 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%350, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %352 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%351, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %353 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%352, %49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:396:0
  %354 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%353, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.2, %354, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.3, %41, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:558:0
  %357 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.6, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %358 : Tensor[] = prim::ListConstruct(%357, %288), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %attn_vec.1 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%40, %358), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %360 : int[] = prim::ListConstruct(%267, %268, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %input.7 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.1, %360), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:565:0
  %362 : Tensor = prim::GetAttr[name="bias"](%258)
  %363 : Tensor = prim::GetAttr[name="weight"](%258)
  %364 : Float(768:1, 768:768) = aten::t(%363), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.7, %364), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %362, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.8, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %attn_out.1, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:568:0
  %369 : Tensor = prim::GetAttr[name="bias"](%257)
  %370 : Tensor = prim::GetAttr[name="weight"](%257)
  %371 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.9, %371, %370, %369, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %373 : __torch__.torch.nn.modules.normalization.___torch_mangle_1960.LayerNorm = prim::GetAttr[name="layer_norm"](%255)
  %374 : __torch__.torch.nn.modules.linear.___torch_mangle_1958.Linear = prim::GetAttr[name="linear_2"](%255)
  %375 : __torch__.torch.nn.modules.linear.___torch_mangle_1956.Linear = prim::GetAttr[name="linear_1"](%255)
  %376 : Tensor = prim::GetAttr[name="bias"](%375)
  %377 : Tensor = prim::GetAttr[name="weight"](%375)
  %378 : Float(768:1, 3072:768) = aten::t(%377), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.10, %378), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.1 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.4, %376, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %381 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.1, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %382 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.1, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %383 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%382, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %384 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.1, %383, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %385 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%384, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %386 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%385), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %387 : Float(17:39936, 13:3072, 3072:1) = aten::add(%386, %29, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%381, %387), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.11, %30, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %390 : Tensor = prim::GetAttr[name="bias"](%374)
  %391 : Tensor = prim::GetAttr[name="weight"](%374)
  %392 : Float(3072:1, 768:3072) = aten::t(%391), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %392), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.5, %390, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%input.10, %h.1, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/modeling_funnel.py:588:0
  %397 : Tensor = prim::GetAttr[name="bias"](%373)
  %398 : Tensor = prim::GetAttr[name="weight"](%373)
  %399 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm
  %query.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %399, %398, %397, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %401 : __torch__.transformers.modeling_funnel.___torch_mangle_1976.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%99)
  %402 : __torch__.transformers.modeling_funnel.___torch_mangle_1970.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%99)
  %403 : __torch__.torch.nn.modules.normalization.___torch_mangle_1969.LayerNorm = prim::GetAttr[name="layer_norm"](%402)
  %404 : __torch__.torch.nn.modules.linear.___torch_mangle_1968.Linear = prim::GetAttr[name="post_proj"](%402)
  %405 : Tensor = prim::GetAttr[name="seg_embed"](%402)
  %406 : Tensor = prim::GetAttr[name="r_s_bias"](%402)
  %407 : Tensor = prim::GetAttr[name="r_kernel"](%402)
  %408 : Tensor = prim::GetAttr[name="r_r_bias"](%402)
  %409 : Tensor = prim::GetAttr[name="r_w_bias"](%402)
  %410 : __torch__.torch.nn.modules.linear.___torch_mangle_1967.Linear = prim::GetAttr[name="v_head"](%402)
  %411 : __torch__.torch.nn.modules.linear.___torch_mangle_1966.Linear = prim::GetAttr[name="k_head"](%402)
  %412 : __torch__.torch.nn.modules.linear.___torch_mangle_1965.Linear = prim::GetAttr[name="q_head"](%402)
  %413 : int = aten::size(%query.1, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %414 : int = aten::size(%query.1, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %415 : int = aten::size(%query.1, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:531:0
  %416 : Tensor = prim::GetAttr[name="weight"](%412)
  %417 : Float(768:1, 768:768) = aten::t(%416), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %418 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %417), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %419 : int[] = prim::ListConstruct(%413, %414, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %q_head.3 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%418, %419), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:535:0
  %421 : Tensor = prim::GetAttr[name="bias"](%411)
  %422 : Tensor = prim::GetAttr[name="weight"](%411)
  %423 : Float(768:1, 768:768) = aten::t(%422), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %423), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %425 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %421, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1678:0
  %426 : int[] = prim::ListConstruct(%413, %415, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %427 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%425, %426), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:537:0
  %428 : Tensor = prim::GetAttr[name="bias"](%410)
  %429 : Tensor = prim::GetAttr[name="weight"](%410)
  %430 : Float(768:1, 768:768) = aten::t(%429), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %430), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %432 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %428, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1678:0
  %433 : int[] = prim::ListConstruct(%413, %415, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %434 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%432, %433), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.3, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.2 : Float(12:64, 64:1) = aten::mul(%409, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:542:0
  %437 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_w_bias.2, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:544:0
  %438 : Tensor[] = prim::ListConstruct(%437, %427), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %content_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%34, %438), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %v.2 : Float(12:64, 64:1) = aten::mul(%408, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:486:0
  %441 : Tensor[] = prim::ListConstruct(%148, %407), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %442 : Float(26:768, 12:64, 64:1) = aten::einsum(%35, %441), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %443 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %v.2, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:493:0
  %444 : Tensor[] = prim::ListConstruct(%443, %442), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.7 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%36, %444), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %446 : int = aten::size(%positional_attn.7, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %447 : int = aten::size(%positional_attn.7, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %448 : int = aten::size(%positional_attn.7, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %449 : int = aten::size(%positional_attn.7, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.2 : Long() = prim::NumToTensor(%449), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %451 : int[] = prim::ListConstruct(%446, %447, %449, %448), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.8 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.7, %451), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:428:0
  %453 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.8, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %454 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%453, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %455 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%454, %19, %49, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.9 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%455, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %457 : Long() = aten::sub(%max_rel_len.2, %14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %458 : int = aten::Int(%457), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %459 : int[] = prim::ListConstruct(%446, %447, %448, %458), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.10 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.9, %459), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.11 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.10, %37, %50, %415, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.12 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.11, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:498:0
  %463 : int = aten::size(%token_type_mat.2, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %464 : int = aten::size(%token_type_mat.2, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %465 : int = aten::size(%token_type_mat.2, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.2 : Float(12:64, 64:1) = aten::mul(%406, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:508:0
  %467 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_s_bias.2, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:511:0
  %468 : Tensor[] = prim::ListConstruct(%467, %405), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %469 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%38, %468), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %470 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %471 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%470, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %472 : int = aten::size(%q_head.4, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %473 : int[] = prim::ListConstruct(%463, %472, %464, %465), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %token_type_mat.4 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%471, %473, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %475 : Tensor[] = aten::split(%469, %49, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:371:0
  %diff_token_type.2 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.2 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%475), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %478 : int = aten::size(%token_type_mat.4, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %479 : int = aten::size(%token_type_mat.4, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %480 : int = aten::size(%token_type_mat.4, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %481 : int = aten::size(%token_type_mat.4, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %482 : int[] = prim::ListConstruct(%478, %479, %480, %481), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %483 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.2, %482, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %484 : int = aten::size(%token_type_mat.4, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %485 : int = aten::size(%token_type_mat.4, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %486 : int = aten::size(%token_type_mat.4, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %487 : int = aten::size(%token_type_mat.4, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %488 : int[] = prim::ListConstruct(%484, %485, %486, %487), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %489 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.2, %488, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.4, %483, %489), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.3, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:522:0
  %492 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.2, %positional_attn.12, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%492, %token_type_attn.4, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.4, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:553:0
  %495 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %496 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%495, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %497 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%496, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %498 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%497, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %499 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%498, %49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:396:0
  %500 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%499, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.5, %500, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %input.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.6, %41, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:558:0
  %503 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.15, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %504 : Tensor[] = prim::ListConstruct(%503, %434), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %attn_vec.2 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%40, %504), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %506 : int[] = prim::ListConstruct(%413, %414, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %input.16 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.2, %506), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:565:0
  %508 : Tensor = prim::GetAttr[name="bias"](%404)
  %509 : Tensor = prim::GetAttr[name="weight"](%404)
  %510 : Float(768:1, 768:768) = aten::t(%509), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.16, %510), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %508, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.17, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::add(%query.1, %attn_out.2, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:568:0
  %515 : Tensor = prim::GetAttr[name="bias"](%403)
  %516 : Tensor = prim::GetAttr[name="weight"](%403)
  %517 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.18, %517, %516, %515, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %519 : __torch__.torch.nn.modules.normalization.___torch_mangle_1975.LayerNorm = prim::GetAttr[name="layer_norm"](%401)
  %520 : __torch__.torch.nn.modules.linear.___torch_mangle_1973.Linear = prim::GetAttr[name="linear_2"](%401)
  %521 : __torch__.torch.nn.modules.linear.___torch_mangle_1971.Linear = prim::GetAttr[name="linear_1"](%401)
  %522 : Tensor = prim::GetAttr[name="bias"](%521)
  %523 : Tensor = prim::GetAttr[name="weight"](%521)
  %524 : Float(768:1, 3072:768) = aten::t(%523), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.9 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.19, %524), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.2 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.9, %522, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %527 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.2, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %528 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.2, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %529 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%528, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %530 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.2, %529, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %531 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%530, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %532 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%531), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %533 : Float(17:39936, 13:3072, 3072:1) = aten::add(%532, %29, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.20 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%527, %533), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.20, %30, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %536 : Tensor = prim::GetAttr[name="bias"](%520)
  %537 : Tensor = prim::GetAttr[name="weight"](%520)
  %538 : Float(3072:1, 768:3072) = aten::t(%537), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.21, %538), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %536, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.22, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add(%input.19, %h.2, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/modeling_funnel.py:588:0
  %543 : Tensor = prim::GetAttr[name="bias"](%519)
  %544 : Tensor = prim::GetAttr[name="weight"](%519)
  %545 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm
  %query.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.23, %545, %544, %543, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %547 : __torch__.transformers.modeling_funnel.___torch_mangle_1991.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%96)
  %548 : __torch__.transformers.modeling_funnel.___torch_mangle_1985.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%96)
  %549 : __torch__.torch.nn.modules.normalization.___torch_mangle_1984.LayerNorm = prim::GetAttr[name="layer_norm"](%548)
  %550 : __torch__.torch.nn.modules.linear.___torch_mangle_1983.Linear = prim::GetAttr[name="post_proj"](%548)
  %551 : Tensor = prim::GetAttr[name="seg_embed"](%548)
  %552 : Tensor = prim::GetAttr[name="r_s_bias"](%548)
  %553 : Tensor = prim::GetAttr[name="r_kernel"](%548)
  %554 : Tensor = prim::GetAttr[name="r_r_bias"](%548)
  %555 : Tensor = prim::GetAttr[name="r_w_bias"](%548)
  %556 : __torch__.torch.nn.modules.linear.___torch_mangle_1982.Linear = prim::GetAttr[name="v_head"](%548)
  %557 : __torch__.torch.nn.modules.linear.___torch_mangle_1981.Linear = prim::GetAttr[name="k_head"](%548)
  %558 : __torch__.torch.nn.modules.linear.___torch_mangle_1980.Linear = prim::GetAttr[name="q_head"](%548)
  %559 : int = aten::size(%query.2, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %560 : int = aten::size(%query.2, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %561 : int = aten::size(%query.2, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:531:0
  %562 : Tensor = prim::GetAttr[name="weight"](%558)
  %563 : Float(768:1, 768:768) = aten::t(%562), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %564 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %563), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %565 : int[] = prim::ListConstruct(%559, %560, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %q_head.5 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%564, %565), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:535:0
  %567 : Tensor = prim::GetAttr[name="bias"](%557)
  %568 : Tensor = prim::GetAttr[name="weight"](%557)
  %569 : Float(768:1, 768:768) = aten::t(%568), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.11 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %569), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %571 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.11, %567, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1678:0
  %572 : int[] = prim::ListConstruct(%559, %561, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %573 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%571, %572), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:537:0
  %574 : Tensor = prim::GetAttr[name="bias"](%556)
  %575 : Tensor = prim::GetAttr[name="weight"](%556)
  %576 : Float(768:1, 768:768) = aten::t(%575), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %576), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %578 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %574, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1678:0
  %579 : int[] = prim::ListConstruct(%559, %561, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %580 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%578, %579), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.5, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.3 : Float(12:64, 64:1) = aten::mul(%555, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:542:0
  %583 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_w_bias.3, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:544:0
  %584 : Tensor[] = prim::ListConstruct(%583, %573), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %content_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%34, %584), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %v.3 : Float(12:64, 64:1) = aten::mul(%554, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:486:0
  %587 : Tensor[] = prim::ListConstruct(%148, %553), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %588 : Float(26:768, 12:64, 64:1) = aten::einsum(%35, %587), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %589 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %v.3, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:493:0
  %590 : Tensor[] = prim::ListConstruct(%589, %588), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.13 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%36, %590), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %592 : int = aten::size(%positional_attn.13, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %593 : int = aten::size(%positional_attn.13, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %594 : int = aten::size(%positional_attn.13, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %595 : int = aten::size(%positional_attn.13, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.3 : Long() = prim::NumToTensor(%595), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %597 : int[] = prim::ListConstruct(%592, %593, %595, %594), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.14 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.13, %597), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:428:0
  %599 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.14, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %600 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%599, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %601 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%600, %19, %49, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.15 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%601, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %603 : Long() = aten::sub(%max_rel_len.3, %14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %604 : int = aten::Int(%603), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %605 : int[] = prim::ListConstruct(%592, %593, %594, %604), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.16 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.15, %605), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.17 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.16, %37, %50, %561, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.18 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.17, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:498:0
  %609 : int = aten::size(%token_type_mat.2, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %610 : int = aten::size(%token_type_mat.2, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %611 : int = aten::size(%token_type_mat.2, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.3 : Float(12:64, 64:1) = aten::mul(%552, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:508:0
  %613 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_s_bias.3, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:511:0
  %614 : Tensor[] = prim::ListConstruct(%613, %551), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %615 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%38, %614), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %616 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %617 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%616, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %618 : int = aten::size(%q_head.6, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %619 : int[] = prim::ListConstruct(%609, %618, %610, %611), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %token_type_mat.5 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%617, %619, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %621 : Tensor[] = aten::split(%615, %49, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:371:0
  %diff_token_type.3 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.3 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%621), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %624 : int = aten::size(%token_type_mat.5, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %625 : int = aten::size(%token_type_mat.5, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %626 : int = aten::size(%token_type_mat.5, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %627 : int = aten::size(%token_type_mat.5, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %628 : int[] = prim::ListConstruct(%624, %625, %626, %627), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %629 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.3, %628, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %630 : int = aten::size(%token_type_mat.5, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %631 : int = aten::size(%token_type_mat.5, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %632 : int = aten::size(%token_type_mat.5, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %633 : int = aten::size(%token_type_mat.5, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %634 : int[] = prim::ListConstruct(%630, %631, %632, %633), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %635 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.3, %634, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.5, %629, %635), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.5, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:522:0
  %638 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.3, %positional_attn.18, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%638, %token_type_attn.6, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.7, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:553:0
  %641 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %642 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%641, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %643 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%642, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %644 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%643, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %645 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%644, %49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:396:0
  %646 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%645, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.8, %646, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %input.24 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.9, %41, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:558:0
  %649 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.24, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %650 : Tensor[] = prim::ListConstruct(%649, %580), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %attn_vec.3 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%40, %650), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %652 : int[] = prim::ListConstruct(%559, %560, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.3, %652), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:565:0
  %654 : Tensor = prim::GetAttr[name="bias"](%550)
  %655 : Tensor = prim::GetAttr[name="weight"](%550)
  %656 : Float(768:1, 768:768) = aten::t(%655), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %656), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.26 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %654, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.26, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.27 : Float(17:9984, 13:768, 768:1) = aten::add(%query.2, %attn_out.3, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:568:0
  %661 : Tensor = prim::GetAttr[name="bias"](%549)
  %662 : Tensor = prim::GetAttr[name="weight"](%549)
  %663 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.27, %663, %662, %661, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %665 : __torch__.torch.nn.modules.normalization.___torch_mangle_1990.LayerNorm = prim::GetAttr[name="layer_norm"](%547)
  %666 : __torch__.torch.nn.modules.linear.___torch_mangle_1988.Linear = prim::GetAttr[name="linear_2"](%547)
  %667 : __torch__.torch.nn.modules.linear.___torch_mangle_1986.Linear = prim::GetAttr[name="linear_1"](%547)
  %668 : Tensor = prim::GetAttr[name="bias"](%667)
  %669 : Tensor = prim::GetAttr[name="weight"](%667)
  %670 : Float(768:1, 3072:768) = aten::t(%669), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.14 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.28, %670), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.3 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.14, %668, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %673 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.3, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %674 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.3, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %675 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%674, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %676 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.3, %675, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %677 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%676, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %678 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%677), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %679 : Float(17:39936, 13:3072, 3072:1) = aten::add(%678, %29, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.29 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%673, %679), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.30 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.29, %30, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %682 : Tensor = prim::GetAttr[name="bias"](%666)
  %683 : Tensor = prim::GetAttr[name="weight"](%666)
  %684 : Float(3072:1, 768:3072) = aten::t(%683), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.30, %684), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %682, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.31, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.32 : Float(17:9984, 13:768, 768:1) = aten::add(%input.28, %h.3, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/modeling_funnel.py:588:0
  %689 : Tensor = prim::GetAttr[name="bias"](%665)
  %690 : Tensor = prim::GetAttr[name="weight"](%665)
  %691 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm
  %query.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.32, %691, %690, %689, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %693 : __torch__.transformers.modeling_funnel.___torch_mangle_2006.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%93)
  %694 : __torch__.transformers.modeling_funnel.___torch_mangle_2000.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%93)
  %695 : __torch__.torch.nn.modules.normalization.___torch_mangle_1999.LayerNorm = prim::GetAttr[name="layer_norm"](%694)
  %696 : __torch__.torch.nn.modules.linear.___torch_mangle_1998.Linear = prim::GetAttr[name="post_proj"](%694)
  %697 : Tensor = prim::GetAttr[name="seg_embed"](%694)
  %698 : Tensor = prim::GetAttr[name="r_s_bias"](%694)
  %699 : Tensor = prim::GetAttr[name="r_kernel"](%694)
  %700 : Tensor = prim::GetAttr[name="r_r_bias"](%694)
  %701 : Tensor = prim::GetAttr[name="r_w_bias"](%694)
  %702 : __torch__.torch.nn.modules.linear.___torch_mangle_1997.Linear = prim::GetAttr[name="v_head"](%694)
  %703 : __torch__.torch.nn.modules.linear.___torch_mangle_1996.Linear = prim::GetAttr[name="k_head"](%694)
  %704 : __torch__.torch.nn.modules.linear.___torch_mangle_1995.Linear = prim::GetAttr[name="q_head"](%694)
  %705 : int = aten::size(%query.3, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %706 : int = aten::size(%query.3, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %707 : int = aten::size(%query.3, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:531:0
  %708 : Tensor = prim::GetAttr[name="weight"](%704)
  %709 : Float(768:1, 768:768) = aten::t(%708), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %710 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %709), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %711 : int[] = prim::ListConstruct(%705, %706, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %q_head.7 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%710, %711), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:535:0
  %713 : Tensor = prim::GetAttr[name="bias"](%703)
  %714 : Tensor = prim::GetAttr[name="weight"](%703)
  %715 : Float(768:1, 768:768) = aten::t(%714), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %715), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %717 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %713, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1678:0
  %718 : int[] = prim::ListConstruct(%705, %707, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %719 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%717, %718), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:537:0
  %720 : Tensor = prim::GetAttr[name="bias"](%702)
  %721 : Tensor = prim::GetAttr[name="weight"](%702)
  %722 : Float(768:1, 768:768) = aten::t(%721), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.17 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %722), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %724 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.17, %720, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1678:0
  %725 : int[] = prim::ListConstruct(%705, %707, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %726 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%724, %725), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.7, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.4 : Float(12:64, 64:1) = aten::mul(%701, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:542:0
  %729 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_w_bias.4, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:544:0
  %730 : Tensor[] = prim::ListConstruct(%729, %719), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %content_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%34, %730), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %v.4 : Float(12:64, 64:1) = aten::mul(%700, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:486:0
  %733 : Tensor[] = prim::ListConstruct(%148, %699), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %734 : Float(26:768, 12:64, 64:1) = aten::einsum(%35, %733), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %735 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %v.4, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:493:0
  %736 : Tensor[] = prim::ListConstruct(%735, %734), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.19 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%36, %736), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %738 : int = aten::size(%positional_attn.19, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %739 : int = aten::size(%positional_attn.19, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %740 : int = aten::size(%positional_attn.19, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %741 : int = aten::size(%positional_attn.19, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.4 : Long() = prim::NumToTensor(%741), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %743 : int[] = prim::ListConstruct(%738, %739, %741, %740), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.20 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.19, %743), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:428:0
  %745 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.20, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %746 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%745, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %747 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%746, %19, %49, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.21 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%747, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %749 : Long() = aten::sub(%max_rel_len.4, %14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %750 : int = aten::Int(%749), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %751 : int[] = prim::ListConstruct(%738, %739, %740, %750), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.22 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.21, %751), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.23 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.22, %37, %50, %707, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.24 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.23, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:498:0
  %755 : int = aten::size(%token_type_mat.2, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %756 : int = aten::size(%token_type_mat.2, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %757 : int = aten::size(%token_type_mat.2, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.4 : Float(12:64, 64:1) = aten::mul(%698, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:508:0
  %759 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_s_bias.4, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:511:0
  %760 : Tensor[] = prim::ListConstruct(%759, %697), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %761 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%38, %760), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %762 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %763 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%762, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %764 : int = aten::size(%q_head.8, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %765 : int[] = prim::ListConstruct(%755, %764, %756, %757), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %token_type_mat.6 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%763, %765, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %767 : Tensor[] = aten::split(%761, %49, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:371:0
  %diff_token_type.4 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.4 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%767), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %770 : int = aten::size(%token_type_mat.6, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %771 : int = aten::size(%token_type_mat.6, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %772 : int = aten::size(%token_type_mat.6, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %773 : int = aten::size(%token_type_mat.6, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %774 : int[] = prim::ListConstruct(%770, %771, %772, %773), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %775 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.4, %774, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %776 : int = aten::size(%token_type_mat.6, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %777 : int = aten::size(%token_type_mat.6, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %778 : int = aten::size(%token_type_mat.6, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %779 : int = aten::size(%token_type_mat.6, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %780 : int[] = prim::ListConstruct(%776, %777, %778, %779), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %781 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.4, %780, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.6, %775, %781), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.7, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:522:0
  %784 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.4, %positional_attn.24, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%784, %token_type_attn.8, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.10, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:553:0
  %787 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %788 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%787, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %789 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%788, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %790 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%789, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %791 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%790, %49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:396:0
  %792 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%791, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.11, %792, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %input.33 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.12, %41, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:558:0
  %795 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.33, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %796 : Tensor[] = prim::ListConstruct(%795, %726), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %attn_vec.4 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%40, %796), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %798 : int[] = prim::ListConstruct(%705, %706, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.4, %798), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:565:0
  %800 : Tensor = prim::GetAttr[name="bias"](%696)
  %801 : Tensor = prim::GetAttr[name="weight"](%696)
  %802 : Float(768:1, 768:768) = aten::t(%801), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.34, %802), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %800, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.35, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.36 : Float(17:9984, 13:768, 768:1) = aten::add(%query.3, %attn_out.4, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:568:0
  %807 : Tensor = prim::GetAttr[name="bias"](%695)
  %808 : Tensor = prim::GetAttr[name="weight"](%695)
  %809 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm
  %input.37 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.36, %809, %808, %807, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %811 : __torch__.torch.nn.modules.normalization.___torch_mangle_2005.LayerNorm = prim::GetAttr[name="layer_norm"](%693)
  %812 : __torch__.torch.nn.modules.linear.___torch_mangle_2003.Linear = prim::GetAttr[name="linear_2"](%693)
  %813 : __torch__.torch.nn.modules.linear.___torch_mangle_2001.Linear = prim::GetAttr[name="linear_1"](%693)
  %814 : Tensor = prim::GetAttr[name="bias"](%813)
  %815 : Tensor = prim::GetAttr[name="weight"](%813)
  %816 : Float(768:1, 3072:768) = aten::t(%815), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.37, %816), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.4 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.19, %814, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %819 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.4, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %820 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.4, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %821 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%820, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %822 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.4, %821, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %823 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%822, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %824 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%823), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %825 : Float(17:39936, 13:3072, 3072:1) = aten::add(%824, %29, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.38 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%819, %825), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.39 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.38, %30, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %828 : Tensor = prim::GetAttr[name="bias"](%812)
  %829 : Tensor = prim::GetAttr[name="weight"](%812)
  %830 : Float(3072:1, 768:3072) = aten::t(%829), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.39, %830), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %828, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.40, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.41 : Float(17:9984, 13:768, 768:1) = aten::add(%input.37, %h.4, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/modeling_funnel.py:588:0
  %835 : Tensor = prim::GetAttr[name="bias"](%811)
  %836 : Tensor = prim::GetAttr[name="weight"](%811)
  %837 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm
  %hidden.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.41, %837, %836, %835, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %839 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %840 : Bool(17:169, 1:13, 13:1) = aten::slice(%839, %49, %50, %49, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %841 : Tensor[] = prim::ListConstruct(%840, %token_type_mat.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.1 : Bool(17:182, 14:13, 13:1) = aten::cat(%841, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %843 : Bool(17:182, 14:13, 13:1) = aten::slice(%tensor.1, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.7 : Bool(17:182, 7:26, 13:1) = aten::slice(%843, %49, %50, %41, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %845 : Float(1:13, 13:1) = aten::slice(%cls_mask.1, %50, %50, %49, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %846 : Tensor[] = prim::ListConstruct(%845, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder
  %tensor.2 : Float(14:13, 13:1) = aten::cat(%846, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.2 : Float(7:26, 13:1) = aten::slice(%tensor.2, %50, %50, %41, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %849 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden.1, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.1 : Float(17:9984, 12:768, 768:1) = aten::slice(%849, %49, %50, %41, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %851 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden.1, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %852 : Float(17:9984, 1:768, 768:1) = aten::slice(%851, %49, %50, %49, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %853 : Tensor[] = prim::ListConstruct(%852, %suffix.1), scope: __module.funnel/__module.funnel.encoder
  %tensor.3 : Float(17:9984, 13:768, 768:1) = aten::cat(%853, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %855 : Float(17:9984, 13:768, 768:1) = aten::slice(%tensor.3, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %856 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::unsqueeze(%855, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %857 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%856, %19, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %tensor.4 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%857, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %859 : int[] = prim::ListConstruct(%19, %49), scope: __module.funnel/__module.funnel.encoder
  %860 : int[] = prim::ListConstruct(%19, %49), scope: __module.funnel/__module.funnel.encoder
  %861 : int[] = prim::ListConstruct(%50, %50), scope: __module.funnel/__module.funnel.encoder
  %tensor.5 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::avg_pool2d(%tensor.4, %859, %860, %861, %42, %42, %13), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
  %863 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%tensor.5, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %query.4 : Float(17:5376, 7:768, 768:1) = aten::select(%863, %49, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %865 : __torch__.transformers.modeling_funnel.___torch_mangle_2022.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%90)
  %866 : __torch__.transformers.modeling_funnel.___torch_mangle_2016.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%90)
  %867 : __torch__.torch.nn.modules.normalization.___torch_mangle_2015.LayerNorm = prim::GetAttr[name="layer_norm"](%866)
  %868 : __torch__.torch.nn.modules.linear.___torch_mangle_2014.Linear = prim::GetAttr[name="post_proj"](%866)
  %869 : Tensor = prim::GetAttr[name="seg_embed"](%866)
  %870 : Tensor = prim::GetAttr[name="r_s_bias"](%866)
  %871 : Tensor = prim::GetAttr[name="r_kernel"](%866)
  %872 : Tensor = prim::GetAttr[name="r_r_bias"](%866)
  %873 : Tensor = prim::GetAttr[name="r_w_bias"](%866)
  %874 : __torch__.torch.nn.modules.linear.___torch_mangle_2013.Linear = prim::GetAttr[name="v_head"](%866)
  %875 : __torch__.torch.nn.modules.linear.___torch_mangle_2012.Linear = prim::GetAttr[name="k_head"](%866)
  %876 : __torch__.torch.nn.modules.linear.___torch_mangle_2011.Linear = prim::GetAttr[name="q_head"](%866)
  %877 : int = aten::size(%query.4, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %878 : int = aten::size(%query.4, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %879 : int = aten::size(%hidden.1, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:531:0
  %880 : Tensor = prim::GetAttr[name="weight"](%876)
  %881 : Float(768:1, 768:768) = aten::t(%880), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %882 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.4, %881), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %883 : int[] = prim::ListConstruct(%877, %878, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %q_head.9 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%882, %883), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:535:0
  %885 : Tensor = prim::GetAttr[name="bias"](%875)
  %886 : Tensor = prim::GetAttr[name="weight"](%875)
  %887 : Float(768:1, 768:768) = aten::t(%886), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %887), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %889 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %885, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1678:0
  %890 : int[] = prim::ListConstruct(%877, %879, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %891 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%889, %890), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:537:0
  %892 : Tensor = prim::GetAttr[name="bias"](%874)
  %893 : Tensor = prim::GetAttr[name="weight"](%874)
  %894 : Float(768:1, 768:768) = aten::t(%893), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %894), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %896 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %892, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1678:0
  %897 : int[] = prim::ListConstruct(%877, %879, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %898 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%896, %897), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.10 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.9, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.5 : Float(12:64, 64:1) = aten::mul(%873, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:542:0
  %901 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_w_bias.5, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:544:0
  %902 : Tensor[] = prim::ListConstruct(%901, %891), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %content_score.5 : Float(17:1092, 12:91, 7:13, 13:1) = aten::einsum(%34, %902), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %v.5 : Float(12:64, 64:1) = aten::mul(%872, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:486:0
  %905 : Tensor[] = prim::ListConstruct(%172, %871), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %906 : Float(27:768, 12:64, 64:1) = aten::einsum(%35, %905), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %907 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %v.5, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:493:0
  %908 : Tensor[] = prim::ListConstruct(%907, %906), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.25 : Float(17:189, 12:3213, 7:27, 27:1) = aten::einsum(%36, %908), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %910 : int = aten::size(%positional_attn.25, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %911 : int = aten::size(%positional_attn.25, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %912 : int = aten::size(%positional_attn.25, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %913 : int = aten::size(%positional_attn.25, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.5 : Long() = prim::NumToTensor(%913), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %915 : int[] = prim::ListConstruct(%910, %911, %913, %912), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.26 : Float(17:189, 12:3213, 27:7, 7:1) = aten::reshape(%positional_attn.25, %915), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:428:0
  %917 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%positional_attn.26, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %918 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%917, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %919 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%918, %19, %19, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.27 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%919, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %921 : Long() = aten::sub(%max_rel_len.5, %15, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %922 : int = aten::Int(%921), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %923 : int[] = prim::ListConstruct(%910, %911, %912, %922), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.28 : Float(17:189, 12:3213, 7:25, 25:1) = aten::reshape(%positional_attn.27, %923), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.29 : Float(17:189, 12:3213, 7:25, 13:1) = aten::slice(%positional_attn.28, %37, %50, %879, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.30 : Float(17:189, 12:3213, 7:25, 13:1) = aten::mul_(%positional_attn.29, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:498:0
  %927 : int = aten::size(%token_type_mat.7, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %928 : int = aten::size(%token_type_mat.7, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %929 : int = aten::size(%token_type_mat.7, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.5 : Float(12:64, 64:1) = aten::mul(%870, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:508:0
  %931 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_s_bias.5, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:511:0
  %932 : Tensor[] = prim::ListConstruct(%931, %869), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %933 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%38, %932), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %934 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %935 : Bool(17:182, 1:182, 7:26, 13:1) = aten::unsqueeze(%934, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %936 : int = aten::size(%q_head.10, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %937 : int[] = prim::ListConstruct(%927, %936, %928, %929), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %token_type_mat.8 : Bool(17:182, 12:0, 7:26, 13:1) = aten::expand(%935, %937, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %939 : Tensor[] = aten::split(%933, %49, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:371:0
  %diff_token_type.5 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.5 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%939), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %942 : int = aten::size(%token_type_mat.8, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %943 : int = aten::size(%token_type_mat.8, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %944 : int = aten::size(%token_type_mat.8, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %945 : int = aten::size(%token_type_mat.8, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %946 : int[] = prim::ListConstruct(%942, %943, %944, %945), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %947 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%same_token_type.5, %946, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %948 : int = aten::size(%token_type_mat.8, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %949 : int = aten::size(%token_type_mat.8, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %950 : int = aten::size(%token_type_mat.8, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %951 : int = aten::size(%token_type_mat.8, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %952 : int[] = prim::ListConstruct(%948, %949, %950, %951), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %953 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%diff_token_type.5, %952, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.9 : Float(17:1092, 12:91, 7:13, 13:1) = aten::where(%token_type_mat.8, %947, %953), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.10 : Float(17:1092, 12:91, 7:13, 13:1) = aten::mul_(%token_type_attn.9, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:522:0
  %956 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%content_score.5, %positional_attn.30, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.13 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%956, %token_type_attn.10, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.14 : Float(17:1092, 12:91, 7:13, 13:1) = aten::to(%attn_score.13, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:553:0
  %959 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %960 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%959, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %961 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%960, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %962 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%961, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %963 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%962, %49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:396:0
  %964 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%963, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.15 : Float(17:1092, 12:91, 7:13, 13:1) = aten::sub(%attn_score.14, %964, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %input.42 : Float(17:1092, 12:91, 7:13, 13:1) = aten::softmax(%attn_score.15, %41, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:558:0
  %967 : Float(17:1092, 12:91, 7:13, 13:1) = aten::dropout(%input.42, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %968 : Tensor[] = prim::ListConstruct(%967, %898), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %attn_vec.5 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%40, %968), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %970 : int[] = prim::ListConstruct(%877, %878, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %input.43 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.5, %970), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:565:0
  %972 : Tensor = prim::GetAttr[name="bias"](%868)
  %973 : Tensor = prim::GetAttr[name="weight"](%868)
  %974 : Float(768:1, 768:768) = aten::t(%973), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.23 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.43, %974), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.44 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.23, %972, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.44, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:5376, 7:768, 768:1) = aten::add(%query.4, %attn_out.5, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:568:0
  %979 : Tensor = prim::GetAttr[name="bias"](%867)
  %980 : Tensor = prim::GetAttr[name="weight"](%867)
  %981 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm
  %input.46 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.45, %981, %980, %979, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %983 : __torch__.torch.nn.modules.normalization.___torch_mangle_2021.LayerNorm = prim::GetAttr[name="layer_norm"](%865)
  %984 : __torch__.torch.nn.modules.linear.___torch_mangle_2019.Linear = prim::GetAttr[name="linear_2"](%865)
  %985 : __torch__.torch.nn.modules.linear.___torch_mangle_2017.Linear = prim::GetAttr[name="linear_1"](%865)
  %986 : Tensor = prim::GetAttr[name="bias"](%985)
  %987 : Tensor = prim::GetAttr[name="weight"](%985)
  %988 : Float(768:1, 3072:768) = aten::t(%987), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.46, %988), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.5 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.24, %986, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %991 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.5, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %992 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.5, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %993 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%992, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %994 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.5, %993, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %995 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%994, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %996 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%995), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %997 : Float(17:21504, 7:3072, 3072:1) = aten::add(%996, %29, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.47 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%991, %997), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.48 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.47, %30, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1000 : Tensor = prim::GetAttr[name="bias"](%984)
  %1001 : Tensor = prim::GetAttr[name="weight"](%984)
  %1002 : Float(3072:1, 768:3072) = aten::t(%1001), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.25 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.48, %1002), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.49 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.25, %1000, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.49, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:5376, 7:768, 768:1) = aten::add(%input.46, %h.5, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/modeling_funnel.py:588:0
  %1007 : Tensor = prim::GetAttr[name="bias"](%983)
  %1008 : Tensor = prim::GetAttr[name="weight"](%983)
  %1009 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm
  %query.5 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.50, %1009, %1008, %1007, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1011 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1012 : Bool(17:182, 7:26, 13:1) = aten::slice(%1011, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1013 : Bool(17:182, 7:26, 1:1) = aten::slice(%1012, %19, %50, %49, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1014 : Tensor[] = prim::ListConstruct(%1013, %token_type_mat.7), scope: __module.funnel/__module.funnel.encoder
  %tensor.6 : Bool(17:98, 7:14, 14:1) = aten::cat(%1014, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1016 : Bool(17:98, 7:14, 14:1) = aten::slice(%tensor.6, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1017 : Bool(17:98, 7:14, 14:1) = aten::slice(%1016, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.9 : Bool(17:98, 7:14, 7:2) = aten::slice(%1017, %19, %50, %41, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1019 : Float(7:26, 13:1) = aten::slice(%cls_mask.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1020 : Float(7:26, 1:1) = aten::slice(%1019, %49, %50, %49, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1021 : Tensor[] = prim::ListConstruct(%1020, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.7 : Float(7:14, 14:1) = aten::cat(%1021, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1023 : Float(7:14, 14:1) = aten::slice(%tensor.7, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask.3 : Float(7:14, 7:2) = aten::slice(%1023, %49, %50, %41, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1025 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.2 : Float(17:13, 12:1) = aten::slice(%1025, %49, %50, %41, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1027 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1028 : Float(17:13, 1:1) = aten::slice(%1027, %49, %50, %49, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1029 : Tensor[] = prim::ListConstruct(%1028, %suffix.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.8 : Float(17:13, 13:1) = aten::cat(%1029, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1031 : Float(17:13, 13:1) = aten::slice(%tensor.8, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1032 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%1031, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1033 : Float(17:13, 1:13, 13:1) = aten::slice(%1032, %19, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %tensor.9 : Float(17:13, 1:13, 13:1, 1:1) = aten::unsqueeze(%1033, %37), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %input.51 : Float(17:13, 1:13, 13:1, 1:1) = aten::neg(%tensor.9), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1036 : int[] = prim::ListConstruct(%19, %49), scope: __module.funnel/__module.funnel.encoder
  %1037 : int[] = prim::ListConstruct(%19, %49), scope: __module.funnel/__module.funnel.encoder
  %1038 : int[] = prim::ListConstruct(%50, %50), scope: __module.funnel/__module.funnel.encoder
  %1039 : int[] = prim::ListConstruct(%49, %49), scope: __module.funnel/__module.funnel.encoder
  %1040 : Float(17:7, 1:7, 7:1, 1:1) = aten::max_pool2d(%input.51, %1036, %1037, %1038, %1039, %42), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
  %tensor.10 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%1040), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1042 : Float(17:7, 1:7, 7:1, 1:1) = aten::slice(%tensor.10, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1043 : Float(17:7, 7:1, 1:1) = aten::select(%1042, %49, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1044 : Float(17:7, 7:1, 1:1) = aten::slice(%1043, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask.3 : Float(17:7, 7:1) = aten::select(%1044, %19, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1046 : __torch__.transformers.modeling_funnel.___torch_mangle_2037.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%87)
  %1047 : __torch__.transformers.modeling_funnel.___torch_mangle_2031.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%87)
  %1048 : __torch__.torch.nn.modules.normalization.___torch_mangle_2030.LayerNorm = prim::GetAttr[name="layer_norm"](%1047)
  %1049 : __torch__.torch.nn.modules.linear.___torch_mangle_2029.Linear = prim::GetAttr[name="post_proj"](%1047)
  %1050 : Tensor = prim::GetAttr[name="seg_embed"](%1047)
  %1051 : Tensor = prim::GetAttr[name="r_s_bias"](%1047)
  %1052 : Tensor = prim::GetAttr[name="r_kernel"](%1047)
  %1053 : Tensor = prim::GetAttr[name="r_r_bias"](%1047)
  %1054 : Tensor = prim::GetAttr[name="r_w_bias"](%1047)
  %1055 : __torch__.torch.nn.modules.linear.___torch_mangle_2028.Linear = prim::GetAttr[name="v_head"](%1047)
  %1056 : __torch__.torch.nn.modules.linear.___torch_mangle_2027.Linear = prim::GetAttr[name="k_head"](%1047)
  %1057 : __torch__.torch.nn.modules.linear.___torch_mangle_2026.Linear = prim::GetAttr[name="q_head"](%1047)
  %1058 : int = aten::size(%query.5, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1059 : int = aten::size(%query.5, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1060 : int = aten::size(%query.5, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:531:0
  %1061 : Tensor = prim::GetAttr[name="weight"](%1057)
  %1062 : Float(768:1, 768:768) = aten::t(%1061), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1063 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1062), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1064 : int[] = prim::ListConstruct(%1058, %1059, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %q_head.11 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1063, %1064), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:535:0
  %1066 : Tensor = prim::GetAttr[name="bias"](%1056)
  %1067 : Tensor = prim::GetAttr[name="weight"](%1056)
  %1068 : Float(768:1, 768:768) = aten::t(%1067), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.26 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1068), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %1070 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.26, %1066, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1678:0
  %1071 : int[] = prim::ListConstruct(%1058, %1060, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1072 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1070, %1071), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:537:0
  %1073 : Tensor = prim::GetAttr[name="bias"](%1055)
  %1074 : Tensor = prim::GetAttr[name="weight"](%1055)
  %1075 : Float(768:1, 768:768) = aten::t(%1074), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.27 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1075), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %1077 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.27, %1073, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1678:0
  %1078 : int[] = prim::ListConstruct(%1058, %1060, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1079 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1077, %1078), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.12 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.11, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.6 : Float(12:64, 64:1) = aten::mul(%1054, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:542:0
  %1082 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_w_bias.6, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:544:0
  %1083 : Tensor[] = prim::ListConstruct(%1082, %1072), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %content_score.6 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%34, %1083), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %v.6 : Float(12:64, 64:1) = aten::mul(%1053, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:486:0
  %1086 : Tensor[] = prim::ListConstruct(%190, %1052), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1087 : Float(14:768, 12:64, 64:1) = aten::einsum(%35, %1086), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1088 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %v.6, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:493:0
  %1089 : Tensor[] = prim::ListConstruct(%1088, %1087), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.31 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%36, %1089), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1091 : int = aten::size(%positional_attn.31, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1092 : int = aten::size(%positional_attn.31, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1093 : int = aten::size(%positional_attn.31, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1094 : int = aten::size(%positional_attn.31, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.6 : Long() = prim::NumToTensor(%1094), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1096 : int[] = prim::ListConstruct(%1091, %1092, %1094, %1093), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.32 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.31, %1096), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:428:0
  %1098 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.32, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1099 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1098, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1100 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1099, %19, %49, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.33 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1100, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1102 : Long() = aten::sub(%max_rel_len.6, %14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %1103 : int = aten::Int(%1102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1104 : int[] = prim::ListConstruct(%1091, %1092, %1093, %1103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.34 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.33, %1104), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.35 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.34, %37, %50, %1060, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.36 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.35, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:498:0
  %1108 : int = aten::size(%token_type_mat.9, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1109 : int = aten::size(%token_type_mat.9, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1110 : int = aten::size(%token_type_mat.9, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.6 : Float(12:64, 64:1) = aten::mul(%1051, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:508:0
  %1112 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_s_bias.6, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:511:0
  %1113 : Tensor[] = prim::ListConstruct(%1112, %1050), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1114 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%38, %1113), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1115 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1116 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1115, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1117 : int = aten::size(%q_head.12, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1118 : int[] = prim::ListConstruct(%1108, %1117, %1109, %1110), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %token_type_mat.10 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1116, %1118, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1120 : Tensor[] = aten::split(%1114, %49, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:371:0
  %diff_token_type.6 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.6 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1120), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1123 : int = aten::size(%token_type_mat.10, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1124 : int = aten::size(%token_type_mat.10, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1125 : int = aten::size(%token_type_mat.10, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1126 : int = aten::size(%token_type_mat.10, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1127 : int[] = prim::ListConstruct(%1123, %1124, %1125, %1126), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1128 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.6, %1127, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1129 : int = aten::size(%token_type_mat.10, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1130 : int = aten::size(%token_type_mat.10, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1131 : int = aten::size(%token_type_mat.10, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1132 : int = aten::size(%token_type_mat.10, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1133 : int[] = prim::ListConstruct(%1129, %1130, %1131, %1132), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1134 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.6, %1133, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.11 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.10, %1128, %1134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.12 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.11, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:522:0
  %1137 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.6, %positional_attn.36, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1137, %token_type_attn.12, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.17 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.16, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:553:0
  %1140 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1141 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1140, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1142 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1141, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1143 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1142, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1144 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1143, %49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:396:0
  %1145 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1144, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.18 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.17, %1145, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %input.52 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.18, %41, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:558:0
  %1148 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.52, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1149 : Tensor[] = prim::ListConstruct(%1148, %1079), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %attn_vec.6 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%40, %1149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1151 : int[] = prim::ListConstruct(%1058, %1059, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %input.53 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.6, %1151), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:565:0
  %1153 : Tensor = prim::GetAttr[name="bias"](%1049)
  %1154 : Tensor = prim::GetAttr[name="weight"](%1049)
  %1155 : Float(768:1, 768:768) = aten::t(%1154), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.28 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.53, %1155), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.54 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.28, %1153, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.54, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.55 : Float(17:5376, 7:768, 768:1) = aten::add(%query.5, %attn_out.6, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:568:0
  %1160 : Tensor = prim::GetAttr[name="bias"](%1048)
  %1161 : Tensor = prim::GetAttr[name="weight"](%1048)
  %1162 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm
  %input.56 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.55, %1162, %1161, %1160, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1164 : __torch__.torch.nn.modules.normalization.___torch_mangle_2036.LayerNorm = prim::GetAttr[name="layer_norm"](%1046)
  %1165 : __torch__.torch.nn.modules.linear.___torch_mangle_2034.Linear = prim::GetAttr[name="linear_2"](%1046)
  %1166 : __torch__.torch.nn.modules.linear.___torch_mangle_2032.Linear = prim::GetAttr[name="linear_1"](%1046)
  %1167 : Tensor = prim::GetAttr[name="bias"](%1166)
  %1168 : Tensor = prim::GetAttr[name="weight"](%1166)
  %1169 : Float(768:1, 3072:768) = aten::t(%1168), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.56, %1169), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.6 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.29, %1167, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1172 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.6, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1173 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.6, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1174 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1173, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1175 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.6, %1174, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1176 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1175, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1177 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1176), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1178 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1177, %29, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.57 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1172, %1178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.58 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.57, %30, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1181 : Tensor = prim::GetAttr[name="bias"](%1165)
  %1182 : Tensor = prim::GetAttr[name="weight"](%1165)
  %1183 : Float(3072:1, 768:3072) = aten::t(%1182), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.58, %1183), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.59 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.30, %1181, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.59, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:5376, 7:768, 768:1) = aten::add(%input.56, %h.6, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/modeling_funnel.py:588:0
  %1188 : Tensor = prim::GetAttr[name="bias"](%1164)
  %1189 : Tensor = prim::GetAttr[name="weight"](%1164)
  %1190 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm
  %query.6 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.60, %1190, %1189, %1188, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1192 : __torch__.transformers.modeling_funnel.___torch_mangle_2052.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%84)
  %1193 : __torch__.transformers.modeling_funnel.___torch_mangle_2046.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%84)
  %1194 : __torch__.torch.nn.modules.normalization.___torch_mangle_2045.LayerNorm = prim::GetAttr[name="layer_norm"](%1193)
  %1195 : __torch__.torch.nn.modules.linear.___torch_mangle_2044.Linear = prim::GetAttr[name="post_proj"](%1193)
  %1196 : Tensor = prim::GetAttr[name="seg_embed"](%1193)
  %1197 : Tensor = prim::GetAttr[name="r_s_bias"](%1193)
  %1198 : Tensor = prim::GetAttr[name="r_kernel"](%1193)
  %1199 : Tensor = prim::GetAttr[name="r_r_bias"](%1193)
  %1200 : Tensor = prim::GetAttr[name="r_w_bias"](%1193)
  %1201 : __torch__.torch.nn.modules.linear.___torch_mangle_2043.Linear = prim::GetAttr[name="v_head"](%1193)
  %1202 : __torch__.torch.nn.modules.linear.___torch_mangle_2042.Linear = prim::GetAttr[name="k_head"](%1193)
  %1203 : __torch__.torch.nn.modules.linear.___torch_mangle_2041.Linear = prim::GetAttr[name="q_head"](%1193)
  %1204 : int = aten::size(%query.6, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1205 : int = aten::size(%query.6, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1206 : int = aten::size(%query.6, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:531:0
  %1207 : Tensor = prim::GetAttr[name="weight"](%1203)
  %1208 : Float(768:1, 768:768) = aten::t(%1207), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1209 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1208), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1210 : int[] = prim::ListConstruct(%1204, %1205, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %q_head.13 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1209, %1210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:535:0
  %1212 : Tensor = prim::GetAttr[name="bias"](%1202)
  %1213 : Tensor = prim::GetAttr[name="weight"](%1202)
  %1214 : Float(768:1, 768:768) = aten::t(%1213), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.31 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1214), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %1216 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.31, %1212, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1678:0
  %1217 : int[] = prim::ListConstruct(%1204, %1206, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1218 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1216, %1217), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:537:0
  %1219 : Tensor = prim::GetAttr[name="bias"](%1201)
  %1220 : Tensor = prim::GetAttr[name="weight"](%1201)
  %1221 : Float(768:1, 768:768) = aten::t(%1220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.32 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1221), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %1223 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.32, %1219, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1678:0
  %1224 : int[] = prim::ListConstruct(%1204, %1206, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1225 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1223, %1224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.14 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.13, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.7 : Float(12:64, 64:1) = aten::mul(%1200, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:542:0
  %1228 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_w_bias.7, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:544:0
  %1229 : Tensor[] = prim::ListConstruct(%1228, %1218), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %content_score.7 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%34, %1229), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %v.7 : Float(12:64, 64:1) = aten::mul(%1199, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:486:0
  %1232 : Tensor[] = prim::ListConstruct(%190, %1198), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1233 : Float(14:768, 12:64, 64:1) = aten::einsum(%35, %1232), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1234 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %v.7, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:493:0
  %1235 : Tensor[] = prim::ListConstruct(%1234, %1233), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.37 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%36, %1235), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1237 : int = aten::size(%positional_attn.37, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1238 : int = aten::size(%positional_attn.37, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1239 : int = aten::size(%positional_attn.37, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1240 : int = aten::size(%positional_attn.37, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.7 : Long() = prim::NumToTensor(%1240), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1242 : int[] = prim::ListConstruct(%1237, %1238, %1240, %1239), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.38 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.37, %1242), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:428:0
  %1244 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.38, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1245 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1244, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1246 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1245, %19, %49, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.39 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1246, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1248 : Long() = aten::sub(%max_rel_len.7, %14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %1249 : int = aten::Int(%1248), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1250 : int[] = prim::ListConstruct(%1237, %1238, %1239, %1249), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.40 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.39, %1250), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.41 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.40, %37, %50, %1206, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.42 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.41, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:498:0
  %1254 : int = aten::size(%token_type_mat.9, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1255 : int = aten::size(%token_type_mat.9, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1256 : int = aten::size(%token_type_mat.9, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.7 : Float(12:64, 64:1) = aten::mul(%1197, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:508:0
  %1258 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_s_bias.7, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:511:0
  %1259 : Tensor[] = prim::ListConstruct(%1258, %1196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1260 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%38, %1259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1261 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1262 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1261, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1263 : int = aten::size(%q_head.14, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1264 : int[] = prim::ListConstruct(%1254, %1263, %1255, %1256), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %token_type_mat.11 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1262, %1264, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1266 : Tensor[] = aten::split(%1260, %49, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:371:0
  %diff_token_type.7 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.7 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1269 : int = aten::size(%token_type_mat.11, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1270 : int = aten::size(%token_type_mat.11, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1271 : int = aten::size(%token_type_mat.11, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1272 : int = aten::size(%token_type_mat.11, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1273 : int[] = prim::ListConstruct(%1269, %1270, %1271, %1272), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1274 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.7, %1273, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1275 : int = aten::size(%token_type_mat.11, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1276 : int = aten::size(%token_type_mat.11, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1277 : int = aten::size(%token_type_mat.11, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1278 : int = aten::size(%token_type_mat.11, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1280 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.7, %1279, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.13 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.11, %1274, %1280), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.14 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.13, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:522:0
  %1283 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.7, %positional_attn.42, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.19 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1283, %token_type_attn.14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.20 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.19, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:553:0
  %1286 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1287 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1286, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1288 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1287, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1289 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1288, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1290 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1289, %49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:396:0
  %1291 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1290, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.21 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.20, %1291, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %input.61 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.21, %41, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:558:0
  %1294 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.61, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1295 : Tensor[] = prim::ListConstruct(%1294, %1225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %attn_vec.7 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%40, %1295), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1297 : int[] = prim::ListConstruct(%1204, %1205, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %input.62 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.7, %1297), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:565:0
  %1299 : Tensor = prim::GetAttr[name="bias"](%1195)
  %1300 : Tensor = prim::GetAttr[name="weight"](%1195)
  %1301 : Float(768:1, 768:768) = aten::t(%1300), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.33 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.62, %1301), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.63 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.33, %1299, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.63, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:5376, 7:768, 768:1) = aten::add(%query.6, %attn_out.7, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:568:0
  %1306 : Tensor = prim::GetAttr[name="bias"](%1194)
  %1307 : Tensor = prim::GetAttr[name="weight"](%1194)
  %1308 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm
  %input.65 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.64, %1308, %1307, %1306, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1310 : __torch__.torch.nn.modules.normalization.___torch_mangle_2051.LayerNorm = prim::GetAttr[name="layer_norm"](%1192)
  %1311 : __torch__.torch.nn.modules.linear.___torch_mangle_2049.Linear = prim::GetAttr[name="linear_2"](%1192)
  %1312 : __torch__.torch.nn.modules.linear.___torch_mangle_2047.Linear = prim::GetAttr[name="linear_1"](%1192)
  %1313 : Tensor = prim::GetAttr[name="bias"](%1312)
  %1314 : Tensor = prim::GetAttr[name="weight"](%1312)
  %1315 : Float(768:1, 3072:768) = aten::t(%1314), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.65, %1315), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.7 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.34, %1313, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1318 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.7, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1319 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.7, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1320 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1319, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1321 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.7, %1320, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1322 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1321, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1323 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1322), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1324 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1323, %29, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.66 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1318, %1324), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.67 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.66, %30, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1327 : Tensor = prim::GetAttr[name="bias"](%1311)
  %1328 : Tensor = prim::GetAttr[name="weight"](%1311)
  %1329 : Float(3072:1, 768:3072) = aten::t(%1328), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.67, %1329), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.68 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.35, %1327, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.68, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(17:5376, 7:768, 768:1) = aten::add(%input.65, %h.7, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/modeling_funnel.py:588:0
  %1334 : Tensor = prim::GetAttr[name="bias"](%1310)
  %1335 : Tensor = prim::GetAttr[name="weight"](%1310)
  %1336 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm
  %query.7 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.69, %1336, %1335, %1334, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1338 : __torch__.transformers.modeling_funnel.___torch_mangle_2067.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%81)
  %1339 : __torch__.transformers.modeling_funnel.___torch_mangle_2061.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%81)
  %1340 : __torch__.torch.nn.modules.normalization.___torch_mangle_2060.LayerNorm = prim::GetAttr[name="layer_norm"](%1339)
  %1341 : __torch__.torch.nn.modules.linear.___torch_mangle_2059.Linear = prim::GetAttr[name="post_proj"](%1339)
  %1342 : Tensor = prim::GetAttr[name="seg_embed"](%1339)
  %1343 : Tensor = prim::GetAttr[name="r_s_bias"](%1339)
  %1344 : Tensor = prim::GetAttr[name="r_kernel"](%1339)
  %1345 : Tensor = prim::GetAttr[name="r_r_bias"](%1339)
  %1346 : Tensor = prim::GetAttr[name="r_w_bias"](%1339)
  %1347 : __torch__.torch.nn.modules.linear.___torch_mangle_2058.Linear = prim::GetAttr[name="v_head"](%1339)
  %1348 : __torch__.torch.nn.modules.linear.___torch_mangle_2057.Linear = prim::GetAttr[name="k_head"](%1339)
  %1349 : __torch__.torch.nn.modules.linear.___torch_mangle_2056.Linear = prim::GetAttr[name="q_head"](%1339)
  %1350 : int = aten::size(%query.7, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1351 : int = aten::size(%query.7, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1352 : int = aten::size(%query.7, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:531:0
  %1353 : Tensor = prim::GetAttr[name="weight"](%1349)
  %1354 : Float(768:1, 768:768) = aten::t(%1353), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1355 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1354), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1356 : int[] = prim::ListConstruct(%1350, %1351, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %q_head.15 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1355, %1356), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:535:0
  %1358 : Tensor = prim::GetAttr[name="bias"](%1348)
  %1359 : Tensor = prim::GetAttr[name="weight"](%1348)
  %1360 : Float(768:1, 768:768) = aten::t(%1359), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.36 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1360), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %1362 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.36, %1358, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1678:0
  %1363 : int[] = prim::ListConstruct(%1350, %1352, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1364 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1362, %1363), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:537:0
  %1365 : Tensor = prim::GetAttr[name="bias"](%1347)
  %1366 : Tensor = prim::GetAttr[name="weight"](%1347)
  %1367 : Float(768:1, 768:768) = aten::t(%1366), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.37 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1367), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %1369 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.37, %1365, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1678:0
  %1370 : int[] = prim::ListConstruct(%1350, %1352, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1371 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1369, %1370), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.16 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.15, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.8 : Float(12:64, 64:1) = aten::mul(%1346, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:542:0
  %1374 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_w_bias.8, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:544:0
  %1375 : Tensor[] = prim::ListConstruct(%1374, %1364), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %content_score.8 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%34, %1375), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %v.8 : Float(12:64, 64:1) = aten::mul(%1345, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:486:0
  %1378 : Tensor[] = prim::ListConstruct(%190, %1344), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1379 : Float(14:768, 12:64, 64:1) = aten::einsum(%35, %1378), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1380 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %v.8, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:493:0
  %1381 : Tensor[] = prim::ListConstruct(%1380, %1379), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.43 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%36, %1381), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1383 : int = aten::size(%positional_attn.43, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1384 : int = aten::size(%positional_attn.43, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1385 : int = aten::size(%positional_attn.43, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1386 : int = aten::size(%positional_attn.43, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.8 : Long() = prim::NumToTensor(%1386), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1388 : int[] = prim::ListConstruct(%1383, %1384, %1386, %1385), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.44 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.43, %1388), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:428:0
  %1390 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.44, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1391 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1390, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1392 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1391, %19, %49, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.45 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1392, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1394 : Long() = aten::sub(%max_rel_len.8, %14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %1395 : int = aten::Int(%1394), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1396 : int[] = prim::ListConstruct(%1383, %1384, %1385, %1395), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.46 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.45, %1396), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.47 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.46, %37, %50, %1352, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.48 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.47, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:498:0
  %1400 : int = aten::size(%token_type_mat.9, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1401 : int = aten::size(%token_type_mat.9, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1402 : int = aten::size(%token_type_mat.9, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.8 : Float(12:64, 64:1) = aten::mul(%1343, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:508:0
  %1404 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_s_bias.8, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:511:0
  %1405 : Tensor[] = prim::ListConstruct(%1404, %1342), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1406 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%38, %1405), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1407 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1408 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1407, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1409 : int = aten::size(%q_head.16, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1410 : int[] = prim::ListConstruct(%1400, %1409, %1401, %1402), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %token_type_mat.12 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1408, %1410, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1412 : Tensor[] = aten::split(%1406, %49, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:371:0
  %diff_token_type.8 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.8 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1412), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1415 : int = aten::size(%token_type_mat.12, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1416 : int = aten::size(%token_type_mat.12, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1417 : int = aten::size(%token_type_mat.12, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1418 : int = aten::size(%token_type_mat.12, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1419 : int[] = prim::ListConstruct(%1415, %1416, %1417, %1418), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1420 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.8, %1419, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1421 : int = aten::size(%token_type_mat.12, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1422 : int = aten::size(%token_type_mat.12, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1423 : int = aten::size(%token_type_mat.12, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1424 : int = aten::size(%token_type_mat.12, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1425 : int[] = prim::ListConstruct(%1421, %1422, %1423, %1424), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1426 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.8, %1425, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.15 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.12, %1420, %1426), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.15, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:522:0
  %1429 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.8, %positional_attn.48, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.22 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1429, %token_type_attn.16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.23 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.22, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:553:0
  %1432 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1433 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1432, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1434 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1433, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1435 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1434, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1436 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1435, %49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:396:0
  %1437 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1436, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.24 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.23, %1437, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %input.70 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.24, %41, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:558:0
  %1440 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.70, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %1441 : Tensor[] = prim::ListConstruct(%1440, %1371), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %attn_vec.8 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%40, %1441), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1443 : int[] = prim::ListConstruct(%1350, %1351, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %input.71 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.8, %1443), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:565:0
  %1445 : Tensor = prim::GetAttr[name="bias"](%1341)
  %1446 : Tensor = prim::GetAttr[name="weight"](%1341)
  %1447 : Float(768:1, 768:768) = aten::t(%1446), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.38 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.71, %1447), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.72 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.38, %1445, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.72, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:5376, 7:768, 768:1) = aten::add(%query.7, %attn_out.8, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:568:0
  %1452 : Tensor = prim::GetAttr[name="bias"](%1340)
  %1453 : Tensor = prim::GetAttr[name="weight"](%1340)
  %1454 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm
  %input.74 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.73, %1454, %1453, %1452, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %1456 : __torch__.torch.nn.modules.normalization.___torch_mangle_2066.LayerNorm = prim::GetAttr[name="layer_norm"](%1338)
  %1457 : __torch__.torch.nn.modules.linear.___torch_mangle_2064.Linear = prim::GetAttr[name="linear_2"](%1338)
  %1458 : __torch__.torch.nn.modules.linear.___torch_mangle_2062.Linear = prim::GetAttr[name="linear_1"](%1338)
  %1459 : Tensor = prim::GetAttr[name="bias"](%1458)
  %1460 : Tensor = prim::GetAttr[name="weight"](%1458)
  %1461 : Float(768:1, 3072:768) = aten::t(%1460), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.39 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.74, %1461), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.8 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.39, %1459, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1464 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.8, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1465 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.8, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1466 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1465, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1467 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.8, %1466, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1468 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1467, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1469 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1468), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1470 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1469, %29, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.75 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1464, %1470), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.76 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.75, %30, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1473 : Tensor = prim::GetAttr[name="bias"](%1457)
  %1474 : Tensor = prim::GetAttr[name="weight"](%1457)
  %1475 : Float(3072:1, 768:3072) = aten::t(%1474), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.40 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.76, %1475), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.77 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.40, %1473, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.77, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(17:5376, 7:768, 768:1) = aten::add(%input.74, %h.8, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/modeling_funnel.py:588:0
  %1480 : Tensor = prim::GetAttr[name="bias"](%1456)
  %1481 : Tensor = prim::GetAttr[name="weight"](%1456)
  %1482 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm
  %hidden : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.78, %1482, %1481, %1480, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1484 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1485 : Bool(17:98, 1:14, 7:2) = aten::slice(%1484, %49, %50, %49, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1486 : Tensor[] = prim::ListConstruct(%1485, %token_type_mat.9), scope: __module.funnel/__module.funnel.encoder
  %tensor.11 : Bool(17:56, 8:7, 7:1) = aten::cat(%1486, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1488 : Bool(17:56, 8:7, 7:1) = aten::slice(%tensor.11, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.13 : Bool(17:56, 4:14, 7:1) = aten::slice(%1488, %49, %50, %41, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1490 : Float(1:14, 7:2) = aten::slice(%cls_mask.3, %50, %50, %49, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1491 : Tensor[] = prim::ListConstruct(%1490, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder
  %tensor.12 : Float(8:7, 7:1) = aten::cat(%1491, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.4 : Float(4:14, 7:1) = aten::slice(%tensor.12, %50, %50, %41, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1494 : Float(17:5376, 7:768, 768:1) = aten::slice(%hidden, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.3 : Float(17:5376, 6:768, 768:1) = aten::slice(%1494, %49, %50, %41, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1496 : Float(17:5376, 7:768, 768:1) = aten::slice(%hidden, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1497 : Float(17:5376, 1:768, 768:1) = aten::slice(%1496, %49, %50, %49, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1498 : Tensor[] = prim::ListConstruct(%1497, %suffix.3), scope: __module.funnel/__module.funnel.encoder
  %tensor.13 : Float(17:5376, 7:768, 768:1) = aten::cat(%1498, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1500 : Float(17:5376, 7:768, 768:1) = aten::slice(%tensor.13, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1501 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::unsqueeze(%1500, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1502 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%1501, %19, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %tensor.14 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%1502, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1504 : int[] = prim::ListConstruct(%19, %49), scope: __module.funnel/__module.funnel.encoder
  %1505 : int[] = prim::ListConstruct(%19, %49), scope: __module.funnel/__module.funnel.encoder
  %1506 : int[] = prim::ListConstruct(%50, %50), scope: __module.funnel/__module.funnel.encoder
  %tensor.15 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::avg_pool2d(%tensor.14, %1504, %1505, %1506, %42, %42, %13), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
  %1508 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::slice(%tensor.15, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %query.8 : Float(17:3072, 4:768, 768:1) = aten::select(%1508, %49, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %1510 : __torch__.transformers.modeling_funnel.___torch_mangle_2083.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%78)
  %1511 : __torch__.transformers.modeling_funnel.___torch_mangle_2077.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%78)
  %1512 : __torch__.torch.nn.modules.normalization.___torch_mangle_2076.LayerNorm = prim::GetAttr[name="layer_norm"](%1511)
  %1513 : __torch__.torch.nn.modules.linear.___torch_mangle_2075.Linear = prim::GetAttr[name="post_proj"](%1511)
  %1514 : Tensor = prim::GetAttr[name="seg_embed"](%1511)
  %1515 : Tensor = prim::GetAttr[name="r_s_bias"](%1511)
  %1516 : Tensor = prim::GetAttr[name="r_kernel"](%1511)
  %1517 : Tensor = prim::GetAttr[name="r_r_bias"](%1511)
  %1518 : Tensor = prim::GetAttr[name="r_w_bias"](%1511)
  %1519 : __torch__.torch.nn.modules.linear.___torch_mangle_2074.Linear = prim::GetAttr[name="v_head"](%1511)
  %1520 : __torch__.torch.nn.modules.linear.___torch_mangle_2073.Linear = prim::GetAttr[name="k_head"](%1511)
  %1521 : __torch__.torch.nn.modules.linear.___torch_mangle_2072.Linear = prim::GetAttr[name="q_head"](%1511)
  %1522 : int = aten::size(%query.8, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1523 : int = aten::size(%query.8, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1524 : int = aten::size(%hidden, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:531:0
  %1525 : Tensor = prim::GetAttr[name="weight"](%1521)
  %1526 : Float(768:1, 768:768) = aten::t(%1525), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1527 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.8, %1526), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1528 : int[] = prim::ListConstruct(%1522, %1523, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %q_head.17 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1527, %1528), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:535:0
  %1530 : Tensor = prim::GetAttr[name="bias"](%1520)
  %1531 : Tensor = prim::GetAttr[name="weight"](%1520)
  %1532 : Float(768:1, 768:768) = aten::t(%1531), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.41 : Float(17:5376, 7:768, 768:1) = aten::matmul(%hidden, %1532), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %1534 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.41, %1530, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1678:0
  %1535 : int[] = prim::ListConstruct(%1522, %1524, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1536 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1534, %1535), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:537:0
  %1537 : Tensor = prim::GetAttr[name="bias"](%1519)
  %1538 : Tensor = prim::GetAttr[name="weight"](%1519)
  %1539 : Float(768:1, 768:768) = aten::t(%1538), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.42 : Float(17:5376, 7:768, 768:1) = aten::matmul(%hidden, %1539), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %1541 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.42, %1537, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1678:0
  %1542 : int[] = prim::ListConstruct(%1522, %1524, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1543 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1541, %1542), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.18 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.17, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.9 : Float(12:64, 64:1) = aten::mul(%1518, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:542:0
  %1546 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_w_bias.9, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:544:0
  %1547 : Tensor[] = prim::ListConstruct(%1546, %1536), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %content_score.9 : Float(17:336, 12:28, 4:7, 7:1) = aten::einsum(%34, %1547), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %v.9 : Float(12:64, 64:1) = aten::mul(%1517, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:486:0
  %1550 : Tensor[] = prim::ListConstruct(%214, %1516), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1551 : Float(15:768, 12:64, 64:1) = aten::einsum(%35, %1550), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1552 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %v.9, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:493:0
  %1553 : Tensor[] = prim::ListConstruct(%1552, %1551), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.49 : Float(17:60, 12:1020, 4:15, 15:1) = aten::einsum(%36, %1553), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1555 : int = aten::size(%positional_attn.49, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1556 : int = aten::size(%positional_attn.49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1557 : int = aten::size(%positional_attn.49, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1558 : int = aten::size(%positional_attn.49, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.9 : Long() = prim::NumToTensor(%1558), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1560 : int[] = prim::ListConstruct(%1555, %1556, %1558, %1557), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.50 : Float(17:60, 12:1020, 15:4, 4:1) = aten::reshape(%positional_attn.49, %1560), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:428:0
  %1562 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%positional_attn.50, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1563 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%1562, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1564 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%1563, %19, %19, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.51 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%1564, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1566 : Long() = aten::sub(%max_rel_len.9, %15, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %1567 : int = aten::Int(%1566), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1568 : int[] = prim::ListConstruct(%1555, %1556, %1557, %1567), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.52 : Float(17:60, 12:1020, 4:13, 13:1) = aten::reshape(%positional_attn.51, %1568), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.53 : Float(17:60, 12:1020, 4:13, 7:1) = aten::slice(%positional_attn.52, %37, %50, %1524, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.54 : Float(17:60, 12:1020, 4:13, 7:1) = aten::mul_(%positional_attn.53, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:498:0
  %1572 : int = aten::size(%token_type_mat.13, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1573 : int = aten::size(%token_type_mat.13, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1574 : int = aten::size(%token_type_mat.13, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.9 : Float(12:64, 64:1) = aten::mul(%1515, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:508:0
  %1576 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_s_bias.9, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:511:0
  %1577 : Tensor[] = prim::ListConstruct(%1576, %1514), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1578 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%38, %1577), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1579 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1580 : Bool(17:56, 1:56, 4:14, 7:1) = aten::unsqueeze(%1579, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1581 : int = aten::size(%q_head.18, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1582 : int[] = prim::ListConstruct(%1572, %1581, %1573, %1574), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %token_type_mat.14 : Bool(17:56, 12:0, 4:14, 7:1) = aten::expand(%1580, %1582, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1584 : Tensor[] = aten::split(%1578, %49, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:371:0
  %diff_token_type.9 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.9 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1584), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1587 : int = aten::size(%token_type_mat.14, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1588 : int = aten::size(%token_type_mat.14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1589 : int = aten::size(%token_type_mat.14, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1590 : int = aten::size(%token_type_mat.14, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1591 : int[] = prim::ListConstruct(%1587, %1588, %1589, %1590), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1592 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%same_token_type.9, %1591, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1593 : int = aten::size(%token_type_mat.14, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1594 : int = aten::size(%token_type_mat.14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1595 : int = aten::size(%token_type_mat.14, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1596 : int = aten::size(%token_type_mat.14, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1597 : int[] = prim::ListConstruct(%1593, %1594, %1595, %1596), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1598 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%diff_token_type.9, %1597, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.17 : Float(17:336, 12:28, 4:7, 7:1) = aten::where(%token_type_mat.14, %1592, %1598), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.18 : Float(17:336, 12:28, 4:7, 7:1) = aten::mul_(%token_type_attn.17, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:522:0
  %1601 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%content_score.9, %positional_attn.54, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.25 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%1601, %token_type_attn.18, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.26 : Float(17:336, 12:28, 4:7, 7:1) = aten::to(%attn_score.25, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:553:0
  %1604 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1605 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1604, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1606 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1605, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1607 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1606, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1608 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1607, %49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:396:0
  %1609 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1608, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.27 : Float(17:336, 12:28, 4:7, 7:1) = aten::sub(%attn_score.26, %1609, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %input.79 : Float(17:336, 12:28, 4:7, 7:1) = aten::softmax(%attn_score.27, %41, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:558:0
  %1612 : Float(17:336, 12:28, 4:7, 7:1) = aten::dropout(%input.79, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %1613 : Tensor[] = prim::ListConstruct(%1612, %1543), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %attn_vec.9 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%40, %1613), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1615 : int[] = prim::ListConstruct(%1522, %1523, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %input.80 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.9, %1615), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:565:0
  %1617 : Tensor = prim::GetAttr[name="bias"](%1513)
  %1618 : Tensor = prim::GetAttr[name="weight"](%1513)
  %1619 : Float(768:1, 768:768) = aten::t(%1618), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.43 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.80, %1619), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.81 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.43, %1617, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.81, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.82 : Float(17:3072, 4:768, 768:1) = aten::add(%query.8, %attn_out.9, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:568:0
  %1624 : Tensor = prim::GetAttr[name="bias"](%1512)
  %1625 : Tensor = prim::GetAttr[name="weight"](%1512)
  %1626 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm
  %input.83 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.82, %1626, %1625, %1624, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %1628 : __torch__.torch.nn.modules.normalization.___torch_mangle_2082.LayerNorm = prim::GetAttr[name="layer_norm"](%1510)
  %1629 : __torch__.torch.nn.modules.linear.___torch_mangle_2080.Linear = prim::GetAttr[name="linear_2"](%1510)
  %1630 : __torch__.torch.nn.modules.linear.___torch_mangle_2078.Linear = prim::GetAttr[name="linear_1"](%1510)
  %1631 : Tensor = prim::GetAttr[name="bias"](%1630)
  %1632 : Tensor = prim::GetAttr[name="weight"](%1630)
  %1633 : Float(768:1, 3072:768) = aten::t(%1632), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.44 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.83, %1633), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.9 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.44, %1631, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1636 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.9, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1637 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.9, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1638 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1637, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1639 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.9, %1638, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1640 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1639, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1641 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1640), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1642 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1641, %29, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.84 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1636, %1642), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.85 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.84, %30, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1645 : Tensor = prim::GetAttr[name="bias"](%1629)
  %1646 : Tensor = prim::GetAttr[name="weight"](%1629)
  %1647 : Float(3072:1, 768:3072) = aten::t(%1646), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.45 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.85, %1647), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.86 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.45, %1645, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.86, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(17:3072, 4:768, 768:1) = aten::add(%input.83, %h.9, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/modeling_funnel.py:588:0
  %1652 : Tensor = prim::GetAttr[name="bias"](%1628)
  %1653 : Tensor = prim::GetAttr[name="weight"](%1628)
  %1654 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm
  %query.9 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.87, %1654, %1653, %1652, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1656 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1657 : Bool(17:56, 4:14, 7:1) = aten::slice(%1656, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1658 : Bool(17:56, 4:14, 1:1) = aten::slice(%1657, %19, %50, %49, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1659 : Tensor[] = prim::ListConstruct(%1658, %token_type_mat.13), scope: __module.funnel/__module.funnel.encoder
  %tensor.16 : Bool(17:32, 4:8, 8:1) = aten::cat(%1659, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1661 : Bool(17:32, 4:8, 8:1) = aten::slice(%tensor.16, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1662 : Bool(17:32, 4:8, 8:1) = aten::slice(%1661, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.15 : Bool(17:32, 4:8, 4:2) = aten::slice(%1662, %19, %50, %41, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1664 : Float(4:14, 7:1) = aten::slice(%cls_mask.4, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1665 : Float(4:14, 1:1) = aten::slice(%1664, %49, %50, %49, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1666 : Tensor[] = prim::ListConstruct(%1665, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder
  %tensor.17 : Float(4:8, 8:1) = aten::cat(%1666, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1668 : Float(4:8, 8:1) = aten::slice(%tensor.17, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask.5 : Float(4:8, 4:2) = aten::slice(%1668, %49, %50, %41, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1670 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix : Float(17:7, 6:1) = aten::slice(%1670, %49, %50, %41, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1672 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1673 : Float(17:7, 1:1) = aten::slice(%1672, %49, %50, %49, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1674 : Tensor[] = prim::ListConstruct(%1673, %suffix), scope: __module.funnel/__module.funnel.encoder
  %tensor.18 : Float(17:7, 7:1) = aten::cat(%1674, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1676 : Float(17:7, 7:1) = aten::slice(%tensor.18, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1677 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1676, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1678 : Float(17:7, 1:7, 7:1) = aten::slice(%1677, %19, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %tensor.19 : Float(17:7, 1:7, 7:1, 1:1) = aten::unsqueeze(%1678, %37), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %input.88 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%tensor.19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1681 : int[] = prim::ListConstruct(%19, %49), scope: __module.funnel/__module.funnel.encoder
  %1682 : int[] = prim::ListConstruct(%19, %49), scope: __module.funnel/__module.funnel.encoder
  %1683 : int[] = prim::ListConstruct(%50, %50), scope: __module.funnel/__module.funnel.encoder
  %1684 : int[] = prim::ListConstruct(%49, %49), scope: __module.funnel/__module.funnel.encoder
  %1685 : Float(17:4, 1:4, 4:1, 1:1) = aten::max_pool2d(%input.88, %1681, %1682, %1683, %1684, %42), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
  %tensor : Float(17:4, 1:4, 4:1, 1:1) = aten::neg(%1685), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1687 : Float(17:4, 1:4, 4:1, 1:1) = aten::slice(%tensor, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1688 : Float(17:4, 4:1, 1:1) = aten::select(%1687, %49, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1689 : Float(17:4, 4:1, 1:1) = aten::slice(%1688, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask : Float(17:4, 4:1) = aten::select(%1689, %19, %50), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1691 : __torch__.transformers.modeling_funnel.___torch_mangle_2098.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%75)
  %1692 : __torch__.transformers.modeling_funnel.___torch_mangle_2092.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%75)
  %1693 : __torch__.torch.nn.modules.normalization.___torch_mangle_2091.LayerNorm = prim::GetAttr[name="layer_norm"](%1692)
  %1694 : __torch__.torch.nn.modules.linear.___torch_mangle_2090.Linear = prim::GetAttr[name="post_proj"](%1692)
  %1695 : Tensor = prim::GetAttr[name="seg_embed"](%1692)
  %1696 : Tensor = prim::GetAttr[name="r_s_bias"](%1692)
  %1697 : Tensor = prim::GetAttr[name="r_kernel"](%1692)
  %1698 : Tensor = prim::GetAttr[name="r_r_bias"](%1692)
  %1699 : Tensor = prim::GetAttr[name="r_w_bias"](%1692)
  %1700 : __torch__.torch.nn.modules.linear.___torch_mangle_2089.Linear = prim::GetAttr[name="v_head"](%1692)
  %1701 : __torch__.torch.nn.modules.linear.___torch_mangle_2088.Linear = prim::GetAttr[name="k_head"](%1692)
  %1702 : __torch__.torch.nn.modules.linear.___torch_mangle_2087.Linear = prim::GetAttr[name="q_head"](%1692)
  %1703 : int = aten::size(%query.9, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1704 : int = aten::size(%query.9, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1705 : int = aten::size(%query.9, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:531:0
  %1706 : Tensor = prim::GetAttr[name="weight"](%1702)
  %1707 : Float(768:1, 768:768) = aten::t(%1706), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1708 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1707), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1709 : int[] = prim::ListConstruct(%1703, %1704, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %q_head.19 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1708, %1709), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:535:0
  %1711 : Tensor = prim::GetAttr[name="bias"](%1701)
  %1712 : Tensor = prim::GetAttr[name="weight"](%1701)
  %1713 : Float(768:1, 768:768) = aten::t(%1712), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.46 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1713), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %1715 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.46, %1711, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1678:0
  %1716 : int[] = prim::ListConstruct(%1703, %1705, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1717 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1715, %1716), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:537:0
  %1718 : Tensor = prim::GetAttr[name="bias"](%1700)
  %1719 : Tensor = prim::GetAttr[name="weight"](%1700)
  %1720 : Float(768:1, 768:768) = aten::t(%1719), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.47 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1720), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %1722 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.47, %1718, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1678:0
  %1723 : int[] = prim::ListConstruct(%1703, %1705, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1724 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1722, %1723), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.20 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.19, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.10 : Float(12:64, 64:1) = aten::mul(%1699, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:542:0
  %1727 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_w_bias.10, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:544:0
  %1728 : Tensor[] = prim::ListConstruct(%1727, %1717), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %content_score.10 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%34, %1728), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %v.10 : Float(12:64, 64:1) = aten::mul(%1698, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:486:0
  %1731 : Tensor[] = prim::ListConstruct(%232, %1697), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1732 : Float(8:768, 12:64, 64:1) = aten::einsum(%35, %1731), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1733 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %v.10, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:493:0
  %1734 : Tensor[] = prim::ListConstruct(%1733, %1732), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.55 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%36, %1734), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1736 : int = aten::size(%positional_attn.55, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1737 : int = aten::size(%positional_attn.55, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1738 : int = aten::size(%positional_attn.55, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1739 : int = aten::size(%positional_attn.55, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.10 : Long() = prim::NumToTensor(%1739), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1741 : int[] = prim::ListConstruct(%1736, %1737, %1739, %1738), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.56 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.55, %1741), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:428:0
  %1743 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.56, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1744 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%1743, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1745 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1744, %19, %49, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.57 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1745, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1747 : Long() = aten::sub(%max_rel_len.10, %14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %1748 : int = aten::Int(%1747), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1749 : int[] = prim::ListConstruct(%1736, %1737, %1738, %1748), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.58 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.57, %1749), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.59 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.58, %37, %50, %1705, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.60 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.59, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:498:0
  %1753 : int = aten::size(%token_type_mat.15, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1754 : int = aten::size(%token_type_mat.15, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1755 : int = aten::size(%token_type_mat.15, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.10 : Float(12:64, 64:1) = aten::mul(%1696, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:508:0
  %1757 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_s_bias.10, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:511:0
  %1758 : Tensor[] = prim::ListConstruct(%1757, %1695), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1759 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%38, %1758), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1760 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1761 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1760, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1762 : int = aten::size(%q_head.20, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1763 : int[] = prim::ListConstruct(%1753, %1762, %1754, %1755), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %token_type_mat.16 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%1761, %1763, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1765 : Tensor[] = aten::split(%1759, %49, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:371:0
  %diff_token_type.10 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.10 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1765), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1768 : int = aten::size(%token_type_mat.16, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1769 : int = aten::size(%token_type_mat.16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1770 : int = aten::size(%token_type_mat.16, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1771 : int = aten::size(%token_type_mat.16, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1772 : int[] = prim::ListConstruct(%1768, %1769, %1770, %1771), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1773 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.10, %1772, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1774 : int = aten::size(%token_type_mat.16, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1775 : int = aten::size(%token_type_mat.16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1776 : int = aten::size(%token_type_mat.16, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1777 : int = aten::size(%token_type_mat.16, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1778 : int[] = prim::ListConstruct(%1774, %1775, %1776, %1777), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1779 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.10, %1778, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.19 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.16, %1773, %1779), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.20 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.19, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:522:0
  %1782 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.10, %positional_attn.60, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.28 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%1782, %token_type_attn.20, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.29 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.28, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:553:0
  %1785 : Float(17:4, 4:1) = aten::slice(%attention_mask, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1786 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%1785, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1787 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1786, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1788 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%1787, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1789 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%1788, %49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:396:0
  %1790 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%1789, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.30 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.29, %1790, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %input.89 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.30, %41, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:558:0
  %1793 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.89, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1794 : Tensor[] = prim::ListConstruct(%1793, %1724), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %attn_vec.10 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%40, %1794), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1796 : int[] = prim::ListConstruct(%1703, %1704, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %input.90 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.10, %1796), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:565:0
  %1798 : Tensor = prim::GetAttr[name="bias"](%1694)
  %1799 : Tensor = prim::GetAttr[name="weight"](%1694)
  %1800 : Float(768:1, 768:768) = aten::t(%1799), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.48 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.90, %1800), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.91 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.48, %1798, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.91, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.92 : Float(17:3072, 4:768, 768:1) = aten::add(%query.9, %attn_out.10, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:568:0
  %1805 : Tensor = prim::GetAttr[name="bias"](%1693)
  %1806 : Tensor = prim::GetAttr[name="weight"](%1693)
  %1807 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm
  %input.93 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.92, %1807, %1806, %1805, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1809 : __torch__.torch.nn.modules.normalization.___torch_mangle_2097.LayerNorm = prim::GetAttr[name="layer_norm"](%1691)
  %1810 : __torch__.torch.nn.modules.linear.___torch_mangle_2095.Linear = prim::GetAttr[name="linear_2"](%1691)
  %1811 : __torch__.torch.nn.modules.linear.___torch_mangle_2093.Linear = prim::GetAttr[name="linear_1"](%1691)
  %1812 : Tensor = prim::GetAttr[name="bias"](%1811)
  %1813 : Tensor = prim::GetAttr[name="weight"](%1811)
  %1814 : Float(768:1, 3072:768) = aten::t(%1813), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.93, %1814), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.10 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.49, %1812, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1817 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.10, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1818 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.10, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1819 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1818, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1820 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.10, %1819, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1821 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1820, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1822 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1821), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1823 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1822, %29, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.94 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1817, %1823), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.95 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.94, %30, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1826 : Tensor = prim::GetAttr[name="bias"](%1810)
  %1827 : Tensor = prim::GetAttr[name="weight"](%1810)
  %1828 : Float(3072:1, 768:3072) = aten::t(%1827), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.95, %1828), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.96 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.50, %1826, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.96, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.97 : Float(17:3072, 4:768, 768:1) = aten::add(%input.93, %h.10, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/modeling_funnel.py:588:0
  %1833 : Tensor = prim::GetAttr[name="bias"](%1809)
  %1834 : Tensor = prim::GetAttr[name="weight"](%1809)
  %1835 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm
  %query.10 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.97, %1835, %1834, %1833, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1837 : __torch__.transformers.modeling_funnel.___torch_mangle_2113.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%72)
  %1838 : __torch__.transformers.modeling_funnel.___torch_mangle_2107.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%72)
  %1839 : __torch__.torch.nn.modules.normalization.___torch_mangle_2106.LayerNorm = prim::GetAttr[name="layer_norm"](%1838)
  %1840 : __torch__.torch.nn.modules.linear.___torch_mangle_2105.Linear = prim::GetAttr[name="post_proj"](%1838)
  %1841 : Tensor = prim::GetAttr[name="seg_embed"](%1838)
  %1842 : Tensor = prim::GetAttr[name="r_s_bias"](%1838)
  %1843 : Tensor = prim::GetAttr[name="r_kernel"](%1838)
  %1844 : Tensor = prim::GetAttr[name="r_r_bias"](%1838)
  %1845 : Tensor = prim::GetAttr[name="r_w_bias"](%1838)
  %1846 : __torch__.torch.nn.modules.linear.___torch_mangle_2104.Linear = prim::GetAttr[name="v_head"](%1838)
  %1847 : __torch__.torch.nn.modules.linear.___torch_mangle_2103.Linear = prim::GetAttr[name="k_head"](%1838)
  %1848 : __torch__.torch.nn.modules.linear.___torch_mangle_2102.Linear = prim::GetAttr[name="q_head"](%1838)
  %1849 : int = aten::size(%query.10, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1850 : int = aten::size(%query.10, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1851 : int = aten::size(%query.10, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:531:0
  %1852 : Tensor = prim::GetAttr[name="weight"](%1848)
  %1853 : Float(768:1, 768:768) = aten::t(%1852), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1854 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1853), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1855 : int[] = prim::ListConstruct(%1849, %1850, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %q_head.21 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1854, %1855), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:535:0
  %1857 : Tensor = prim::GetAttr[name="bias"](%1847)
  %1858 : Tensor = prim::GetAttr[name="weight"](%1847)
  %1859 : Float(768:1, 768:768) = aten::t(%1858), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.51 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1859), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %1861 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.51, %1857, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1678:0
  %1862 : int[] = prim::ListConstruct(%1849, %1851, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1863 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1861, %1862), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:537:0
  %1864 : Tensor = prim::GetAttr[name="bias"](%1846)
  %1865 : Tensor = prim::GetAttr[name="weight"](%1846)
  %1866 : Float(768:1, 768:768) = aten::t(%1865), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.52 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1866), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %1868 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.52, %1864, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1678:0
  %1869 : int[] = prim::ListConstruct(%1849, %1851, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1870 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1868, %1869), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.22 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.21, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.11 : Float(12:64, 64:1) = aten::mul(%1845, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:542:0
  %1873 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_w_bias.11, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:544:0
  %1874 : Tensor[] = prim::ListConstruct(%1873, %1863), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %content_score.11 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%34, %1874), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %v.11 : Float(12:64, 64:1) = aten::mul(%1844, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:486:0
  %1877 : Tensor[] = prim::ListConstruct(%232, %1843), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1878 : Float(8:768, 12:64, 64:1) = aten::einsum(%35, %1877), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1879 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %v.11, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:493:0
  %1880 : Tensor[] = prim::ListConstruct(%1879, %1878), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.61 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%36, %1880), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1882 : int = aten::size(%positional_attn.61, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1883 : int = aten::size(%positional_attn.61, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1884 : int = aten::size(%positional_attn.61, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1885 : int = aten::size(%positional_attn.61, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.11 : Long() = prim::NumToTensor(%1885), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1887 : int[] = prim::ListConstruct(%1882, %1883, %1885, %1884), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.62 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.61, %1887), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:428:0
  %1889 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.62, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1890 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%1889, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1891 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1890, %19, %49, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.63 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1891, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1893 : Long() = aten::sub(%max_rel_len.11, %14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %1894 : int = aten::Int(%1893), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1895 : int[] = prim::ListConstruct(%1882, %1883, %1884, %1894), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.64 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.63, %1895), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.65 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.64, %37, %50, %1851, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.66 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.65, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:498:0
  %1899 : int = aten::size(%token_type_mat.15, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1900 : int = aten::size(%token_type_mat.15, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1901 : int = aten::size(%token_type_mat.15, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.11 : Float(12:64, 64:1) = aten::mul(%1842, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:508:0
  %1903 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_s_bias.11, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:511:0
  %1904 : Tensor[] = prim::ListConstruct(%1903, %1841), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1905 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%38, %1904), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1906 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1907 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1906, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1908 : int = aten::size(%q_head.22, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1909 : int[] = prim::ListConstruct(%1899, %1908, %1900, %1901), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %token_type_mat.17 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%1907, %1909, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1911 : Tensor[] = aten::split(%1905, %49, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:371:0
  %diff_token_type.11 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.11 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1911), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1914 : int = aten::size(%token_type_mat.17, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1915 : int = aten::size(%token_type_mat.17, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1916 : int = aten::size(%token_type_mat.17, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1917 : int = aten::size(%token_type_mat.17, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1918 : int[] = prim::ListConstruct(%1914, %1915, %1916, %1917), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1919 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.11, %1918, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1920 : int = aten::size(%token_type_mat.17, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1921 : int = aten::size(%token_type_mat.17, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1922 : int = aten::size(%token_type_mat.17, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1923 : int = aten::size(%token_type_mat.17, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1924 : int[] = prim::ListConstruct(%1920, %1921, %1922, %1923), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1925 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.11, %1924, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.21 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.17, %1919, %1925), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.22 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.21, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:522:0
  %1928 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.11, %positional_attn.66, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.31 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%1928, %token_type_attn.22, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.32 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.31, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:553:0
  %1931 : Float(17:4, 4:1) = aten::slice(%attention_mask, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1932 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%1931, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1933 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1932, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1934 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%1933, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1935 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%1934, %49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:396:0
  %1936 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%1935, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.33 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.32, %1936, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %input.98 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.33, %41, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:558:0
  %1939 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.98, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1940 : Tensor[] = prim::ListConstruct(%1939, %1870), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %attn_vec.11 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%40, %1940), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1942 : int[] = prim::ListConstruct(%1849, %1850, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %input.99 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.11, %1942), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:565:0
  %1944 : Tensor = prim::GetAttr[name="bias"](%1840)
  %1945 : Tensor = prim::GetAttr[name="weight"](%1840)
  %1946 : Float(768:1, 768:768) = aten::t(%1945), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.53 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.99, %1946), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.100 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.53, %1944, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.100, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.101 : Float(17:3072, 4:768, 768:1) = aten::add(%query.10, %attn_out.11, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:568:0
  %1951 : Tensor = prim::GetAttr[name="bias"](%1839)
  %1952 : Tensor = prim::GetAttr[name="weight"](%1839)
  %1953 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm
  %input.102 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.101, %1953, %1952, %1951, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1955 : __torch__.torch.nn.modules.normalization.___torch_mangle_2112.LayerNorm = prim::GetAttr[name="layer_norm"](%1837)
  %1956 : __torch__.torch.nn.modules.linear.___torch_mangle_2110.Linear = prim::GetAttr[name="linear_2"](%1837)
  %1957 : __torch__.torch.nn.modules.linear.___torch_mangle_2108.Linear = prim::GetAttr[name="linear_1"](%1837)
  %1958 : Tensor = prim::GetAttr[name="bias"](%1957)
  %1959 : Tensor = prim::GetAttr[name="weight"](%1957)
  %1960 : Float(768:1, 3072:768) = aten::t(%1959), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.102, %1960), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.11 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.54, %1958, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1963 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.11, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1964 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.11, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1965 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1964, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1966 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.11, %1965, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1967 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1966, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1968 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1967), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1969 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1968, %29, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.103 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1963, %1969), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.104 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.103, %30, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1972 : Tensor = prim::GetAttr[name="bias"](%1956)
  %1973 : Tensor = prim::GetAttr[name="weight"](%1956)
  %1974 : Float(3072:1, 768:3072) = aten::t(%1973), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.55 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.104, %1974), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.105 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.55, %1972, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.105, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.106 : Float(17:3072, 4:768, 768:1) = aten::add(%input.102, %h.11, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/modeling_funnel.py:588:0
  %1979 : Tensor = prim::GetAttr[name="bias"](%1955)
  %1980 : Tensor = prim::GetAttr[name="weight"](%1955)
  %1981 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm
  %query.11 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.106, %1981, %1980, %1979, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1983 : __torch__.transformers.modeling_funnel.___torch_mangle_2128.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%69)
  %1984 : __torch__.transformers.modeling_funnel.___torch_mangle_2122.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%69)
  %1985 : __torch__.torch.nn.modules.normalization.___torch_mangle_2121.LayerNorm = prim::GetAttr[name="layer_norm"](%1984)
  %1986 : __torch__.torch.nn.modules.linear.___torch_mangle_2120.Linear = prim::GetAttr[name="post_proj"](%1984)
  %1987 : Tensor = prim::GetAttr[name="seg_embed"](%1984)
  %1988 : Tensor = prim::GetAttr[name="r_s_bias"](%1984)
  %1989 : Tensor = prim::GetAttr[name="r_kernel"](%1984)
  %1990 : Tensor = prim::GetAttr[name="r_r_bias"](%1984)
  %1991 : Tensor = prim::GetAttr[name="r_w_bias"](%1984)
  %1992 : __torch__.torch.nn.modules.linear.___torch_mangle_2119.Linear = prim::GetAttr[name="v_head"](%1984)
  %1993 : __torch__.torch.nn.modules.linear.___torch_mangle_2118.Linear = prim::GetAttr[name="k_head"](%1984)
  %1994 : __torch__.torch.nn.modules.linear.___torch_mangle_2117.Linear = prim::GetAttr[name="q_head"](%1984)
  %1995 : int = aten::size(%query.11, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %1996 : int = aten::size(%query.11, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %1997 : int = aten::size(%query.11, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:531:0
  %1998 : Tensor = prim::GetAttr[name="weight"](%1994)
  %1999 : Float(768:1, 768:768) = aten::t(%1998), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2000 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.11, %1999), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2001 : int[] = prim::ListConstruct(%1995, %1996, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %q_head.23 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2000, %2001), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:535:0
  %2003 : Tensor = prim::GetAttr[name="bias"](%1993)
  %2004 : Tensor = prim::GetAttr[name="weight"](%1993)
  %2005 : Float(768:1, 768:768) = aten::t(%2004), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.56 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.11, %2005), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %2007 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.56, %2003, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1678:0
  %2008 : int[] = prim::ListConstruct(%1995, %1997, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2009 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2007, %2008), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:537:0
  %2010 : Tensor = prim::GetAttr[name="bias"](%1992)
  %2011 : Tensor = prim::GetAttr[name="weight"](%1992)
  %2012 : Float(768:1, 768:768) = aten::t(%2011), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.57 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.11, %2012), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %2014 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.57, %2010, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1678:0
  %2015 : int[] = prim::ListConstruct(%1995, %1997, %31, %32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2016 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2014, %2015), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.24 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.23, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.12 : Float(12:64, 64:1) = aten::mul(%1991, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:542:0
  %2019 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %r_w_bias.12, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:544:0
  %2020 : Tensor[] = prim::ListConstruct(%2019, %2009), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %content_score.12 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%34, %2020), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %v.12 : Float(12:64, 64:1) = aten::mul(%1990, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:486:0
  %2023 : Tensor[] = prim::ListConstruct(%232, %1989), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2024 : Float(8:768, 12:64, 64:1) = aten::einsum(%35, %2023), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2025 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %v.12, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:493:0
  %2026 : Tensor[] = prim::ListConstruct(%2025, %2024), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.67 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%36, %2026), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2028 : int = aten::size(%positional_attn.67, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2029 : int = aten::size(%positional_attn.67, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2030 : int = aten::size(%positional_attn.67, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2031 : int = aten::size(%positional_attn.67, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.12 : Long() = prim::NumToTensor(%2031), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2033 : int[] = prim::ListConstruct(%2028, %2029, %2031, %2030), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.68 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.67, %2033), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:428:0
  %2035 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.68, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2036 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%2035, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2037 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%2036, %19, %49, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.69 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%2037, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2039 : Long() = aten::sub(%max_rel_len.12, %14, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %2040 : int = aten::Int(%2039), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2041 : int[] = prim::ListConstruct(%2028, %2029, %2030, %2040), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.70 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.69, %2041), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.71 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.70, %37, %50, %1997, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.72 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.71, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:498:0
  %2045 : int = aten::size(%token_type_mat.15, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2046 : int = aten::size(%token_type_mat.15, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2047 : int = aten::size(%token_type_mat.15, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.12 : Float(12:64, 64:1) = aten::mul(%1988, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:508:0
  %2049 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %r_s_bias.12, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:511:0
  %2050 : Tensor[] = prim::ListConstruct(%2049, %1987), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2051 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%38, %2050), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2052 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2053 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%2052, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2054 : int = aten::size(%q_head.24, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2055 : int[] = prim::ListConstruct(%2045, %2054, %2046, %2047), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %token_type_mat.18 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%2053, %2055, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2057 : Tensor[] = aten::split(%2051, %49, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:371:0
  %diff_token_type.12 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.12 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%2057), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2060 : int = aten::size(%token_type_mat.18, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2061 : int = aten::size(%token_type_mat.18, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2062 : int = aten::size(%token_type_mat.18, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2063 : int = aten::size(%token_type_mat.18, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2064 : int[] = prim::ListConstruct(%2060, %2061, %2062, %2063), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2065 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.12, %2064, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2066 : int = aten::size(%token_type_mat.18, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2067 : int = aten::size(%token_type_mat.18, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2068 : int = aten::size(%token_type_mat.18, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2069 : int = aten::size(%token_type_mat.18, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2070 : int[] = prim::ListConstruct(%2066, %2067, %2068, %2069), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2071 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.12, %2070, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.23 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.18, %2065, %2071), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.24 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.23, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:522:0
  %2074 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.12, %positional_attn.72, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.34 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%2074, %token_type_attn.24, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.35 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.34, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:553:0
  %2077 : Float(17:4, 4:1) = aten::slice(%attention_mask, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2078 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%2077, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2079 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%2078, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2080 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%2079, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2081 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%2080, %49, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:396:0
  %2082 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%2081, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.36 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.35, %2082, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %input.107 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.36, %41, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:558:0
  %2085 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.107, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %2086 : Tensor[] = prim::ListConstruct(%2085, %2016), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %attn_vec.12 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%40, %2086), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2088 : int[] = prim::ListConstruct(%1995, %1996, %44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %input.108 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.12, %2088), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:565:0
  %2090 : Tensor = prim::GetAttr[name="bias"](%1986)
  %2091 : Tensor = prim::GetAttr[name="weight"](%1986)
  %2092 : Float(768:1, 768:768) = aten::t(%2091), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.58 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.108, %2092), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.109 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.58, %2090, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.12 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.109, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:3072, 4:768, 768:1) = aten::add(%query.11, %attn_out.12, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:568:0
  %2097 : Tensor = prim::GetAttr[name="bias"](%1985)
  %2098 : Tensor = prim::GetAttr[name="weight"](%1985)
  %2099 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm
  %input.111 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.110, %2099, %2098, %2097, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %2101 : __torch__.torch.nn.modules.normalization.___torch_mangle_2127.LayerNorm = prim::GetAttr[name="layer_norm"](%1983)
  %2102 : __torch__.torch.nn.modules.linear.___torch_mangle_2125.Linear = prim::GetAttr[name="linear_2"](%1983)
  %2103 : __torch__.torch.nn.modules.linear.___torch_mangle_2123.Linear = prim::GetAttr[name="linear_1"](%1983)
  %2104 : Tensor = prim::GetAttr[name="bias"](%2103)
  %2105 : Tensor = prim::GetAttr[name="weight"](%2103)
  %2106 : Float(768:1, 3072:768) = aten::t(%2105), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.111, %2106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.12 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.59, %2104, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2109 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.12, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2110 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.12, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2111 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2110, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2112 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.12, %2111, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2113 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2112, %28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2114 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%2113), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2115 : Float(17:12288, 4:3072, 3072:1) = aten::add(%2114, %29, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.112 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2109, %2115), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.113 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.112, %30, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2118 : Tensor = prim::GetAttr[name="bias"](%2102)
  %2119 : Tensor = prim::GetAttr[name="weight"](%2102)
  %2120 : Float(3072:1, 768:3072) = aten::t(%2119), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.60 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.113, %2120), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.114 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.60, %2118, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.12 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.114, %45, %46), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.115 : Float(17:3072, 4:768, 768:1) = aten::add(%input.111, %h.12, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/modeling_funnel.py:588:0
  %2125 : Tensor = prim::GetAttr[name="bias"](%2101)
  %2126 : Tensor = prim::GetAttr[name="weight"](%2101)
  %2127 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm
  %x.13 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.115, %2127, %2126, %2125, %43, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2129 : (Float(17:9984, 13:768, 768:1), Float(17:9984, 13:768, 768:1), Float(17:9984, 13:768, 768:1), Float(17:3072, 4:768, 768:1), Float(17:9984, 13:768, 768:1)) = prim::TupleConstruct(%hidden.1, %hidden.1, %hidden.1, %x.13, %hidden.1)
  %2130 : Float(17:9984, 13:768, 768:1), %2131 : Float(17:9984, 13:768, 768:1), %2132 : Float(17:9984, 13:768, 768:1), %2133 : Float(17:3072, 4:768, 768:1), %2134 : Float(17:9984, 13:768, 768:1) = prim::TupleUnpack(%2129)
  %2135 : __torch__.torch.nn.modules.container.___torch_mangle_2166.ModuleList = prim::GetAttr[name="layers"](%51)
  %2136 : __torch__.transformers.modeling_funnel.___torch_mangle_2165.FunnelLayer = prim::GetAttr[name="1"](%2135)
  %2137 : __torch__.torch.nn.modules.container.___torch_mangle_2166.ModuleList = prim::GetAttr[name="layers"](%51)
  %2138 : __torch__.transformers.modeling_funnel.___torch_mangle_2150.FunnelLayer = prim::GetAttr[name="0"](%2137)
  %2139 : int = aten::size(%2131, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:705:0
  %target_len : Long() = prim::NumToTensor(%2139), scope: __module.funnel/__module.funnel.decoder
  %2141 : Float(17:3072, 4:768, 768:1) = aten::slice(%2133, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
  %cls : Float(17:3072, 1:768, 768:1) = aten::slice(%2141, %49, %50, %49, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
  %2143 : Float(17:3072, 4:768, 768:1) = aten::slice(%2133, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
  %x.14 : Float(17:3072, 3:768, 768:1) = aten::slice(%2143, %49, %49, %16, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
  %input.116 : Float(17:9216, 12:768, 768:1) = aten::repeat_interleave(%x.14, %48, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:674:0
  %2146 : int[] = prim::ListConstruct(%50, %50, %50, %37, %50, %50), scope: __module.funnel/__module.funnel.decoder
  %output.61 : Float(17:11520, 15:768, 768:1) = aten::constant_pad_nd(%input.116, %2146, %50), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
  %2148 : Long() = aten::sub(%target_len, %14, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
  %2149 : int = aten::Int(%2148), scope: __module.funnel/__module.funnel.decoder
  %2150 : Float(17:11520, 15:768, 768:1) = aten::slice(%output.61, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
  %output.62 : Float(17:11520, 12:768, 768:1) = aten::slice(%2150, %49, %50, %2149, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
  %2152 : Tensor[] = prim::ListConstruct(%cls, %output.62), scope: __module.funnel/__module.funnel.decoder
  %upsampled_hidden : Float(17:9984, 13:768, 768:1) = aten::cat(%2152, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:679:0
  %input_embeds : Float(17:9984, 13:768, 768:1) = aten::add(%upsampled_hidden, %2134, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:710:0
  %2155 : int = aten::size(%input_embeds, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:195:0
  %seq_len.38 : Long() = prim::NumToTensor(%2155), scope: __module.funnel/__module.funnel.decoder
  %freq_seq : Float(384:1) = aten::arange(%50, %8, %9, %10, %50, %47, %46), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:247:0
  %2158 : Float(384:1) = aten::div(%freq_seq, %11), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:248:0
  %2159 : Float() = aten::to(%12, %47, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
  %2160 : Float() = aten::detach(%2159), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
  %2161 : Float(384:1) = aten::pow(%2160, %2158), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
  %2162 : Float(384:1) = aten::reciprocal(%2161), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:400:0
  %inv_freq : Float(384:1) = aten::mul(%2162, %14), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:400:0
  %2164 : Long() = aten::neg(%seq_len.38), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
  %2165 : Long() = aten::mul(%2164, %15), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
  %2166 : Scalar = aten::ScalarImplicit(%2165), scope: __module.funnel/__module.funnel.decoder
  %2167 : Long() = aten::mul(%seq_len.38, %15), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
  %2168 : Scalar = aten::ScalarImplicit(%2167), scope: __module.funnel/__module.funnel.decoder
  %rel_pos_id : Float(52:1) = aten::arange(%2166, %2168, %9, %10, %50, %47, %46), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
  %zero_offset : Long() = aten::mul(%seq_len.38, %15), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:251:0
  %2171 : Float(52:1) = aten::slice(%rel_pos_id, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
  %2172 : Float(52:1, 1:1) = aten::unsqueeze(%2171, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
  %2173 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq, %50), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
  %sinusoid : Float(52:384, 384:1) = aten::mul(%2172, %2173), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
  %input.117 : Float(52:384, 384:1) = aten::sin(%sinusoid), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:253:0
  %sin_embed : Float(52:384, 384:1) = aten::dropout(%input.117, %45, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %input.118 : Float(52:384, 384:1) = aten::cos(%sinusoid), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:254:0
  %cos_embed : Float(52:384, 384:1) = aten::dropout(%input.118, %45, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
  %2179 : Tensor[] = prim::ListConstruct(%sin_embed, %cos_embed), scope: __module.funnel/__module.funnel.decoder
  %pos_embed : Float(52:768, 768:1) = aten::cat(%2179, %41), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:255:0
  %pos : Float(13:1) = aten::arange(%50, %2155, %49, %10, %50, %47, %46), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:257:0
  %2182 : Float() = aten::select(%pos, %50, %50), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
  %2183 : Float() = aten::select(%pos, %50, %50), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
  %ref_point.6 : Float() = aten::sub(%2182, %2183, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
  %max_dist.6 : Float() = aten::add(%ref_point.6, %17, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:315:0
  %2186 : Scalar = aten::ScalarImplicit(%max_dist.6), scope: __module.funnel/__module.funnel.decoder
  %2187 : Float() = aten::select(%pos, %50, %50), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
  %2188 : Float() = aten::select(%pos, %50, %41), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
  %min_dist.6 : Float() = aten::sub(%2187, %2188, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
  %2190 : Float() = aten::sub(%min_dist.6, %14, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
  %2191 : Scalar = aten::ScalarImplicit(%2190), scope: __module.funnel/__module.funnel.decoder
  %rel_pos.16 : Long(26:1) = aten::arange(%2186, %2191, %41, %48, %50, %47, %46), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
  %2193 : Long(26:1) = aten::slice(%rel_pos.16, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
  %2194 : Long(26:1, 1:1) = aten::unsqueeze(%2193, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
  %rel_pos.17 : Long(26:1, 1:1) = aten::add(%2194, %zero_offset, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
  %2196 : int = aten::size(%rel_pos.17, %50), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:285:0
  %2197 : int[] = prim::ListConstruct(%2196, %44), scope: __module.funnel/__module.funnel.decoder
  %rel_pos.18 : Long(26:1, 768:0) = aten::expand(%rel_pos.17, %2197, %46), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:285:0
  %2199 : Float(26:768, 768:1) = aten::gather(%pos_embed, %50, %rel_pos.18, %46), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:286:0
  %2200 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %2201 : Long(17:13, 13:1) = aten::slice(%2200, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %2202 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%2201, %19), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %2203 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %2204 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%2203, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
  %token_type_mat.19 : Bool(17:169, 13:13, 13:1) = aten::eq(%2202, %2204), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:22:0
  %cls_ids : Bool(17:13, 13:1) = aten::eq(%token_type_ids, %19), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:22:0
  %2207 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %2208 : Bool(17:13, 13:1) = aten::slice(%2207, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %2209 : Bool(17:13, 13:1, 1:1) = aten::unsqueeze(%2208, %19), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %2210 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %2211 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%2210, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %cls_mat : Bool(17:169, 13:13, 13:1) = aten::__or__(%2209, %2211), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
  %token_type_mat.20 : Bool(17:169, 13:13, 13:1) = aten::__or__(%cls_mat, %token_type_mat.19), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:211:0
  %2214 : Long() = aten::sub(%seq_len.38, %14, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
  %2215 : int = aten::Int(%2214), scope: __module.funnel/__module.funnel.decoder
  %2216 : Long() = aten::sub(%seq_len.38, %14, %49), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
  %2217 : int = aten::Int(%2216), scope: __module.funnel/__module.funnel.decoder
  %2218 : int[] = prim::ListConstruct(%2215, %2217), scope: __module.funnel/__module.funnel.decoder
  %input.119 : Float(12:12, 12:1) = aten::ones(%2218, %10, %50, %47, %46), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
  %2220 : int[] = prim::ListConstruct(%49, %50, %49, %50), scope: __module.funnel/__module.funnel.decoder
  %cls_mask : Float(13:13, 13:1) = aten::constant_pad_nd(%input.119, %2220, %50), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
  %2222 : __torch__.transformers.modeling_funnel.___torch_mangle_2149.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%2138)
  %2223 : __torch__.transformers.modeling_funnel.___torch_mangle_2143.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%2138)
  %2224 : __torch__.torch.nn.modules.normalization.___torch_mangle_2142.LayerNorm = prim::GetAttr[name="layer_norm"](%2223)
  %2225 : __torch__.torch.nn.modules.linear.___torch_mangle_2141.Linear = prim::GetAttr[name="post_proj"](%2223)
  %2226 : Tensor = prim::GetAttr[name="seg_embed"](%2223)
  %2227 : Tensor = prim::GetAttr[name="r_s_bias"](%2223)
  %2228 : Tensor = prim::GetAttr[name="r_kernel"](%2223)
  %2229 : Tensor = prim::GetAttr[name="r_r_bias"](%2223)
  %2230 : Tensor = prim::GetAttr[name="r_w_bias"](%2223)
  %2231 : __torch__.torch.nn.modules.linear.___torch_mangle_2140.Linear = prim::GetAttr[name="v_head"](%2223)
  %2232 : __torch__.torch.nn.modules.linear.___torch_mangle_2139.Linear = prim::GetAttr[name="k_head"](%2223)
  %2233 : __torch__.torch.nn.modules.linear.___torch_mangle_2138.Linear = prim::GetAttr[name="q_head"](%2223)
  %2234 : int = aten::size(%input_embeds, %50), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:530:0
  %2235 : int = aten::size(%input_embeds, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:530:0
  %2236 : int = aten::size(%input_embeds, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:531:0
  %2237 : Tensor = prim::GetAttr[name="weight"](%2233)
  %2238 : Float(768:1, 768:768) = aten::t(%2237), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.q_head # torch/nn/functional.py:1676:0
  %2239 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %2238), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.q_head # torch/nn/functional.py:1676:0
  %2240 : int[] = prim::ListConstruct(%2234, %2235, %31, %32), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %q_head.25 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2239, %2240), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:535:0
  %2242 : Tensor = prim::GetAttr[name="bias"](%2232)
  %2243 : Tensor = prim::GetAttr[name="weight"](%2232)
  %2244 : Float(768:1, 768:768) = aten::t(%2243), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.63 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %2244), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1676:0
  %2246 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.63, %2242, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1678:0
  %2247 : int[] = prim::ListConstruct(%2234, %2236, %31, %32), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2248 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2246, %2247), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:537:0
  %2249 : Tensor = prim::GetAttr[name="bias"](%2231)
  %2250 : Tensor = prim::GetAttr[name="weight"](%2231)
  %2251 : Float(768:1, 768:768) = aten::t(%2250), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.64 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %2251), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1676:0
  %2253 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.64, %2249, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1678:0
  %2254 : int[] = prim::ListConstruct(%2234, %2236, %31, %32), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2255 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2253, %2254), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.25, %33), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.13 : Float(12:64, 64:1) = aten::mul(%2230, %33), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:542:0
  %2258 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %r_w_bias.13, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:544:0
  %2259 : Tensor[] = prim::ListConstruct(%2258, %2248), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %content_score.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%34, %2259), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %v.13 : Float(12:64, 64:1) = aten::mul(%2229, %33), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:486:0
  %2262 : Tensor[] = prim::ListConstruct(%2199, %2228), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2263 : Float(26:768, 12:64, 64:1) = aten::einsum(%35, %2262), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %2264 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %v.13, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:493:0
  %2265 : Tensor[] = prim::ListConstruct(%2264, %2263), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %positional_attn.73 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%36, %2265), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %2267 : int = aten::size(%positional_attn.73, %50), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %2268 : int = aten::size(%positional_attn.73, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %2269 : int = aten::size(%positional_attn.73, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %2270 : int = aten::size(%positional_attn.73, %37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.13 : Long() = prim::NumToTensor(%2270), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2272 : int[] = prim::ListConstruct(%2267, %2268, %2270, %2269), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %positional_attn.74 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.73, %2272), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:428:0
  %2274 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.74, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %2275 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%2274, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %2276 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2275, %19, %49, %16, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.75 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2276, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %2278 : Long() = aten::sub(%max_rel_len.13, %14, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:430:0
  %2279 : int = aten::Int(%2278), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2280 : int[] = prim::ListConstruct(%2267, %2268, %2269, %2279), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %positional_attn.76 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.75, %2280), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.77 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.76, %37, %50, %2236, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.78 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.77, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:498:0
  %2284 : int = aten::size(%token_type_mat.20, %50), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
  %2285 : int = aten::size(%token_type_mat.20, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
  %2286 : int = aten::size(%token_type_mat.20, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.13 : Float(12:64, 64:1) = aten::mul(%2227, %33), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:508:0
  %2288 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %r_s_bias.13, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:511:0
  %2289 : Tensor[] = prim::ListConstruct(%2288, %2226), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2290 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%38, %2289), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %2291 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.20, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2292 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%2291, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2293 : int = aten::size(%q_head.26, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2294 : int[] = prim::ListConstruct(%2284, %2293, %2285, %2286), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %token_type_mat.21 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%2292, %2294, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2296 : Tensor[] = aten::split(%2290, %49, %41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/tensor.py:371:0
  %diff_token_type.13 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.13 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%2296), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2299 : int = aten::size(%token_type_mat.21, %50), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2300 : int = aten::size(%token_type_mat.21, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2301 : int = aten::size(%token_type_mat.21, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2302 : int = aten::size(%token_type_mat.21, %37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2303 : int[] = prim::ListConstruct(%2299, %2300, %2301, %2302), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2304 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.13, %2303, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2305 : int = aten::size(%token_type_mat.21, %50), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2306 : int = aten::size(%token_type_mat.21, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2307 : int = aten::size(%token_type_mat.21, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2308 : int = aten::size(%token_type_mat.21, %37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2309 : int[] = prim::ListConstruct(%2305, %2306, %2307, %2308), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %2310 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.13, %2309, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.25 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.21, %2304, %2310), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.25, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:522:0
  %2313 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.13, %positional_attn.78, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%2313, %token_type_attn.26, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.38 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.37, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:553:0
  %2316 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2317 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%2316, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2318 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2317, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2319 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%2318, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2320 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%2319, %49, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/tensor.py:396:0
  %2321 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%2320, %39), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.39 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.38, %2321, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %input.120 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.39, %41, %10), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:558:0
  %2324 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.120, %45, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %2325 : Tensor[] = prim::ListConstruct(%2324, %2255), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %attn_vec.13 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%40, %2325), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
  %2327 : int[] = prim::ListConstruct(%2234, %2235, %44), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
  %input.121 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.13, %2327), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:565:0
  %2329 : Tensor = prim::GetAttr[name="bias"](%2225)
  %2330 : Tensor = prim::GetAttr[name="weight"](%2225)
  %2331 : Float(768:1, 768:768) = aten::t(%2330), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.65 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.121, %2331), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.122 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.65, %2329, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.122, %45, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.123 : Float(17:9984, 13:768, 768:1) = aten::add(%input_embeds, %attn_out.13, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:568:0
  %2336 : Tensor = prim::GetAttr[name="bias"](%2224)
  %2337 : Tensor = prim::GetAttr[name="weight"](%2224)
  %2338 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.layer_norm
  %input.124 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.123, %2338, %2337, %2336, %43, %42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %2340 : __torch__.torch.nn.modules.normalization.___torch_mangle_2148.LayerNorm = prim::GetAttr[name="layer_norm"](%2222)
  %2341 : __torch__.torch.nn.modules.linear.___torch_mangle_2146.Linear = prim::GetAttr[name="linear_2"](%2222)
  %2342 : __torch__.torch.nn.modules.linear.___torch_mangle_2144.Linear = prim::GetAttr[name="linear_1"](%2222)
  %2343 : Tensor = prim::GetAttr[name="bias"](%2342)
  %2344 : Tensor = prim::GetAttr[name="weight"](%2342)
  %2345 : Float(768:1, 3072:768) = aten::t(%2344), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.66 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.124, %2345), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.15 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.66, %2343, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2348 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.15, %25), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2349 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.15, %26), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2350 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2349, %27), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2351 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.15, %2350, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2352 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2351, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2353 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%2352), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2354 : Float(17:39936, 13:3072, 3072:1) = aten::add(%2353, %29, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %input.125 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2348, %2354), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
  %input.126 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.125, %30, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2357 : Tensor = prim::GetAttr[name="bias"](%2341)
  %2358 : Tensor = prim::GetAttr[name="weight"](%2341)
  %2359 : Float(3072:1, 768:3072) = aten::t(%2358), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.67 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.126, %2359), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.127 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.67, %2357, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.127, %45, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.128 : Float(17:9984, 13:768, 768:1) = aten::add(%input.124, %h.13, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/modeling_funnel.py:588:0
  %2364 : Tensor = prim::GetAttr[name="bias"](%2340)
  %2365 : Tensor = prim::GetAttr[name="weight"](%2340)
  %2366 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.layer_norm
  %query : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.128, %2366, %2365, %2364, %43, %42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2368 : __torch__.transformers.modeling_funnel.___torch_mangle_2164.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%2136)
  %2369 : __torch__.transformers.modeling_funnel.___torch_mangle_2158.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%2136)
  %2370 : __torch__.torch.nn.modules.normalization.___torch_mangle_2157.LayerNorm = prim::GetAttr[name="layer_norm"](%2369)
  %2371 : __torch__.torch.nn.modules.linear.___torch_mangle_2156.Linear = prim::GetAttr[name="post_proj"](%2369)
  %2372 : Tensor = prim::GetAttr[name="seg_embed"](%2369)
  %2373 : Tensor = prim::GetAttr[name="r_s_bias"](%2369)
  %2374 : Tensor = prim::GetAttr[name="r_kernel"](%2369)
  %2375 : Tensor = prim::GetAttr[name="r_r_bias"](%2369)
  %2376 : Tensor = prim::GetAttr[name="r_w_bias"](%2369)
  %2377 : __torch__.torch.nn.modules.linear.___torch_mangle_2155.Linear = prim::GetAttr[name="v_head"](%2369)
  %2378 : __torch__.torch.nn.modules.linear.___torch_mangle_2154.Linear = prim::GetAttr[name="k_head"](%2369)
  %2379 : __torch__.torch.nn.modules.linear.___torch_mangle_2153.Linear = prim::GetAttr[name="q_head"](%2369)
  %2380 : int = aten::size(%query, %50), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:530:0
  %2381 : int = aten::size(%query, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:530:0
  %2382 : int = aten::size(%query, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:531:0
  %2383 : Tensor = prim::GetAttr[name="weight"](%2379)
  %2384 : Float(768:1, 768:768) = aten::t(%2383), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.q_head # torch/nn/functional.py:1676:0
  %2385 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query, %2384), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.q_head # torch/nn/functional.py:1676:0
  %2386 : int[] = prim::ListConstruct(%2380, %2381, %31, %32), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %q_head.27 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2385, %2386), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:535:0
  %2388 : Tensor = prim::GetAttr[name="bias"](%2378)
  %2389 : Tensor = prim::GetAttr[name="weight"](%2378)
  %2390 : Float(768:1, 768:768) = aten::t(%2389), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.68 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query, %2390), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1676:0
  %2392 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.68, %2388, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1678:0
  %2393 : int[] = prim::ListConstruct(%2380, %2382, %31, %32), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2394 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2392, %2393), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:537:0
  %2395 : Tensor = prim::GetAttr[name="bias"](%2377)
  %2396 : Tensor = prim::GetAttr[name="weight"](%2377)
  %2397 : Float(768:1, 768:768) = aten::t(%2396), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.69 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query, %2397), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1676:0
  %2399 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.69, %2395, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1678:0
  %2400 : int[] = prim::ListConstruct(%2380, %2382, %31, %32), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2401 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2399, %2400), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:538:0
  %q_head : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.27, %33), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias : Float(12:64, 64:1) = aten::mul(%2376, %33), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:542:0
  %2404 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %r_w_bias, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:544:0
  %2405 : Tensor[] = prim::ListConstruct(%2404, %2394), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %content_score : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%34, %2405), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %v : Float(12:64, 64:1) = aten::mul(%2375, %33), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:486:0
  %2408 : Tensor[] = prim::ListConstruct(%2199, %2374), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2409 : Float(26:768, 12:64, 64:1) = aten::einsum(%35, %2408), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %2410 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %v, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:493:0
  %2411 : Tensor[] = prim::ListConstruct(%2410, %2409), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %positional_attn.79 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%36, %2411), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %2413 : int = aten::size(%positional_attn.79, %50), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %2414 : int = aten::size(%positional_attn.79, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %2415 : int = aten::size(%positional_attn.79, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %2416 : int = aten::size(%positional_attn.79, %37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len : Long() = prim::NumToTensor(%2416), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2418 : int[] = prim::ListConstruct(%2413, %2414, %2416, %2415), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %positional_attn.80 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.79, %2418), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:428:0
  %2420 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.80, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %2421 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%2420, %49, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %2422 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2421, %19, %49, %16, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.81 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2422, %37, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %2424 : Long() = aten::sub(%max_rel_len, %14, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:430:0
  %2425 : int = aten::Int(%2424), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2426 : int[] = prim::ListConstruct(%2413, %2414, %2415, %2425), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %positional_attn.82 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.81, %2426), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.83 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.82, %37, %50, %2382, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.83, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:498:0
  %2430 : int = aten::size(%token_type_mat.20, %50), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
  %2431 : int = aten::size(%token_type_mat.20, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
  %2432 : int = aten::size(%token_type_mat.20, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias : Float(12:64, 64:1) = aten::mul(%2373, %33), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:508:0
  %2434 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %r_s_bias, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:511:0
  %2435 : Tensor[] = prim::ListConstruct(%2434, %2372), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2436 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%38, %2435), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %2437 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.20, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2438 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%2437, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2439 : int = aten::size(%q_head, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2440 : int[] = prim::ListConstruct(%2430, %2439, %2431, %2432), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %token_type_mat : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%2438, %2440, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2442 : Tensor[] = aten::split(%2436, %49, %41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/tensor.py:371:0
  %diff_token_type : Float(17:26, 12:442, 13:2, 1:1), %same_token_type : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%2442), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2445 : int = aten::size(%token_type_mat, %50), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2446 : int = aten::size(%token_type_mat, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2447 : int = aten::size(%token_type_mat, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2448 : int = aten::size(%token_type_mat, %37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2449 : int[] = prim::ListConstruct(%2445, %2446, %2447, %2448), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2450 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type, %2449, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2451 : int = aten::size(%token_type_mat, %50), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2452 : int = aten::size(%token_type_mat, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2453 : int = aten::size(%token_type_mat, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2454 : int = aten::size(%token_type_mat, %37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2455 : int[] = prim::ListConstruct(%2451, %2452, %2453, %2454), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %2456 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type, %2455, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat, %2450, %2456), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.27, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:522:0
  %2459 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score, %positional_attn, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.40 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%2459, %token_type_attn, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.41 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.40, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:553:0
  %2462 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %50, %50, %16, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2463 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%2462, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2464 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2463, %19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2465 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%2464, %10, %46, %46, %13), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2466 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%2465, %49, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/tensor.py:396:0
  %2467 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%2466, %39), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.41, %2467, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %input.129 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score, %41, %10), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:558:0
  %2470 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.129, %45, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %2471 : Tensor[] = prim::ListConstruct(%2470, %2401), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %attn_vec : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%40, %2471), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
  %2473 : int[] = prim::ListConstruct(%2380, %2381, %44), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
  %input.130 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec, %2473), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:565:0
  %2475 : Tensor = prim::GetAttr[name="bias"](%2371)
  %2476 : Tensor = prim::GetAttr[name="weight"](%2371)
  %2477 : Float(768:1, 768:768) = aten::t(%2476), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.70 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.130, %2477), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.131 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.70, %2475, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.131, %45, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.132 : Float(17:9984, 13:768, 768:1) = aten::add(%query, %attn_out, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:568:0
  %2482 : Tensor = prim::GetAttr[name="bias"](%2370)
  %2483 : Tensor = prim::GetAttr[name="weight"](%2370)
  %2484 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.layer_norm
  %input.133 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.132, %2484, %2483, %2482, %43, %42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %2486 : __torch__.torch.nn.modules.normalization.___torch_mangle_2163.LayerNorm = prim::GetAttr[name="layer_norm"](%2368)
  %2487 : __torch__.torch.nn.modules.linear.___torch_mangle_2161.Linear = prim::GetAttr[name="linear_2"](%2368)
  %2488 : __torch__.torch.nn.modules.linear.___torch_mangle_2159.Linear = prim::GetAttr[name="linear_1"](%2368)
  %2489 : Tensor = prim::GetAttr[name="bias"](%2488)
  %2490 : Tensor = prim::GetAttr[name="weight"](%2488)
  %2491 : Float(768:1, 3072:768) = aten::t(%2490), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.71 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.133, %2491), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.71, %2489, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2494 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x, %25), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2495 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x, %26), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2496 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2495, %27), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2497 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x, %2496, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2498 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2497, %28), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2499 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%2498), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2500 : Float(17:39936, 13:3072, 3072:1) = aten::add(%2499, %29, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %input.134 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2494, %2500), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
  %input.135 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.134, %30, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2503 : Tensor = prim::GetAttr[name="bias"](%2487)
  %2504 : Tensor = prim::GetAttr[name="weight"](%2487)
  %2505 : Float(3072:1, 768:3072) = aten::t(%2504), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.72 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.135, %2505), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.136 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.72, %2503, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.136, %45, %46), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.137 : Float(17:9984, 13:768, 768:1) = aten::add(%input.133, %h, %49), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/modeling_funnel.py:588:0
  %2510 : Tensor = prim::GetAttr[name="bias"](%2486)
  %2511 : Tensor = prim::GetAttr[name="weight"](%2486)
  %2512 : int[] = prim::ListConstruct(%44), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.layer_norm
  %input : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.137, %2512, %2511, %2510, %43, %42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2514 : int = prim::Constant[value=1](), scope: __module.lm_head # torch/nn/functional.py:1678:0
  %2515 : Tensor = prim::GetAttr[name="bias"](%3)
  %2516 : Tensor = prim::GetAttr[name="weight"](%3)
  %2517 : Float(768:1, 30522:768) = aten::t(%2516), scope: __module.lm_head # torch/nn/functional.py:1676:0
  %output : Float(17:396786, 13:30522, 30522:1) = aten::matmul(%input, %2517), scope: __module.lm_head # torch/nn/functional.py:1676:0
  %2519 : Float(17:396786, 13:30522, 30522:1) = aten::add_(%output, %2515, %2514), scope: __module.lm_head # torch/nn/functional.py:1678:0
  %7 : (Float(17:396786, 13:30522, 30522:1)) = prim::TupleConstruct(%2519)
  return (%7)
